[
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk32",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A_ab",
        "annotation": null
      },
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk32(",
      "    A_ab,",
      "    A_ab_inv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    p_Aab = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_Aab_inv = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    b_A_ab = tl.load(p_Aab, boundary_check=(0, 1))",
      "    b_A_ab = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A_ab, 0)",
      "    for i in range(1, BT):",
      "        mask = tl.arange(0, BT) == i",
      "        b_a = tl.sum(tl.where(mask[:, None], b_A_ab, 0), 0)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A_ab, 0) * (tl.arange(0, BT) < i)",
      "        b_A_ab = tl.where(mask[:, None], b_a, b_A_ab)",
      "    b_A_ab += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]",
      "    tl.store(p_Aab_inv, b_A_ab.to(p_Aab_inv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/389.py",
    "header": "def prepare_wy_repr_fwd_kernel_chunk32(A_ab, A_ab_inv, cu_seqlens, chunk_indices, T, H: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_Aab = tl.make_block_ptr(A_ab + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_Aab_inv = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_A_ab = tl.load(p_Aab, boundary_check=(0, 1))\nb_A_ab = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A_ab, 0)\nfor i in range(1, BT):\n    mask = tl.arange(0, BT) == i\n    b_a = tl.sum(tl.where(mask[:, None], b_A_ab, 0), 0)\n    b_a = b_a + tl.sum(b_a[:, None] * b_A_ab, 0) * (tl.arange(0, BT) < i)\n    b_A_ab = tl.where(mask[:, None], b_a, b_A_ab)\nb_A_ab += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\ntl.store(p_Aab_inv, b_A_ab.to(p_Aab_inv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BC'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A_ab",
        "annotation": null
      },
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GATHER_SUPPORTED",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk64(",
      "    A_ab,",
      "    A_ab_inv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    GATHER_SUPPORTED: tl.constexpr = is_gather_supported,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_A1 = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A2 = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A3 = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv1 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv2 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv3 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv4 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A = tl.load(p_A1, boundary_check=(0, 1))",
      "    b_A2 = tl.load(p_A2, boundary_check=(0, 1))",
      "    b_A3 = tl.load(p_A3, boundary_check=(0, 1))",
      "    b_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)",
      "    b_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)",
      "",
      "    for i in range(1, BC):",
      "        if GATHER_SUPPORTED:",
      "            row_idx = tl.full([1, BC], i, dtype=tl.int16)",
      "",
      "            b_a = tl.sum(gather(b_A, row_idx, axis=0), 0)",
      "            b_a2 = tl.sum(gather(b_A2, row_idx, axis=0), 0)",
      "        else:",
      "            mask = tl.arange(0, BC) == i",
      "            b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)",
      "            b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)",
      "        mask = tl.arange(0, BC) == i",
      "",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)",
      "        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "        b_A2 = tl.where(mask[:, None], b_a2, b_A2)",
      "",
      "    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A3 = tl.dot(tl.dot(b_A2, b_A3), b_A)",
      "",
      "    tl.store(",
      "        p_A_inv1,",
      "        b_A.to(p_A_inv1.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_A_inv2,",
      "        b_A2.to(p_A_inv2.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_A_inv3,",
      "        b_A3.to(p_A_inv3.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    tl.store(",
      "        p_A_inv4,",
      "        tl.zeros([BC, BC], dtype=tl.float32).to(p_A_inv4.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/389.py",
    "header": "def prepare_wy_repr_fwd_kernel_chunk64(A_ab, A_ab_inv, cu_seqlens, chunk_indices, T, H: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, IS_VARLEN: tl.constexpr, GATHER_SUPPORTED: tl.constexpr = is_gather_supported):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_A1 = tl.make_block_ptr(A_ab + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0))\np_A2 = tl.make_block_ptr(A_ab + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + BC, BC), (BC, BC), (1, 0))\np_A3 = tl.make_block_ptr(A_ab + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + BC, 0), (BC, BC), (1, 0))\np_A_inv1 = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0))\np_A_inv2 = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + BC, BC), (BC, BC), (1, 0))\np_A_inv3 = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + BC, 0), (BC, BC), (1, 0))\np_A_inv4 = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0))\nb_A = tl.load(p_A1, boundary_check=(0, 1))\nb_A2 = tl.load(p_A2, boundary_check=(0, 1))\nb_A3 = tl.load(p_A3, boundary_check=(0, 1))\nb_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)\nb_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)\nfor i in range(1, BC):\n    if GATHER_SUPPORTED:\n        row_idx = tl.full([1, BC], i, dtype=tl.int16)\n        b_a = tl.sum(gather(b_A, row_idx, axis=0), 0)\n        b_a2 = tl.sum(gather(b_A2, row_idx, axis=0), 0)\n    else:\n        mask = tl.arange(0, BC) == i\n        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n    mask = tl.arange(0, BC) == i\n    b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n    b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n    b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A2 = tl.where(mask[:, None], b_a2, b_A2)\nb_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\nb_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\nb_A3 = tl.dot(tl.dot(b_A2, b_A3), b_A)\ntl.store(p_A_inv1, b_A.to(p_A_inv1.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_A_inv2, b_A2.to(p_A_inv2.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_A_inv3, b_A3.to(p_A_inv3.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_A_inv4, tl.zeros([BC, BC], dtype=tl.float32).to(p_A_inv4.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "wu_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "ag",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "A_ak",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def wu_fwd_kernel(",
      "    w,",
      "    u,",
      "    ag,",
      "    v,",
      "    A_ab_inv,",
      "    A_ak,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_s = tl.arange(0, BT)",
      "",
      "    p_A_ab_inv = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_A_ak = tl.make_block_ptr(",
      "        A_ak + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_Aab_inv = tl.load(p_A_ab_inv, boundary_check=(0, 1))",
      "    b_Aak = tl.load(p_A_ak, boundary_check=(0, 1))",
      "    b_Aab_inv = tl.where(o_s[:, None] >= o_s[None, :], b_Aab_inv, 0)",
      "    b_Aak = tl.where(o_s[:, None] > o_s[None, :], b_Aak, 0)",
      "",
      "    b_Aak = tl.dot(b_Aab_inv, b_Aak)",
      "",
      "    b_Aak = b_Aak.to(v.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "    b_Aab_inv = b_Aab_inv.to(ag.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_ag = tl.make_block_ptr(",
      "            ag + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_ag = tl.load(p_ag, boundary_check=(0, 1))",
      "        b_w = tl.dot(b_Aab_inv, b_ag)",
      "        tl.store(",
      "            p_w,",
      "            b_w.to(p_w.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_u = tl.dot(b_Aak, b_v)",
      "        tl.store(",
      "            p_u,",
      "            b_u.to(p_u.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            boundary_check=(0, 1),",
      "        )"
    ],
    "file": "codes/389.py",
    "header": "def wu_fwd_kernel(w, u, ag, v, A_ab_inv, A_ak, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\no_s = tl.arange(0, BT)\np_A_ab_inv = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_A_ak = tl.make_block_ptr(A_ak + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_Aab_inv = tl.load(p_A_ab_inv, boundary_check=(0, 1))\nb_Aak = tl.load(p_A_ak, boundary_check=(0, 1))\nb_Aab_inv = tl.where(o_s[:, None] >= o_s[None, :], b_Aab_inv, 0)\nb_Aak = tl.where(o_s[:, None] > o_s[None, :], b_Aak, 0)\nb_Aak = tl.dot(b_Aab_inv, b_Aak)\nb_Aak = b_Aak.to(v.dtype.element_ty, fp_downcast_rounding='rtne')\nb_Aab_inv = b_Aab_inv.to(ag.dtype.element_ty, fp_downcast_rounding='rtne')\nfor i_k in range(tl.cdiv(K, BK)):\n    p_ag = tl.make_block_ptr(ag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_ag = tl.load(p_ag, boundary_check=(0, 1))\n    b_w = tl.dot(b_Aab_inv, b_ag)\n    tl.store(p_w, b_w.to(p_w.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_u = tl.dot(b_Aak, b_v)\n    tl.store(p_u, b_u.to(p_u.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "mean_pooling_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mean_pooling_fwd_kernel(",
      "    x,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_x = tl.make_block_ptr(",
      "        x + (bos * H + i_h) * D,",
      "        (T, D),",
      "        (H * D, 1),",
      "        (i_t * BT, i_d * BD),",
      "        (BT, BD),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,)",
      "    )",
      "",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    b_o = tl.sum(b_x, axis=0) / min(BT, T - i_t * BT)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/434.py",
    "header": "def mean_pooling_fwd_kernel(x, o, cu_seqlens, chunk_indices, T, H: tl.constexpr, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_d, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\np_x = tl.make_block_ptr(x + (bos * H + i_h) * D, (T, D), (H * D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\np_o = tl.make_block_ptr(o + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,))\nb_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\nb_o = tl.sum(b_x, axis=0) / min(BT, T - i_t * BT)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "mean_pooling_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mean_pooling_bwd_kernel(",
      "    do,",
      "    dx,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_dx = tl.make_block_ptr(",
      "        dx + (bos * H + i_h) * D,",
      "        (T, D),",
      "        (H * D, 1),",
      "        (i_t * BT, i_d * BD),",
      "        (BT, BD),",
      "        (1, 0),",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,)",
      "    )",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0,)).to(tl.float32)",
      "",
      "    b_dx = b_do / tl.full((BT,), min(BT, T - i_t * BT), dtype=tl.float32)[:, None]",
      "    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/434.py",
    "header": "def mean_pooling_bwd_kernel(do, dx, cu_seqlens, chunk_indices, T, H: tl.constexpr, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_d, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\np_dx = tl.make_block_ptr(dx + (bos * H + i_h) * D, (T, D), (H * D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\np_do = tl.make_block_ptr(do + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,))\nb_do = tl.load(p_do, boundary_check=(0,)).to(tl.float32)\nb_dx = b_do / tl.full((BT,), min(BT, T - i_t * BT), dtype=tl.float32)[:, None]\ntl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_rms_norm_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "stride_x",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "stride_y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "block_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_norm_fwd_kernel(",
      "    X,",
      "    stride_x,",
      "    Y,",
      "    stride_y,",
      "    W,",
      "    Rstd,",
      "    eps,",
      "    M,",
      "    N,",
      "    block_N: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    cols = tl.arange(0, block_N)",
      "",
      "    mask = cols < N",
      "    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    w = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)",
      "",
      "    xbar = tl.where(cols < N, x, 0.0)",
      "    var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    x_hat = x * rstd",
      "    y = x_hat * w",
      "",
      "    tl.store(Y + row * stride_y + cols, y, mask=mask)"
    ],
    "file": "codes/574.py",
    "header": "def _rms_norm_fwd_kernel(X, stride_x, Y, stride_y, W, Rstd, eps, M, N, block_N: tl.constexpr):",
    "body": "row = tl.program_id(0)\ncols = tl.arange(0, block_N)\nmask = cols < N\nx = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)\nw = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)\nxbar = tl.where(cols < N, x, 0.0)\nvar = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nx_hat = x * rstd\ny = x_hat * w\ntl.store(Y + row * stride_y + cols, y, mask=mask)"
  },
  {
    "name": "_rms_norm_bwd_kernel_sm",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "stride_x",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_dy",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "stride_dx",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "block_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_norm_bwd_kernel_sm(",
      "    X,",
      "    stride_x,",
      "    W,",
      "    DY,",
      "    stride_dy,",
      "    DX,",
      "    stride_dx,",
      "    Rstd,",
      "    DW,",
      "    eps,",
      "    M,",
      "    N,",
      "    rows_per_program,",
      "    block_N: tl.constexpr,",
      "):",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, block_N)",
      "    mask = cols < N",
      "",
      "    w = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)",
      "",
      "    dw = tl.zeros((block_N,), dtype=tl.float32)",
      "",
      "    row_end = min(row_start + rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)",
      "        dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0).to(tl.float32)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        x_hat = x * rstd",
      "        wdy = w * dy",
      "        dw += dy * x_hat",
      "        c1 = tl.sum(x_hat * wdy, axis=0) / N",
      "        dx = (wdy - x_hat * c1) * rstd",
      "",
      "        tl.store(DX + row * stride_dx + cols, dx, mask=mask)",
      "",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)"
    ],
    "file": "codes/574.py",
    "header": "def _rms_norm_bwd_kernel_sm(X, stride_x, W, DY, stride_dy, DX, stride_dx, Rstd, DW, eps, M, N, rows_per_program, block_N: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, block_N)\nmask = cols < N\nw = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)\ndw = tl.zeros((block_N,), dtype=tl.float32)\nrow_end = min(row_start + rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)\n    dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0).to(tl.float32)\n    rstd = tl.load(Rstd + row)\n    x_hat = x * rstd\n    wdy = w * dy\n    dw += dy * x_hat\n    c1 = tl.sum(x_hat * wdy, axis=0) / N\n    dx = (wdy - x_hat * c1) * rstd\n    tl.store(DX + row * stride_dx + cols, dx, mask=mask)\ntl.store(DW + row_block_id * N + cols, dw, mask=mask)"
  },
  {
    "name": "fused_recurrent_linear_attn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_linear_attn_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    h0,",
      "    ht,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)",
      "    p_o = o + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    mask_bk = (i_k * BK + tl.arange(0, BK)) < K",
      "    mask_bv = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_kv = mask_bk[None, :] & mask_bv[:, None]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_bh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale",
      "",
      "        b_h += b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)",
      "",
      "        p_q += K",
      "        p_k += K",
      "        p_o += V",
      "        p_v += V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_bh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)"
    ],
    "file": "codes/400.py",
    "header": "def fused_recurrent_linear_attn_fwd_kernel(q, k, v, o, h0, ht, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\np_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\np_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\np_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\np_o = o + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV)\nmask_bk = i_k * BK + tl.arange(0, BK) < K\nmask_bv = i_v * BV + tl.arange(0, BV) < V\nmask_kv = mask_bk[None, :] & mask_bv[:, None]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n    b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n    b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n    b_h += b_k[None, :] * b_v[:, None]\n    b_o = b_h * b_q[None, :]\n    b_o = tl.sum(b_o, axis=1)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)\n    p_q += K\n    p_k += K\n    p_o += V\n    p_v += V\nif STORE_FINAL_STATE:\n    p_ht = ht + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)"
  },
  {
    "name": "fused_recurrent_linear_attn_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_linear_attn_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)",
      "    p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    p_dq = dq + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK)",
      "    mask_bk = i_k * BK + tl.arange(0, BK) < K",
      "    mask_bv = i_v * BV + tl.arange(0, BV) < V",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        mask_kv = mask_bk[:, None] & mask_bv[None, :]",
      "        p_h0 = (",
      "            h0",
      "            + i_bh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)",
      "",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        _d_q = b_h * b_do[None, :]",
      "        d_q = tl.sum(_d_q, axis=1) * scale",
      "        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)",
      "",
      "        p_k += K",
      "        p_do += V",
      "        p_v += V",
      "        p_dq += K",
      "",
      "    tl.debug_barrier()",
      "",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K",
      "    p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V",
      "    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V",
      "    p_dk = dk + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K",
      "    p_dv = dv + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V",
      "    d_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for _ in range(T):",
      "        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)",
      "        d_h += b_q[:, None] * b_do[None, :]",
      "        d_k = tl.sum(d_h * b_v[None, :], axis=1)",
      "        d_v = tl.sum(d_h * b_k[:, None], axis=0)",
      "",
      "        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)",
      "        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)",
      "",
      "        p_do -= V",
      "        p_q -= K",
      "        p_k -= K",
      "        p_v -= V",
      "        p_dk -= K",
      "        p_dv -= V"
    ],
    "file": "codes/400.py",
    "header": "def fused_recurrent_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\np_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)\np_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)\np_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)\np_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)\np_dq = dq + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK)\nmask_bk = i_k * BK + tl.arange(0, BK) < K\nmask_bv = i_v * BV + tl.arange(0, BV) < V\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    mask_kv = mask_bk[:, None] & mask_bv[None, :]\n    p_h0 = h0 + i_bh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n    b_h += b_k[:, None] * b_v[None, :]\n    _d_q = b_h * b_do[None, :]\n    d_q = tl.sum(_d_q, axis=1) * scale\n    tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)\n    p_k += K\n    p_do += V\n    p_v += V\n    p_dq += K\ntl.debug_barrier()\np_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\np_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\np_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\np_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\np_dk = dk + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K\np_dv = dv + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V\nd_h = tl.zeros([BK, BV], dtype=tl.float32)\nfor _ in range(T):\n    b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)\n    b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)\n    d_h += b_q[:, None] * b_do[None, :]\n    d_k = tl.sum(d_h * b_v[None, :], axis=1)\n    d_v = tl.sum(d_h * b_k[:, None], axis=0)\n    tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)\n    tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)\n    p_do -= V\n    p_q -= K\n    p_k -= K\n    p_v -= V\n    p_dk -= K\n    p_dv -= V"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,))",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        input = tl.where(condition, input, float(\"-inf\"))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    output = tl.argmax(input, 1)",
      "    tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "codes/511.py",
    "header": "def forward(output_ptr: tl.tensor, input_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, y_stride: tl.int32, x_stride: tl.int32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\ninput_block_ptr = tl.make_block_ptr(input_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nif require_x_boundary_check:\n    input = tl.load(input_block_ptr, boundary_check=(1,))\n    condition = tl.arange(0, x_block_size) < x_size\n    input = tl.where(condition, input, float('-inf'))\nelse:\n    input = tl.load(input_block_ptr)\noutput = tl.argmax(input, 1)\ntl.store(output_block_ptr, output.to(dtype))"
  },
  {
    "name": "batch_norm_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'spatial_dim'], restore_value=['running_mean_pointer', 'running_var_pointer'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args['batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "bias_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "pre_act_add_pointer",
        "annotation": null
      },
      {
        "name": "pre_act_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "running_mean_pointer",
        "annotation": null
      },
      {
        "name": "running_var_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "spatial_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_spatial_stride",
        "annotation": null
      },
      {
        "name": "pre_act_add_batch_stride",
        "annotation": null
      },
      {
        "name": "pre_act_add_feat_stride",
        "annotation": null
      },
      {
        "name": "pre_act_add_spatial_stride",
        "annotation": null
      },
      {
        "name": "pre_act_batch_stride",
        "annotation": null
      },
      {
        "name": "pre_act_feat_stride",
        "annotation": null
      },
      {
        "name": "pre_act_spatial_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "output_spatial_stride",
        "annotation": null
      },
      {
        "name": "momentum",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "affine",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "track_running_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "is_train",
        "annotation": "tl.constexpr"
      },
      {
        "name": "add_pre_act",
        "annotation": "tl.constexpr"
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_pre_act",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_SPATIAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def batch_norm_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    bias_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    pre_act_add_pointer,",
      "    pre_act_pointer,",
      "    output_pointer,",
      "    running_mean_pointer,",
      "    running_var_pointer,",
      "    batch_dim,",
      "    spatial_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_spatial_stride,",
      "    pre_act_add_batch_stride,",
      "    pre_act_add_feat_stride,",
      "    pre_act_add_spatial_stride,",
      "    pre_act_batch_stride,",
      "    pre_act_feat_stride,",
      "    pre_act_spatial_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    output_spatial_stride,",
      "    momentum,",
      "    eps,",
      "    param,",
      "    affine: tl.constexpr,",
      "    save_stats: tl.constexpr,",
      "    track_running_stats: tl.constexpr,",
      "    is_train: tl.constexpr,",
      "    add_pre_act: tl.constexpr,",
      "    act_func: tl.constexpr,",
      "    save_pre_act: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_SPATIAL: tl.constexpr,",
      "):",
      "",
      "    feat_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)",
      "    batch_mask = batch_offset < batch_dim",
      "",
      "    if is_train or not track_running_stats:",
      "        count = 0",
      "        mean = 0.0",
      "        var = 0.0",
      "",
      "        for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "            spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "                0, BLOCK_SIZE_SPATIAL",
      "            )",
      "            spatial_mask = spatial_offset < spatial_dim",
      "",
      "            curr_input_pointer = (",
      "                input_pointer",
      "                + input_feat_stride * feat_pid",
      "                + input_batch_stride * batch_offset[:, None]",
      "                + input_spatial_stride * spatial_offset[None, :]",
      "            )",
      "            curr_input = tl.load(",
      "                curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "            ).to(tl.float32)",
      "",
      "            spatial_count = min(",
      "                BLOCK_SIZE_SPATIAL, spatial_dim - block_ind * BLOCK_SIZE_SPATIAL",
      "            )",
      "            curr_count = spatial_count * batch_dim",
      "            count += curr_count",
      "",
      "            prev_mean = mean",
      "            mean += (tl.sum(curr_input) - curr_count * mean) / count",
      "            deltas = tl.where(",
      "                batch_mask[:, None] & spatial_mask[None, :],",
      "                (curr_input - mean) * (curr_input - prev_mean),",
      "                0.0,",
      "            )",
      "            var += tl.sum(deltas)",
      "",
      "        var /= count",
      "        inv_std = tl.rsqrt(var + eps)",
      "",
      "        if save_stats:",
      "            tl.store(feat_pid + mean_pointer, mean)",
      "            tl.store(feat_pid + inv_std_pointer, inv_std)",
      "",
      "        if track_running_stats:",
      "            running_mean_pointer += feat_pid",
      "            running_var_pointer += feat_pid",
      "",
      "            running_mean = tl.load(running_mean_pointer)",
      "            running_var = tl.load(running_var_pointer)",
      "",
      "            n = batch_dim * spatial_dim",
      "            tl.store(",
      "                running_mean_pointer, (1 - momentum) * running_mean + momentum * mean",
      "            )",
      "            tl.store(",
      "                running_var_pointer,",
      "                (1 - momentum) * running_var + momentum * var * n / (n - 1),",
      "            )",
      "",
      "    else:",
      "        mean = tl.load(feat_pid + running_mean_pointer)",
      "        inv_std = tl.rsqrt(tl.load(feat_pid + running_var_pointer) + eps)",
      "",
      "    if affine:",
      "        weight = tl.load(feat_pid + weight_pointer)",
      "        bias = tl.load(feat_pid + bias_pointer)",
      "",
      "    else:",
      "        weight = 1.0",
      "        bias = 0.0",
      "",
      "    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "            0, BLOCK_SIZE_SPATIAL",
      "        )",
      "        spatial_mask = spatial_offset < spatial_dim",
      "",
      "        curr_input_pointer = (",
      "            input_pointer",
      "            + input_feat_stride * feat_pid",
      "            + input_batch_stride * batch_offset[:, None]",
      "            + input_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_output_pointer = (",
      "            output_pointer",
      "            + output_feat_stride * feat_pid",
      "            + output_batch_stride * batch_offset[:, None]",
      "            + output_spatial_stride * spatial_offset[None, :]",
      "        )",
      "",
      "        curr_input = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        output = weight * (curr_input - mean) * inv_std + bias",
      "",
      "        if add_pre_act:",
      "            curr_pre_act_add_pointer = (",
      "                pre_act_add_pointer",
      "                + pre_act_add_feat_stride * feat_pid",
      "                + pre_act_add_batch_stride * batch_offset[:, None]",
      "                + pre_act_add_spatial_stride * spatial_offset[None, :]",
      "            )",
      "            curr_pre_act_add = tl.load(",
      "                curr_pre_act_add_pointer,",
      "                mask=batch_mask[:, None] & spatial_mask[None, :],",
      "            )",
      "            output += curr_pre_act_add",
      "",
      "        if act_func is not None:",
      "            if save_pre_act:",
      "                curr_pre_act_pointer = (",
      "                    pre_act_pointer",
      "                    + pre_act_feat_stride * feat_pid",
      "                    + pre_act_batch_stride * batch_offset[:, None]",
      "                    + pre_act_spatial_stride * spatial_offset[None, :]",
      "                )",
      "                tl.store(",
      "                    curr_pre_act_pointer,",
      "                    output,",
      "                    mask=batch_mask[:, None] & spatial_mask[None, :],",
      "                )",
      "",
      "            output = apply_act_func(output, None, None, None, param, act_func, False)",
      "",
      "        tl.store(",
      "            curr_output_pointer,",
      "            output,",
      "            mask=batch_mask[:, None] & spatial_mask[None, :],",
      "        )"
    ],
    "file": "codes/21.py",
    "header": "def batch_norm_forward_kernel(input_pointer, weight_pointer, bias_pointer, mean_pointer, inv_std_pointer, pre_act_add_pointer, pre_act_pointer, output_pointer, running_mean_pointer, running_var_pointer, batch_dim, spatial_dim, input_batch_stride, input_feat_stride, input_spatial_stride, pre_act_add_batch_stride, pre_act_add_feat_stride, pre_act_add_spatial_stride, pre_act_batch_stride, pre_act_feat_stride, pre_act_spatial_stride, output_batch_stride, output_feat_stride, output_spatial_stride, momentum, eps, param, affine: tl.constexpr, save_stats: tl.constexpr, track_running_stats: tl.constexpr, is_train: tl.constexpr, add_pre_act: tl.constexpr, act_func: tl.constexpr, save_pre_act: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_SPATIAL: tl.constexpr):",
    "body": "feat_pid = tl.program_id(axis=0)\nbatch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\nbatch_mask = batch_offset < batch_dim\nif is_train or not track_running_stats:\n    count = 0\n    mean = 0.0\n    var = 0.0\n    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0, BLOCK_SIZE_SPATIAL)\n        spatial_mask = spatial_offset < spatial_dim\n        curr_input_pointer = input_pointer + input_feat_stride * feat_pid + input_batch_stride * batch_offset[:, None] + input_spatial_stride * spatial_offset[None, :]\n        curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(tl.float32)\n        spatial_count = min(BLOCK_SIZE_SPATIAL, spatial_dim - block_ind * BLOCK_SIZE_SPATIAL)\n        curr_count = spatial_count * batch_dim\n        count += curr_count\n        prev_mean = mean\n        mean += (tl.sum(curr_input) - curr_count * mean) / count\n        deltas = tl.where(batch_mask[:, None] & spatial_mask[None, :], (curr_input - mean) * (curr_input - prev_mean), 0.0)\n        var += tl.sum(deltas)\n    var /= count\n    inv_std = tl.rsqrt(var + eps)\n    if save_stats:\n        tl.store(feat_pid + mean_pointer, mean)\n        tl.store(feat_pid + inv_std_pointer, inv_std)\n    if track_running_stats:\n        running_mean_pointer += feat_pid\n        running_var_pointer += feat_pid\n        running_mean = tl.load(running_mean_pointer)\n        running_var = tl.load(running_var_pointer)\n        n = batch_dim * spatial_dim\n        tl.store(running_mean_pointer, (1 - momentum) * running_mean + momentum * mean)\n        tl.store(running_var_pointer, (1 - momentum) * running_var + momentum * var * n / (n - 1))\nelse:\n    mean = tl.load(feat_pid + running_mean_pointer)\n    inv_std = tl.rsqrt(tl.load(feat_pid + running_var_pointer) + eps)\nif affine:\n    weight = tl.load(feat_pid + weight_pointer)\n    bias = tl.load(feat_pid + bias_pointer)\nelse:\n    weight = 1.0\n    bias = 0.0\nfor block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n    spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0, BLOCK_SIZE_SPATIAL)\n    spatial_mask = spatial_offset < spatial_dim\n    curr_input_pointer = input_pointer + input_feat_stride * feat_pid + input_batch_stride * batch_offset[:, None] + input_spatial_stride * spatial_offset[None, :]\n    curr_output_pointer = output_pointer + output_feat_stride * feat_pid + output_batch_stride * batch_offset[:, None] + output_spatial_stride * spatial_offset[None, :]\n    curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(tl.float32)\n    output = weight * (curr_input - mean) * inv_std + bias\n    if add_pre_act:\n        curr_pre_act_add_pointer = pre_act_add_pointer + pre_act_add_feat_stride * feat_pid + pre_act_add_batch_stride * batch_offset[:, None] + pre_act_add_spatial_stride * spatial_offset[None, :]\n        curr_pre_act_add = tl.load(curr_pre_act_add_pointer, mask=batch_mask[:, None] & spatial_mask[None, :])\n        output += curr_pre_act_add\n    if act_func is not None:\n        if save_pre_act:\n            curr_pre_act_pointer = pre_act_pointer + pre_act_feat_stride * feat_pid + pre_act_batch_stride * batch_offset[:, None] + pre_act_spatial_stride * spatial_offset[None, :]\n            tl.store(curr_pre_act_pointer, output, mask=batch_mask[:, None] & spatial_mask[None, :])\n        output = apply_act_func(output, None, None, None, param, act_func, False)\n    tl.store(curr_output_pointer, output, mask=batch_mask[:, None] & spatial_mask[None, :])"
  },
  {
    "name": "batch_norm_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'spatial_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args['batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "weight_grad_pointer",
        "annotation": null
      },
      {
        "name": "bias_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "spatial_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "output_grad_spatial_stride",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_spatial_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_spatial_stride",
        "annotation": null
      },
      {
        "name": "affine",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_SPATIAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def batch_norm_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    weight_pointer,",
      "    input_grad_pointer,",
      "    weight_grad_pointer,",
      "    bias_grad_pointer,",
      "    batch_dim,",
      "    spatial_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    output_grad_spatial_stride,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_spatial_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    input_grad_spatial_stride,",
      "    affine: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_SPATIAL: tl.constexpr,",
      "):",
      "",
      "    feat_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)",
      "    batch_mask = batch_offset < batch_dim",
      "",
      "    mean = tl.load(feat_pid + mean_pointer)",
      "    inv_std = tl.load(feat_pid + inv_std_pointer)",
      "",
      "    term1 = 0.0",
      "    term2 = 0.0",
      "",
      "    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "            0, BLOCK_SIZE_SPATIAL",
      "        )",
      "        spatial_mask = spatial_offset < spatial_dim",
      "",
      "        curr_output_grad_pointer = (",
      "            output_grad_pointer",
      "            + output_grad_feat_stride * feat_pid",
      "            + output_grad_batch_stride * batch_offset[:, None]",
      "            + output_grad_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_input_pointer = (",
      "            input_pointer",
      "            + input_feat_stride * feat_pid",
      "            + input_batch_stride * batch_offset[:, None]",
      "            + input_spatial_stride * spatial_offset[None, :]",
      "        )",
      "",
      "        curr_input = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        curr_pre_lin = (curr_input - mean) * inv_std",
      "        curr_output_grad = tl.load(",
      "            curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "",
      "        term1 += tl.sum(curr_pre_lin * curr_output_grad)",
      "        term2 += tl.sum(curr_output_grad)",
      "",
      "    if affine:",
      "        weight = tl.load(feat_pid + weight_pointer)",
      "        weight_grad = 0.0",
      "        bias_grad = 0.0",
      "",
      "    else:",
      "        weight = 1.0",
      "",
      "    count = batch_dim * spatial_dim",
      "    term1 *= weight / count",
      "    term2 *= weight / count",
      "",
      "    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "            0, BLOCK_SIZE_SPATIAL",
      "        )",
      "        spatial_mask = spatial_offset < spatial_dim",
      "",
      "        curr_output_grad_pointer = (",
      "            output_grad_pointer",
      "            + output_grad_feat_stride * feat_pid",
      "            + output_grad_batch_stride * batch_offset[:, None]",
      "            + output_grad_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_input_pointer = (",
      "            input_pointer",
      "            + input_feat_stride * feat_pid",
      "            + input_batch_stride * batch_offset[:, None]",
      "            + input_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_input_grad_pointer = (",
      "            input_grad_pointer",
      "            + input_grad_feat_stride * feat_pid",
      "            + input_grad_batch_stride * batch_offset[:, None]",
      "            + input_grad_spatial_stride * spatial_offset[None, :]",
      "        )",
      "",
      "        curr_input = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        curr_pre_lin = (curr_input - mean) * inv_std",
      "        curr_output_grad = tl.load(",
      "            curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        curr_input_grad = inv_std * (",
      "            weight * curr_output_grad - (term1 * curr_pre_lin + term2)",
      "        )",
      "        tl.store(",
      "            curr_input_grad_pointer,",
      "            curr_input_grad,",
      "            mask=batch_mask[:, None] & spatial_mask[None, :],",
      "        )",
      "",
      "        if affine:",
      "            weight_grad += tl.sum(curr_pre_lin * curr_output_grad)",
      "            bias_grad += tl.sum(curr_output_grad)",
      "",
      "    if affine:",
      "        tl.store(feat_pid + weight_grad_pointer, weight_grad)",
      "        tl.store(feat_pid + bias_grad_pointer, bias_grad)"
    ],
    "file": "codes/21.py",
    "header": "def batch_norm_backward_kernel(output_grad_pointer, input_pointer, mean_pointer, inv_std_pointer, weight_pointer, input_grad_pointer, weight_grad_pointer, bias_grad_pointer, batch_dim, spatial_dim, output_grad_batch_stride, output_grad_feat_stride, output_grad_spatial_stride, input_batch_stride, input_feat_stride, input_spatial_stride, input_grad_batch_stride, input_grad_feat_stride, input_grad_spatial_stride, affine: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_SPATIAL: tl.constexpr):",
    "body": "feat_pid = tl.program_id(axis=0)\nbatch_offset = tl.arange(0, BLOCK_SIZE_BATCH)\nbatch_mask = batch_offset < batch_dim\nmean = tl.load(feat_pid + mean_pointer)\ninv_std = tl.load(feat_pid + inv_std_pointer)\nterm1 = 0.0\nterm2 = 0.0\nfor block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n    spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0, BLOCK_SIZE_SPATIAL)\n    spatial_mask = spatial_offset < spatial_dim\n    curr_output_grad_pointer = output_grad_pointer + output_grad_feat_stride * feat_pid + output_grad_batch_stride * batch_offset[:, None] + output_grad_spatial_stride * spatial_offset[None, :]\n    curr_input_pointer = input_pointer + input_feat_stride * feat_pid + input_batch_stride * batch_offset[:, None] + input_spatial_stride * spatial_offset[None, :]\n    curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(tl.float32)\n    curr_pre_lin = (curr_input - mean) * inv_std\n    curr_output_grad = tl.load(curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(tl.float32)\n    term1 += tl.sum(curr_pre_lin * curr_output_grad)\n    term2 += tl.sum(curr_output_grad)\nif affine:\n    weight = tl.load(feat_pid + weight_pointer)\n    weight_grad = 0.0\n    bias_grad = 0.0\nelse:\n    weight = 1.0\ncount = batch_dim * spatial_dim\nterm1 *= weight / count\nterm2 *= weight / count\nfor block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):\n    spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(0, BLOCK_SIZE_SPATIAL)\n    spatial_mask = spatial_offset < spatial_dim\n    curr_output_grad_pointer = output_grad_pointer + output_grad_feat_stride * feat_pid + output_grad_batch_stride * batch_offset[:, None] + output_grad_spatial_stride * spatial_offset[None, :]\n    curr_input_pointer = input_pointer + input_feat_stride * feat_pid + input_batch_stride * batch_offset[:, None] + input_spatial_stride * spatial_offset[None, :]\n    curr_input_grad_pointer = input_grad_pointer + input_grad_feat_stride * feat_pid + input_grad_batch_stride * batch_offset[:, None] + input_grad_spatial_stride * spatial_offset[None, :]\n    curr_input = tl.load(curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(tl.float32)\n    curr_pre_lin = (curr_input - mean) * inv_std\n    curr_output_grad = tl.load(curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(tl.float32)\n    curr_input_grad = inv_std * (weight * curr_output_grad - (term1 * curr_pre_lin + term2))\n    tl.store(curr_input_grad_pointer, curr_input_grad, mask=batch_mask[:, None] & spatial_mask[None, :])\n    if affine:\n        weight_grad += tl.sum(curr_pre_lin * curr_output_grad)\n        bias_grad += tl.sum(curr_output_grad)\nif affine:\n    tl.store(feat_pid + weight_grad_pointer, weight_grad)\n    tl.store(feat_pid + bias_grad_pointer, bias_grad)"
  },
  {
    "name": "parallel_path_bwd_dq_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "hc_whole",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dhc_whole",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_bwd_dq_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    hc_whole,",
      "    scale,",
      "    L,",
      "    D,",
      "    dq,",
      "    do,",
      "    dhc_whole,",
      "    dg_cumsum,",
      "    cu_seqlens,",
      "    indices,",
      "    split_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    dq += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * HQ + i_hq) * V",
      "    hc_whole += (boh_large * H + i_h) * K * K",
      "    dhc_whole += (boh_large * HQ + i_hq) * K * K",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "",
      "    stride_h = H * K * K",
      "    stride_hq = HQ * K * K",
      "    sm_scale = scale * 1.44269504",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_q_origin = tl.load(p_q, boundary_check=(0, 1))",
      "    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    p_d = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_l, boundary_check=(0,))",
      "    b_delta = tl.load(p_d, boundary_check=(0,))",
      "",
      "    if USE_GATE:",
      "        b_g_cumsum_q = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        p_g_cumsum_q = tl.make_block_ptr(",
      "            g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g_cumsum_q += tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "        b_dg_cumsum_q = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_g_cumsum_q = None",
      "        b_dg_cumsum_q = None",
      "",
      "    idx_i = i_t * BT // S",
      "    curr_end = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)",
      "    b_dq = tl.zeros([BT, K], dtype=tl.float32)",
      "",
      "    for offset_outer in range(0, curr_end, S):",
      "        idx_j = offset_outer // S",
      "        b_q_accum = tl.zeros([BT, BK], dtype=tl.float32)",
      "        b_q_accum += b_q_origin",
      "        for i in range(idx_i - 1, idx_j, -1):",
      "            p_h = tl.make_block_ptr(",
      "                hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "            )",
      "            b_h = tl.load(p_h, boundary_check=(0, 1))",
      "            b_q_accum = b_q_accum - tl.dot(b_q_accum.to(b_h.dtype), b_h)",
      "        b_q = b_q_accum.to(b_q_origin.dtype)",
      "        b_dh = -tl.dot(tl.trans(b_q), b_dq.to(b_q.dtype))",
      "",
      "        tl.atomic_add(",
      "            dhc_whole",
      "            + idx_j * stride_hq",
      "            + tl.arange(0, K)[:, None] * K",
      "            + tl.arange(0, K)[None, :],",
      "            b_dh,",
      "            sem=\"relaxed\",",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            hc_whole + idx_j * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dq = b_dq - tl.dot(b_dq.to(b_h.dtype), tl.trans(b_h))",
      "",
      "        for offset in range(offset_outer, min(offset_outer + S, i_t * BT), BS):",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (H * K, 1), (offset, 0), (BS, BK), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_A = tl.dot(b_q, tl.trans(b_k))",
      "            if USE_GATE:",
      "                p_g_cumsum_k = tl.make_block_ptr(",
      "                    g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)",
      "                )",
      "                b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "                b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "                b_A = tl.where(",
      "                    (i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\")",
      "                )",
      "            b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "            p_v = tl.make_block_ptr(",
      "                v, (V, T), (1, V * H), (0, offset), (BK, BS), (0, 1)",
      "            )",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_dp = tl.dot(b_do, b_v)",
      "            b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "            b_dq += tl.dot(b_dA.to(b_k.dtype), b_k)",
      "            if USE_GATE:",
      "                b_dg_cumsum_q += tl.sum(b_dA, axis=1)",
      "",
      "    p_dq = tl.make_block_ptr(dq, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    tl.store(p_dq, b_dq.to(dq.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_GATE:",
      "        tl.atomic_add(",
      "            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_q, sem=\"relaxed\"",
      "        )"
    ],
    "file": "codes/410.py",
    "header": "def parallel_path_bwd_dq_kernel(q, k, v, g_cumsum, hc_whole, scale, L, D, dq, do, dhc_whole, dg_cumsum, cu_seqlens, indices, split_offsets, T, G: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, S: tl.constexpr, IS_VARLEN: tl.constexpr, USE_GATE: tl.constexpr):",
    "body": "i_t, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    boh_large = tl.load(split_offsets + i_n).to(tl.int32)\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    boh_large = i_n * tl.cdiv(T, S)\nq += (bos * HQ + i_hq) * K\ndq += (bos * HQ + i_hq) * K\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\ndo += (bos * HQ + i_hq) * V\nhc_whole += (boh_large * H + i_h) * K * K\ndhc_whole += (boh_large * HQ + i_hq) * K * K\nL += bos * HQ + i_hq\nD += bos * HQ + i_hq\nif USE_GATE:\n    g_cumsum += bos * HQ + i_hq\n    dg_cumsum += bos * HQ + i_hq\nstride_h = H * K * K\nstride_hq = HQ * K * K\nsm_scale = scale * 1.44269504\np_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_q_origin = tl.load(p_q, boundary_check=(0, 1))\np_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_do = tl.load(p_do, boundary_check=(0, 1))\np_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\np_d = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_l = tl.load(p_l, boundary_check=(0,))\nb_delta = tl.load(p_d, boundary_check=(0,))\nif USE_GATE:\n    b_g_cumsum_q = tl.zeros([BT], dtype=tl.float32)\n    p_g_cumsum_q = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_g_cumsum_q += tl.load(p_g_cumsum_q, boundary_check=(0,))\n    b_dg_cumsum_q = tl.zeros([BT], dtype=tl.float32)\nelse:\n    b_g_cumsum_q = None\n    b_dg_cumsum_q = None\nidx_i = i_t * BT // S\ncurr_end = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)\nb_dq = tl.zeros([BT, K], dtype=tl.float32)\nfor offset_outer in range(0, curr_end, S):\n    idx_j = offset_outer // S\n    b_q_accum = tl.zeros([BT, BK], dtype=tl.float32)\n    b_q_accum += b_q_origin\n    for i in range(idx_i - 1, idx_j, -1):\n        p_h = tl.make_block_ptr(hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_q_accum = b_q_accum - tl.dot(b_q_accum.to(b_h.dtype), b_h)\n    b_q = b_q_accum.to(b_q_origin.dtype)\n    b_dh = -tl.dot(tl.trans(b_q), b_dq.to(b_q.dtype))\n    tl.atomic_add(dhc_whole + idx_j * stride_hq + tl.arange(0, K)[:, None] * K + tl.arange(0, K)[None, :], b_dh, sem='relaxed')\n    p_h = tl.make_block_ptr(hc_whole + idx_j * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dq = b_dq - tl.dot(b_dq.to(b_h.dtype), tl.trans(b_h))\n    for offset in range(offset_outer, min(offset_outer + S, i_t * BT), BS):\n        p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (offset, 0), (BS, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_A = tl.dot(b_q, tl.trans(b_k))\n        if USE_GATE:\n            p_g_cumsum_k = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,))\n            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))\n            b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n            b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float('-inf'))\n        b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n        p_v = tl.make_block_ptr(v, (V, T), (1, V * H), (0, offset), (BK, BS), (0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_dp = tl.dot(b_do, b_v)\n        b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n        b_dq += tl.dot(b_dA.to(b_k.dtype), b_k)\n        if USE_GATE:\n            b_dg_cumsum_q += tl.sum(b_dA, axis=1)\np_dq = tl.make_block_ptr(dq, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_dq, b_dq.to(dq.dtype.element_ty), boundary_check=(0, 1))\nif USE_GATE:\n    tl.atomic_add(dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_q, sem='relaxed')"
  },
  {
    "name": "threadblock_swizzle_gemm_reduce_scatter_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank'])"
    ],
    "args": [
      {
        "name": "tiled_m",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NNODES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DEBUG",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def threadblock_swizzle_gemm_reduce_scatter_kernel(",
      "    tiled_m,",
      "    M,",
      "    rank,",
      "    WORLD_SIZE: tl.constexpr,",
      "    NNODES: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    DEBUG: tl.constexpr = False,",
      "):",
      "    LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES",
      "    node_id = rank // LOCAL_WORLD_SIZE",
      "    M_per_rank = M // WORLD_SIZE",
      "    M_per_node = M // NNODES",
      "    node_start = node_id + 1",
      "",
      "    lane_id = laneid()",
      "",
      "    if lane_id < NNODES:",
      "        n = (lane_id + node_start) % NNODES",
      "        M_node_start = M_per_node * n",
      "        M_node_end = M_per_node * (n + 1)",
      "        tiled_m_node_start = M_node_start // BLOCK_SIZE_M",
      "",
      "        prev_tiled_m_node_end = (M_node_start - 1) // BLOCK_SIZE_M",
      "        if lane_id != 0 and M_node_start != 0:",
      "            if prev_tiled_m_node_end == tiled_m_node_start:",
      "                tiled_m_node_start += 1",
      "",
      "        tiled_m_node_end = (M_node_end - 1) // BLOCK_SIZE_M",
      "        next_tiled_m_node_start = M_node_end // BLOCK_SIZE_M",
      "        if lane_id == NNODES - 1 and M_node_end != M:",
      "            if next_tiled_m_node_start == tiled_m_node_end:",
      "                tiled_m_node_end -= 1",
      "",
      "        swizzled_tiled_m_size = tiled_m_node_end - tiled_m_node_start + 1",
      "    else:",
      "        swizzled_tiled_m_size = 0",
      "",
      "    if DEBUG and lane_id < NNODES:",
      "        print(\"swizzled_tiled_m_size\", swizzled_tiled_m_size, lane_id)",
      "    swizzled_tiled_m_size_accum = (",
      "        warp_prefix_sum_kernel(swizzled_tiled_m_size, lane_id, NNODES)",
      "        - swizzled_tiled_m_size",
      "    )",
      "    if DEBUG and lane_id < NNODES:",
      "        print(\"swizzled_tiled_m_size_accum\", swizzled_tiled_m_size_accum)",
      "",
      "    tiled_m_size_l = __shfl_down_sync_i32(",
      "        0xFFFFFFFF, swizzled_tiled_m_size, NNODES - node_start",
      "    )",
      "    tiled_m_size_r = __shfl_up_sync_i32(0xFFFFFFFF, swizzled_tiled_m_size, node_start)",
      "    tiled_m_size = 0",
      "    if lane_id < node_start:",
      "        tiled_m_size = tiled_m_size_l",
      "    elif lane_id < NNODES:",
      "        tiled_m_size = tiled_m_size_r",
      "",
      "    if DEBUG and lane_id < NNODES:",
      "        print(\"tiled_m_size\", tiled_m_size)",
      "",
      "    tiled_m_size_accum = (",
      "        warp_prefix_sum_kernel(tiled_m_size, lane_id, NNODES) - tiled_m_size",
      "    )",
      "    mask = __ballot_sync(0xFFFFFFFF, tiled_m < swizzled_tiled_m_size_accum)",
      "    n = ffs(mask) - 1 - 1",
      "    if DEBUG and lane_id < NNODES + 1:",
      "        print(\"tiled_m_size_accum\", tiled_m_size_accum)",
      "        print(\"n\", n, tiled_m, swizzled_tiled_m_size_accum, mask)",
      "",
      "    nid = (n + node_start) % NNODES",
      "    node_offset = __shfl_sync_i32(0xFFFFFFFF, swizzled_tiled_m_size_accum, n)",
      "",
      "    tile_size = __shfl_sync_i32(0xFFFFFFFF, swizzled_tiled_m_size, n)",
      "",
      "    tiled_m_intra_node = tiled_m - node_offset",
      "    local_rank = rank % LOCAL_WORLD_SIZE",
      "    m_start = M_per_node * nid + M_per_rank * (local_rank + 1)",
      "    tiled_m_start = m_start // BLOCK_SIZE_M",
      "    swizzled_node_offset = __shfl_sync_i32(0xFFFFFFFF, tiled_m_size_accum, nid)",
      "    rank_offset = max(0, tiled_m_start - swizzled_node_offset)",
      "",
      "    tiled_m_intra_node_new = (tiled_m_intra_node + rank_offset) % tile_size",
      "    return swizzled_node_offset + tiled_m_intra_node_new"
    ],
    "file": "codes/44.py",
    "header": "def threadblock_swizzle_gemm_reduce_scatter_kernel(tiled_m, M, rank, WORLD_SIZE: tl.constexpr, NNODES: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, DEBUG: tl.constexpr = False):",
    "body": "LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES\nnode_id = rank // LOCAL_WORLD_SIZE\nM_per_rank = M // WORLD_SIZE\nM_per_node = M // NNODES\nnode_start = node_id + 1\nlane_id = laneid()\nif lane_id < NNODES:\n    n = (lane_id + node_start) % NNODES\n    M_node_start = M_per_node * n\n    M_node_end = M_per_node * (n + 1)\n    tiled_m_node_start = M_node_start // BLOCK_SIZE_M\n    prev_tiled_m_node_end = (M_node_start - 1) // BLOCK_SIZE_M\n    if lane_id != 0 and M_node_start != 0:\n        if prev_tiled_m_node_end == tiled_m_node_start:\n            tiled_m_node_start += 1\n    tiled_m_node_end = (M_node_end - 1) // BLOCK_SIZE_M\n    next_tiled_m_node_start = M_node_end // BLOCK_SIZE_M\n    if lane_id == NNODES - 1 and M_node_end != M:\n        if next_tiled_m_node_start == tiled_m_node_end:\n            tiled_m_node_end -= 1\n    swizzled_tiled_m_size = tiled_m_node_end - tiled_m_node_start + 1\nelse:\n    swizzled_tiled_m_size = 0\nif DEBUG and lane_id < NNODES:\n    print('swizzled_tiled_m_size', swizzled_tiled_m_size, lane_id)\nswizzled_tiled_m_size_accum = warp_prefix_sum_kernel(swizzled_tiled_m_size, lane_id, NNODES) - swizzled_tiled_m_size\nif DEBUG and lane_id < NNODES:\n    print('swizzled_tiled_m_size_accum', swizzled_tiled_m_size_accum)\ntiled_m_size_l = __shfl_down_sync_i32(4294967295, swizzled_tiled_m_size, NNODES - node_start)\ntiled_m_size_r = __shfl_up_sync_i32(4294967295, swizzled_tiled_m_size, node_start)\ntiled_m_size = 0\nif lane_id < node_start:\n    tiled_m_size = tiled_m_size_l\nelif lane_id < NNODES:\n    tiled_m_size = tiled_m_size_r\nif DEBUG and lane_id < NNODES:\n    print('tiled_m_size', tiled_m_size)\ntiled_m_size_accum = warp_prefix_sum_kernel(tiled_m_size, lane_id, NNODES) - tiled_m_size\nmask = __ballot_sync(4294967295, tiled_m < swizzled_tiled_m_size_accum)\nn = ffs(mask) - 1 - 1\nif DEBUG and lane_id < NNODES + 1:\n    print('tiled_m_size_accum', tiled_m_size_accum)\n    print('n', n, tiled_m, swizzled_tiled_m_size_accum, mask)\nnid = (n + node_start) % NNODES\nnode_offset = __shfl_sync_i32(4294967295, swizzled_tiled_m_size_accum, n)\ntile_size = __shfl_sync_i32(4294967295, swizzled_tiled_m_size, n)\ntiled_m_intra_node = tiled_m - node_offset\nlocal_rank = rank % LOCAL_WORLD_SIZE\nm_start = M_per_node * nid + M_per_rank * (local_rank + 1)\ntiled_m_start = m_start // BLOCK_SIZE_M\nswizzled_node_offset = __shfl_sync_i32(4294967295, tiled_m_size_accum, nid)\nrank_offset = max(0, tiled_m_start - swizzled_node_offset)\ntiled_m_intra_node_new = (tiled_m_intra_node + rank_offset) % tile_size\nreturn swizzled_node_offset + tiled_m_intra_node_new"
  },
  {
    "name": "parallel_simple_gla_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']), 'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None, 'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BT', 'BS', 'BK', 'BV', 'USE_G'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "attn",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "OUTPUT_ATTENTIONS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    o,",
      "    attn,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NV: tl.constexpr,",
      "    OUTPUT_ATTENTIONS: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    o += i_k * B * T * H * V",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "    if OUTPUT_ATTENTIONS:",
      "        attn += (bos * H + i_h * T) * T + i_k * B * H * T * T",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_g = H",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "",
      "    o_k = i_t * BT + tl.arange(0, BS)",
      "",
      "    if USE_G:",
      "        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "",
      "        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "",
      "    else:",
      "        b_gq = None",
      "",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_s = tl.dot(b_q, b_k)",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gk = tl.load(p_gk, boundary_check=(0,))",
      "            b_s *= safe_exp(b_gq[:, None] - b_gk[None, :])",
      "            b_s = tl.where(m_s, b_s, 0)",
      "        else:",
      "            b_s = tl.where(m_s, b_s, 0)",
      "",
      "        if i_s >= 0:",
      "            b_o += tl.dot(b_s.to(b_q.dtype), b_v)",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0)",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "        o_k += BS",
      "",
      "    for i_s in range(i_t * BT - BS, -BS, -BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_s = tl.dot(b_q, b_k)",
      "        if USE_G:",
      "            p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_g = tl.load(p_g, boundary_check=(0,))",
      "            b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)",
      "            b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0",
      "",
      "            b_s *= safe_exp(b_gq[:, None] + (b_gn - b_g)[None, :])",
      "            b_gq += b_gn - b_gp",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0)",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_s >= 0:",
      "            b_o += tl.dot(b_s.to(b_v.dtype), b_v)",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/424.py",
    "header": "def parallel_simple_gla_fwd_kernel(q, k, v, g, o, attn, scale, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NV: tl.constexpr, OUTPUT_ATTENTIONS: tl.constexpr, IS_VARLEN: tl.constexpr, USE_G: tl.constexpr):",
    "body": "i_kv, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_k, i_v = (i_kv // NV, i_kv % NV)\ni_b, i_h = (i_bh // H, i_bh % H)\no += i_k * B * T * H * V\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\no += (bos * H + i_h) * V\nif USE_G:\n    g += bos * H + i_h\nif OUTPUT_ATTENTIONS:\n    attn += (bos * H + i_h * T) * T + i_k * B * H * T * T\nstride_qk = H * K\nstride_vo = H * V\nstride_g = H\np_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\no_q = i_t * BT + tl.arange(0, BT)\no_k = i_t * BT + tl.arange(0, BS)\nif USE_G:\n    p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n    b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\nelse:\n    b_gq = None\nfor i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n    p_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    m_s = o_q[:, None] >= o_k[None, :]\n    b_s = tl.dot(b_q, b_k)\n    if USE_G:\n        p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        b_s *= safe_exp(b_gq[:, None] - b_gk[None, :])\n        b_s = tl.where(m_s, b_s, 0)\n    else:\n        b_s = tl.where(m_s, b_s, 0)\n    if i_s >= 0:\n        b_o += tl.dot(b_s.to(b_q.dtype), b_v)\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0))\n        tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n    o_k += BS\nfor i_s in range(i_t * BT - BS, -BS, -BS):\n    p_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    if USE_G:\n        p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)\n        b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0\n        b_s *= safe_exp(b_gq[:, None] + (b_gn - b_g)[None, :])\n        b_gq += b_gn - b_gp\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0))\n        tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\n    if i_s >= 0:\n        b_o += tl.dot(b_s.to(b_v.dtype), b_v)\np_o = tl.make_block_ptr(o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_simple_gla_bwd_kernel_dq",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_t",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_vo",
        "annotation": null
      },
      {
        "name": "stride_g",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_bwd_kernel_dq(",
      "    i_t,",
      "    i_k,",
      "    i_v,",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dq,",
      "    dg,",
      "    stride_qk,",
      "    stride_vo,",
      "    stride_g,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    p_do = tl.make_block_ptr(",
      "        do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "    for i_s in range(0, i_t * BT, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v)",
      "        if USE_G:",
      "            p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_g = tl.load(p_g, boundary_check=(0,))",
      "            b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)",
      "            b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0",
      "            b_ds *= safe_exp(b_gn - b_g)[None, :]",
      "            if i_s > 0:",
      "                b_dq *= safe_exp(b_gn - b_gp)",
      "",
      "        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k)",
      "",
      "    if USE_G:",
      "        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "",
      "        b_gq = tl.load(p_gq, boundary_check=(0,))",
      "",
      "        b_dq *= safe_exp(b_gq)[:, None]",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "",
      "    o_k = i_t * BT + tl.arange(0, BS)",
      "",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v)",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gk = tl.load(p_gk, boundary_check=(0,))",
      "            b_ds *= safe_exp(b_gq[:, None] - b_gk[None, :])",
      "        b_ds = tl.where(o_q[:, None] >= o_k[None, :], b_ds, 0)",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k)",
      "        o_k += BS",
      "",
      "    b_dq *= scale",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_dg = tl.sum(b_dq * b_q, 1)",
      "        p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/424.py",
    "header": "def parallel_simple_gla_bwd_kernel_dq(i_t, i_k, i_v, q, k, v, g, do, dq, dg, stride_qk, stride_vo, stride_g, scale, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr):",
    "body": "p_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nfor i_s in range(0, i_t * BT, BS):\n    p_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_ds = tl.dot(b_do, b_v)\n    if USE_G:\n        p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n        b_g = tl.load(p_g, boundary_check=(0,))\n        b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)\n        b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0\n        b_ds *= safe_exp(b_gn - b_g)[None, :]\n        if i_s > 0:\n            b_dq *= safe_exp(b_gn - b_gp)\n    b_dq += tl.dot(b_ds.to(b_v.dtype), b_k)\nif USE_G:\n    p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n    b_gq = tl.load(p_gq, boundary_check=(0,))\n    b_dq *= safe_exp(b_gq)[:, None]\no_q = i_t * BT + tl.arange(0, BT)\no_k = i_t * BT + tl.arange(0, BS)\nfor i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n    p_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_ds = tl.dot(b_do, b_v)\n    if USE_G:\n        p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,))\n        b_ds *= safe_exp(b_gq[:, None] - b_gk[None, :])\n    b_ds = tl.where(o_q[:, None] >= o_k[None, :], b_ds, 0)\n    b_dq += tl.dot(b_ds.to(b_k.dtype), b_k)\n    o_k += BS\nb_dq *= scale\np_dq = tl.make_block_ptr(dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\nif USE_G:\n    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_dg = tl.sum(b_dq * b_q, 1)\n    p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "parallel_simple_gla_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_t",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_vo",
        "annotation": null
      },
      {
        "name": "stride_g",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_bwd_kernel_dkv(",
      "    i_t,",
      "    i_k,",
      "    i_v,",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dk,",
      "    dv,",
      "    dg,",
      "    scale,",
      "    stride_qk,",
      "    stride_vo,",
      "    stride_g,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "    if USE_G:",
      "        p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "        b_gk = tl.load(p_gk, boundary_check=(0,))",
      "    NTS = tl.cdiv(T, BS)",
      "",
      "    for i_s in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_ds = tl.dot(b_v, tl.trans(b_do))",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gq = tl.load(p_gq, boundary_check=(0,))",
      "            b_gp = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)",
      "            b_gn = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0",
      "            if i_s >= 0:",
      "                tmp = safe_exp(b_gp - b_gn)",
      "                b_dk *= tmp",
      "                b_dv *= tmp",
      "                tmp2 = safe_exp(b_gq - b_gn)",
      "                b_ds *= tmp2[None, :]",
      "                b_s *= tmp2[None, :]",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "        b_dv += tl.dot(b_s.to(b_do.dtype), b_do)",
      "",
      "    if USE_G:",
      "        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * stride_g)",
      "        if i_t >= 0:",
      "            tmp2 = safe_exp(b_g_last - b_gk)[:, None]",
      "            b_dk *= tmp2",
      "            b_dv *= tmp2",
      "",
      "    o_q = i_t * BT + tl.arange(0, BS)",
      "    o_k = i_t * BT + tl.arange(0, BT)",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_v, tl.trans(b_do))",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gq = tl.load(p_gq, boundary_check=(0,))",
      "            if i_s >= 0:",
      "                tmp = safe_exp(-b_gk[:, None] + b_gq[None, :])",
      "                b_ds *= tmp",
      "                b_s *= tmp",
      "        m_s = o_k[:, None] <= o_q[None, :]",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_ds = tl.where(m_s, b_ds, 0)",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "        b_dv += tl.dot(b_s.to(b_do.dtype), b_do)",
      "        o_q += BS",
      "    b_dk *= scale",
      "    b_dv *= scale",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "        b_dg = tl.load(p_dg, boundary_check=(0,))",
      "        b_dg -= tl.sum(b_dk * b_k, 1)",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/424.py",
    "header": "def parallel_simple_gla_bwd_kernel_dkv(i_t, i_k, i_v, q, k, v, g, do, dk, dv, dg, scale, stride_qk, stride_vo, stride_g, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr):",
    "body": "p_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\np_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BT, BV], dtype=tl.float32)\nif USE_G:\n    p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n    b_gk = tl.load(p_gk, boundary_check=(0,))\nNTS = tl.cdiv(T, BS)\nfor i_s in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):\n    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0))\n    p_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_ds = tl.dot(b_v, tl.trans(b_do))\n    b_s = tl.dot(b_k, tl.trans(b_q))\n    if USE_G:\n        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n        b_gp = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)\n        b_gn = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0\n        if i_s >= 0:\n            tmp = safe_exp(b_gp - b_gn)\n            b_dk *= tmp\n            b_dv *= tmp\n            tmp2 = safe_exp(b_gq - b_gn)\n            b_ds *= tmp2[None, :]\n            b_s *= tmp2[None, :]\n    b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n    b_dv += tl.dot(b_s.to(b_do.dtype), b_do)\nif USE_G:\n    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * stride_g)\n    if i_t >= 0:\n        tmp2 = safe_exp(b_g_last - b_gk)[:, None]\n        b_dk *= tmp2\n        b_dv *= tmp2\no_q = i_t * BT + tl.arange(0, BS)\no_k = i_t * BT + tl.arange(0, BT)\nfor i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0))\n    p_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_ds = tl.dot(b_v, tl.trans(b_do))\n    b_s = tl.dot(b_k, tl.trans(b_q))\n    if USE_G:\n        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))\n        b_gq = tl.load(p_gq, boundary_check=(0,))\n        if i_s >= 0:\n            tmp = safe_exp(-b_gk[:, None] + b_gq[None, :])\n            b_ds *= tmp\n            b_s *= tmp\n    m_s = o_k[:, None] <= o_q[None, :]\n    b_s = tl.where(m_s, b_s, 0)\n    b_ds = tl.where(m_s, b_ds, 0)\n    b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n    b_dv += tl.dot(b_s.to(b_do.dtype), b_do)\n    o_q += BS\nb_dk *= scale\nb_dv *= scale\np_dk = tl.make_block_ptr(dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dv = tl.make_block_ptr(dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\nif USE_G:\n    p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\n    b_dg = tl.load(p_dg, boundary_check=(0,))\n    b_dg -= tl.sum(b_dk * b_k, 1)\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "parallel_simple_gla_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']), 'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config(triton_config, num_warps=num_warps) for num_warps in NUM_WARPS], key=['BT', 'BS', 'BK', 'BV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dg,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    dq += i_v * B * H * T * K",
      "    dk += i_v * B * H * T * K",
      "    dv += i_k * B * H * T * V",
      "    if USE_G:",
      "        dg += i_kv * B * H * T",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    dv += (bos * H + i_h) * V",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        dg += bos * H + i_h",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_g = H",
      "",
      "    parallel_simple_gla_bwd_kernel_dq(",
      "        i_t=i_t,",
      "        i_k=i_k,",
      "        i_v=i_v,",
      "        q=q,",
      "        k=k,",
      "        v=v,",
      "        g=g,",
      "        do=do,",
      "        dq=dq,",
      "        dg=dg,",
      "        scale=scale,",
      "        stride_qk=stride_qk,",
      "        stride_vo=stride_vo,",
      "        stride_g=stride_g,",
      "        T=T,",
      "        K=K,",
      "        V=V,",
      "        BT=BT,",
      "        BS=BS,",
      "        BK=BK,",
      "        BV=BV,",
      "        USE_G=USE_G,",
      "    )",
      "    tl.debug_barrier()",
      "    parallel_simple_gla_bwd_kernel_dkv(",
      "        i_t=i_t,",
      "        i_k=i_k,",
      "        i_v=i_v,",
      "        q=q,",
      "        k=k,",
      "        v=v,",
      "        g=g,",
      "        do=do,",
      "        dk=dk,",
      "        dv=dv,",
      "        dg=dg,",
      "        scale=scale,",
      "        stride_qk=stride_qk,",
      "        stride_vo=stride_vo,",
      "        stride_g=stride_g,",
      "        T=T,",
      "        K=K,",
      "        V=V,",
      "        BT=BT,",
      "        BS=BS,",
      "        BK=BK,",
      "        BV=BV,",
      "        USE_G=USE_G,",
      "    )"
    ],
    "file": "codes/424.py",
    "header": "def parallel_simple_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, dg, scale, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NV: tl.constexpr, IS_VARLEN: tl.constexpr, USE_G: tl.constexpr):",
    "body": "i_kv, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_k, i_v = (i_kv // NV, i_kv % NV)\ni_b, i_h = (i_bh // H, i_bh % H)\ndq += i_v * B * H * T * K\ndk += i_v * B * H * T * K\ndv += i_k * B * H * T * V\nif USE_G:\n    dg += i_kv * B * H * T\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\ndo += (bos * H + i_h) * V\ndq += (bos * H + i_h) * K\ndk += (bos * H + i_h) * K\ndv += (bos * H + i_h) * V\nif USE_G:\n    g += bos * H + i_h\n    dg += bos * H + i_h\nstride_qk = H * K\nstride_vo = H * V\nstride_g = H\nparallel_simple_gla_bwd_kernel_dq(i_t=i_t, i_k=i_k, i_v=i_v, q=q, k=k, v=v, g=g, do=do, dq=dq, dg=dg, scale=scale, stride_qk=stride_qk, stride_vo=stride_vo, stride_g=stride_g, T=T, K=K, V=V, BT=BT, BS=BS, BK=BK, BV=BV, USE_G=USE_G)\ntl.debug_barrier()\nparallel_simple_gla_bwd_kernel_dkv(i_t=i_t, i_k=i_k, i_v=i_v, q=q, k=k, v=v, g=g, do=do, dk=dk, dv=dv, dg=dg, scale=scale, stride_qk=stride_qk, stride_vo=stride_vo, stride_g=stride_g, T=T, K=K, V=V, BT=BT, BS=BS, BK=BK, BV=BV, USE_G=USE_G)"
  },
  {
    "name": "dropout_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dropout_forward_kernel(",
      "    input_pointer,",
      "    output_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input = tl.load(input_pointer + offset, mask=mask)",
      "    output = apply_dropout(input, drop_p, seed, offset)",
      "    tl.store(output_pointer + offset, output, mask=mask)"
    ],
    "file": "codes/15.py",
    "header": "def dropout_forward_kernel(input_pointer, output_pointer, size, drop_p, seed, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\noffset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = offset < size\ninput = tl.load(input_pointer + offset, mask=mask)\noutput = apply_dropout(input, drop_p, seed, offset)\ntl.store(output_pointer + offset, output, mask=mask)"
  },
  {
    "name": "dropout_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dropout_backward_kernel(",
      "    output_grad_pointer,",
      "    input_grad_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad = tl.load(output_grad_pointer + offset, mask=mask)",
      "    input_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)",
      "    tl.store(input_grad_pointer + offset, input_grad, mask=mask)"
    ],
    "file": "codes/15.py",
    "header": "def dropout_backward_kernel(output_grad_pointer, input_grad_pointer, size, drop_p, seed, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\noffset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = offset < size\noutput_grad = tl.load(output_grad_pointer + offset, mask=mask)\ninput_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)\ntl.store(input_grad_pointer + offset, input_grad, mask=mask)"
  },
  {
    "name": "fused_chunk_linear_attn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [4] for num_stages in [1]], key=['B', 'H', 'K', 'V', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_linear_attn_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    h0,",
      "    ht,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    o_i = tl.arange(0, BT)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(0, tl.cdiv(T, BT)):",
      "        p_q = tl.make_block_ptr(",
      "            q + (i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (i_b * T * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (i_k * B * T * H + i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = tl.where(m_s, b_s, 0)",
      "",
      "        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "        if CHECK and i_t == 0:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)",
      "        else:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/399.py",
    "header": "def fused_chunk_linear_attn_fwd_kernel(q, k, v, o, h0, ht, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, CHECK: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(0, tl.cdiv(T, BT)):\n    p_q = tl.make_block_ptr(q + (i_b * T * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + (i_b * T * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (i_b * T * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (i_k * B * T * H + i_b * T * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n    if CHECK and i_t == 0:\n        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n        b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n    else:\n        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n        b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_chunk_linear_attn_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [4] for num_stages in [1]], key=['B', 'H', 'K', 'V', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_linear_attn_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    o_i = tl.arange(0, BT)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(0, tl.cdiv(T, BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (i_b * T * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (i_v * B * T * H + i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        b_ds = tl.where(m_s, b_ds, 0)",
      "",
      "        b_dq = tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)",
      "",
      "        if CHECK and i_t == 0:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)",
      "        else:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)",
      "        b_dq *= scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_h = None",
      "    tl.debug_barrier()",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    m_s = o_i[:, None] <= o_i[None, :]",
      "    for i_t in range(1, tl.cdiv(T, BT) + 1):",
      "        p_q = tl.make_block_ptr(",
      "            q + (i_b * T * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, T - i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (T - i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (T - i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (T - i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (i_v * B * T * H + i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (T - i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * B * T * H + i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (T - i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_k, b_q, allow_tf32=False)",
      "        b_s = tl.where(m_s, b_s, 0).to(b_q.dtype)",
      "",
      "        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)",
      "        b_ds = tl.where(m_s, b_ds, 0).to(b_q.dtype)",
      "",
      "        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)",
      "",
      "        b_dv = tl.dot(b_s, b_do, allow_tf32=False)",
      "        if CHECK and i_t == 1:",
      "            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)",
      "            b_dh += tl.dot(b_q, b_do, allow_tf32=False)",
      "        else:",
      "            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)",
      "            b_dh += tl.dot(b_q, b_do, allow_tf32=False)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/399.py",
    "header": "def fused_chunk_linear_attn_bwd_kernel(q, k, v, do, dq, dk, dv, h0, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, CHECK: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(0, tl.cdiv(T, BT)):\n    p_k = tl.make_block_ptr(k + (i_b * T * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + (i_b * T * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_t * BT), (BV, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + (i_b * T * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dq = tl.make_block_ptr(dq + (i_v * B * T * H + i_b * T * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n    b_ds = tl.where(m_s, b_ds, 0)\n    b_dq = tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)\n    if CHECK and i_t == 0:\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n        b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n    else:\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n        b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\nb_h = None\ntl.debug_barrier()\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nm_s = o_i[:, None] <= o_i[None, :]\nfor i_t in range(1, tl.cdiv(T, BT) + 1):\n    p_q = tl.make_block_ptr(q + (i_b * T * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, T - i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + (i_b * T * H + i_h) * K, (T, K), (H * K, 1), (T - i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + (i_b * T * H + i_h) * V, (T, V), (H * V, 1), (T - i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + (i_b * T * H + i_h) * V, (T, V), (H * V, 1), (T - i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (i_v * B * T * H + i_b * T * H + i_h) * K, (T, K), (H * K, 1), (T - i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * B * T * H + i_b * T * H + i_h) * V, (T, V), (H * V, 1), (T - i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_s = tl.dot(b_k, b_q, allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0).to(b_q.dtype)\n    b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n    b_ds = tl.where(m_s, b_ds, 0).to(b_q.dtype)\n    b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n    b_dv = tl.dot(b_s, b_do, allow_tf32=False)\n    if CHECK and i_t == 1:\n        b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n    else:\n        b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)\n        b_dh += tl.dot(b_q, b_do, allow_tf32=False)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_chunk_scan_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_f_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_b_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    C_ptr,",
      "    prev_states_f_ptr,",
      "    prev_states_b_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_states_f_batch,",
      "    stride_states_f_chunk,",
      "    stride_states_f_head,",
      "    stride_states_f_hdim,",
      "    stride_states_f_dstate,",
      "    stride_states_b_batch,",
      "    stride_states_b_chunk,",
      "    stride_states_b_head,",
      "    stride_states_b_hdim,",
      "    stride_states_b_dstate,",
      "    stride_D_head,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    prev_states_f_ptr += (",
      "        pid_b * stride_states_f_batch",
      "        + pid_c * stride_states_f_chunk",
      "        + pid_h * stride_states_f_head",
      "    )",
      "    prev_states_b_ptr += (",
      "        pid_b * stride_states_b_batch",
      "        + pid_c * stride_states_b_chunk",
      "        + pid_h * stride_states_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dA_cs_f_m = tl.load(",
      "        dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "        mask=offs_m < chunk_size,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dA_cs_b_m = tl.load(",
      "        dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "        mask=offs_m < chunk_size,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    if IS_TRITON_22 or pid_c > -1:",
      "",
      "        offs_k_dstate = tl.arange(",
      "            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "        )",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "        )",
      "        prev_states_f_ptrs = prev_states_f_ptr + (",
      "            offs_n[None, :] * stride_states_f_hdim",
      "            + offs_k_dstate[:, None] * stride_states_f_dstate",
      "        )",
      "        prev_states_b_ptrs = prev_states_b_ptr + (",
      "            offs_n[None, :] * stride_states_b_hdim",
      "            + offs_k_dstate[:, None] * stride_states_b_dstate",
      "        )",
      "        scale_f_m = tl.exp(dA_cs_f_m)",
      "        scale_b_m = tl.exp(dA_cs_b_m)",
      "        if BLOCK_SIZE_DSTATE <= 128:",
      "            C = tl.load(",
      "                C_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k_dstate[None, :] < dstate),",
      "                other=0.0,",
      "            )",
      "            prev_states_f = tl.load(",
      "                prev_states_f_ptrs,",
      "                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)",
      "            acc = tl.dot(C, prev_states_f) * scale_f_m[:, None]",
      "",
      "            prev_states_b = tl.load(",
      "                prev_states_b_ptrs,",
      "                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)",
      "            acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]",
      "        else:",
      "            for k in range(0, dstate, BLOCK_SIZE_K):",
      "                C = tl.load(",
      "                    C_ptrs,",
      "                    mask=(offs_m[:, None] < chunk_size_limit)",
      "                    & (offs_k_dstate[None, :] < dstate - k),",
      "                    other=0.0,",
      "                )",
      "",
      "                prev_states_f = tl.load(",
      "                    prev_states_f_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate - k)",
      "                    & (offs_n[None, :] < hdim),",
      "                    other=0.0,",
      "                )",
      "                prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states_f) * scale_f_m[:, None]",
      "",
      "                prev_states_b = tl.load(",
      "                    prev_states_f_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                    other=0.0,",
      "                )",
      "                prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]",
      "                C_ptrs += BLOCK_SIZE_K",
      "                prev_states_f_ptrs += BLOCK_SIZE_K",
      "                prev_states_b_ptrs += BLOCK_SIZE_K",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "    K_F_MAX = min((pid_m) * BLOCK_SIZE_M, chunk_size_limit)",
      "    K_F_MAX_BEG = K_F_MAX - BLOCK_SIZE_K",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if k <= K_F_MAX and k >= K_F_MAX_BEG:",
      "",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_f = offs_m[:, None] >= (k + offs_k[None, :])",
      "            scale_f = tl.where(",
      "                mask_f, tl.exp((dA_cs_f_m[:, None] - dA_cs_f_k[None, :])), 0.0",
      "            )",
      "",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_b = offs_m[:, None] <= (k + offs_k[None, :])",
      "            scale_b = tl.where(",
      "                mask_b, tl.exp((dA_cs_b_m[:, None] - dA_cs_b_k[None, :])), 0.0",
      "            )",
      "",
      "            cb = cb * (scale_f + scale_b)",
      "",
      "        elif k < K_F_MAX:",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] >= k + offs_k[None, :]",
      "            cb *= tl.where(mask, tl.exp((dA_cs_f_m[:, None] - dA_cs_f_k[None, :])), 0.0)",
      "        elif k > K_F_MAX:",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] <= k + offs_k[None, :]",
      "            cb *= tl.where(mask, tl.exp((dA_cs_b_m[:, None] - dA_cs_b_k[None, :])), 0.0)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)",
      "        cb *= dt_k",
      "        cb = cb.to(x_ptr.dtype.element_ty)",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        acc += tl.dot(cb, x)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "",
      "    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    if HAS_D:",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        x_residual = tl.load(",
      "            x_ptr",
      "            + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim),",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc += x_residual * D",
      "",
      "    if HAS_Z:",
      "        out_x_ptr += (",
      "            pid_b * stride_out_batch",
      "            + pid_c * chunk_size * stride_out_seqlen",
      "            + pid_h * stride_out_head",
      "        )",
      "        out_x_ptrs = out_x_ptr + (",
      "            stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :]",
      "        )",
      "        tl.store(",
      "            out_x_ptrs,",
      "            acc,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "        )",
      "",
      "        z_ptr += (",
      "            pid_b * stride_z_batch",
      "            + pid_c * chunk_size * stride_z_seqlen",
      "            + pid_h * stride_z_head",
      "        )",
      "        z_ptrs = z_ptr + (",
      "            stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :]",
      "        )",
      "        z = tl.load(",
      "            z_ptrs,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= z * tl.sigmoid(z)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim",
      "    )",
      "    tl.store(",
      "        out_ptrs,",
      "        acc,",
      "        mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim),",
      "    )"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_fwd_kernel(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr, dt_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, C_ptr, prev_states_f_ptr, prev_states_b_ptr, D_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_states_f_batch, stride_states_f_chunk, stride_states_f_head, stride_states_f_hdim, stride_states_f_dstate, stride_states_b_batch, stride_states_b_chunk, stride_states_b_head, stride_states_b_hdim, stride_states_b_dstate, stride_D_head, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_Z: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr, IS_TRITON_22: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + pid_h * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + pid_h * stride_dA_cs_b_head\nC_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_h // nheads_ngroups_ratio * stride_C_head\nprev_states_f_ptr += pid_b * stride_states_f_batch + pid_c * stride_states_f_chunk + pid_h * stride_states_f_head\nprev_states_b_ptr += pid_b * stride_states_b_batch + pid_c * stride_states_b_chunk + pid_h * stride_states_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndA_cs_f_m = tl.load(dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndA_cs_b_m = tl.load(dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif IS_TRITON_22 or pid_c > -1:\n    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate)\n    prev_states_f_ptrs = prev_states_f_ptr + (offs_n[None, :] * stride_states_f_hdim + offs_k_dstate[:, None] * stride_states_f_dstate)\n    prev_states_b_ptrs = prev_states_b_ptr + (offs_n[None, :] * stride_states_b_hdim + offs_k_dstate[:, None] * stride_states_b_dstate)\n    scale_f_m = tl.exp(dA_cs_f_m)\n    scale_b_m = tl.exp(dA_cs_b_m)\n    if BLOCK_SIZE_DSTATE <= 128:\n        C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k_dstate[None, :] < dstate), other=0.0)\n        prev_states_f = tl.load(prev_states_f_ptrs, mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n        prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)\n        acc = tl.dot(C, prev_states_f) * scale_f_m[:, None]\n        prev_states_b = tl.load(prev_states_b_ptrs, mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n        prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)\n        acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]\n    else:\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k_dstate[None, :] < dstate - k), other=0.0)\n            prev_states_f = tl.load(prev_states_f_ptrs, mask=(offs_k_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n            prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)\n            acc += tl.dot(C, prev_states_f) * scale_f_m[:, None]\n            prev_states_b = tl.load(prev_states_f_ptrs, mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n            prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)\n            acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]\n            C_ptrs += BLOCK_SIZE_K\n            prev_states_f_ptrs += BLOCK_SIZE_K\n            prev_states_b_ptrs += BLOCK_SIZE_K\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k)\nx_ptrs = x_ptr + (offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_k * stride_dt_csize\ndA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\ndA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\nK_F_MAX = min(pid_m * BLOCK_SIZE_M, chunk_size_limit)\nK_F_MAX_BEG = K_F_MAX - BLOCK_SIZE_K\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k), other=0.0).to(tl.float32)\n    if k <= K_F_MAX and k >= K_F_MAX_BEG:\n        dA_cs_f_k = tl.load(dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask_f = offs_m[:, None] >= k + offs_k[None, :]\n        scale_f = tl.where(mask_f, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_k[None, :]), 0.0)\n        dA_cs_b_k = tl.load(dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask_b = offs_m[:, None] <= k + offs_k[None, :]\n        scale_b = tl.where(mask_b, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_k[None, :]), 0.0)\n        cb = cb * (scale_f + scale_b)\n    elif k < K_F_MAX:\n        dA_cs_f_k = tl.load(dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask = offs_m[:, None] >= k + offs_k[None, :]\n        cb *= tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_k[None, :]), 0.0)\n    elif k > K_F_MAX:\n        dA_cs_b_k = tl.load(dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask = offs_m[:, None] <= k + offs_k[None, :]\n        cb *= tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_k[None, :]), 0.0)\n    dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n    cb *= dt_k\n    cb = cb.to(x_ptr.dtype.element_ty)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim), other=0.0)\n    acc += tl.dot(cb, x)\n    cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n    x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n    dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n    dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\noffs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_D:\n    if D_HAS_HDIM:\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    x_residual = tl.load(x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim), mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    acc += x_residual * D\nif HAS_Z:\n    out_x_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\n    out_x_ptrs = out_x_ptr + (stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :])\n    tl.store(out_x_ptrs, acc, mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim))\n    z_ptr += pid_b * stride_z_batch + pid_c * chunk_size * stride_z_seqlen + pid_h * stride_z_head\n    z_ptrs = z_ptr + (stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :])\n    z = tl.load(z_ptrs, mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim), other=0.0).to(tl.float32)\n    acc *= z * tl.sigmoid(z)\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\nout_ptrs = out_ptr + (stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim)\ntl.store(out_ptrs, acc, mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim))"
  },
  {
    "name": "_chunk_scan_fwd_kernel_wip",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_seqlen",
        "annotation": null
      },
      {
        "name": "stride_B_head",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel_wip(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    B_ptr,",
      "    prev_states_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_B_batch,",
      "    stride_B_seqlen,",
      "    stride_B_head,",
      "    stride_B_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_D_head,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_n = tl.program_id(axis=0)",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    B_ptr += (",
      "        pid_b * stride_B_batch",
      "        + pid_c * chunk_size * stride_B_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_B_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "",
      "    offs_m = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "    )",
      "    B_ptrs = B_ptr + (",
      "        offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_states_hdim",
      "        + offs_k_dstate[:, None] * stride_states_dstate",
      "    )",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)",
      "        dA_cs_m = tl.load(",
      "            dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize,",
      "            mask=offs_m < chunk_size - start_m,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_prev = tl.load(",
      "                seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "            )",
      "            seq_idx_m = tl.load(",
      "                seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen,",
      "                mask=offs_m < chunk_size_limit - start_m,",
      "                other=-1,",
      "            )",
      "        if not HAS_SEQ_IDX:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        else:",
      "            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        C = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_k_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]",
      "",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(",
      "            tl.float32",
      "        )",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        if HAS_D:",
      "            if D_HAS_HDIM:",
      "                D = tl.load(",
      "                    D_ptr + pid_h * stride_D_head + offs_n,",
      "                    mask=offs_n < hdim,",
      "                    other=0.0,",
      "                ).to(tl.float32)",
      "            else:",
      "                D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "            acc += x.to(tl.float32) * D",
      "",
      "        tl.store(",
      "            out_ptrs,",
      "            acc,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "        )",
      "",
      "        if start_m + BLOCK_SIZE_M < chunk_size_limit:",
      "",
      "            B = tl.load(",
      "                B_ptrs,",
      "                mask=(offs_m[None, :] < chunk_size_limit - start_m)",
      "                & (offs_k_dstate[:, None] < dstate),",
      "                other=0.0,",
      "            )",
      "            dA_cs_last = tl.load(",
      "                dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize",
      "            ).to(tl.float32)",
      "",
      "            scale = tl.exp((dA_cs_last - dA_cs_m)) * dt_m",
      "",
      "            B = B.to(x_ptr.dtype.element_ty)",
      "            tmp = tl.dot(B, x)",
      "            prev_states += tmp.to(prev_states.dtype)",
      "",
      "        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen",
      "        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen",
      "        cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize",
      "        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_fwd_kernel_wip(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, C_ptr, B_ptr, prev_states_ptr, D_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_B_batch, stride_B_seqlen, stride_B_head, stride_B_dstate, stride_states_batch, stride_states_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_D_head, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_Z: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_n = tl.program_id(axis=0)\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nC_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_h // nheads_ngroups_ratio * stride_C_head\nB_ptr += pid_b * stride_B_batch + pid_c * chunk_size * stride_B_seqlen + pid_h // nheads_ngroups_ratio * stride_B_head\nprev_states_ptr += pid_b * stride_states_batch + pid_c * stride_states_chunk + pid_h * stride_states_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\noffs_m = tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)\nC_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate)\nB_ptrs = B_ptr + (offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate)\nprev_states_ptrs = prev_states_ptr + (offs_n[None, :] * stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\nout_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim)\nprev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nfor start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):\n    start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)\n    dA_cs_m = tl.load(dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize, mask=offs_m < chunk_size - start_m, other=0.0).to(tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit - start_m, other=-1)\n    if not HAS_SEQ_IDX:\n        scale_m = tl.exp(dA_cs_m)\n    else:\n        scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit - start_m) & (offs_k_dstate[None, :] < dstate), other=0.0)\n    acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(tl.float32)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit - start_m) & (offs_n[None, :] < hdim), other=0.0)\n    if HAS_D:\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        acc += x.to(tl.float32) * D\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit - start_m) & (offs_n[None, :] < hdim))\n    if start_m + BLOCK_SIZE_M < chunk_size_limit:\n        B = tl.load(B_ptrs, mask=(offs_m[None, :] < chunk_size_limit - start_m) & (offs_k_dstate[:, None] < dstate), other=0.0)\n        dA_cs_last = tl.load(dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize).to(tl.float32)\n        scale = tl.exp(dA_cs_last - dA_cs_m) * dt_m\n        B = B.to(x_ptr.dtype.element_ty)\n        tmp = tl.dot(B, x)\n        prev_states += tmp.to(prev_states.dtype)\n    C_ptrs += BLOCK_SIZE_M * stride_C_seqlen\n    B_ptrs += BLOCK_SIZE_M * stride_B_seqlen\n    cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k\n    x_ptrs += BLOCK_SIZE_M * stride_x_seqlen\n    dt_ptrs += BLOCK_SIZE_M * stride_dt_csize\n    out_ptrs += BLOCK_SIZE_M * stride_out_seqlen"
  },
  {
    "name": "_chunk_scan_bwd_dz_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "outz_ptr",
        "annotation": null
      },
      {
        "name": "dz_ptr",
        "annotation": null
      },
      {
        "name": "dout_x_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_outz_batch",
        "annotation": null
      },
      {
        "name": "stride_outz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_outz_head",
        "annotation": null
      },
      {
        "name": "stride_outz_hdim",
        "annotation": null
      },
      {
        "name": "stride_dz_batch",
        "annotation": null
      },
      {
        "name": "stride_dz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dz_head",
        "annotation": null
      },
      {
        "name": "stride_dz_hdim",
        "annotation": null
      },
      {
        "name": "stride_doutx_batch",
        "annotation": null
      },
      {
        "name": "stride_doutx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_doutx_head",
        "annotation": null
      },
      {
        "name": "stride_doutx_hdim",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DDACS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dz_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    z_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    outz_ptr,",
      "    dz_ptr,",
      "    dout_x_ptr,",
      "    dD_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_outz_batch,",
      "    stride_outz_seqlen,",
      "    stride_outz_head,",
      "    stride_outz_hdim,",
      "    stride_dz_batch,",
      "    stride_dz_seqlen,",
      "    stride_dz_head,",
      "    stride_dz_hdim,",
      "    stride_doutx_batch,",
      "    stride_doutx_seqlen,",
      "    stride_doutx_head,",
      "    stride_doutx_hdim,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_DDACS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dout_x_ptr += (",
      "        pid_b * stride_doutx_batch",
      "        + pid_c * chunk_size * stride_doutx_seqlen",
      "        + pid_h * stride_doutx_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    z_ptr += (",
      "        pid_b * stride_z_batch",
      "        + pid_c * chunk_size * stride_z_seqlen",
      "        + pid_h * stride_z_head",
      "    )",
      "    dz_ptr += (",
      "        pid_b * stride_dz_batch",
      "        + pid_c * chunk_size * stride_dz_seqlen",
      "        + pid_h * stride_dz_head",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptr += (",
      "            pid_b * stride_outz_batch",
      "            + pid_c * chunk_size * stride_outz_seqlen",
      "            + pid_h * stride_outz_head",
      "        )",
      "    if HAS_DDACS:",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + pid_h * stride_ddA_cs_head",
      "        )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dout_x_ptrs = dout_x_ptr + (",
      "        offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    z_ptrs = z_ptr + (",
      "        offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim",
      "    )",
      "    dz_ptrs = dz_ptr + (",
      "        offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptrs = outz_ptr + (",
      "            offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim",
      "        )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z = tl.load(",
      "        z_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z_sigmoid = tl.sigmoid(z)",
      "    if RECOMPUTE_OUTPUT:",
      "        outz = out * z * z_sigmoid",
      "        tl.store(",
      "            outz_ptrs,",
      "            outz,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        )",
      "    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "    tl.store(",
      "        dz_ptrs,",
      "        dz,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    dout *= z * z_sigmoid",
      "    tl.store(",
      "        dout_x_ptrs,",
      "        dout,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    if HAS_DDACS:",
      "        ddA_cs = tl.sum(dout * out, axis=1)",
      "        tl.store(",
      "            ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize,",
      "            ddA_cs,",
      "            mask=offs_m < chunk_size,",
      "        )"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_dz_kernel(dout_ptr, out_ptr, z_ptr, x_ptr, D_ptr, outz_ptr, dz_ptr, dout_x_ptr, dD_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_D_head, stride_outz_batch, stride_outz_seqlen, stride_outz_head, stride_outz_hdim, stride_dz_batch, stride_dz_seqlen, stride_dz_head, stride_dz_hdim, stride_doutx_batch, stride_doutx_seqlen, stride_doutx_head, stride_doutx_hdim, stride_dD_batch, stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_DDACS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndout_x_ptr += pid_b * stride_doutx_batch + pid_c * chunk_size * stride_doutx_seqlen + pid_h * stride_doutx_head\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\nz_ptr += pid_b * stride_z_batch + pid_c * chunk_size * stride_z_seqlen + pid_h * stride_z_head\ndz_ptr += pid_b * stride_dz_batch + pid_c * chunk_size * stride_dz_seqlen + pid_h * stride_dz_head\nif RECOMPUTE_OUTPUT:\n    outz_ptr += pid_b * stride_outz_batch + pid_c * chunk_size * stride_outz_seqlen + pid_h * stride_outz_head\nif HAS_DDACS:\n    ddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\nif HAS_D:\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dD_ptr += pid_b * stride_dD_batch + pid_c * stride_dD_chunk + pid_h * stride_dD_head + pid_m * stride_dD_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\ndout_x_ptrs = dout_x_ptr + (offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim)\nout_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim)\nz_ptrs = z_ptr + (offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim)\ndz_ptrs = dz_ptr + (offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim)\nif RECOMPUTE_OUTPUT:\n    outz_ptrs = outz_ptr + (offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim)\nif HAS_D:\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\n    if D_HAS_HDIM:\n        dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nout = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nz = tl.load(z_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nz_sigmoid = tl.sigmoid(z)\nif RECOMPUTE_OUTPUT:\n    outz = out * z * z_sigmoid\n    tl.store(outz_ptrs, outz, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\ndz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))\ntl.store(dz_ptrs, dz, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\ndout *= z * z_sigmoid\ntl.store(dout_x_ptrs, dout, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\nif HAS_D:\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        dD = tl.sum(dout * x, axis=0)\n        tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        dD = tl.sum(dout * x)\n        tl.store(dD_ptr, dD)\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    out -= x * D\nif HAS_DDACS:\n    ddA_cs = tl.sum(dout * out, axis=1)\n    tl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_bwd_dstates_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "dprev_states_f_ptr",
        "annotation": null
      },
      {
        "name": "dprev_states_b_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_c_batch",
        "annotation": null
      },
      {
        "name": "stride_c_seqlen",
        "annotation": null
      },
      {
        "name": "stride_c_head",
        "annotation": null
      },
      {
        "name": "stride_c_dstate",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dstates_kernel(",
      "    dout_ptr,",
      "    c_ptr,",
      "    dprev_states_f_ptr,",
      "    dprev_states_b_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_c_batch,",
      "    stride_c_seqlen,",
      "    stride_c_head,",
      "    stride_c_dstate,",
      "    stride_dprev_states_f_batch,",
      "    stride_dprev_states_f_chunk,",
      "    stride_dprev_states_f_head,",
      "    stride_dprev_states_f_hdim,",
      "    stride_dprev_states_f_dstate,",
      "    stride_dprev_states_b_batch,",
      "    stride_dprev_states_b_chunk,",
      "    stride_dprev_states_b_head,",
      "    stride_dprev_states_b_hdim,",
      "    stride_dprev_states_b_dstate,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    c_ptr += (",
      "        pid_b * stride_c_batch",
      "        + pid_c * chunk_size * stride_c_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_c_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen",
      "    )",
      "    c_ptrs = c_ptr + (",
      "        offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen",
      "    )",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    acc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_f_k = tl.load(",
      "            dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "        ).to(tl.float32)",
      "        dA_cs_b_k = tl.load(",
      "            dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "        ).to(tl.float32)",
      "        scale_f_k = tl.exp(dA_cs_f_k)",
      "        scale_b_k = tl.exp(dA_cs_b_k)",
      "        dout_f = (dout * scale_f_k).to(dout_ptr.dtype.element_ty)",
      "        dout_b = (dout * scale_b_k).to(dout_ptr.dtype.element_ty)",
      "        c = tl.load(",
      "            c_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc_f += tl.dot(dout_f, c)",
      "        acc_b += tl.dot(dout_b, c)",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "    out_f = acc_f.to(dprev_states_f_ptr.dtype.element_ty)",
      "    out_b = acc_b.to(dprev_states_b_ptr.dtype.element_ty)",
      "",
      "    dprev_states_f_ptr += (",
      "        pid_b * stride_dprev_states_f_batch",
      "        + pid_c * stride_dprev_states_f_chunk",
      "        + pid_h * stride_dprev_states_f_head",
      "    )",
      "    dprev_states_b_ptr += (",
      "        pid_b * stride_dprev_states_b_batch",
      "        + pid_c * stride_dprev_states_b_chunk",
      "        + pid_h * stride_dprev_states_b_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dprev_states_f_ptrs = dprev_states_f_ptr + (",
      "        offs_m[:, None] * stride_dprev_states_f_hdim",
      "        + offs_n[None, :] * stride_dprev_states_f_dstate",
      "    )",
      "    dprev_states_b_ptrs = dprev_states_b_ptr + (",
      "        offs_m[:, None] * stride_dprev_states_b_hdim",
      "        + offs_n[None, :] * stride_dprev_states_b_dstate",
      "    )",
      "    tl.store(",
      "        dprev_states_f_ptrs,",
      "        out_f,",
      "        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "    )",
      "    tl.store(",
      "        dprev_states_b_ptrs,",
      "        out_b,",
      "        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_dstates_kernel(dout_ptr, c_ptr, dprev_states_f_ptr, dprev_states_b_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, hdim, dstate, chunk_size, batch, seqlen, nchunks, nheads_ngroups_ratio, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_c_batch, stride_c_seqlen, stride_c_head, stride_c_dstate, stride_dprev_states_f_batch, stride_dprev_states_f_chunk, stride_dprev_states_f_head, stride_dprev_states_f_hdim, stride_dprev_states_f_dstate, stride_dprev_states_b_batch, stride_dprev_states_b_chunk, stride_dprev_states_b_head, stride_dprev_states_b_hdim, stride_dprev_states_b_dstate, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nc_ptr += pid_b * stride_c_batch + pid_c * chunk_size * stride_c_seqlen + pid_h // nheads_ngroups_ratio * stride_c_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + pid_h * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + pid_h * stride_dA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen)\nc_ptrs = c_ptr + (offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen)\ndA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\ndA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nacc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k), other=0.0).to(tl.float32)\n    dA_cs_f_k = tl.load(dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n    dA_cs_b_k = tl.load(dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n    scale_f_k = tl.exp(dA_cs_f_k)\n    scale_b_k = tl.exp(dA_cs_b_k)\n    dout_f = (dout * scale_f_k).to(dout_ptr.dtype.element_ty)\n    dout_b = (dout * scale_b_k).to(dout_ptr.dtype.element_ty)\n    c = tl.load(c_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate), other=0.0)\n    acc_f += tl.dot(dout_f, c)\n    acc_b += tl.dot(dout_b, c)\n    dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n    c_ptrs += BLOCK_SIZE_K * stride_c_seqlen\n    dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\nout_f = acc_f.to(dprev_states_f_ptr.dtype.element_ty)\nout_b = acc_b.to(dprev_states_b_ptr.dtype.element_ty)\ndprev_states_f_ptr += pid_b * stride_dprev_states_f_batch + pid_c * stride_dprev_states_f_chunk + pid_h * stride_dprev_states_f_head\ndprev_states_b_ptr += pid_b * stride_dprev_states_b_batch + pid_c * stride_dprev_states_b_chunk + pid_h * stride_dprev_states_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndprev_states_f_ptrs = dprev_states_f_ptr + (offs_m[:, None] * stride_dprev_states_f_hdim + offs_n[None, :] * stride_dprev_states_f_dstate)\ndprev_states_b_ptrs = dprev_states_b_ptr + (offs_m[:, None] * stride_dprev_states_b_hdim + offs_n[None, :] * stride_dprev_states_b_dstate)\ntl.store(dprev_states_f_ptrs, out_f, mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate))\ntl.store(dprev_states_b_ptrs, out_b, mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate))"
  },
  {
    "name": "_chunk_scan_bwd_dc_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_f_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_b_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "dc_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_dc_batch",
        "annotation": null
      },
      {
        "name": "stride_dc_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dc_split",
        "annotation": null
      },
      {
        "name": "stride_dc_group",
        "annotation": null
      },
      {
        "name": "stride_dc_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dc_kernel(",
      "    dout_ptr,",
      "    prev_states_f_ptr,",
      "    prev_states_b_ptr,",
      "    C_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    dc_ptr,",
      "    ddA_cumsum_f_ptr,",
      "    ddA_cumsum_b_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_f_batch,",
      "    stride_prev_states_f_chunk,",
      "    stride_prev_states_f_head,",
      "    stride_prev_states_f_hdim,",
      "    stride_prev_states_f_dstate,",
      "    stride_prev_states_b_batch,",
      "    stride_prev_states_b_chunk,",
      "    stride_prev_states_b_head,",
      "    stride_prev_states_b_hdim,",
      "    stride_prev_states_b_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_dc_batch,",
      "    stride_dc_seqlen,",
      "    stride_dc_split,",
      "    stride_dc_group,",
      "    stride_dc_dstate,",
      "    stride_ddA_cs_f_batch,",
      "    stride_ddA_cs_f_chunk,",
      "    stride_ddA_cs_f_head,",
      "    stride_ddA_cs_f_csize,",
      "    stride_ddA_cs_b_batch,",
      "    stride_ddA_cs_b_chunk,",
      "    stride_ddA_cs_b_head,",
      "    stride_ddA_cs_b_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dc_ptr += (",
      "        pid_b * stride_dc_batch",
      "        + pid_c * chunk_size * stride_dc_seqlen",
      "        + pid_g * stride_dc_group",
      "        + pid_s * stride_dc_split",
      "    )",
      "    prev_states_f_ptr += (",
      "        pid_b * stride_prev_states_f_batch",
      "        + pid_c * stride_prev_states_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_prev_states_f_head",
      "    )",
      "    prev_states_b_ptr += (",
      "        pid_b * stride_prev_states_b_batch",
      "        + pid_c * stride_prev_states_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_prev_states_b_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_b_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        C_ptr += (",
      "            pid_b * stride_C_batch",
      "            + pid_c * chunk_size * stride_C_seqlen",
      "            + pid_g * stride_C_head",
      "        )",
      "        ddA_cumsum_f_ptr += (",
      "            pid_b * stride_ddA_cs_f_batch",
      "            + pid_c * stride_ddA_cs_f_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_f_head",
      "        )",
      "        ddA_cumsum_b_ptr += (",
      "            pid_b * stride_ddA_cs_b_batch",
      "            + pid_c * stride_ddA_cs_b_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_b_head",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_f_ptrs = prev_states_f_ptr + (",
      "        offs_n[None, :] * stride_prev_states_f_dstate",
      "        + offs_k[:, None] * stride_prev_states_f_hdim",
      "    )",
      "    prev_states_b_ptrs = prev_states_b_ptr + (",
      "        offs_n[None, :] * stride_prev_states_b_dstate",
      "        + offs_k[:, None] * stride_prev_states_b_hdim",
      "    )",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize",
      "    if HAS_DDA_CS:",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "        )",
      "        ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize",
      "        ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        c = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        prev_states_f = tl.load(",
      "            prev_states_f_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        prev_states_f = prev_states_f.to(dout_ptrs.dtype.element_ty)",
      "        dc_f = tl.dot(dout, prev_states_f)",
      "        dA_cs_f_m = tl.load(",
      "            dA_cumsum_f_ptrs, mask=offs_m < chunk_size_limit, other=0.0",
      "        ).to(tl.float32)",
      "        scale_f = tl.exp(dA_cs_f_m)",
      "        dc_f *= scale_f[:, None]",
      "        if HAS_DDA_CS:",
      "            ddA_cs_f = tl.sum(dc_f * c, axis=1)",
      "            tl.atomic_add(ddA_cumsum_f_ptrs, ddA_cs_f, mask=offs_m < chunk_size)",
      "",
      "        prev_states_b = tl.load(",
      "            prev_states_b_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        prev_states_b = prev_states_b.to(dout_ptrs.dtype.element_ty)",
      "        dc_b = tl.dot(dout, prev_states_b)",
      "        dA_cs_b_m = tl.load(",
      "            dA_cumsum_b_ptrs, mask=offs_m < chunk_size_limit, other=0.0",
      "        ).to(tl.float32)",
      "        scale_b = tl.exp(dA_cs_b_m)",
      "        dc_b *= scale_b[:, None]",
      "        if HAS_DDA_CS:",
      "            ddA_cs_b = tl.sum(dc_b * c, axis=1)",
      "            tl.atomic_add(ddA_cumsum_b_ptrs, ddA_cs_b, mask=offs_m < chunk_size)",
      "",
      "        acc += dc_f + dc_b",
      "        dout_ptrs += stride_dout_head",
      "        prev_states_f_ptrs += stride_prev_states_f_head",
      "        prev_states_b_ptrs += stride_prev_states_b_head",
      "        dA_cumsum_f_ptrs += stride_dA_cs_f_head",
      "        dA_cumsum_b_ptrs += stride_dA_cs_b_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_f_ptrs += stride_ddA_cs_f_head",
      "            ddA_cumsum_b_ptrs += stride_ddA_cs_b_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dc_ptrs = dc_ptr + (",
      "        offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate",
      "    )",
      "    tl.store(",
      "        dc_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_dc_kernel(dout_ptr, prev_states_f_ptr, prev_states_b_ptr, C_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, dc_ptr, ddA_cumsum_f_ptr, ddA_cumsum_b_ptr, chunk_size, dstate, hdim, batch, seqlen, nheads, nheads_per_program, ngroups, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_prev_states_f_batch, stride_prev_states_f_chunk, stride_prev_states_f_head, stride_prev_states_f_hdim, stride_prev_states_f_dstate, stride_prev_states_b_batch, stride_prev_states_b_chunk, stride_prev_states_b_head, stride_prev_states_b_hdim, stride_prev_states_b_dstate, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, stride_dc_batch, stride_dc_seqlen, stride_dc_split, stride_dc_group, stride_dc_dstate, stride_ddA_cs_f_batch, stride_ddA_cs_f_chunk, stride_ddA_cs_f_head, stride_ddA_cs_f_csize, stride_ddA_cs_b_batch, stride_ddA_cs_b_chunk, stride_ddA_cs_b_head, stride_ddA_cs_b_csize, HAS_DDA_CS: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_sg = tl.program_id(axis=2)\npid_s = pid_sg // ngroups\npid_g = pid_sg - pid_s * ngroups\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\ndc_ptr += pid_b * stride_dc_batch + pid_c * chunk_size * stride_dc_seqlen + pid_g * stride_dc_group + pid_s * stride_dc_split\nprev_states_f_ptr += pid_b * stride_prev_states_f_batch + pid_c * stride_prev_states_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_prev_states_f_head\nprev_states_b_ptr += pid_b * stride_prev_states_b_batch + pid_c * stride_prev_states_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_prev_states_b_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_b_head\nif HAS_DDA_CS:\n    C_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_g * stride_C_head\n    ddA_cumsum_f_ptr += pid_b * stride_ddA_cs_f_batch + pid_c * stride_ddA_cs_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_f_head\n    ddA_cumsum_b_ptr += pid_b * stride_ddA_cs_b_batch + pid_c * stride_ddA_cs_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nprev_states_f_ptrs = prev_states_f_ptr + (offs_n[None, :] * stride_prev_states_f_dstate + offs_k[:, None] * stride_prev_states_f_hdim)\nprev_states_b_ptrs = prev_states_b_ptr + (offs_n[None, :] * stride_prev_states_b_dstate + offs_k[:, None] * stride_prev_states_b_hdim)\ndA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize\ndA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize\nif HAS_DDA_CS:\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate)\n    ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize\n    ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif HAS_DDA_CS:\n    c = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\nnheads_iter = min(nheads_per_program, nheads // ngroups - pid_s * nheads_per_program)\nfor h in range(nheads_iter):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\n    prev_states_f = tl.load(prev_states_f_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\n    prev_states_f = prev_states_f.to(dout_ptrs.dtype.element_ty)\n    dc_f = tl.dot(dout, prev_states_f)\n    dA_cs_f_m = tl.load(dA_cumsum_f_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    scale_f = tl.exp(dA_cs_f_m)\n    dc_f *= scale_f[:, None]\n    if HAS_DDA_CS:\n        ddA_cs_f = tl.sum(dc_f * c, axis=1)\n        tl.atomic_add(ddA_cumsum_f_ptrs, ddA_cs_f, mask=offs_m < chunk_size)\n    prev_states_b = tl.load(prev_states_b_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    prev_states_b = prev_states_b.to(dout_ptrs.dtype.element_ty)\n    dc_b = tl.dot(dout, prev_states_b)\n    dA_cs_b_m = tl.load(dA_cumsum_b_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    scale_b = tl.exp(dA_cs_b_m)\n    dc_b *= scale_b[:, None]\n    if HAS_DDA_CS:\n        ddA_cs_b = tl.sum(dc_b * c, axis=1)\n        tl.atomic_add(ddA_cumsum_b_ptrs, ddA_cs_b, mask=offs_m < chunk_size)\n    acc += dc_f + dc_b\n    dout_ptrs += stride_dout_head\n    prev_states_f_ptrs += stride_prev_states_f_head\n    prev_states_b_ptrs += stride_prev_states_b_head\n    dA_cumsum_f_ptrs += stride_dA_cs_f_head\n    dA_cumsum_b_ptrs += stride_dA_cs_b_head\n    if HAS_DDA_CS:\n        ddA_cumsum_f_ptrs += stride_ddA_cs_f_head\n        ddA_cumsum_b_ptrs += stride_ddA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndc_ptrs = dc_ptr + (offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate)\ntl.store(dc_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate))"
  },
  {
    "name": "_chunk_scan_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    D_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_D_head,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    K_MAX = chunk_size_limit",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])",
      "",
      "        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)",
      "        cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, D_ptr, dx_ptr, ddt_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_D_head, stride_dx_batch, stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k)\ndout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nK_MAX = chunk_size_limit\nfor k in range(0, K_MAX, BLOCK_SIZE_K):\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k), other=0.0)\n    dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim), other=0.0)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(tl.float32)\n    cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n    mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n    cb = tl.where(mask, cb, 0.0)\n    cb = cb.to(dout_ptr.dtype.element_ty)\n    acc += tl.dot(cb, dout)\n    cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n    dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndx = acc * dt_m[:, None]\ndx_ptr += pid_b * stride_dx_batch + pid_c * chunk_size * stride_dx_seqlen + pid_h * stride_dx_head\ndx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim)\nif HAS_D:\n    dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\n    dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    dx += dout_res * D\ntl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nddt = tl.sum(acc * x, axis=1)\nddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\ntl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_bwd_dcb_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "dcb_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_dcb_batch",
        "annotation": null
      },
      {
        "name": "stride_dcb_chunk",
        "annotation": null
      },
      {
        "name": "stride_dcb_split",
        "annotation": null
      },
      {
        "name": "stride_dcb_group",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dcb_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    dcb_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_dcb_batch,",
      "    stride_dcb_chunk,",
      "    stride_dcb_split,",
      "    stride_dcb_group,",
      "    stride_dcb_csize_m,",
      "    stride_dcb_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        )",
      "        dcb = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "        dcb *= dt_n",
      "        if (pid_n * BLOCK_SIZE_N < (pid_m + 1) * BLOCK_SIZE_M) and (",
      "            (pid_n + 1) * BLOCK_SIZE_N > (pid_m) * BLOCK_SIZE_M",
      "        ):",
      "",
      "            dA_cs_f_m = tl.load(",
      "                dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_f_n = tl.load(",
      "                dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] >= offs_n[None, :]",
      "            A_f = tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)",
      "            dA_cs_b_m = tl.load(",
      "                dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_b_n = tl.load(",
      "                dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] <= offs_n[None, :]",
      "            A_b = tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)",
      "            dcb *= A_f + A_b",
      "        elif pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "",
      "            dA_cs_b_m = tl.load(",
      "                dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_b_n = tl.load(",
      "                dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] <= offs_n[None, :]",
      "            dcb *= tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)",
      "        else:",
      "",
      "            dA_cs_f_m = tl.load(",
      "                dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_f_n = tl.load(",
      "                dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] >= offs_n[None, :]",
      "            dcb *= tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)",
      "        acc += dcb",
      "        dout_ptrs += stride_dout_head",
      "        x_ptrs += stride_x_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_f_ptr += stride_dA_cs_f_head",
      "        dA_cumsum_b_ptr += stride_dA_cs_b_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dcb_ptr += (",
      "        pid_b * stride_dcb_batch",
      "        + pid_c * stride_dcb_chunk",
      "        + pid_g * stride_dcb_group",
      "        + pid_s * stride_dcb_split",
      "    )",
      "    dcb_ptrs = dcb_ptr + (",
      "        offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n",
      "    )",
      "    tl.store(",
      "        dcb_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_dcb_kernel(x_ptr, dout_ptr, dt_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, dcb_ptr, chunk_size, hdim, batch, seqlen, nheads, nheads_per_program, ngroups, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, stride_dcb_batch, stride_dcb_chunk, stride_dcb_split, stride_dcb_group, stride_dcb_csize_m, stride_dcb_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_sg = tl.program_id(axis=2)\npid_s = pid_sg // ngroups\npid_g = pid_sg - pid_s * ngroups\nnum_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_n * stride_dt_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nnheads_iter = min(nheads_per_program, nheads // ngroups - pid_s * nheads_per_program)\nfor h in range(nheads_iter):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit), other=0.0)\n    dcb = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\n    dcb *= dt_n\n    if pid_n * BLOCK_SIZE_N < (pid_m + 1) * BLOCK_SIZE_M and (pid_n + 1) * BLOCK_SIZE_N > pid_m * BLOCK_SIZE_M:\n        dA_cs_f_m = tl.load(dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n        dA_cs_f_n = tl.load(dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize, mask=offs_n < chunk_size_limit, other=0.0).to(tl.float32)\n        mask = offs_m[:, None] >= offs_n[None, :]\n        A_f = tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)\n        dA_cs_b_m = tl.load(dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n        dA_cs_b_n = tl.load(dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize, mask=offs_n < chunk_size_limit, other=0.0).to(tl.float32)\n        mask = offs_m[:, None] <= offs_n[None, :]\n        A_b = tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)\n        dcb *= A_f + A_b\n    elif pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n        dA_cs_b_m = tl.load(dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n        dA_cs_b_n = tl.load(dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize, mask=offs_n < chunk_size_limit, other=0.0).to(tl.float32)\n        mask = offs_m[:, None] <= offs_n[None, :]\n        dcb *= tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)\n    else:\n        dA_cs_f_m = tl.load(dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n        dA_cs_f_n = tl.load(dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize, mask=offs_n < chunk_size_limit, other=0.0).to(tl.float32)\n        mask = offs_m[:, None] >= offs_n[None, :]\n        dcb *= tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)\n    acc += dcb\n    dout_ptrs += stride_dout_head\n    x_ptrs += stride_x_head\n    dt_ptrs += stride_dt_head\n    dA_cumsum_f_ptr += stride_dA_cs_f_head\n    dA_cumsum_b_ptr += stride_dA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndcb_ptr += pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + pid_g * stride_dcb_group + pid_s * stride_dcb_split\ndcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n)\ntl.store(dcb_ptrs, acc, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_unstable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SUBTRACT_DDTDT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_unstable_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dt_ptr,",
      "    ddt_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    ddA_cumsum_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    SUBTRACT_DDTDT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    ddA_cs = tl.sum(dout * out, axis=1)",
      "    if SUBTRACT_DDTDT:",
      "        dt = tl.load(",
      "            dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddt = tl.load(",
      "            ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddA_cs -= dt * ddt",
      "    tl.store(",
      "        ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size",
      "    )"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_unstable_kernel(dout_ptr, out_ptr, dt_ptr, ddt_ptr, x_ptr, D_ptr, ddA_cumsum_ptr, dD_ptr, chunk_size, hdim, batch, seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_D_head, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, stride_dD_batch, stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, SUBTRACT_DDTDT: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\nif HAS_D:\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dD_ptr += pid_b * stride_dD_batch + pid_c * stride_dD_chunk + pid_h * stride_dD_head + pid_m * stride_dD_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\nout_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim)\nif HAS_D:\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\n    if D_HAS_HDIM:\n        dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nout = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nif HAS_D:\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        dD = tl.sum(dout * x, axis=0)\n        tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        dD = tl.sum(dout * x)\n        tl.store(dD_ptr, dD)\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    out -= x * D\nddA_cs = tl.sum(dout * out, axis=1)\nif SUBTRACT_DDTDT:\n    dt = tl.load(dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    ddt = tl.load(ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    ddA_cs -= dt * ddt\ntl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_kernel_old",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 16}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 16}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddAcs_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_head",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_kernel_old(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddAcs_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddAcs_batch,",
      "    stride_ddAcs_chunk,",
      "    stride_ddAcs_head,",
      "    stride_ddAcs_csize_m,",
      "    stride_ddAcs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)",
      "",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),",
      "        other=0.0,",
      "    )",
      "    acc = tl.dot(dout, x)",
      "    cb = tl.load(",
      "        cb_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    acc *= cb",
      "    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= dt_n",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_n = tl.load(",
      "        dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "    mask = offs_m[:, None] >= offs_n[None, :] + 1",
      "    acc = tl.where(mask, acc, 0.0)",
      "    acc = tl.cumsum(acc, axis=1)",
      "    acc = tl.where(mask, acc, 0.0)",
      "    ddA_cs = tl.sum(acc, axis=0)",
      "    ddAcs_ptr += (",
      "        pid_b * stride_ddAcs_batch",
      "        + pid_c * stride_ddAcs_chunk",
      "        + pid_h * stride_ddAcs_head",
      "        + pid_m * stride_ddAcs_csize_m",
      "    )",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n",
      "    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)",
      "    tl.store(ddAcs_ptr, 0.0)"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_kernel_old(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddAcs_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddAcs_batch, stride_ddAcs_chunk, stride_ddAcs_head, stride_ddAcs_csize_m, stride_ddAcs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_n * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nchunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\nx = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n), other=0.0)\nacc = tl.dot(dout, x)\ncb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size), other=0.0).to(tl.float32)\nacc *= cb\ndt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\nacc *= dt_n\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\nacc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\nmask = offs_m[:, None] >= offs_n[None, :] + 1\nacc = tl.where(mask, acc, 0.0)\nacc = tl.cumsum(acc, axis=1)\nacc = tl.where(mask, acc, 0.0)\nddA_cs = tl.sum(acc, axis=0)\nddAcs_ptr += pid_b * stride_ddAcs_batch + pid_c * stride_ddAcs_chunk + pid_h * stride_ddAcs_head + pid_m * stride_ddAcs_csize_m\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n\ntl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)\ntl.store(ddAcs_ptr, 0.0)"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_bwd_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    start = chunk_size - BLOCK_SIZE_N",
      "",
      "    offs_m = (chunk_size - (pid_m + 1) * BLOCK_SIZE_M) + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        (start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m",
      "        + (start + offs_n[None, :]) * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit)",
      "        & (offs_m[:, None] >= 0)",
      "        & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m >= 0, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M",
      "",
      "    for start_n in range(lo, hi, BLOCK_SIZE_N):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit + start_n - start)",
      "            & (offs_n >= start_n - start),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n >= start_n - start, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] >= 0) & (offs_n[None, :] >= start_n - start),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + (start - start_n + offs_n) * stride_dA_cs_csize,",
      "            mask=(offs_n >= start_n - start),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] <= start - start_n + offs_n[None, :] - 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=(offs_n >= 1 + start_n - start),",
      "        )",
      "        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n >= 1 + start_n - start,",
      "        )",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_bwd_kernel(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m * stride_ddA_cs_csize_m\nstart = chunk_size - BLOCK_SIZE_N\noffs_m = chunk_size - (pid_m + 1) * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + ((start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + (start + offs_n[None, :]) * stride_cb_csize_n)\nddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n\ntl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nrowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_m[:, None] >= 0) & (offs_k[None, :] < hdim), other=0.0)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m >= 0, other=0.0).to(tl.float32)\nlo, hi = (0, (pid_m + 1) * BLOCK_SIZE_M)\nfor start_n in range(lo, hi, BLOCK_SIZE_N):\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit + start_n - start) & (offs_n >= start_n - start), other=0.0)\n    acc = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n >= start_n - start, other=0.0).to(tl.float32)\n    acc *= dt_n\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] >= 0) & (offs_n[None, :] >= start_n - start), other=0.0).to(tl.float32)\n    acc *= cb\n    dA_cs_n = tl.load(dA_cumsum_ptr + (start - start_n + offs_n) * stride_dA_cs_csize, mask=offs_n >= start_n - start, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] <= start - start_n + offs_n[None, :] - 1\n    acc = tl.where(mask, acc, 0.0)\n    rowsum_new = rowsum + tl.sum(acc, axis=1)\n    acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n    rowsum = rowsum_new\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >= 1 + start_n - start)\n    x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n    dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n    cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n    ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\nfor start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n    tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32), mask=offs_n >= 1 + start_n - start)\n    ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    start = (chunk_size - 1 // BLOCK_SIZE_N) * BLOCK_SIZE_N",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        (start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m",
      "        + (start + offs_n[None, :]) * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = start, ((pid_m * BLOCK_SIZE_M) // BLOCK_SIZE_N) * BLOCK_SIZE_N - 1",
      "",
      "    for start_n in range(lo, hi, -BLOCK_SIZE_N):",
      "        tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - start_n),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_n[None, :] < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize,",
      "            mask=(offs_n < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] <= start_n + offs_n[None, :] - 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=(offs_n < chunk_size - start_n) & (offs_n >= 1 - start_n),",
      "        )",
      "        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, -1, -BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n >= 1 - start_n,",
      "        )",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m * stride_ddA_cs_csize_m\nstart = (chunk_size - 1 // BLOCK_SIZE_N) * BLOCK_SIZE_N\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + ((start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + (start + offs_n[None, :]) * stride_cb_csize_n)\nddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n\ntl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nrowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nlo, hi = (start, pid_m * BLOCK_SIZE_M // BLOCK_SIZE_N * BLOCK_SIZE_N - 1)\nfor start_n in range(lo, hi, -BLOCK_SIZE_N):\n    tl.multiple_of(start_n, BLOCK_SIZE_N)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit - start_n), other=0.0)\n    acc = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(tl.float32)\n    acc *= dt_n\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size - start_n), other=0.0).to(tl.float32)\n    acc *= cb\n    dA_cs_n = tl.load(dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] <= start_n + offs_n[None, :] - 1\n    acc = tl.where(mask, acc, 0.0)\n    rowsum_new = rowsum + tl.sum(acc, axis=1)\n    acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n    rowsum = rowsum_new\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=(offs_n < chunk_size - start_n) & (offs_n >= 1 - start_n))\n    x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n    dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n    cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n    ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\nfor start_n in range(hi, -1, -BLOCK_SIZE_N):\n    tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32), mask=offs_n >= 1 - start_n)\n    ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_bwd_old_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_bwd_old_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    end_offset = chunk_size - BLOCK_SIZE_N",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        (end_offset + offs_n[None, :]) * stride_x_seqlen",
      "        + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + (end_offset + offs_n) * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m",
      "        + (end_offset + offs_n[None, :]) * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + (end_offset + offs_n) * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    for start_n in range(0, chunk_size - pid_m * BLOCK_SIZE_M, BLOCK_SIZE_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "        offset = (chunk_size - BLOCK_SIZE_N) - start_n",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - offset)",
      "            & (offs_n[None, :] >= -offset),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n >= (-offset), other=0.0).to(tl.float32)",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] >= (-offset)),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + offset + offs_n * stride_dA_cs_csize,",
      "            mask=offs_n >= (-offset),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] <= offset + offs_n[None, :] - 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >= 1 - offset)",
      "        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(pid_m * BLOCK_SIZE_M, 0, -BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n >= 1 - start_n,",
      "        )",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_bwd_old_kernel(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m * stride_ddA_cs_csize_m\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nend_offset = chunk_size - BLOCK_SIZE_N\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + ((end_offset + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + (end_offset + offs_n) * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + (end_offset + offs_n[None, :]) * stride_cb_csize_n)\nddAcs_ptrs = ddA_cumsum_ptr + (end_offset + offs_n) * stride_ddA_cs_csize_n\ntl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nrowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nfor start_n in range(0, chunk_size - pid_m * BLOCK_SIZE_M, BLOCK_SIZE_N):\n    start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n    offset = chunk_size - BLOCK_SIZE_N - start_n\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit - offset) & (offs_n[None, :] >= -offset), other=0.0)\n    acc = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n >= -offset, other=0.0).to(tl.float32)\n    acc *= dt_n\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] >= -offset), other=0.0).to(tl.float32)\n    acc *= cb\n    dA_cs_n = tl.load(dA_cumsum_ptr + offset + offs_n * stride_dA_cs_csize, mask=offs_n >= -offset, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] <= offset + offs_n[None, :] - 1\n    acc = tl.where(mask, acc, 0.0)\n    rowsum_new = rowsum + tl.sum(acc, axis=1)\n    acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)\n    rowsum = rowsum_new\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >= 1 - offset)\n    x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen\n    dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize\n    cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n\n    ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n\nfor start_n in range(pid_m * BLOCK_SIZE_M, 0, -BLOCK_SIZE_N):\n    tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32), mask=offs_n >= 1 - start_n)\n    ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_fwd_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M",
      "",
      "    for start_n in range(lo, hi, BLOCK_SIZE_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - start_n),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_n[None, :] < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + start_n + offs_n * stride_dA_cs_csize,",
      "            mask=offs_n < chunk_size - start_n,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_fwd_kernel(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m * stride_ddA_cs_csize_m\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_n * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n)\nddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\ntl.store(ddA_cumsum_ptr, 0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nrowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nlo, hi = (0, (pid_m + 1) * BLOCK_SIZE_M)\nfor start_n in range(lo, hi, BLOCK_SIZE_N):\n    start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit - start_n), other=0.0)\n    acc = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(tl.float32)\n    acc *= dt_n\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size - start_n), other=0.0).to(tl.float32)\n    acc *= cb\n    dA_cs_n = tl.load(dA_cumsum_ptr + start_n + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n    acc = tl.where(mask, acc, 0.0)\n    rowsum_new = rowsum + tl.sum(acc, axis=1)\n    acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n    rowsum = rowsum_new\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=offs_n < chunk_size - start_n - 1)\n    x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n    dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n    cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n    ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\nfor start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n    tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32), mask=offs_n < chunk_size - start_n - 1)\n    ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_prev_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_prev_kernel(",
      "    dout_ptr,",
      "    prev_states_ptr,",
      "    C_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_batch,",
      "    stride_prev_states_chunk,",
      "    stride_prev_states_head,",
      "    stride_prev_states_hdim,",
      "    stride_prev_states_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_prev_states_batch",
      "        + pid_c * stride_prev_states_chunk",
      "        + pid_h * stride_prev_states_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_prev_states_dstate",
      "        + offs_k[:, None] * stride_prev_states_hdim",
      "    )",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    )",
      "    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)",
      "    acc = tl.dot(dout, prev_states)",
      "    c = tl.load(",
      "        C_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddA_cs = tl.sum(acc * c, axis=1)",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_m)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "    ddA_cs *= scale",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)"
    ],
    "file": "codes/119.py",
    "header": "def _chunk_scan_bwd_ddAcs_prev_kernel(dout_ptr, prev_states_ptr, C_ptr, dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, dstate, hdim, batch, seqlen, nchunks, nheads_ngroups_ratio, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_prev_states_batch, stride_prev_states_chunk, stride_prev_states_head, stride_prev_states_hdim, stride_prev_states_dstate, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\nprev_states_ptr += pid_b * stride_prev_states_batch + pid_c * stride_prev_states_chunk + pid_h * stride_prev_states_head\nC_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_h // nheads_ngroups_ratio * stride_C_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nprev_states_ptrs = prev_states_ptr + (offs_n[None, :] * stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\nC_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\nprev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\nprev_states = prev_states.to(dout_ptrs.dtype.element_ty)\nacc = tl.dot(dout, prev_states)\nc = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\nddA_cs = tl.sum(acc * c, axis=1)\ndA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\nif not HAS_SEQ_IDX:\n    scale = tl.exp(dA_cs_m)\nif HAS_SEQ_IDX:\n    seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\nddA_cs *= scale\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\ntl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)"
  },
  {
    "name": "kernel_gemm_rs_producer_fuse_scatter",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_hip_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "scatter_bufs_ptr",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M_PER_COPY_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_gemm_rs_producer_fuse_scatter(",
      "    a_ptr,",
      "    b_ptr,",
      "    scatter_bufs_ptr,",
      "    rank,",
      "    num_ranks,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    M_PER_COPY_CHUNK: tl.constexpr,",
      "):",
      "",
      "    tl.assume(stride_am > 0)",
      "    tl.assume(stride_ak > 0)",
      "    tl.assume(stride_bk > 0)",
      "    tl.assume(stride_bn > 0)",
      "    tl.assume(stride_cm > 0)",
      "    tl.assume(stride_cn > 0)",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    M_per_rank = M // num_ranks",
      "    num_pid_m_per_copy_chunk = M_PER_COPY_CHUNK // BLOCK_SIZE_M",
      "    chunk_offset = pid_m // (num_pid_m_per_copy_chunk * num_ranks)",
      "    rank_offset = (",
      "        pid_m % (num_pid_m_per_copy_chunk * num_ranks) // num_pid_m_per_copy_chunk",
      "    )",
      "    block_offset = pid_m % num_pid_m_per_copy_chunk",
      "",
      "    rank_offset = (rank_offset + rank + 1) % num_ranks",
      "    pid_m = (",
      "        rank_offset * M_per_rank",
      "        + chunk_offset * M_PER_COPY_CHUNK",
      "        + block_offset * BLOCK_SIZE_M",
      "    ) // BLOCK_SIZE_M",
      "    thread_idx = libdevice.thread_idx(axis=0)",
      "",
      "    tl.assume(pid_m >= 0)",
      "    tl.assume(pid_n >= 0)",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    dtype = a_ptr.dtype.element_ty",
      "    c = accumulator.to(dtype)",
      "",
      "    target_m = (pid_m * BLOCK_SIZE_M % M_per_rank) + M_per_rank * rank",
      "    offs_cm = target_m + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptr = tl.load(scatter_bufs_ptr + rank_offset).to(tl.pointer_type(dtype))",
      "    c_ptr = tl.multiple_of(c_ptr, 16)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/35.py",
    "header": "def kernel_gemm_rs_producer_fuse_scatter(a_ptr, b_ptr, scatter_bufs_ptr, rank, num_ranks, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, M_PER_COPY_CHUNK: tl.constexpr):",
    "body": "tl.assume(stride_am > 0)\ntl.assume(stride_ak > 0)\ntl.assume(stride_bk > 0)\ntl.assume(stride_bn > 0)\ntl.assume(stride_cm > 0)\ntl.assume(stride_cn > 0)\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\nM_per_rank = M // num_ranks\nnum_pid_m_per_copy_chunk = M_PER_COPY_CHUNK // BLOCK_SIZE_M\nchunk_offset = pid_m // (num_pid_m_per_copy_chunk * num_ranks)\nrank_offset = pid_m % (num_pid_m_per_copy_chunk * num_ranks) // num_pid_m_per_copy_chunk\nblock_offset = pid_m % num_pid_m_per_copy_chunk\nrank_offset = (rank_offset + rank + 1) % num_ranks\npid_m = (rank_offset * M_per_rank + chunk_offset * M_PER_COPY_CHUNK + block_offset * BLOCK_SIZE_M) // BLOCK_SIZE_M\nthread_idx = libdevice.thread_idx(axis=0)\ntl.assume(pid_m >= 0)\ntl.assume(pid_n >= 0)\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs)\n    b = tl.load(b_ptrs)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\ndtype = a_ptr.dtype.element_ty\nc = accumulator.to(dtype)\ntarget_m = pid_m * BLOCK_SIZE_M % M_per_rank + M_per_rank * rank\noffs_cm = target_m + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptr = tl.load(scatter_bufs_ptr + rank_offset).to(tl.pointer_type(dtype))\nc_ptr = tl.multiple_of(c_ptr, 16)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "p",
        "annotation": "tl.float32"
      },
      {
        "name": "seed",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    x_size: tl.int32,",
      "    p: tl.float32,",
      "    seed: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    x_offset = pid * x_block_size",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(0,))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    condition = tl.rand(seed, tl.arange(0, x_block_size) + x_offset) > p",
      "    output = tl.where(condition, input / (1.0 - p + language.eps), 0.0)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "codes/515.py",
    "header": "def forward(output_ptr: tl.tensor, input_ptr: tl.tensor, x_size: tl.int32, p: tl.float32, seed: tl.int32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nx_offset = pid * x_block_size\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(x_size,), strides=(1,), offsets=(x_offset,), block_shape=(x_block_size,), order=(0,))\ninput_block_ptr = tl.make_block_ptr(input_ptr, shape=(x_size,), strides=(1,), offsets=(x_offset,), block_shape=(x_block_size,), order=(0,))\nif require_x_boundary_check:\n    input = tl.load(input_block_ptr, boundary_check=(0,))\nelse:\n    input = tl.load(input_block_ptr)\ncondition = tl.rand(seed, tl.arange(0, x_block_size) + x_offset) > p\noutput = tl.where(condition, input / (1.0 - p + language.eps), 0.0)\nif require_x_boundary_check:\n    tl.store(output_block_ptr, output.to(dtype), boundary_check=(0,))\nelse:\n    tl.store(output_block_ptr, output.to(dtype))"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "p",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    x_size: tl.int32,",
      "    p: tl.float32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    x_offset = pid * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,))",
      "        output = tl.load(output_block_ptr, boundary_check=(0,))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        output = tl.load(output_block_ptr)",
      "",
      "    condition = (p == 0.0) | (output > 0.0)",
      "    grad_input = tl.where(condition, grad_output * (1.0 - p + language.eps), 0.0)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))"
    ],
    "file": "codes/515.py",
    "header": "def backward(grad_input_ptr: tl.tensor, grad_output_ptr: tl.tensor, output_ptr: tl.tensor, x_size: tl.int32, p: tl.float32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nx_offset = pid * x_block_size\ngrad_input_block_ptr = tl.make_block_ptr(grad_input_ptr, shape=(x_size,), strides=(1,), offsets=(x_offset,), block_shape=(x_block_size,), order=(0,))\ngrad_output_block_ptr = tl.make_block_ptr(grad_output_ptr, shape=(x_size,), strides=(1,), offsets=(x_offset,), block_shape=(x_block_size,), order=(0,))\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(x_size,), strides=(1,), offsets=(x_offset,), block_shape=(x_block_size,), order=(0,))\nif require_x_boundary_check:\n    grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,))\n    output = tl.load(output_block_ptr, boundary_check=(0,))\nelse:\n    grad_output = tl.load(grad_output_block_ptr)\n    output = tl.load(output_block_ptr)\ncondition = (p == 0.0) | (output > 0.0)\ngrad_input = tl.where(condition, grad_output * (1.0 - p + language.eps), 0.0)\nif require_x_boundary_check:\n    tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0,))\nelse:\n    tl.store(grad_input_block_ptr, grad_input.to(dtype))"
  },
  {
    "name": "chunk_cumprod_householder_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hc_suffix",
        "annotation": null
      },
      {
        "name": "dhc_whole",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dk_new",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_cumprod_householder_bwd_kernel(",
      "    h,",
      "    hc_suffix,",
      "    dhc_whole,",
      "    dh,",
      "    k,",
      "    dk,",
      "    dk_new,",
      "    cu_seqlens,",
      "    split_indices,",
      "    chunk_offsets,",
      "    split_offsets,",
      "    BT: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    G: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_ss, i_hq = tl.program_id(0), tl.program_id(1)",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh = i_n * tl.cdiv(T, BT)",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    h += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    k += (bos * H + i_h) * K",
      "",
      "    dh += ((boh + tl.cdiv(i_s * S, BT)) * HQ + i_hq) * K * K",
      "    dhc_whole += ((boh_large + i_s) * HQ + i_hq) * K * K",
      "    dk += (bos * HQ + i_hq) * K",
      "    dk_new += (bos * HQ + i_hq) * K",
      "",
      "    stride_hq = HQ * K * K",
      "    stride_h = H * K * K",
      "    NT_small = tl.cdiv(min(S, T - i_s * S), BT)",
      "    p_dhc_whole = tl.make_block_ptr(dhc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "    b_dhc = tl.zeros([BK, BK], dtype=tl.float32)",
      "    b_dhc += tl.load(p_dhc_whole, boundary_check=(0, 1))",
      "",
      "    for i_t_small in range(0, NT_small):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_dk_new = tl.make_block_ptr(",
      "            dk_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_hc = tl.make_block_ptr(",
      "            hc_suffix + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_t_small * stride_hq, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_hc = tl.load(p_hc, boundary_check=(0, 1))",
      "        b_dk_new = b_dk - tl.dot(b_dk.to(b_hc.dtype), b_hc)",
      "        tl.store(p_dk_new, b_dk_new.to(dk.dtype.element_ty), boundary_check=(0, 1))",
      "        b_dh = b_dhc - tl.dot(tl.trans(b_hc), b_dhc.to(b_hc.dtype))",
      "        tl.store(p_dh, b_dh.to(dh.dtype.element_ty), boundary_check=(0, 1))",
      "        b_dhc = b_dhc - tl.dot(b_dhc.to(b_h.dtype), tl.trans(b_h))",
      "        b_dhc -= tl.dot(tl.trans(b_dk).to(b_k.dtype), b_k)"
    ],
    "file": "codes/404.py",
    "header": "def chunk_cumprod_householder_bwd_kernel(h, hc_suffix, dhc_whole, dh, k, dk, dk_new, cu_seqlens, split_indices, chunk_offsets, split_offsets, BT: tl.constexpr, K: tl.constexpr, BK: tl.constexpr, T: tl.constexpr, S: tl.constexpr, G: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_ss, i_hq = (tl.program_id(0), tl.program_id(1))\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_s = (tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(split_indices + i_ss * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NS = tl.cdiv(T, S)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    boh_large = tl.load(split_offsets + i_n).to(tl.int32)\nelse:\n    NS = tl.cdiv(T, S)\n    i_n, i_s = (i_ss // NS, i_ss % NS)\n    bos, eos = (i_n * T, i_n * T + T)\n    boh = i_n * tl.cdiv(T, BT)\n    boh_large = i_n * tl.cdiv(T, S)\nh += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\nhc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\nk += (bos * H + i_h) * K\ndh += ((boh + tl.cdiv(i_s * S, BT)) * HQ + i_hq) * K * K\ndhc_whole += ((boh_large + i_s) * HQ + i_hq) * K * K\ndk += (bos * HQ + i_hq) * K\ndk_new += (bos * HQ + i_hq) * K\nstride_hq = HQ * K * K\nstride_h = H * K * K\nNT_small = tl.cdiv(min(S, T - i_s * S), BT)\np_dhc_whole = tl.make_block_ptr(dhc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\nb_dhc = tl.zeros([BK, BK], dtype=tl.float32)\nb_dhc += tl.load(p_dhc_whole, boundary_check=(0, 1))\nfor i_t_small in range(0, NT_small):\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n    p_dk_new = tl.make_block_ptr(dk_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n    p_hc = tl.make_block_ptr(hc_suffix + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    p_h = tl.make_block_ptr(h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    p_dh = tl.make_block_ptr(dh + i_t_small * stride_hq, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.load(p_dk, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_hc = tl.load(p_hc, boundary_check=(0, 1))\n    b_dk_new = b_dk - tl.dot(b_dk.to(b_hc.dtype), b_hc)\n    tl.store(p_dk_new, b_dk_new.to(dk.dtype.element_ty), boundary_check=(0, 1))\n    b_dh = b_dhc - tl.dot(tl.trans(b_hc), b_dhc.to(b_hc.dtype))\n    tl.store(p_dh, b_dh.to(dh.dtype.element_ty), boundary_check=(0, 1))\n    b_dhc = b_dhc - tl.dot(b_dhc.to(b_h.dtype), tl.trans(b_h))\n    b_dhc -= tl.dot(tl.trans(b_dk).to(b_k.dtype), b_k)"
  },
  {
    "name": "bmm_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "o_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_al",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bl",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_ol",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def bmm_kernel(",
      "    x_ptr,",
      "    y_ptr,",
      "    o_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_al,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bl,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_ol,",
      "    stride_om,",
      "    stride_on,",
      "    **meta,",
      "):",
      "    BLOCK_SIZE_M = meta[\"BLOCK_SIZE_M\"]",
      "    BLOCK_SIZE_N = meta[\"BLOCK_SIZE_N\"]",
      "    BLOCK_SIZE_K = meta[\"BLOCK_SIZE_K\"]",
      "    GROUP_SIZE_M = 8",
      "",
      "    pid_batch = tl.program_id(0)",
      "    pid = tl.program_id(1)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_am[:, None] * stride_am",
      "        + offs_k[None, :] * stride_ak",
      "        + pid_batch * stride_al",
      "    )",
      "    y_ptrs = y_ptr + (",
      "        offs_k[:, None] * stride_bk",
      "        + offs_bn[None, :] * stride_bn",
      "        + pid_batch * stride_bl",
      "    )",
      "",
      "    o = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, K, BLOCK_SIZE_K):",
      "        x = tl.load(x_ptrs)",
      "        y = tl.load(y_ptrs)",
      "        o += tl.dot(x, y)",
      "",
      "        x_ptrs += BLOCK_SIZE_K * stride_ak",
      "        y_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if exists(meta[\"ACTIVATION\"]):",
      "        o = meta[\"ACTIVATION\"](o)",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)",
      "",
      "    o_ptrs = (",
      "        o_ptr",
      "        + stride_om * offs_m[:, None]",
      "        + stride_on * offs_n[None, :]",
      "        + stride_ol * pid_batch",
      "    )",
      "    tl.store(o_ptrs, o, mask=mask)"
    ],
    "file": "codes/612.py",
    "header": "def bmm_kernel(x_ptr, y_ptr, o_ptr, M, N, K, stride_al, stride_am, stride_ak, stride_bl, stride_bk, stride_bn, stride_ol, stride_om, stride_on):",
    "body": "BLOCK_SIZE_M = meta['BLOCK_SIZE_M']\nBLOCK_SIZE_N = meta['BLOCK_SIZE_N']\nBLOCK_SIZE_K = meta['BLOCK_SIZE_K']\nGROUP_SIZE_M = 8\npid_batch = tl.program_id(0)\npid = tl.program_id(1)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak + pid_batch * stride_al)\ny_ptrs = y_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn + pid_batch * stride_bl)\no = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, K, BLOCK_SIZE_K):\n    x = tl.load(x_ptrs)\n    y = tl.load(y_ptrs)\n    o += tl.dot(x, y)\n    x_ptrs += BLOCK_SIZE_K * stride_ak\n    y_ptrs += BLOCK_SIZE_K * stride_bk\nif exists(meta['ACTIVATION']):\n    o = meta['ACTIVATION'](o)\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nmask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\no_ptrs = o_ptr + stride_om * offs_m[:, None] + stride_on * offs_n[None, :] + stride_ol * pid_batch\ntl.store(o_ptrs, o, mask=mask)"
  },
  {
    "name": "logsumexp_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SCALE': lambda args: args['scale'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SCALE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_fwd_kernel(",
      "    x, z, scale, D: tl.constexpr, B: tl.constexpr, HAS_SCALE: tl.constexpr",
      "):",
      "    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    o_d = i_d * B + tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))",
      "    if HAS_SCALE:",
      "        b_x = b_x * scale",
      "    b_m = tl.max(b_x, 0)",
      "    b_z = log(tl.sum(exp(b_x - b_m), 0)) + b_m",
      "    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)"
    ],
    "file": "codes/430.py",
    "header": "def logsumexp_fwd_kernel(x, z, scale, D: tl.constexpr, B: tl.constexpr, HAS_SCALE: tl.constexpr):",
    "body": "i_n, i_d = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64))\no_d = i_d * B + tl.arange(0, B)\nm_d = o_d < D\nb_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\nif HAS_SCALE:\n    b_x = b_x * scale\nb_m = tl.max(b_x, 0)\nb_z = log(tl.sum(exp(b_x - b_m), 0)) + b_m\ntl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)"
  },
  {
    "name": "fused_addcmul_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "hidden",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "ixr",
        "annotation": null
      },
      {
        "name": "ixw",
        "annotation": null
      },
      {
        "name": "ixk",
        "annotation": null
      },
      {
        "name": "ixv",
        "annotation": null
      },
      {
        "name": "ixa",
        "annotation": null
      },
      {
        "name": "ixg",
        "annotation": null
      },
      {
        "name": "oxr",
        "annotation": null
      },
      {
        "name": "oxw",
        "annotation": null
      },
      {
        "name": "oxk",
        "annotation": null
      },
      {
        "name": "oxv",
        "annotation": null
      },
      {
        "name": "oxa",
        "annotation": null
      },
      {
        "name": "oxg",
        "annotation": null
      },
      {
        "name": "use_xg",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_addcmul_fwd_kernel(",
      "    hidden,",
      "    delta,",
      "    ixr,",
      "    ixw,",
      "    ixk,",
      "    ixv,",
      "    ixa,",
      "    ixg,",
      "    oxr,",
      "    oxw,",
      "    oxk,",
      "    oxv,",
      "    oxa,",
      "    oxg,",
      "    use_xg: tl.constexpr,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "    xoffset = i_b * T * D + i_t * D",
      "    indices = tl.arange(0, BD)",
      "    xindex = xoffset + indices",
      "    xmask = indices < D",
      "    b_hiddn = tl.load(hidden + xindex, xmask)",
      "    b_x = tl.load(delta + xindex, xmask)",
      "    b_ixr = tl.load(ixr + indices, xmask)",
      "    b_ixw = tl.load(ixw + indices, xmask)",
      "    b_ixk = tl.load(ixk + indices, xmask)",
      "    b_ixv = tl.load(ixv + indices, xmask)",
      "    b_ixa = tl.load(ixa + indices, xmask)",
      "    b_oxr = tl.fma(b_x, b_ixr, b_hiddn)",
      "    b_oxw = tl.fma(b_x, b_ixw, b_hiddn)",
      "    b_oxk = tl.fma(b_x, b_ixk, b_hiddn)",
      "    b_oxv = tl.fma(b_x, b_ixv, b_hiddn)",
      "    b_oxa = tl.fma(b_x, b_ixa, b_hiddn)",
      "",
      "    tl.store(oxr + xindex, b_oxr.to(oxr.dtype.element_ty), xmask)",
      "    tl.store(oxw + xindex, b_oxw.to(oxw.dtype.element_ty), xmask)",
      "    tl.store(oxk + xindex, b_oxk.to(oxk.dtype.element_ty), xmask)",
      "    tl.store(oxv + xindex, b_oxv.to(oxv.dtype.element_ty), xmask)",
      "    tl.store(oxa + xindex, b_oxa.to(oxa.dtype.element_ty), xmask)",
      "",
      "    if use_xg:",
      "        b_ixg = tl.load(ixg + indices)",
      "        b_oxg = tl.fma(b_x, b_ixg, b_hiddn)",
      "        tl.store(oxg + xindex, b_oxg.to(oxg.dtype.element_ty), xmask)"
    ],
    "file": "codes/420.py",
    "header": "def fused_addcmul_fwd_kernel(hidden, delta, ixr, ixw, ixk, ixv, ixa, ixg, oxr, oxw, oxk, oxv, oxa, oxg, use_xg: tl.constexpr, T, D: tl.constexpr, BD: tl.constexpr):",
    "body": "i_b, i_t = (tl.program_id(0), tl.program_id(1))\nxoffset = i_b * T * D + i_t * D\nindices = tl.arange(0, BD)\nxindex = xoffset + indices\nxmask = indices < D\nb_hiddn = tl.load(hidden + xindex, xmask)\nb_x = tl.load(delta + xindex, xmask)\nb_ixr = tl.load(ixr + indices, xmask)\nb_ixw = tl.load(ixw + indices, xmask)\nb_ixk = tl.load(ixk + indices, xmask)\nb_ixv = tl.load(ixv + indices, xmask)\nb_ixa = tl.load(ixa + indices, xmask)\nb_oxr = tl.fma(b_x, b_ixr, b_hiddn)\nb_oxw = tl.fma(b_x, b_ixw, b_hiddn)\nb_oxk = tl.fma(b_x, b_ixk, b_hiddn)\nb_oxv = tl.fma(b_x, b_ixv, b_hiddn)\nb_oxa = tl.fma(b_x, b_ixa, b_hiddn)\ntl.store(oxr + xindex, b_oxr.to(oxr.dtype.element_ty), xmask)\ntl.store(oxw + xindex, b_oxw.to(oxw.dtype.element_ty), xmask)\ntl.store(oxk + xindex, b_oxk.to(oxk.dtype.element_ty), xmask)\ntl.store(oxv + xindex, b_oxv.to(oxv.dtype.element_ty), xmask)\ntl.store(oxa + xindex, b_oxa.to(oxa.dtype.element_ty), xmask)\nif use_xg:\n    b_ixg = tl.load(ixg + indices)\n    b_oxg = tl.fma(b_x, b_ixg, b_hiddn)\n    tl.store(oxg + xindex, b_oxg.to(oxg.dtype.element_ty), xmask)"
  },
  {
    "name": "addcmul_bwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "ixr",
        "annotation": null
      },
      {
        "name": "ixw",
        "annotation": null
      },
      {
        "name": "ixk",
        "annotation": null
      },
      {
        "name": "ixv",
        "annotation": null
      },
      {
        "name": "ixa",
        "annotation": null
      },
      {
        "name": "ixg",
        "annotation": null
      },
      {
        "name": "dxr",
        "annotation": null
      },
      {
        "name": "dxw",
        "annotation": null
      },
      {
        "name": "dxk",
        "annotation": null
      },
      {
        "name": "dxv",
        "annotation": null
      },
      {
        "name": "dxa",
        "annotation": null
      },
      {
        "name": "dxg",
        "annotation": null
      },
      {
        "name": "ghidden",
        "annotation": null
      },
      {
        "name": "gx",
        "annotation": null
      },
      {
        "name": "use_xg",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DTYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def addcmul_bwd_kernel1(",
      "    ixr,",
      "    ixw,",
      "    ixk,",
      "    ixv,",
      "    ixa,",
      "    ixg,",
      "    dxr,",
      "    dxw,",
      "    dxk,",
      "    dxv,",
      "    dxa,",
      "    dxg,",
      "    ghidden,",
      "    gx,",
      "    use_xg: tl.constexpr,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    DTYPE: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "    xoffset = i_b * T * D + i_t * D",
      "    indices = tl.arange(0, BD)",
      "    xindex = xoffset + indices",
      "    xmask = indices < D",
      "    b_dxr = tl.load(dxr + xindex, xmask).to(DTYPE)",
      "    b_dxw = tl.load(dxw + xindex, xmask).to(DTYPE)",
      "    b_dxk = tl.load(dxk + xindex, xmask).to(DTYPE)",
      "    b_dxv = tl.load(dxv + xindex, xmask).to(DTYPE)",
      "    b_dxa = tl.load(dxa + xindex, xmask).to(DTYPE)",
      "    b_ixr = tl.load(ixr + indices, xmask).to(DTYPE)",
      "    b_ixw = tl.load(ixw + indices, xmask).to(DTYPE)",
      "    b_iwk = tl.load(ixk + indices, xmask).to(DTYPE)",
      "    b_ixv = tl.load(ixv + indices, xmask).to(DTYPE)",
      "    b_ixa = tl.load(ixa + indices, xmask).to(DTYPE)",
      "",
      "    if use_xg:",
      "        b_dxg = tl.load(dxg + xindex, xmask).to(DTYPE)",
      "        b_ixg = tl.load(ixg + indices, xmask).to(DTYPE)",
      "        g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa + b_dxg",
      "        g_x = (",
      "            b_dxr * b_ixr",
      "            + b_dxw * b_ixw",
      "            + b_dxk * b_iwk",
      "            + b_dxv * b_ixv",
      "            + b_dxa * b_ixa",
      "            + b_dxg * b_ixg",
      "        )",
      "    else:",
      "        g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa",
      "        g_x = (",
      "            b_dxr * b_ixr",
      "            + b_dxw * b_ixw",
      "            + b_dxk * b_iwk",
      "            + b_dxv * b_ixv",
      "            + b_dxa * b_ixa",
      "        )",
      "",
      "    tl.store(ghidden + xindex, g_hidden.to(ghidden.dtype.element_ty), xmask)",
      "    tl.store(gx + xindex, g_x.to(gx.dtype.element_ty), xmask)"
    ],
    "file": "codes/420.py",
    "header": "def addcmul_bwd_kernel1(ixr, ixw, ixk, ixv, ixa, ixg, dxr, dxw, dxk, dxv, dxa, dxg, ghidden, gx, use_xg: tl.constexpr, T, D: tl.constexpr, BD: tl.constexpr, DTYPE: tl.constexpr):",
    "body": "i_b, i_t = (tl.program_id(0), tl.program_id(1))\nxoffset = i_b * T * D + i_t * D\nindices = tl.arange(0, BD)\nxindex = xoffset + indices\nxmask = indices < D\nb_dxr = tl.load(dxr + xindex, xmask).to(DTYPE)\nb_dxw = tl.load(dxw + xindex, xmask).to(DTYPE)\nb_dxk = tl.load(dxk + xindex, xmask).to(DTYPE)\nb_dxv = tl.load(dxv + xindex, xmask).to(DTYPE)\nb_dxa = tl.load(dxa + xindex, xmask).to(DTYPE)\nb_ixr = tl.load(ixr + indices, xmask).to(DTYPE)\nb_ixw = tl.load(ixw + indices, xmask).to(DTYPE)\nb_iwk = tl.load(ixk + indices, xmask).to(DTYPE)\nb_ixv = tl.load(ixv + indices, xmask).to(DTYPE)\nb_ixa = tl.load(ixa + indices, xmask).to(DTYPE)\nif use_xg:\n    b_dxg = tl.load(dxg + xindex, xmask).to(DTYPE)\n    b_ixg = tl.load(ixg + indices, xmask).to(DTYPE)\n    g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa + b_dxg\n    g_x = b_dxr * b_ixr + b_dxw * b_ixw + b_dxk * b_iwk + b_dxv * b_ixv + b_dxa * b_ixa + b_dxg * b_ixg\nelse:\n    g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa\n    g_x = b_dxr * b_ixr + b_dxw * b_ixw + b_dxk * b_iwk + b_dxv * b_ixv + b_dxa * b_ixa\ntl.store(ghidden + xindex, g_hidden.to(ghidden.dtype.element_ty), xmask)\ntl.store(gx + xindex, g_x.to(gx.dtype.element_ty), xmask)"
  },
  {
    "name": "parallel_rebased_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_rebased_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    z,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % (NV)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_o = tl.zeros([BTL, BV], dtype=tl.float32)",
      "    b_z = tl.zeros([BTL], dtype=tl.float32)",
      "",
      "    for _ in range(0, i_c * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, (b_k), allow_tf32=False)",
      "        b_s = b_s * b_s",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "",
      "    tl.debug_barrier()",
      "    o_q = tl.arange(0, BTL)",
      "",
      "    o_k = tl.arange(0, BTS)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "        o_k += BTS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + B * H * i_k) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_c * BTL, i_v * BV),",
      "        (BTL, BV),",
      "        (1, 0),",
      "    )",
      "    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(",
      "        p_z, b_z.to(p_z.dtype.element_ty), mask=((i_c * BTL + tl.arange(0, BTL)) < T)",
      "    )"
    ],
    "file": "codes/414.py",
    "header": "def parallel_rebased_fwd_kernel(q, k, v, o, z, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "i_kv, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNV = tl.cdiv(V, BV)\ni_k = i_kv // NV\ni_v = i_kv % NV\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nb_o = tl.zeros([BTL, BV], dtype=tl.float32)\nb_z = tl.zeros([BTL], dtype=tl.float32)\nfor _ in range(0, i_c * BTL, BTS):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = b_s * b_s\n    b_z += tl.sum(b_s, axis=1)\n    b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n    p_k = tl.advance(p_k, (0, BTS))\n    p_v = tl.advance(p_v, (BTS, 0))\ntl.debug_barrier()\no_q = tl.arange(0, BTL)\no_k = tl.arange(0, BTS)\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\nfor _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    m_s = o_q[:, None] >= o_k[None, :]\n    b_s = tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = b_s * b_s\n    b_s = tl.where(m_s, b_s, 0)\n    b_z += tl.sum(b_s, axis=1)\n    b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n    p_k = tl.advance(p_k, (0, BTS))\n    p_v = tl.advance(p_v, (BTS, 0))\n    o_k += BTS\np_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\np_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_z, b_z.to(p_z.dtype.element_ty), mask=i_c * BTL + tl.arange(0, BTL) < T)"
  },
  {
    "name": "_parallel_rebased_bwd_dq",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_bh",
        "annotation": null
      },
      {
        "name": "i_c",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "i_h",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _parallel_rebased_bwd_dq(",
      "    i_bh,",
      "    i_c,",
      "    i_k,",
      "    i_v,",
      "    i_h,",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dq,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    p_do = tl.make_block_ptr(",
      "        do + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0)",
      "    )",
      "    p_q = tl.make_block_ptr(",
      "        q + (i_bh) * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BTS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (V, T), (1, V), (i_v * BV, 0), (BV, BTS), (0, 1)",
      "    )",
      "    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)",
      "    b_dz = tl.load(p_dz, mask=(i_c * BTL + tl.arange(0, BTL)) < T)",
      "",
      "    for _ in range(0, i_c * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        if i_v == 0:",
      "            b_ds += b_dz[:, None]",
      "        else:",
      "            b_ds = b_ds",
      "        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)",
      "",
      "        b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (BTS, 0))",
      "        p_v = tl.advance(p_v, (0, BTS))",
      "",
      "    b_dq *= scale",
      "    o_q = tl.arange(0, BTL)",
      "    o_k = tl.arange(0, BTS)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1)",
      "    )",
      "",
      "    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        if i_v == 0:",
      "            b_ds += b_dz[:, None]",
      "        else:",
      "            b_ds = b_ds",
      "        b_ds = tl.where(m_s, b_ds, 0) * scale",
      "        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)",
      "        b_s = tl.where(m_s, b_s, 0)",
      "",
      "        b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype), b_k, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (BTS, 0))",
      "        p_v = tl.advance(p_v, (0, BTS))",
      "        o_k += BTS",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (i_bh + B * H * i_v) * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_c * BTL, i_k * BK),",
      "        (BTL, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    return"
    ],
    "file": "codes/414.py",
    "header": "def _parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\nb_q = (b_q * scale).to(b_q.dtype)\nb_dq = tl.zeros([BTL, BK], dtype=tl.float32)\np_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BTS, BK), (1, 0))\np_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, 0), (BV, BTS), (0, 1))\np_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)\nb_dz = tl.load(p_dz, mask=i_c * BTL + tl.arange(0, BTL) < T)\nfor _ in range(0, i_c * BTL, BTS):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n    if i_v == 0:\n        b_ds += b_dz[:, None]\n    else:\n        b_ds = b_ds\n    b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n    b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)\n    p_k = tl.advance(p_k, (BTS, 0))\n    p_v = tl.advance(p_v, (0, BTS))\nb_dq *= scale\no_q = tl.arange(0, BTL)\no_k = tl.arange(0, BTS)\np_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0))\np_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1))\nfor _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    m_s = o_q[:, None] >= o_k[None, :]\n    b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n    if i_v == 0:\n        b_ds += b_dz[:, None]\n    else:\n        b_ds = b_ds\n    b_ds = tl.where(m_s, b_ds, 0) * scale\n    b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)\n    b_s = tl.where(m_s, b_s, 0)\n    b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype), b_k, allow_tf32=False)\n    p_k = tl.advance(p_k, (BTS, 0))\n    p_v = tl.advance(p_v, (0, BTS))\n    o_k += BTS\np_dq = tl.make_block_ptr(dq + (i_bh + B * H * i_v) * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\nreturn"
  },
  {
    "name": "_parallel_rebased_bwd_dkv",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_bh",
        "annotation": null
      },
      {
        "name": "i_c",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "i_h",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _parallel_rebased_bwd_dkv(",
      "    i_bh,",
      "    i_c,",
      "    i_k,",
      "    i_v,",
      "    i_h,",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dk,",
      "    dv,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0)",
      "    )",
      "    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v, boundary_check=(0, 1))",
      "    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(",
      "        [BTL, BV], dtype=tl.float32",
      "    )",
      "",
      "    for i in range((tl.cdiv(T, BTS) * BTS) - BTS, (i_c + 1) * BTL - BTS, -BTS):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1)",
      "        )",
      "        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)",
      "        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)",
      "",
      "        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * scale",
      "        b_s2 = b_s * b_s",
      "        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)",
      "        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale",
      "        if i_v == 0:",
      "            b_ds += b_dz[None, :] * scale",
      "        else:",
      "            b_ds = b_ds",
      "        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)",
      "",
      "    tl.debug_barrier()",
      "    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)",
      "    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1)",
      "        )",
      "        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)",
      "        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)",
      "",
      "        m_s = o_k[:, None] <= o_q[None, :]",
      "        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale",
      "        b_s2 = b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_s2 = tl.where(m_s, b_s2, 0)",
      "",
      "        b_ds = tl.dot(b_v, b_do, allow_tf32=False)",
      "        if i_v == 0:",
      "            b_ds += b_dz[None, :]",
      "        else:",
      "            b_ds = b_ds",
      "        b_ds = tl.where(m_s, b_ds, 0) * scale",
      "",
      "        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)",
      "        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)",
      "        o_q += BTS",
      "",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_bh + B * H * i_v) * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_c * BTL, i_k * BK),",
      "        (BTL, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (i_bh + B * H * i_k) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_c * BTL, i_v * BV),",
      "        (BTL, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    return"
    ],
    "file": "codes/414.py",
    "header": "def _parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk, dv, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\nb_k, b_v = (tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v, boundary_check=(0, 1)))\nb_dk, b_dv = (tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros([BTL, BV], dtype=tl.float32))\nfor i in range(tl.cdiv(T, BTS) * BTS - BTS, (i_c + 1) * BTL - BTS, -BTS):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n    b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * scale\n    b_s2 = b_s * b_s\n    b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n    b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale\n    if i_v == 0:\n        b_ds += b_dz[None, :] * scale\n    else:\n        b_ds = b_ds\n    b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\ntl.debug_barrier()\no_q, o_k = (tl.arange(0, BTS), tl.arange(0, BTL))\nfor i in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1))\n    p_dz = dz + i_bh * T + i + tl.arange(0, BTS)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)\n    b_dz = tl.load(p_dz, mask=i + tl.arange(0, BTS) < T)\n    m_s = o_k[:, None] <= o_q[None, :]\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale\n    b_s2 = b_s * b_s\n    b_s = tl.where(m_s, b_s, 0)\n    b_s2 = tl.where(m_s, b_s2, 0)\n    b_ds = tl.dot(b_v, b_do, allow_tf32=False)\n    if i_v == 0:\n        b_ds += b_dz[None, :]\n    else:\n        b_ds = b_ds\n    b_ds = tl.where(m_s, b_ds, 0) * scale\n    b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)\n    b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)\n    o_q += BTS\np_dk = tl.make_block_ptr(dk + (i_bh + B * H * i_v) * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\np_dv = tl.make_block_ptr(dv + (i_bh + B * H * i_k) * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\nreturn"
  },
  {
    "name": "parallel_rebased_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_rebased_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % (NV)",
      "    i_h = i_bh % H",
      "    _parallel_rebased_bwd_dq(",
      "        i_bh,",
      "        i_c,",
      "        i_k,",
      "        i_v,",
      "        i_h,",
      "        q,",
      "        k,",
      "        v,",
      "        do,",
      "        dz,",
      "        dq,",
      "        scale,",
      "        B=B,",
      "        H=H,",
      "        T=T,",
      "        K=K,",
      "        V=V,",
      "        BTL=BTL,",
      "        BTS=BTS,",
      "        BK=BK,",
      "        BV=BV,",
      "    )",
      "    tl.debug_barrier()",
      "    _parallel_rebased_bwd_dkv(",
      "        i_bh,",
      "        i_c,",
      "        i_k,",
      "        i_v,",
      "        i_h,",
      "        q,",
      "        k,",
      "        v,",
      "        do,",
      "        dz,",
      "        dk,",
      "        dv,",
      "        scale,",
      "        B=B,",
      "        H=H,",
      "        T=T,",
      "        K=K,",
      "        V=V,",
      "        BTL=BTL,",
      "        BTS=BTS,",
      "        BK=BK,",
      "        BV=BV,",
      "    )"
    ],
    "file": "codes/414.py",
    "header": "def parallel_rebased_bwd_kernel(q, k, v, do, dz, dq, dk, dv, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "i_kv, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNV = tl.cdiv(V, BV)\ni_k = i_kv // NV\ni_v = i_kv % NV\ni_h = i_bh % H\n_parallel_rebased_bwd_dq(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dq, scale, B=B, H=H, T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)\ntl.debug_barrier()\n_parallel_rebased_bwd_dkv(i_bh, i_c, i_k, i_v, i_h, q, k, v, do, dz, dk, dv, scale, B=B, H=H, T=T, K=K, V=V, BTL=BTL, BTS=BTS, BK=BK, BV=BV)"
  },
  {
    "name": "cross_entropy_loss_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "sum_weights_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "weighted",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_loss_forward_kernel(",
      "    input_pointer,",
      "    target_pointer,",
      "    weight_pointer,",
      "    sum_weights_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    weighted: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    target = tl.load(target_pointer + batch_offset, mask=batch_mask)",
      "",
      "    pred_pointer = (",
      "        input_pointer + input_feat_stride * target + input_batch_stride * batch_offset",
      "    )",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(",
      "        input_pointer,",
      "        mask=batch_mask[:, None] & feat_mask[None, :],",
      "        other=-float(\"inf\"),",
      "    ).to(tl.float32)",
      "    pred = tl.load(pred_pointer, mask=batch_mask).to(tl.float32)",
      "    mx = tl.max(input, axis=1)",
      "    input -= mx[:, None]",
      "    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx",
      "",
      "    if weighted:",
      "        weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)",
      "        loss *= weight",
      "        tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))",
      "",
      "    else:",
      "        loss /= batch_dim",
      "",
      "    tl.store(output_pointer + batch_pid, tl.sum(loss))"
    ],
    "file": "codes/25.py",
    "header": "def cross_entropy_loss_forward_kernel(input_pointer, target_pointer, weight_pointer, sum_weights_pointer, output_pointer, batch_dim, feat_dim, input_batch_stride, input_feat_stride, weighted: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr):",
    "body": "batch_pid = tl.program_id(axis=0)\nbatch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\nfeat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\nbatch_mask = batch_offset < batch_dim\nfeat_mask = feat_offset < feat_dim\ntarget = tl.load(target_pointer + batch_offset, mask=batch_mask)\npred_pointer = input_pointer + input_feat_stride * target + input_batch_stride * batch_offset\ninput_pointer += input_batch_stride * batch_offset[:, None] + input_feat_stride * feat_offset[None, :]\ninput = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :], other=-float('inf')).to(tl.float32)\npred = tl.load(pred_pointer, mask=batch_mask).to(tl.float32)\nmx = tl.max(input, axis=1)\ninput -= mx[:, None]\nloss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx\nif weighted:\n    weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)\n    loss *= weight\n    tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))\nelse:\n    loss /= batch_dim\ntl.store(output_pointer + batch_pid, tl.sum(loss))"
  },
  {
    "name": "cross_entropy_loss_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "sum_weights_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "weighted",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_loss_backward_kernel(",
      "    output_grad_pointer,",
      "    target_pointer,",
      "    input_pointer,",
      "    weight_pointer,",
      "    sum_weights_pointer,",
      "    input_grad_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    weighted: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_grad_pointer += (",
      "        input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(",
      "        input_pointer,",
      "        mask=batch_mask[:, None] & feat_mask[None, :],",
      "        other=-float(\"inf\"),",
      "    ).to(tl.float32)",
      "    input -= tl.max(input, axis=1)[:, None]",
      "    numerator = tl.exp(input)",
      "    softmax = numerator / tl.sum(numerator, axis=1)[:, None]",
      "",
      "    output_grad = tl.load(output_grad_pointer).to(tl.float32)",
      "    target = tl.load(target_pointer + batch_offset, mask=batch_mask)",
      "    broadcasted_feat_offset = tl.broadcast_to(",
      "        feat_offset[None, :], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT)",
      "    )",
      "    broadcasted_target = tl.broadcast_to(",
      "        target[:, None], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT)",
      "    )",
      "    input_grad = output_grad * (",
      "        softmax - (broadcasted_feat_offset == broadcasted_target)",
      "    )",
      "",
      "    if weighted:",
      "        weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)",
      "        sum_weights = tl.load(sum_weights_pointer)",
      "        input_grad *= weight[:, None] / sum_weights",
      "",
      "    else:",
      "        input_grad /= batch_dim",
      "",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    )"
    ],
    "file": "codes/25.py",
    "header": "def cross_entropy_loss_backward_kernel(output_grad_pointer, target_pointer, input_pointer, weight_pointer, sum_weights_pointer, input_grad_pointer, batch_dim, feat_dim, input_batch_stride, input_feat_stride, input_grad_batch_stride, input_grad_feat_stride, weighted: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr):",
    "body": "batch_pid = tl.program_id(axis=0)\nbatch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\nfeat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\nbatch_mask = batch_offset < batch_dim\nfeat_mask = feat_offset < feat_dim\ninput_pointer += input_batch_stride * batch_offset[:, None] + input_feat_stride * feat_offset[None, :]\ninput_grad_pointer += input_grad_batch_stride * batch_offset[:, None] + input_grad_feat_stride * feat_offset[None, :]\ninput = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :], other=-float('inf')).to(tl.float32)\ninput -= tl.max(input, axis=1)[:, None]\nnumerator = tl.exp(input)\nsoftmax = numerator / tl.sum(numerator, axis=1)[:, None]\noutput_grad = tl.load(output_grad_pointer).to(tl.float32)\ntarget = tl.load(target_pointer + batch_offset, mask=batch_mask)\nbroadcasted_feat_offset = tl.broadcast_to(feat_offset[None, :], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT))\nbroadcasted_target = tl.broadcast_to(target[:, None], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT))\ninput_grad = output_grad * (softmax - (broadcasted_feat_offset == broadcasted_target))\nif weighted:\n    weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)\n    sum_weights = tl.load(sum_weights_pointer)\n    input_grad *= weight[:, None] / sum_weights\nelse:\n    input_grad /= batch_dim\ntl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :])"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    eps: tl.float32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,), padding_option=\"zero\")",
      "        mean = tl.sum(input / x_size, 1)",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        centered_mean = tl.where(condition, input - mean, 0)",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        mean = tl.sum(input / x_size, 1)",
      "        centered_mean = input - mean",
      "",
      "    var = tl.sum(centered_mean * centered_mean / x_size, 1)",
      "    rstd = tl.math.rsqrt(var + eps)",
      "    output = centered_mean * rstd",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(x_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(x_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "        else:",
      "            weight = tl.load(weight_block_ptr)",
      "",
      "        output *= weight",
      "",
      "    if bias_ptr is not None:",
      "        bias_block_ptr = tl.make_block_ptr(",
      "            bias_ptr,",
      "            shape=(x_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(x_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            bias = tl.load(bias_block_ptr, boundary_check=(0,))",
      "        else:",
      "            bias = tl.load(bias_block_ptr)",
      "",
      "        output += bias",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))",
      "",
      "    tl.store(rstd_block_ptr, rstd.to(dtype))",
      "    tl.store(mean_block_ptr, mean.to(dtype))"
    ],
    "file": "codes/505.py",
    "header": "def forward(output_ptr: tl.tensor, rstd_ptr: tl.tensor, mean_ptr: tl.tensor, input_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, weight_ptr: tl.tensor, bias_ptr: tl.tensor, eps: tl.float32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(y_size, x_size), strides=(x_size, 1), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nrstd_block_ptr = tl.make_block_ptr(rstd_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\nmean_block_ptr = tl.make_block_ptr(mean_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\ninput_block_ptr = tl.make_block_ptr(input_ptr, shape=(y_size, x_size), strides=(x_size, 1), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nif require_x_boundary_check:\n    input = tl.load(input_block_ptr, boundary_check=(1,), padding_option='zero')\n    mean = tl.sum(input / x_size, 1)\n    condition = tl.arange(0, x_block_size) < x_size\n    centered_mean = tl.where(condition, input - mean, 0)\nelse:\n    input = tl.load(input_block_ptr)\n    mean = tl.sum(input / x_size, 1)\n    centered_mean = input - mean\nvar = tl.sum(centered_mean * centered_mean / x_size, 1)\nrstd = tl.math.rsqrt(var + eps)\noutput = centered_mean * rstd\nif weight_ptr is not None:\n    weight_block_ptr = tl.make_block_ptr(weight_ptr, shape=(x_size,), strides=(1,), offsets=(0,), block_shape=(x_block_size,), order=(0,))\n    if require_x_boundary_check:\n        weight = tl.load(weight_block_ptr, boundary_check=(0,))\n    else:\n        weight = tl.load(weight_block_ptr)\n    output *= weight\nif bias_ptr is not None:\n    bias_block_ptr = tl.make_block_ptr(bias_ptr, shape=(x_size,), strides=(1,), offsets=(0,), block_shape=(x_block_size,), order=(0,))\n    if require_x_boundary_check:\n        bias = tl.load(bias_block_ptr, boundary_check=(0,))\n    else:\n        bias = tl.load(bias_block_ptr)\n    output += bias\nif require_x_boundary_check:\n    tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))\nelse:\n    tl.store(output_block_ptr, output.to(dtype))\ntl.store(rstd_block_ptr, rstd.to(dtype))\ntl.store(mean_block_ptr, mean.to(dtype))"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(",
      "            grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"",
      "        )",
      "        input = tl.load(input_block_ptr, boundary_check=(1,), padding_option=\"zero\")",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        input = tl.load(input_block_ptr)",
      "",
      "    rstd = tl.load(rstd_block_ptr)",
      "    mean = tl.load(mean_block_ptr)",
      "    centered_mean = input - mean",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(1, x_size),",
      "            strides=(x_size, 1),",
      "            offsets=(0, 0),",
      "            block_shape=(1, x_block_size),",
      "            order=(1, 0),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            weight = tl.load(weight_block_ptr, boundary_check=(1,))",
      "        else:",
      "            weight = tl.load(weight_block_ptr)",
      "",
      "        grad_norm = weight * grad_output",
      "    else:",
      "        grad_norm = grad_output",
      "",
      "    grad_std = tl.sum(grad_norm * centered_mean, 1)",
      "    grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / x_size",
      "    grad_distance = 2 * centered_mean * grad_var",
      "    grad_centered_mean = grad_norm * rstd + grad_distance",
      "    grad_mean = -tl.sum(grad_centered_mean, 1) / x_size",
      "    grad_input = grad_centered_mean + grad_mean",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))",
      "",
      "    if grad_weight_staging_ptr is not None:",
      "        grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "            grad_weight_staging_ptr,",
      "            shape=(y_size, x_size),",
      "            strides=(x_size, 1),",
      "            offsets=(y_offset, 0),",
      "            block_shape=(1, x_block_size),",
      "            order=(1, 0),",
      "        )",
      "",
      "        norm = centered_mean * rstd",
      "        grad_weight = norm * grad_output",
      "",
      "        if require_x_boundary_check:",
      "            tl.store(",
      "                grad_weight_staging_block_ptr,",
      "                grad_weight.to(dtype),",
      "                boundary_check=(1,),",
      "            )",
      "        else:",
      "            tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))"
    ],
    "file": "codes/505.py",
    "header": "def backward(grad_input_ptr: tl.tensor, grad_weight_staging_ptr: tl.tensor, grad_output_ptr: tl.tensor, input_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, weight_ptr: tl.tensor, rstd_ptr: tl.tensor, mean_ptr: tl.tensor, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\ngrad_input_block_ptr = tl.make_block_ptr(grad_input_ptr, shape=(y_size, x_size), strides=(x_size, 1), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\ngrad_output_block_ptr = tl.make_block_ptr(grad_output_ptr, shape=(y_size, x_size), strides=(x_size, 1), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\ninput_block_ptr = tl.make_block_ptr(input_ptr, shape=(y_size, x_size), strides=(x_size, 1), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nrstd_block_ptr = tl.make_block_ptr(rstd_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\nmean_block_ptr = tl.make_block_ptr(mean_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\nif require_x_boundary_check:\n    grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,), padding_option='zero')\n    input = tl.load(input_block_ptr, boundary_check=(1,), padding_option='zero')\nelse:\n    grad_output = tl.load(grad_output_block_ptr)\n    input = tl.load(input_block_ptr)\nrstd = tl.load(rstd_block_ptr)\nmean = tl.load(mean_block_ptr)\ncentered_mean = input - mean\nif weight_ptr is not None:\n    weight_block_ptr = tl.make_block_ptr(weight_ptr, shape=(1, x_size), strides=(x_size, 1), offsets=(0, 0), block_shape=(1, x_block_size), order=(1, 0))\n    if require_x_boundary_check:\n        weight = tl.load(weight_block_ptr, boundary_check=(1,))\n    else:\n        weight = tl.load(weight_block_ptr)\n    grad_norm = weight * grad_output\nelse:\n    grad_norm = grad_output\ngrad_std = tl.sum(grad_norm * centered_mean, 1)\ngrad_var = grad_std * -(0.5 * rstd * rstd * rstd) / x_size\ngrad_distance = 2 * centered_mean * grad_var\ngrad_centered_mean = grad_norm * rstd + grad_distance\ngrad_mean = -tl.sum(grad_centered_mean, 1) / x_size\ngrad_input = grad_centered_mean + grad_mean\nif require_x_boundary_check:\n    tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))\nelse:\n    tl.store(grad_input_block_ptr, grad_input.to(dtype))\nif grad_weight_staging_ptr is not None:\n    grad_weight_staging_block_ptr = tl.make_block_ptr(grad_weight_staging_ptr, shape=(y_size, x_size), strides=(x_size, 1), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\n    norm = centered_mean * rstd\n    grad_weight = norm * grad_output\n    if require_x_boundary_check:\n        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype), boundary_check=(1,))\n    else:\n        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args['BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args['BLOCK_N'] == 0})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "H_KV",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "ROUND_CTX",
        "annotation": null
      },
      {
        "name": "NKV_CTX",
        "annotation": null
      },
      {
        "name": "sliding_window_offset",
        "annotation": null
      },
      {
        "name": "sliding_window_size",
        "annotation": null
      },
      {
        "name": "IS_EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "END",
        "annotation": "tl.constexpr"
      },
      {
        "name": "INIT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "COMPLEMENT_SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    L,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    H_KV,",
      "    N_CTX,",
      "    ROUND_CTX,",
      "    NKV_CTX,",
      "    sliding_window_offset,",
      "    sliding_window_size,",
      "    IS_EVEN_M: tl.constexpr,",
      "    IS_EVEN_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    END: tl.constexpr,",
      "    INIT: tl.constexpr,",
      "    SLIDING_WINDOW: tl.constexpr,",
      "    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,",
      "):",
      "",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    off_hkv = off_h // (H // H_KV)",
      "    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh",
      "    v_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64) * stride_vh",
      "    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + q_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + v_offset,",
      "        shape=(NKV_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + k_offset,",
      "        shape=(BLOCK_DMODEL, NKV_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + o_offset,",
      "        shape=(ROUND_CTX, BLOCK_DMODEL),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    m_ptrs = M + off_hz * ROUND_CTX + offs_m",
      "    l_ptrs = L + off_hz * ROUND_CTX + offs_m",
      "    if INIT:",
      "        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0",
      "        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "    else:",
      "",
      "        m_i = tl.load(m_ptrs).to(tl.float32)",
      "        l_i = tl.load(l_ptrs).to(tl.float32)",
      "        acc = tl.load(O_block_ptr).to(tl.float32)",
      "",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.4426950408889634",
      "",
      "    if IS_EVEN_M:",
      "        q = tl.load(Q_block_ptr)",
      "    else:",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    acc, l_i, m_i = _attn_fwd_inner(",
      "        acc,",
      "        l_i,",
      "        m_i,",
      "        q,",
      "        K_block_ptr,",
      "        V_block_ptr,",
      "        start_m,",
      "        qk_scale,",
      "        NKV_CTX,",
      "        sliding_window_offset,",
      "        sliding_window_size,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        SLIDING_WINDOW,",
      "        IS_EVEN_M,",
      "        IS_EVEN_N,",
      "        COMPLEMENT_SLIDING_WINDOW,",
      "    )",
      "",
      "    if END:",
      "        m_i += tl.math.log2(l_i)",
      "        acc = acc / l_i[:, None]",
      "    else:",
      "        tl.store(l_ptrs, l_i)",
      "",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))"
    ],
    "file": "codes/727.py",
    "header": "def _attn_fwd(Q, K, V, sm_scale, M, Out, L, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset, sliding_window_size, IS_EVEN_M: tl.constexpr, IS_EVEN_N: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, END: tl.constexpr, INIT: tl.constexpr, SLIDING_WINDOW: tl.constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr):",
    "body": "start_m = tl.program_id(0)\noff_hz = tl.program_id(1)\noff_z = off_hz // H\noff_h = off_hz % H\noff_hkv = off_h // (H // H_KV)\nq_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\nk_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\nv_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64) * stride_vh\no_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\nQ_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX, BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\nV_block_ptr = tl.make_block_ptr(base=V + v_offset, shape=(NKV_CTX, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\nK_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL, NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\nO_block_ptr = tl.make_block_ptr(base=Out + o_offset, shape=(ROUND_CTX, BLOCK_DMODEL), strides=(stride_om, stride_on), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\nm_ptrs = M + off_hz * ROUND_CTX + offs_m\nl_ptrs = L + off_hz * ROUND_CTX + offs_m\nif INIT:\n    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\n    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\n    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\nelse:\n    m_i = tl.load(m_ptrs).to(tl.float32)\n    l_i = tl.load(l_ptrs).to(tl.float32)\n    acc = tl.load(O_block_ptr).to(tl.float32)\nqk_scale = sm_scale\nqk_scale *= 1.4426950408889634\nif IS_EVEN_M:\n    q = tl.load(Q_block_ptr)\nelse:\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\nacc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, start_m, qk_scale, NKV_CTX, sliding_window_offset, sliding_window_size, BLOCK_M, BLOCK_DMODEL, BLOCK_N, SLIDING_WINDOW, IS_EVEN_M, IS_EVEN_N, COMPLEMENT_SLIDING_WINDOW)\nif END:\n    m_i += tl.math.log2(l_i)\n    acc = acc / l_i[:, None]\nelse:\n    tl.store(l_ptrs, l_i)\ntl.store(m_ptrs, m_i)\ntl.store(O_block_ptr, acc.to(Out.type.element_ty))"
  },
  {
    "name": "_chunk_scan_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'hdim', 'dstate', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    prev_states_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_D_head,",
      "    IS_CAUSAL: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    if IS_TRITON_22 or pid_c > -1:",
      "",
      "        offs_k_dstate = tl.arange(",
      "            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "        )",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "        )",
      "        prev_states_ptrs = prev_states_ptr + (",
      "            offs_n[None, :] * stride_states_hdim",
      "            + offs_k_dstate[:, None] * stride_states_dstate",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        else:",
      "            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        if BLOCK_SIZE_DSTATE <= 128:",
      "            C = tl.load(",
      "                C_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k_dstate[None, :] < dstate),",
      "                other=0.0,",
      "            )",
      "            prev_states = tl.load(",
      "                prev_states_ptrs,",
      "                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            prev_states = prev_states.to(C_ptr.dtype.element_ty)",
      "            acc = tl.dot(C, prev_states) * scale_m[:, None]",
      "        else:",
      "            for k in range(0, dstate, BLOCK_SIZE_K):",
      "                C = tl.load(",
      "                    C_ptrs,",
      "                    mask=(offs_m[:, None] < chunk_size_limit)",
      "                    & (offs_k_dstate[None, :] < dstate - k),",
      "                    other=0.0,",
      "                )",
      "",
      "                prev_states = tl.load(",
      "                    prev_states_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate - k)",
      "                    & (offs_n[None, :] < hdim),",
      "                    other=0.0,",
      "                )",
      "                prev_states = prev_states.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states)",
      "                C_ptrs += BLOCK_SIZE_K",
      "                prev_states_ptrs += BLOCK_SIZE_K",
      "            acc *= scale_m[:, None]",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    K_MAX = (",
      "        chunk_size_limit",
      "        if not IS_CAUSAL",
      "        else min((pid_m + 1) * BLOCK_SIZE_M, chunk_size_limit)",
      "    )",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "",
      "        cb *= tl.exp((dA_cs_m[:, None] - dA_cs_k[None, :]))",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)",
      "        cb *= dt_k",
      "        if IS_CAUSAL:",
      "            mask = offs_m[:, None] >= k + offs_k[None, :]",
      "            cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(x_ptr.dtype.element_ty)",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        acc += tl.dot(cb, x)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    if HAS_D:",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        x_residual = tl.load(",
      "            x_ptr",
      "            + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim),",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc += x_residual * D",
      "",
      "    if HAS_Z:",
      "        out_x_ptr += (",
      "            pid_b * stride_out_batch",
      "            + pid_c * chunk_size * stride_out_seqlen",
      "            + pid_h * stride_out_head",
      "        )",
      "        out_x_ptrs = out_x_ptr + (",
      "            stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :]",
      "        )",
      "        tl.store(",
      "            out_x_ptrs,",
      "            acc,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "        )",
      "",
      "        z_ptr += (",
      "            pid_b * stride_z_batch",
      "            + pid_c * chunk_size * stride_z_seqlen",
      "            + pid_h * stride_z_head",
      "        )",
      "        z_ptrs = z_ptr + (",
      "            stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :]",
      "        )",
      "        z = tl.load(",
      "            z_ptrs,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= z * tl.sigmoid(z)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim",
      "    )",
      "    tl.store(",
      "        out_ptrs,",
      "        acc,",
      "        mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim),",
      "    )"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_fwd_kernel(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, C_ptr, prev_states_ptr, D_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_states_batch, stride_states_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_D_head, IS_CAUSAL: tl.constexpr, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_Z: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr, IS_TRITON_22: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nC_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_h // nheads_ngroups_ratio * stride_C_head\nprev_states_ptr += pid_b * stride_states_batch + pid_c * stride_states_chunk + pid_h * stride_states_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nif HAS_SEQ_IDX:\n    seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif IS_TRITON_22 or pid_c > -1:\n    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate)\n    prev_states_ptrs = prev_states_ptr + (offs_n[None, :] * stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\n    if not HAS_SEQ_IDX:\n        scale_m = tl.exp(dA_cs_m)\n    else:\n        scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    if BLOCK_SIZE_DSTATE <= 128:\n        C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k_dstate[None, :] < dstate), other=0.0)\n        prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n        prev_states = prev_states.to(C_ptr.dtype.element_ty)\n        acc = tl.dot(C, prev_states) * scale_m[:, None]\n    else:\n        for k in range(0, dstate, BLOCK_SIZE_K):\n            C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k_dstate[None, :] < dstate - k), other=0.0)\n            prev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n            prev_states = prev_states.to(C_ptr.dtype.element_ty)\n            acc += tl.dot(C, prev_states)\n            C_ptrs += BLOCK_SIZE_K\n            prev_states_ptrs += BLOCK_SIZE_K\n        acc *= scale_m[:, None]\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k)\nx_ptrs = x_ptr + (offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_k * stride_dt_csize\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nK_MAX = chunk_size_limit if not IS_CAUSAL else min((pid_m + 1) * BLOCK_SIZE_M, chunk_size_limit)\nfor k in range(0, K_MAX, BLOCK_SIZE_K):\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k), other=0.0).to(tl.float32)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n    cb *= tl.exp(dA_cs_m[:, None] - dA_cs_k[None, :])\n    dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n    cb *= dt_k\n    if IS_CAUSAL:\n        mask = offs_m[:, None] >= k + offs_k[None, :]\n        cb = tl.where(mask, cb, 0.0)\n    cb = cb.to(x_ptr.dtype.element_ty)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim), other=0.0)\n    acc += tl.dot(cb, x)\n    cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n    x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n    dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\noffs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_D:\n    if D_HAS_HDIM:\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    x_residual = tl.load(x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim), mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    acc += x_residual * D\nif HAS_Z:\n    out_x_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\n    out_x_ptrs = out_x_ptr + (stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :])\n    tl.store(out_x_ptrs, acc, mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim))\n    z_ptr += pid_b * stride_z_batch + pid_c * chunk_size * stride_z_seqlen + pid_h * stride_z_head\n    z_ptrs = z_ptr + (stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :])\n    z = tl.load(z_ptrs, mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim), other=0.0).to(tl.float32)\n    acc *= z * tl.sigmoid(z)\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\nout_ptrs = out_ptr + (stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim)\ntl.store(out_ptrs, acc, mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim))"
  },
  {
    "name": "_chunk_scan_fwd_kernel_wip",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_seqlen",
        "annotation": null
      },
      {
        "name": "stride_B_head",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel_wip(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    B_ptr,",
      "    prev_states_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_B_batch,",
      "    stride_B_seqlen,",
      "    stride_B_head,",
      "    stride_B_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_D_head,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_n = tl.program_id(axis=0)",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    B_ptr += (",
      "        pid_b * stride_B_batch",
      "        + pid_c * chunk_size * stride_B_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_B_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "",
      "    offs_m = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "    )",
      "    B_ptrs = B_ptr + (",
      "        offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_states_hdim",
      "        + offs_k_dstate[:, None] * stride_states_dstate",
      "    )",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)",
      "        dA_cs_m = tl.load(",
      "            dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize,",
      "            mask=offs_m < chunk_size - start_m,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_prev = tl.load(",
      "                seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "            )",
      "            seq_idx_m = tl.load(",
      "                seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen,",
      "                mask=offs_m < chunk_size_limit - start_m,",
      "                other=-1,",
      "            )",
      "        if not HAS_SEQ_IDX:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        else:",
      "            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        C = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_k_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]",
      "",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(",
      "            tl.float32",
      "        )",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        if HAS_D:",
      "            if D_HAS_HDIM:",
      "                D = tl.load(",
      "                    D_ptr + pid_h * stride_D_head + offs_n,",
      "                    mask=offs_n < hdim,",
      "                    other=0.0,",
      "                ).to(tl.float32)",
      "            else:",
      "                D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "            acc += x.to(tl.float32) * D",
      "",
      "        tl.store(",
      "            out_ptrs,",
      "            acc,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "        )",
      "",
      "        if start_m + BLOCK_SIZE_M < chunk_size_limit:",
      "",
      "            B = tl.load(",
      "                B_ptrs,",
      "                mask=(offs_m[None, :] < chunk_size_limit - start_m)",
      "                & (offs_k_dstate[:, None] < dstate),",
      "                other=0.0,",
      "            )",
      "            dA_cs_last = tl.load(",
      "                dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize",
      "            ).to(tl.float32)",
      "",
      "            scale = tl.exp((dA_cs_last - dA_cs_m)) * dt_m",
      "",
      "            B = B.to(x_ptr.dtype.element_ty)",
      "            tmp = tl.dot(B, x)",
      "            prev_states += tmp.to(prev_states.dtype)",
      "",
      "        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen",
      "        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen",
      "        cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize",
      "        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_fwd_kernel_wip(cb_ptr, x_ptr, z_ptr, out_ptr, out_x_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, C_ptr, B_ptr, prev_states_ptr, D_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_B_batch, stride_B_seqlen, stride_B_head, stride_B_dstate, stride_states_batch, stride_states_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_D_head, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_Z: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_n = tl.program_id(axis=0)\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nC_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_h // nheads_ngroups_ratio * stride_C_head\nB_ptr += pid_b * stride_B_batch + pid_c * chunk_size * stride_B_seqlen + pid_h // nheads_ngroups_ratio * stride_B_head\nprev_states_ptr += pid_b * stride_states_batch + pid_c * stride_states_chunk + pid_h * stride_states_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\noffs_m = tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)\nC_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate)\nB_ptrs = B_ptr + (offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate)\nprev_states_ptrs = prev_states_ptr + (offs_n[None, :] * stride_states_hdim + offs_k_dstate[:, None] * stride_states_dstate)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\nout_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim)\nprev_states = tl.load(prev_states_ptrs, mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nfor start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):\n    start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)\n    dA_cs_m = tl.load(dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize, mask=offs_m < chunk_size - start_m, other=0.0).to(tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_prev = tl.load(seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n        seq_idx_m = tl.load(seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit - start_m, other=-1)\n    if not HAS_SEQ_IDX:\n        scale_m = tl.exp(dA_cs_m)\n    else:\n        scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    C = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit - start_m) & (offs_k_dstate[None, :] < dstate), other=0.0)\n    acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(tl.float32)\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit - start_m) & (offs_n[None, :] < hdim), other=0.0)\n    if HAS_D:\n        if D_HAS_HDIM:\n            D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n        else:\n            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n        acc += x.to(tl.float32) * D\n    tl.store(out_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit - start_m) & (offs_n[None, :] < hdim))\n    if start_m + BLOCK_SIZE_M < chunk_size_limit:\n        B = tl.load(B_ptrs, mask=(offs_m[None, :] < chunk_size_limit - start_m) & (offs_k_dstate[:, None] < dstate), other=0.0)\n        dA_cs_last = tl.load(dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize).to(tl.float32)\n        scale = tl.exp(dA_cs_last - dA_cs_m) * dt_m\n        B = B.to(x_ptr.dtype.element_ty)\n        tmp = tl.dot(B, x)\n        prev_states += tmp.to(prev_states.dtype)\n    C_ptrs += BLOCK_SIZE_M * stride_C_seqlen\n    B_ptrs += BLOCK_SIZE_M * stride_B_seqlen\n    cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k\n    x_ptrs += BLOCK_SIZE_M * stride_x_seqlen\n    dt_ptrs += BLOCK_SIZE_M * stride_dt_csize\n    out_ptrs += BLOCK_SIZE_M * stride_out_seqlen"
  },
  {
    "name": "_chunk_scan_bwd_dz_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "outz_ptr",
        "annotation": null
      },
      {
        "name": "dz_ptr",
        "annotation": null
      },
      {
        "name": "dout_x_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_outz_batch",
        "annotation": null
      },
      {
        "name": "stride_outz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_outz_head",
        "annotation": null
      },
      {
        "name": "stride_outz_hdim",
        "annotation": null
      },
      {
        "name": "stride_dz_batch",
        "annotation": null
      },
      {
        "name": "stride_dz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dz_head",
        "annotation": null
      },
      {
        "name": "stride_dz_hdim",
        "annotation": null
      },
      {
        "name": "stride_doutx_batch",
        "annotation": null
      },
      {
        "name": "stride_doutx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_doutx_head",
        "annotation": null
      },
      {
        "name": "stride_doutx_hdim",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DDACS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dz_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    z_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    outz_ptr,",
      "    dz_ptr,",
      "    dout_x_ptr,",
      "    dD_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_outz_batch,",
      "    stride_outz_seqlen,",
      "    stride_outz_head,",
      "    stride_outz_hdim,",
      "    stride_dz_batch,",
      "    stride_dz_seqlen,",
      "    stride_dz_head,",
      "    stride_dz_hdim,",
      "    stride_doutx_batch,",
      "    stride_doutx_seqlen,",
      "    stride_doutx_head,",
      "    stride_doutx_hdim,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_DDACS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dout_x_ptr += (",
      "        pid_b * stride_doutx_batch",
      "        + pid_c * chunk_size * stride_doutx_seqlen",
      "        + pid_h * stride_doutx_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    z_ptr += (",
      "        pid_b * stride_z_batch",
      "        + pid_c * chunk_size * stride_z_seqlen",
      "        + pid_h * stride_z_head",
      "    )",
      "    dz_ptr += (",
      "        pid_b * stride_dz_batch",
      "        + pid_c * chunk_size * stride_dz_seqlen",
      "        + pid_h * stride_dz_head",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptr += (",
      "            pid_b * stride_outz_batch",
      "            + pid_c * chunk_size * stride_outz_seqlen",
      "            + pid_h * stride_outz_head",
      "        )",
      "    if HAS_DDACS:",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + pid_h * stride_ddA_cs_head",
      "        )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dout_x_ptrs = dout_x_ptr + (",
      "        offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    z_ptrs = z_ptr + (",
      "        offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim",
      "    )",
      "    dz_ptrs = dz_ptr + (",
      "        offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptrs = outz_ptr + (",
      "            offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim",
      "        )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z = tl.load(",
      "        z_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z_sigmoid = tl.sigmoid(z)",
      "    if RECOMPUTE_OUTPUT:",
      "        outz = out * z * z_sigmoid",
      "        tl.store(",
      "            outz_ptrs,",
      "            outz,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        )",
      "    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "    tl.store(",
      "        dz_ptrs,",
      "        dz,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    dout *= z * z_sigmoid",
      "    tl.store(",
      "        dout_x_ptrs,",
      "        dout,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    if HAS_DDACS:",
      "        ddA_cs = tl.sum(dout * out, axis=1)",
      "        tl.store(",
      "            ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize,",
      "            ddA_cs,",
      "            mask=offs_m < chunk_size,",
      "        )"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_dz_kernel(dout_ptr, out_ptr, z_ptr, x_ptr, D_ptr, outz_ptr, dz_ptr, dout_x_ptr, dD_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_z_batch, stride_z_seqlen, stride_z_head, stride_z_hdim, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_D_head, stride_outz_batch, stride_outz_seqlen, stride_outz_head, stride_outz_hdim, stride_dz_batch, stride_dz_seqlen, stride_dz_head, stride_dz_hdim, stride_doutx_batch, stride_doutx_seqlen, stride_doutx_head, stride_doutx_hdim, stride_dD_batch, stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_DDACS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndout_x_ptr += pid_b * stride_doutx_batch + pid_c * chunk_size * stride_doutx_seqlen + pid_h * stride_doutx_head\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\nz_ptr += pid_b * stride_z_batch + pid_c * chunk_size * stride_z_seqlen + pid_h * stride_z_head\ndz_ptr += pid_b * stride_dz_batch + pid_c * chunk_size * stride_dz_seqlen + pid_h * stride_dz_head\nif RECOMPUTE_OUTPUT:\n    outz_ptr += pid_b * stride_outz_batch + pid_c * chunk_size * stride_outz_seqlen + pid_h * stride_outz_head\nif HAS_DDACS:\n    ddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\nif HAS_D:\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dD_ptr += pid_b * stride_dD_batch + pid_c * stride_dD_chunk + pid_h * stride_dD_head + pid_m * stride_dD_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\ndout_x_ptrs = dout_x_ptr + (offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim)\nout_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim)\nz_ptrs = z_ptr + (offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim)\ndz_ptrs = dz_ptr + (offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim)\nif RECOMPUTE_OUTPUT:\n    outz_ptrs = outz_ptr + (offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim)\nif HAS_D:\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\n    if D_HAS_HDIM:\n        dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nout = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nz = tl.load(z_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nz_sigmoid = tl.sigmoid(z)\nif RECOMPUTE_OUTPUT:\n    outz = out * z * z_sigmoid\n    tl.store(outz_ptrs, outz, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\ndz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))\ntl.store(dz_ptrs, dz, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\ndout *= z * z_sigmoid\ntl.store(dout_x_ptrs, dout, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\nif HAS_D:\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        dD = tl.sum(dout * x, axis=0)\n        tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        dD = tl.sum(dout * x)\n        tl.store(dD_ptr, dD)\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    out -= x * D\nif HAS_DDACS:\n    ddA_cs = tl.sum(dout * out, axis=1)\n    tl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_bwd_dstates_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "dprev_states_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_c_batch",
        "annotation": null
      },
      {
        "name": "stride_c_seqlen",
        "annotation": null
      },
      {
        "name": "stride_c_head",
        "annotation": null
      },
      {
        "name": "stride_c_dstate",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_head",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dstates_kernel(",
      "    dout_ptr,",
      "    c_ptr,",
      "    dprev_states_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_c_batch,",
      "    stride_c_seqlen,",
      "    stride_c_head,",
      "    stride_c_dstate,",
      "    stride_dprev_states_batch,",
      "    stride_dprev_states_chunk,",
      "    stride_dprev_states_head,",
      "    stride_dprev_states_hdim,",
      "    stride_dprev_states_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    c_ptr += (",
      "        pid_b * stride_c_batch",
      "        + pid_c * chunk_size * stride_c_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_c_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen",
      "    )",
      "    c_ptrs = c_ptr + (",
      "        offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale_k = tl.exp(dA_cs_k)",
      "        else:",
      "            seq_idx_k = tl.load(",
      "                seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1",
      "            )",
      "            scale_k = tl.where(seq_idx_k == seq_idx_prev, tl.exp(dA_cs_k), 0.0)",
      "        dout = (dout * scale_k).to(dout_ptr.dtype.element_ty)",
      "        c = tl.load(",
      "            c_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc += tl.dot(dout, c)",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen",
      "    out = acc.to(dprev_states_ptr.dtype.element_ty)",
      "",
      "    dprev_states_ptr += (",
      "        pid_b * stride_dprev_states_batch",
      "        + pid_c * stride_dprev_states_chunk",
      "        + pid_h * stride_dprev_states_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dprev_states_ptrs = dprev_states_ptr + (",
      "        offs_m[:, None] * stride_dprev_states_hdim",
      "        + offs_n[None, :] * stride_dprev_states_dstate",
      "    )",
      "    tl.store(",
      "        dprev_states_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_dstates_kernel(dout_ptr, c_ptr, dprev_states_ptr, dA_cumsum_ptr, seq_idx_ptr, hdim, dstate, chunk_size, batch, seqlen, nchunks, nheads_ngroups_ratio, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_c_batch, stride_c_seqlen, stride_c_head, stride_c_dstate, stride_dprev_states_batch, stride_dprev_states_chunk, stride_dprev_states_head, stride_dprev_states_hdim, stride_dprev_states_dstate, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nc_ptr += pid_b * stride_c_batch + pid_c * chunk_size * stride_c_seqlen + pid_h // nheads_ngroups_ratio * stride_c_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen)\nc_ptrs = c_ptr + (offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nif HAS_SEQ_IDX:\n    seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif HAS_SEQ_IDX:\n    seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k), other=0.0).to(tl.float32)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n    if not HAS_SEQ_IDX:\n        scale_k = tl.exp(dA_cs_k)\n    else:\n        seq_idx_k = tl.load(seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1)\n        scale_k = tl.where(seq_idx_k == seq_idx_prev, tl.exp(dA_cs_k), 0.0)\n    dout = (dout * scale_k).to(dout_ptr.dtype.element_ty)\n    c = tl.load(c_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate), other=0.0)\n    acc += tl.dot(dout, c)\n    dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n    c_ptrs += BLOCK_SIZE_K * stride_c_seqlen\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\nout = acc.to(dprev_states_ptr.dtype.element_ty)\ndprev_states_ptr += pid_b * stride_dprev_states_batch + pid_c * stride_dprev_states_chunk + pid_h * stride_dprev_states_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndprev_states_ptrs = dprev_states_ptr + (offs_m[:, None] * stride_dprev_states_hdim + offs_n[None, :] * stride_dprev_states_dstate)\ntl.store(dprev_states_ptrs, out, mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate))"
  },
  {
    "name": "_chunk_scan_bwd_dc_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dc_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dc_batch",
        "annotation": null
      },
      {
        "name": "stride_dc_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dc_split",
        "annotation": null
      },
      {
        "name": "stride_dc_group",
        "annotation": null
      },
      {
        "name": "stride_dc_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dc_kernel(",
      "    dout_ptr,",
      "    prev_states_ptr,",
      "    C_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    dc_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_batch,",
      "    stride_prev_states_chunk,",
      "    stride_prev_states_head,",
      "    stride_prev_states_hdim,",
      "    stride_prev_states_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_dc_batch,",
      "    stride_dc_seqlen,",
      "    stride_dc_split,",
      "    stride_dc_group,",
      "    stride_dc_dstate,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dc_ptr += (",
      "        pid_b * stride_dc_batch",
      "        + pid_c * chunk_size * stride_dc_seqlen",
      "        + pid_g * stride_dc_group",
      "        + pid_s * stride_dc_split",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_prev_states_batch",
      "        + pid_c * stride_prev_states_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_prev_states_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        C_ptr += (",
      "            pid_b * stride_C_batch",
      "            + pid_c * chunk_size * stride_C_seqlen",
      "            + pid_g * stride_C_head",
      "        )",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_prev_states_dstate",
      "        + offs_k[:, None] * stride_prev_states_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    if HAS_DDA_CS:",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "        )",
      "        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        c = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        prev_states = tl.load(",
      "            prev_states_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        prev_states = prev_states.to(dout_ptrs.dtype.element_ty)",
      "        dc = tl.dot(dout, prev_states)",
      "        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp(dA_cs_m)",
      "        else:",
      "            scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        dc *= scale[:, None]",
      "        if HAS_DDA_CS:",
      "            ddA_cs = tl.sum(dc * c, axis=1)",
      "            tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)",
      "        acc += dc",
      "        dout_ptrs += stride_dout_head",
      "        prev_states_ptrs += stride_prev_states_head",
      "        dA_cumsum_ptrs += stride_dA_cs_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_ptrs += stride_ddA_cs_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dc_ptrs = dc_ptr + (",
      "        offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate",
      "    )",
      "    tl.store(",
      "        dc_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_dc_kernel(dout_ptr, prev_states_ptr, C_ptr, dA_cumsum_ptr, seq_idx_ptr, dc_ptr, ddA_cumsum_ptr, chunk_size, dstate, hdim, batch, seqlen, nheads, nheads_per_program, ngroups, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_prev_states_batch, stride_prev_states_chunk, stride_prev_states_head, stride_prev_states_hdim, stride_prev_states_dstate, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_dc_batch, stride_dc_seqlen, stride_dc_split, stride_dc_group, stride_dc_dstate, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_DDA_CS: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_sg = tl.program_id(axis=2)\npid_s = pid_sg // ngroups\npid_g = pid_sg - pid_s * ngroups\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\ndc_ptr += pid_b * stride_dc_batch + pid_c * chunk_size * stride_dc_seqlen + pid_g * stride_dc_group + pid_s * stride_dc_split\nprev_states_ptr += pid_b * stride_prev_states_batch + pid_c * stride_prev_states_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_prev_states_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head\nif HAS_DDA_CS:\n    C_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_g * stride_C_head\n    ddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nprev_states_ptrs = prev_states_ptr + (offs_n[None, :] * stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\nif HAS_DDA_CS:\n    C_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif HAS_DDA_CS:\n    c = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\nif HAS_SEQ_IDX:\n    seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\nnheads_iter = min(nheads_per_program, nheads // ngroups - pid_s * nheads_per_program)\nfor h in range(nheads_iter):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\n    prev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\n    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)\n    dc = tl.dot(dout, prev_states)\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_m)\n    else:\n        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\n    dc *= scale[:, None]\n    if HAS_DDA_CS:\n        ddA_cs = tl.sum(dc * c, axis=1)\n        tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\n    acc += dc\n    dout_ptrs += stride_dout_head\n    prev_states_ptrs += stride_prev_states_head\n    dA_cumsum_ptrs += stride_dA_cs_head\n    if HAS_DDA_CS:\n        ddA_cumsum_ptrs += stride_ddA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndc_ptrs = dc_ptr + (offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate)\ntl.store(dc_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate))"
  },
  {
    "name": "_chunk_scan_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    D_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_D_head,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    K_MAX = chunk_size_limit",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])",
      "",
      "        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)",
      "        cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, D_ptr, dx_ptr, ddt_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_D_head, stride_dx_batch, stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k)\ndout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nK_MAX = chunk_size_limit\nfor k in range(0, K_MAX, BLOCK_SIZE_K):\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k), other=0.0)\n    dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim), other=0.0)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(tl.float32)\n    cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n    mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n    cb = tl.where(mask, cb, 0.0)\n    cb = cb.to(dout_ptr.dtype.element_ty)\n    acc += tl.dot(cb, dout)\n    cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n    dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndx = acc * dt_m[:, None]\ndx_ptr += pid_b * stride_dx_batch + pid_c * chunk_size * stride_dx_seqlen + pid_h * stride_dx_head\ndx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim)\nif HAS_D:\n    dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\n    dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    dx += dout_res * D\ntl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nddt = tl.sum(acc * x, axis=1)\nddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\ntl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_bwd_dcb_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dcb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dcb_batch",
        "annotation": null
      },
      {
        "name": "stride_dcb_chunk",
        "annotation": null
      },
      {
        "name": "stride_dcb_split",
        "annotation": null
      },
      {
        "name": "stride_dcb_group",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dcb_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    cb_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    dcb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_dcb_batch,",
      "    stride_dcb_chunk,",
      "    stride_dcb_split,",
      "    stride_dcb_group,",
      "    stride_dcb_csize_m,",
      "    stride_dcb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        cb_ptr += (",
      "            pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_g * stride_cb_head",
      "        )",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_head",
      "            + pid_m * stride_ddA_cs_csize_m",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    if HAS_DDA_CS:",
      "        cb_ptrs = cb_ptr + (",
      "            offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "        )",
      "        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n",
      "",
      "    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "        dcb_ptr += (",
      "            pid_b * stride_dcb_batch",
      "            + pid_c * stride_dcb_chunk",
      "            + pid_g * stride_dcb_group",
      "            + pid_s * stride_dcb_split",
      "        )",
      "        dcb_ptrs = dcb_ptr + (",
      "            offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n",
      "        )",
      "        tl.store(",
      "            dcb_ptrs,",
      "            tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=dcb_ptr.dtype.element_ty),",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "        )",
      "        return",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),",
      "            other=0.0,",
      "        )",
      "        dcb = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "        dcb *= dt_n",
      "        dA_cs_m = tl.load(",
      "            dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + offs_n * stride_dA_cs_csize,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dcb *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        if HAS_DDA_CS:",
      "            tl.static_assert(",
      "                not HAS_SEQ_IDX, \"HAS_SEQ_IDX not supported with HAS_DDA_CS yet\"",
      "            )",
      "            ddA_cs = dcb * cb",
      "            mask = offs_m[:, None] >= offs_n[None, :] + 1",
      "            ddA_cs = tl.where(mask, ddA_cs, 0.0)",
      "            ddA_cs = tl.cumsum(ddA_cs, axis=1)",
      "            ddA_cs = tl.where(mask, ddA_cs, 0.0)",
      "            ddA_cs = tl.sum(ddA_cs, axis=0)",
      "            tl.store(",
      "                ddA_cumsum_ptrs + stride_ddA_cs_csize_n,",
      "                ddA_cs,",
      "                mask=offs_n < chunk_size - 1,",
      "            )",
      "            tl.store(ddA_cumsum_ptr, 0.0)",
      "        acc += dcb",
      "        dout_ptrs += stride_dout_head",
      "        x_ptrs += stride_x_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_ptr += stride_dA_cs_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_ptr += stride_ddA_cs_head",
      "            ddA_cumsum_ptrs += stride_ddA_cs_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    mask = offs_m[:, None] >= offs_n[None, :]",
      "    acc = tl.where(mask, acc, 0.0)",
      "    dcb_ptr += (",
      "        pid_b * stride_dcb_batch",
      "        + pid_c * stride_dcb_chunk",
      "        + pid_g * stride_dcb_group",
      "        + pid_s * stride_dcb_split",
      "    )",
      "    dcb_ptrs = dcb_ptr + (",
      "        offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n",
      "    )",
      "    tl.store(",
      "        dcb_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_dcb_kernel(x_ptr, dout_ptr, cb_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, dcb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, nheads, nheads_per_program, ngroups, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_dcb_batch, stride_dcb_chunk, stride_dcb_split, stride_dcb_group, stride_dcb_csize_m, stride_dcb_csize_n, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, HAS_DDA_CS: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_sg = tl.program_id(axis=2)\npid_s = pid_sg // ngroups\npid_g = pid_sg - pid_s * ngroups\nnum_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head\nif HAS_DDA_CS:\n    cb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_g * stride_cb_head\n    ddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_head + pid_m * stride_ddA_cs_csize_m\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_n * stride_dt_csize\nif HAS_DDA_CS:\n    cb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\nif pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n    dcb_ptr += pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + pid_g * stride_dcb_group + pid_s * stride_dcb_split\n    dcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n)\n    tl.store(dcb_ptrs, tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=dcb_ptr.dtype.element_ty), mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))\n    return\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nchunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif HAS_DDA_CS:\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size), other=0.0).to(tl.float32)\nnheads_iter = min(nheads_per_program, nheads // ngroups - pid_s * nheads_per_program)\nfor h in range(nheads_iter):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n), other=0.0)\n    dcb = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\n    dcb *= dt_n\n    dA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\n    dA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size_limit, other=0.0).to(tl.float32)\n    dcb *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    if HAS_DDA_CS:\n        tl.static_assert(not HAS_SEQ_IDX, 'HAS_SEQ_IDX not supported with HAS_DDA_CS yet')\n        ddA_cs = dcb * cb\n        mask = offs_m[:, None] >= offs_n[None, :] + 1\n        ddA_cs = tl.where(mask, ddA_cs, 0.0)\n        ddA_cs = tl.cumsum(ddA_cs, axis=1)\n        ddA_cs = tl.where(mask, ddA_cs, 0.0)\n        ddA_cs = tl.sum(ddA_cs, axis=0)\n        tl.store(ddA_cumsum_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)\n        tl.store(ddA_cumsum_ptr, 0.0)\n    acc += dcb\n    dout_ptrs += stride_dout_head\n    x_ptrs += stride_x_head\n    dt_ptrs += stride_dt_head\n    dA_cumsum_ptr += stride_dA_cs_head\n    if HAS_DDA_CS:\n        ddA_cumsum_ptr += stride_ddA_cs_head\n        ddA_cumsum_ptrs += stride_ddA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_SEQ_IDX:\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen, mask=offs_n < chunk_size_limit, other=-2)\n    acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\nmask = offs_m[:, None] >= offs_n[None, :]\nacc = tl.where(mask, acc, 0.0)\ndcb_ptr += pid_b * stride_dcb_batch + pid_c * stride_dcb_chunk + pid_g * stride_dcb_group + pid_s * stride_dcb_split\ndcb_ptrs = dcb_ptr + (offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n)\ntl.store(dcb_ptrs, acc, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_unstable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SUBTRACT_DDTDT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_unstable_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dt_ptr,",
      "    ddt_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    ddA_cumsum_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    SUBTRACT_DDTDT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    ddA_cs = tl.sum(dout * out, axis=1)",
      "    if SUBTRACT_DDTDT:",
      "        dt = tl.load(",
      "            dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddt = tl.load(",
      "            ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddA_cs -= dt * ddt",
      "    tl.store(",
      "        ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size",
      "    )"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_ddAcs_unstable_kernel(dout_ptr, out_ptr, dt_ptr, ddt_ptr, x_ptr, D_ptr, ddA_cumsum_ptr, dD_ptr, chunk_size, hdim, batch, seqlen, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_out_batch, stride_out_seqlen, stride_out_head, stride_out_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_D_head, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, stride_dD_batch, stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, SUBTRACT_DDTDT: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\nout_ptr += pid_b * stride_out_batch + pid_c * chunk_size * stride_out_seqlen + pid_h * stride_out_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\nif HAS_D:\n    x_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\n    dD_ptr += pid_b * stride_dD_batch + pid_c * stride_dD_chunk + pid_h * stride_dD_head + pid_m * stride_dD_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\nout_ptrs = out_ptr + (offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim)\nif HAS_D:\n    x_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\n    if D_HAS_HDIM:\n        dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nout = tl.load(out_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nif HAS_D:\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        dD = tl.sum(dout * x, axis=0)\n        tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        dD = tl.sum(dout * x)\n        tl.store(dD_ptr, dD)\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    out -= x * D\nddA_cs = tl.sum(dout * out, axis=1)\nif SUBTRACT_DDTDT:\n    dt = tl.load(dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    ddt = tl.load(ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    ddA_cs -= dt * ddt\ntl.store(ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_kernel_old",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 16}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 16}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddAcs_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_head",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_kernel_old(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddAcs_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddAcs_batch,",
      "    stride_ddAcs_chunk,",
      "    stride_ddAcs_head,",
      "    stride_ddAcs_csize_m,",
      "    stride_ddAcs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)",
      "",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),",
      "        other=0.0,",
      "    )",
      "    acc = tl.dot(dout, x)",
      "    cb = tl.load(",
      "        cb_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    acc *= cb",
      "    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= dt_n",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_n = tl.load(",
      "        dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "    mask = offs_m[:, None] >= offs_n[None, :] + 1",
      "    acc = tl.where(mask, acc, 0.0)",
      "    acc = tl.cumsum(acc, axis=1)",
      "    acc = tl.where(mask, acc, 0.0)",
      "    ddA_cs = tl.sum(acc, axis=0)",
      "    ddAcs_ptr += (",
      "        pid_b * stride_ddAcs_batch",
      "        + pid_c * stride_ddAcs_chunk",
      "        + pid_h * stride_ddAcs_head",
      "        + pid_m * stride_ddAcs_csize_m",
      "    )",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n",
      "    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)",
      "    tl.store(ddAcs_ptr, 0.0)"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_kernel_old(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddAcs_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddAcs_batch, stride_ddAcs_chunk, stride_ddAcs_head, stride_ddAcs_csize_m, stride_ddAcs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_n * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nchunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\nx = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n), other=0.0)\nacc = tl.dot(dout, x)\ncb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size), other=0.0).to(tl.float32)\nacc *= cb\ndt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\nacc *= dt_n\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndA_cs_n = tl.load(dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0).to(tl.float32)\nacc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\nmask = offs_m[:, None] >= offs_n[None, :] + 1\nacc = tl.where(mask, acc, 0.0)\nacc = tl.cumsum(acc, axis=1)\nacc = tl.where(mask, acc, 0.0)\nddA_cs = tl.sum(acc, axis=0)\nddAcs_ptr += pid_b * stride_ddAcs_batch + pid_c * stride_ddAcs_chunk + pid_h * stride_ddAcs_head + pid_m * stride_ddAcs_csize_m\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n\ntl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)\ntl.store(ddAcs_ptr, 0.0)"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M",
      "",
      "    for start_n in range(lo, hi, BLOCK_SIZE_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - start_n),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_n[None, :] < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize,",
      "            mask=offs_n < chunk_size - start_n,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_ddAcs_stable_kernel(x_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, cb_ptr, ddA_cumsum_ptr, chunk_size, hdim, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_n, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize_m, stride_ddA_cs_csize_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head + pid_m * stride_ddA_cs_csize_m\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nx_ptrs = x_ptr + (offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim)\ndt_ptrs = dt_ptr + offs_n * stride_dt_csize\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n)\nddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n\ntl.store(ddA_cumsum_ptr, 0.0)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nrowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nlo, hi = (0, (pid_m + 1) * BLOCK_SIZE_M)\nfor start_n in range(lo, hi, BLOCK_SIZE_N):\n    start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)\n    x = tl.load(x_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit - start_n), other=0.0)\n    acc = tl.dot(dout, x)\n    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(tl.float32)\n    acc *= dt_n\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size - start_n), other=0.0).to(tl.float32)\n    acc *= cb\n    dA_cs_n = tl.load(dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize, mask=offs_n < chunk_size - start_n, other=0.0).to(tl.float32)\n    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])\n    mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1\n    acc = tl.where(mask, acc, 0.0)\n    rowsum_new = rowsum + tl.sum(acc, axis=1)\n    acc = rowsum[:, None] + tl.cumsum(acc, axis=1)\n    rowsum = rowsum_new\n    acc = tl.where(mask, acc, 0.0)\n    ddA_cs = tl.sum(acc, axis=0)\n    tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, ddA_cs, mask=offs_n < chunk_size - start_n - 1)\n    x_ptrs += BLOCK_SIZE_N * stride_x_seqlen\n    dt_ptrs += BLOCK_SIZE_N * stride_dt_csize\n    cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n\n    ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n\nfor start_n in range(hi, chunk_size, BLOCK_SIZE_N):\n    tl.store(ddAcs_ptrs + stride_ddA_cs_csize_n, tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32), mask=offs_n < chunk_size - start_n - 1)\n    ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_prev_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_prev_kernel(",
      "    dout_ptr,",
      "    prev_states_ptr,",
      "    C_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_batch,",
      "    stride_prev_states_chunk,",
      "    stride_prev_states_head,",
      "    stride_prev_states_hdim,",
      "    stride_prev_states_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_prev_states_batch",
      "        + pid_c * stride_prev_states_chunk",
      "        + pid_h * stride_prev_states_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_prev_states_dstate",
      "        + offs_k[:, None] * stride_prev_states_hdim",
      "    )",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    )",
      "    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)",
      "    acc = tl.dot(dout, prev_states)",
      "    c = tl.load(",
      "        C_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddA_cs = tl.sum(acc * c, axis=1)",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_m)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "    ddA_cs *= scale",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)"
    ],
    "file": "codes/129.py",
    "header": "def _chunk_scan_bwd_ddAcs_prev_kernel(dout_ptr, prev_states_ptr, C_ptr, dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, dstate, hdim, batch, seqlen, nchunks, nheads_ngroups_ratio, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_prev_states_batch, stride_prev_states_chunk, stride_prev_states_head, stride_prev_states_hdim, stride_prev_states_dstate, stride_C_batch, stride_C_seqlen, stride_C_head, stride_C_dstate, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\nprev_states_ptr += pid_b * stride_prev_states_batch + pid_c * stride_prev_states_chunk + pid_h * stride_prev_states_head\nC_ptr += pid_b * stride_C_batch + pid_c * chunk_size * stride_C_seqlen + pid_h // nheads_ngroups_ratio * stride_C_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim)\nprev_states_ptrs = prev_states_ptr + (offs_n[None, :] * stride_prev_states_dstate + offs_k[:, None] * stride_prev_states_hdim)\nC_ptrs = C_ptr + (offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\nprev_states = tl.load(prev_states_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\nprev_states = prev_states.to(dout_ptrs.dtype.element_ty)\nacc = tl.dot(dout, prev_states)\nc = tl.load(C_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\nddA_cs = tl.sum(acc * c, axis=1)\ndA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\nif not HAS_SEQ_IDX:\n    scale = tl.exp(dA_cs_m)\nif HAS_SEQ_IDX:\n    seq_idx_prev = tl.load(seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0)\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)\nddA_cs *= scale\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\ntl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    reduction = tl.load(reduction_ptr)",
      "    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction",
      "",
      "    tl.store(",
      "        z_block_ptr, (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty)",
      "    )",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))",
      "    lse = tl.where(",
      "        y != ignore_index,",
      "        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,",
      "        zero_lse_constant,",
      "    )",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "codes/169.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nnum_idx_N, num_idx_V_group = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_V_group = tl.swizzle2d(idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)\nV_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nreduction = tl.load(reduction_ptr)\nmask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]\nloss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction\ntl.store(z_block_ptr, (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty))\nm = tl.max(z_j_to_k, 1)\nzero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))\nlse = tl.where(y != ignore_index, tl.log(tl.sum(tl.exp(z_j_to_k - m[:, None]), axis=1)) + m, zero_lse_constant)\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\ntl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\ntl.store(lse_row_ptr, lse[:, None])"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == V_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            A_v.type.element_ty",
      "        )",
      "",
      "        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
    ],
    "file": "codes/169.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(z_nv_ptr, y_ptr, A_t_ptr, x_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization: tl.constexpr, fp32_grad_accumulators: tl.constexpr, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0) // SPLIT_V\nidx_H = tl.program_id(axis=1)\nidx_V_tile = tl.program_id(axis=0) % SPLIT_V\nnum_idx_N, num_idx_H = (tl.num_programs(0) - triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N, tl.num_programs(1))\nidx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE)\nV_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, V_split_offset), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, V_split_offset), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range, eviction_policy='evict_last')\nlse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), eviction_policy='evict_last')\nreduction = tl.load(reduction_ptr)\nacc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)\nfor _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n    mask = y[:, None] == V_range[None, :]\n    A_v = tl.load(A_t_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    if z_regularization > 0:\n        softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n    z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)\n    valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(A_v.type.element_ty)\n    x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    V_range += V_BLOCK_SIZE\nif SPLIT_V == 1:\n    x_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))\nelse:\n    row_n = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    x_grad_simple_ptr = x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n    tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            x_ptr.type.element_ty",
      "        )",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
    ],
    "file": "codes/169.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(z_nv_ptr, y_ptr, x_ptr, A_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization: tl.constexpr, fp32_grad_accumulators: tl.constexpr, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\nidx_H = tl.program_id(axis=1)\nidx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\nnum_idx_V, num_idx_H = (tl.num_programs(0) - N_group // N_BLOCK_SIZE * SPLIT_V, tl.num_programs(1))\nidx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE)\nN_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(N_split_offset, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nreduction = tl.load(reduction_ptr)\nacc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty\nA_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)\nfor _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range, eviction_policy='evict_last')\n    lse = tl.load(lse_ptr + N_range, eviction_policy='evict_last')\n    mask = y[:, None] == V_range[None, :]\n    x_chunk = tl.load(x_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    if z_regularization > 0:\n        softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n    z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)\n    valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(x_ptr.type.element_ty)\n    A_grad_acc = tl.dot(x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n    N_range += N_BLOCK_SIZE\nif SPLIT_N == 1:\n    A_grad_T_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\n    if idx_N_group > 0:\n        tl.store(A_grad_T_block_ptr, tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty))\n    else:\n        tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))\nelse:\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    A_grad_T_simple_ptr = A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n    tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_dispatcher",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=bwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad",
        "annotation": null
      },
      {
        "name": "At_grad",
        "annotation": null
      },
      {
        "name": "lse_global",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_dispatcher(",
      "    logits_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_t_ptr,",
      "    x_grad,",
      "    At_grad,",
      "    lse_global,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 128,",
      "    N_BLOCK_SIZE: tl.constexpr = 128,",
      "    H_BLOCK_SIZE: tl.constexpr = 128,",
      "    GROUP_SIZE: tl.constexpr = 32,",
      "    SPLIT_N: tl.constexpr = 2,",
      "    SPLIT_V: tl.constexpr = 2,",
      "):",
      "    idx_NV = tl.program_id(axis=0)",
      "    tl.static_print(",
      "        V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V",
      "    )",
      "    if idx_NV < (N_group // N_BLOCK_SIZE * SPLIT_V):",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "            logits_ptr,",
      "            y_ptr,",
      "            A_t_ptr,",
      "            x_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            z_regularization,",
      "            fp32_grad_accumulators,",
      "            reduction_ptr,",
      "            ignore_index,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N,",
      "            SPLIT_V,",
      "        )",
      "    else:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "            logits_ptr,",
      "            y_ptr,",
      "            x_ptr,",
      "            At_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            z_regularization,",
      "            fp32_grad_accumulators,",
      "            reduction_ptr,",
      "            ignore_index,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N,",
      "            SPLIT_V,",
      "        )"
    ],
    "file": "codes/169.py",
    "header": "def linear_xent_bwd_dispatcher(logits_ptr, y_ptr, x_ptr, A_t_ptr, x_grad, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization: tl.constexpr, fp32_grad_accumulators: tl.constexpr, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 128, N_BLOCK_SIZE: tl.constexpr = 128, H_BLOCK_SIZE: tl.constexpr = 128, GROUP_SIZE: tl.constexpr = 32, SPLIT_N: tl.constexpr = 2, SPLIT_V: tl.constexpr = 2):",
    "body": "idx_NV = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)\nif idx_NV < N_group // N_BLOCK_SIZE * SPLIT_V:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dx(logits_ptr, y_ptr, A_t_ptr, x_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization, fp32_grad_accumulators, reduction_ptr, ignore_index, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)\nelse:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dA(logits_ptr, y_ptr, x_ptr, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization, fp32_grad_accumulators, reduction_ptr, ignore_index, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)"
  },
  {
    "name": "logsumexp_reduction_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'N_BLOCK_SIZE': 2}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 4}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=16, num_stages=5), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=1)], key=['N_group', 'V', 'V_BLOCK_SIZE'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "lse_local_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "lse_sum_ptr",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_reduction_kernel(",
      "    lse_local_ptr,",
      "    lse_global_ptr,",
      "    lse_sum_ptr,",
      "    reduction_ptr,",
      "    z_regularization: tl.constexpr,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr = 32,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_local_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_local = tl.load(lse_row_ptr)",
      "    m = tl.max(lse_local, 1)",
      "    lse = tl.log(tl.sum(tl.exp((lse_local - m[:, None])), axis=1)) + m",
      "",
      "    lse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(",
      "        reduction_ptr",
      "    )",
      "",
      "    tl.atomic_add(lse_sum_ptr, lse_reduction)",
      "    tl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)"
    ],
    "file": "codes/169.py",
    "header": "def logsumexp_reduction_kernel(lse_local_ptr, lse_global_ptr, lse_sum_ptr, reduction_ptr, z_regularization: tl.constexpr, stride_lse_N, stride_lse_B, N_group, V: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr = 32):",
    "body": "idx_N = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)\nlse_row_ptr = tl.make_block_ptr(base=lse_local_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE), order=(1, 0))\nlse_local = tl.load(lse_row_ptr)\nm = tl.max(lse_local, 1)\nlse = tl.log(tl.sum(tl.exp(lse_local - m[:, None]), axis=1)) + m\nlse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(reduction_ptr)\ntl.atomic_add(lse_sum_ptr, lse_reduction)\ntl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_boundary_check': lambda args: args['size_along_dim'] % args['block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "denominator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "numerator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x1_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x2_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "z_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "z_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "size_along_dim",
        "annotation": "tl.int32"
      },
      {
        "name": "output_y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "output_x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    denominator_ptr: tl.tensor,",
      "    numerator_ptr: tl.tensor,",
      "    x1_ptr: tl.tensor,",
      "    x2_ptr: tl.tensor,",
      "    z_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    z_stride: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    eps: tl.float32,",
      "    size_along_dim: tl.int32,",
      "    output_y_size: tl.int32,",
      "    output_x_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    block_size: tl.constexpr,",
      "    require_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "",
      "    num_output_y = pid // output_x_size",
      "    num_output_x = pid % output_x_size",
      "",
      "    x1_block_ptr = tl.make_block_ptr(",
      "        x1_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    x2_block_ptr = tl.make_block_ptr(",
      "        x2_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    denominator_block_ptr = tl.make_block_ptr(",
      "        denominator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    numerator_block_ptr = tl.make_block_ptr(",
      "        numerator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "",
      "    denominator_accumulation1 = tl.zeros((block_size, 1, 1), tl.float32)",
      "    denominator_accumulation2 = tl.zeros((block_size, 1, 1), tl.float32)",
      "    numerator_accumulation = tl.zeros((block_size, 1, 1), tl.float32)",
      "",
      "    for _ in range(0, size_along_dim, block_size):",
      "        if require_boundary_check:",
      "            x1 = tl.load(x1_block_ptr, boundary_check=(0,), padding_option=\"zero\")",
      "            x2 = tl.load(x2_block_ptr, boundary_check=(0,), padding_option=\"zero\")",
      "        else:",
      "            x1 = tl.load(x1_block_ptr)",
      "            x2 = tl.load(x2_block_ptr)",
      "",
      "        denominator_accumulation1 += x1 * x1",
      "        denominator_accumulation2 += x2 * x2",
      "        numerator_accumulation += x1 * x2",
      "",
      "        x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))",
      "        x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))",
      "",
      "    denominator1 = tl.sum(denominator_accumulation1, 0)",
      "    denominator2 = tl.sum(denominator_accumulation2, 0)",
      "    denominator = tl.sqrt(denominator1) * tl.sqrt(denominator2)",
      "",
      "    numerator = tl.sum(numerator_accumulation, 0)",
      "    output = numerator / tl.math.max(denominator, eps)",
      "",
      "    tl.store(output_block_ptr, output.to(dtype))",
      "    tl.store(denominator_block_ptr, denominator.to(dtype))",
      "    tl.store(numerator_block_ptr, numerator.to(dtype))"
    ],
    "file": "codes/514.py",
    "header": "def forward(output_ptr: tl.tensor, denominator_ptr: tl.tensor, numerator_ptr: tl.tensor, x1_ptr: tl.tensor, x2_ptr: tl.tensor, z_size: tl.int32, y_size: tl.int32, x_size: tl.int32, z_stride: tl.int32, y_stride: tl.int32, x_stride: tl.int32, eps: tl.float32, size_along_dim: tl.int32, output_y_size: tl.int32, output_x_size: tl.int32, dtype: tl.constexpr, block_size: tl.constexpr, require_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_output_y = pid // output_x_size\nnum_output_x = pid % output_x_size\nx1_block_ptr = tl.make_block_ptr(x1_ptr, shape=(z_size, y_size, x_size), strides=(z_stride, y_stride, x_stride), offsets=(0, num_output_y, num_output_x), block_shape=(block_size, 1, 1), order=(2, 1, 0))\nx2_block_ptr = tl.make_block_ptr(x2_ptr, shape=(z_size, y_size, x_size), strides=(z_stride, y_stride, x_stride), offsets=(0, num_output_y, num_output_x), block_shape=(block_size, 1, 1), order=(2, 1, 0))\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(output_y_size, output_x_size), strides=(output_x_size, 1), offsets=(num_output_y, num_output_x), block_shape=(1, 1), order=(1, 0))\ndenominator_block_ptr = tl.make_block_ptr(denominator_ptr, shape=(output_y_size, output_x_size), strides=(output_x_size, 1), offsets=(num_output_y, num_output_x), block_shape=(1, 1), order=(1, 0))\nnumerator_block_ptr = tl.make_block_ptr(numerator_ptr, shape=(output_y_size, output_x_size), strides=(output_x_size, 1), offsets=(num_output_y, num_output_x), block_shape=(1, 1), order=(1, 0))\ndenominator_accumulation1 = tl.zeros((block_size, 1, 1), tl.float32)\ndenominator_accumulation2 = tl.zeros((block_size, 1, 1), tl.float32)\nnumerator_accumulation = tl.zeros((block_size, 1, 1), tl.float32)\nfor _ in range(0, size_along_dim, block_size):\n    if require_boundary_check:\n        x1 = tl.load(x1_block_ptr, boundary_check=(0,), padding_option='zero')\n        x2 = tl.load(x2_block_ptr, boundary_check=(0,), padding_option='zero')\n    else:\n        x1 = tl.load(x1_block_ptr)\n        x2 = tl.load(x2_block_ptr)\n    denominator_accumulation1 += x1 * x1\n    denominator_accumulation2 += x2 * x2\n    numerator_accumulation += x1 * x2\n    x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))\n    x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))\ndenominator1 = tl.sum(denominator_accumulation1, 0)\ndenominator2 = tl.sum(denominator_accumulation2, 0)\ndenominator = tl.sqrt(denominator1) * tl.sqrt(denominator2)\nnumerator = tl.sum(numerator_accumulation, 0)\noutput = numerator / tl.math.max(denominator, eps)\ntl.store(output_block_ptr, output.to(dtype))\ntl.store(denominator_block_ptr, denominator.to(dtype))\ntl.store(numerator_block_ptr, numerator.to(dtype))"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_boundary_check': lambda args: args['size_along_dim'] % args['block_size']})"
    ],
    "args": [
      {
        "name": "grad_x1_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_x2_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "denominator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "numerator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x1_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x2_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "z_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "z_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "size_along_dim",
        "annotation": "tl.int32"
      },
      {
        "name": "output_y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "output_x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_x1_ptr: tl.tensor,",
      "    grad_x2_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    denominator_ptr: tl.tensor,",
      "    numerator_ptr: tl.tensor,",
      "    x1_ptr: tl.tensor,",
      "    x2_ptr: tl.tensor,",
      "    z_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    z_stride: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    size_along_dim: tl.int32,",
      "    output_y_size: tl.int32,",
      "    output_x_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    block_size: tl.constexpr,",
      "    require_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_output_y = pid // output_x_size",
      "    num_output_x = pid % output_x_size",
      "",
      "    grad_x1_block_ptr = tl.make_block_ptr(",
      "        grad_x1_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    grad_x2_block_ptr = tl.make_block_ptr(",
      "        grad_x2_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    x1_block_ptr = tl.make_block_ptr(",
      "        x1_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    x2_block_ptr = tl.make_block_ptr(",
      "        x2_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    denominator_block_ptr = tl.make_block_ptr(",
      "        denominator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    numerator_block_ptr = tl.make_block_ptr(",
      "        numerator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for _ in range(0, size_along_dim, block_size):",
      "        if require_boundary_check:",
      "            x1 = tl.load(",
      "                x1_block_ptr, boundary_check=(0,), padding_option=\"zero\"",
      "            ).to(tl.float32)",
      "            x2 = tl.load(",
      "                x2_block_ptr, boundary_check=(0,), padding_option=\"zero\"",
      "            ).to(tl.float32)",
      "        else:",
      "            x1 = tl.load(x1_block_ptr)",
      "            x2 = tl.load(x2_block_ptr)",
      "",
      "        denominator = tl.load(denominator_block_ptr)",
      "        numerator = tl.load(numerator_block_ptr)",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "        squared_x1 = x1 * x1",
      "        squared_x2 = x2 * x2",
      "        squared_x1_sum = tl.sum(squared_x1, 0)",
      "        squared_x2_sum = tl.sum(squared_x2, 0)",
      "",
      "        grad_denominator = (",
      "            grad_output * numerator * (-1 / (denominator * denominator))",
      "        )",
      "",
      "        grad_mul1 = grad_denominator * tl.sqrt(tl.sum(squared_x2, 0))",
      "        grad_mul2 = grad_denominator * tl.sqrt(tl.sum(squared_x1, 0))",
      "",
      "        grad_sqrt1 = grad_mul1 / (2 * tl.sqrt(squared_x1_sum))",
      "        grad_sqrt2 = grad_mul2 / (2 * tl.sqrt(squared_x2_sum))",
      "",
      "        grad_to_dot = grad_output / denominator",
      "",
      "        grad_x1 = (grad_sqrt1 * 2 * x1) + (grad_to_dot * x2)",
      "        grad_x2 = (grad_sqrt2 * 2 * x2) + (grad_to_dot * x1)",
      "",
      "        if require_boundary_check:",
      "            tl.store(grad_x1_block_ptr, grad_x1.to(dtype), boundary_check=(0,))",
      "            tl.store(grad_x2_block_ptr, grad_x2.to(dtype), boundary_check=(0,))",
      "        else:",
      "            tl.store(grad_x1_block_ptr, grad_x1.to(dtype))",
      "            tl.store(grad_x2_block_ptr, grad_x2.to(dtype))",
      "",
      "        x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))",
      "        x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))",
      "        grad_x1_block_ptr = tl.advance(grad_x1_block_ptr, (block_size, 0, 0))",
      "        grad_x2_block_ptr = tl.advance(grad_x2_block_ptr, (block_size, 0, 0))"
    ],
    "file": "codes/514.py",
    "header": "def backward(grad_x1_ptr: tl.tensor, grad_x2_ptr: tl.tensor, grad_output_ptr: tl.tensor, denominator_ptr: tl.tensor, numerator_ptr: tl.tensor, x1_ptr: tl.tensor, x2_ptr: tl.tensor, z_size: tl.int32, y_size: tl.int32, x_size: tl.int32, z_stride: tl.int32, y_stride: tl.int32, x_stride: tl.int32, size_along_dim: tl.int32, output_y_size: tl.int32, output_x_size: tl.int32, dtype: tl.constexpr, block_size: tl.constexpr, require_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_output_y = pid // output_x_size\nnum_output_x = pid % output_x_size\ngrad_x1_block_ptr = tl.make_block_ptr(grad_x1_ptr, shape=(z_size, y_size, x_size), strides=(z_stride, y_stride, x_stride), offsets=(0, num_output_y, num_output_x), block_shape=(block_size, 1, 1), order=(2, 1, 0))\ngrad_x2_block_ptr = tl.make_block_ptr(grad_x2_ptr, shape=(z_size, y_size, x_size), strides=(z_stride, y_stride, x_stride), offsets=(0, num_output_y, num_output_x), block_shape=(block_size, 1, 1), order=(2, 1, 0))\ngrad_output_block_ptr = tl.make_block_ptr(grad_output_ptr, shape=(output_y_size, output_x_size), strides=(output_x_size, 1), offsets=(num_output_y, num_output_x), block_shape=(1, 1), order=(1, 0))\nx1_block_ptr = tl.make_block_ptr(x1_ptr, shape=(z_size, y_size, x_size), strides=(z_stride, y_stride, x_stride), offsets=(0, num_output_y, num_output_x), block_shape=(block_size, 1, 1), order=(2, 1, 0))\nx2_block_ptr = tl.make_block_ptr(x2_ptr, shape=(z_size, y_size, x_size), strides=(z_stride, y_stride, x_stride), offsets=(0, num_output_y, num_output_x), block_shape=(block_size, 1, 1), order=(2, 1, 0))\ndenominator_block_ptr = tl.make_block_ptr(denominator_ptr, shape=(output_y_size, output_x_size), strides=(output_x_size, 1), offsets=(num_output_y, num_output_x), block_shape=(1, 1), order=(1, 0))\nnumerator_block_ptr = tl.make_block_ptr(numerator_ptr, shape=(output_y_size, output_x_size), strides=(output_x_size, 1), offsets=(num_output_y, num_output_x), block_shape=(1, 1), order=(1, 0))\nfor _ in range(0, size_along_dim, block_size):\n    if require_boundary_check:\n        x1 = tl.load(x1_block_ptr, boundary_check=(0,), padding_option='zero').to(tl.float32)\n        x2 = tl.load(x2_block_ptr, boundary_check=(0,), padding_option='zero').to(tl.float32)\n    else:\n        x1 = tl.load(x1_block_ptr)\n        x2 = tl.load(x2_block_ptr)\n    denominator = tl.load(denominator_block_ptr)\n    numerator = tl.load(numerator_block_ptr)\n    grad_output = tl.load(grad_output_block_ptr)\n    squared_x1 = x1 * x1\n    squared_x2 = x2 * x2\n    squared_x1_sum = tl.sum(squared_x1, 0)\n    squared_x2_sum = tl.sum(squared_x2, 0)\n    grad_denominator = grad_output * numerator * (-1 / (denominator * denominator))\n    grad_mul1 = grad_denominator * tl.sqrt(tl.sum(squared_x2, 0))\n    grad_mul2 = grad_denominator * tl.sqrt(tl.sum(squared_x1, 0))\n    grad_sqrt1 = grad_mul1 / (2 * tl.sqrt(squared_x1_sum))\n    grad_sqrt2 = grad_mul2 / (2 * tl.sqrt(squared_x2_sum))\n    grad_to_dot = grad_output / denominator\n    grad_x1 = grad_sqrt1 * 2 * x1 + grad_to_dot * x2\n    grad_x2 = grad_sqrt2 * 2 * x2 + grad_to_dot * x1\n    if require_boundary_check:\n        tl.store(grad_x1_block_ptr, grad_x1.to(dtype), boundary_check=(0,))\n        tl.store(grad_x2_block_ptr, grad_x2.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(grad_x1_block_ptr, grad_x1.to(dtype))\n        tl.store(grad_x2_block_ptr, grad_x2.to(dtype))\n    x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))\n    x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))\n    grad_x1_block_ptr = tl.advance(grad_x1_block_ptr, (block_size, 0, 0))\n    grad_x2_block_ptr = tl.advance(grad_x2_block_ptr, (block_size, 0, 0))"
  },
  {
    "name": "chunk_cumprod_householder_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hc_suffix",
        "annotation": null
      },
      {
        "name": "hc_prefix",
        "annotation": null
      },
      {
        "name": "hc_whole",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_cumprod_householder_fwd_kernel(",
      "    q,",
      "    q_new,",
      "    k,",
      "    k_new,",
      "    h,",
      "    hc_suffix,",
      "    hc_prefix,",
      "    hc_whole,",
      "    cu_seqlens,",
      "    split_indices,",
      "    chunk_offsets,",
      "    split_offsets,",
      "    BT: tl.constexpr,",
      "    K: tl.constexpr,",
      "    G: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_ss, i_hq = tl.program_id(0), tl.program_id(1)",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "        boh = i_n * tl.cdiv(T, BT)",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    NT_small = tl.cdiv(min(S, T - i_s * S), BT)",
      "    stride_h = H * K * K",
      "",
      "    h += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_prefix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_whole += ((boh_large + i_s) * H + i_h) * K * K",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    q_new += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "",
      "    p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "    b_h = tl.zeros([BK, BK], dtype=tl.float32)",
      "    b_h += tl.load(p_h, boundary_check=(0, 1))",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_hc_prefix = tl.make_block_ptr(hc_prefix, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "    tl.store(",
      "        p_hc_prefix,",
      "        tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_prefix.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    for i_t_small in range(1, NT_small):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q - tl.dot(b_q, b_h.to(b_q.dtype))).to(b_q.dtype)",
      "        p_q_new = tl.make_block_ptr(",
      "            q_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))",
      "        if HQ % G == 0:",
      "            p_hc_prefix = tl.make_block_ptr(",
      "                hc_prefix + i_t_small * stride_h,",
      "                (K, K),",
      "                (K, 1),",
      "                (0, 0),",
      "                (BK, BK),",
      "                (1, 0),",
      "            )",
      "            tl.store(",
      "                p_hc_prefix, b_h.to(hc_prefix.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "        p_h_new = tl.make_block_ptr(",
      "            h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_h_new = tl.load(p_h_new, boundary_check=(0, 1))",
      "        b_h = b_h + b_h_new - tl.dot(b_h_new, b_h.to(b_h_new.dtype))",
      "",
      "    tl.debug_barrier()",
      "",
      "    if HQ % G == 0:",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_s * S + (NT_small - 1) * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        p_k_new = tl.make_block_ptr(",
      "            k_new,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_s * S + (NT_small - 1) * BT, 0),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))",
      "        p_hc_suffix = tl.make_block_ptr(",
      "            hc_suffix + (NT_small - 1) * stride_h,",
      "            (K, K),",
      "            (K, 1),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (1, 0),",
      "        )",
      "        tl.store(",
      "            p_hc_suffix,",
      "            tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_suffix.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "        p_h = tl.make_block_ptr(",
      "            h + (NT_small - 1) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_h = tl.zeros([BK, BK], dtype=tl.float32)",
      "        b_h += tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        for i_t_small in range(NT_small - 2, -1, -1):",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_k = (b_k - tl.dot(b_k, tl.trans(b_h).to(b_k.dtype))).to(b_k.dtype)",
      "            p_k_new = tl.make_block_ptr(",
      "                k_new,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_s * S + i_t_small * BT, 0),",
      "                (BT, BK),",
      "                (1, 0),",
      "            )",
      "            tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))",
      "            p_hc_suffix = tl.make_block_ptr(",
      "                hc_suffix + i_t_small * stride_h,",
      "                (K, K),",
      "                (K, 1),",
      "                (0, 0),",
      "                (BK, BK),",
      "                (1, 0),",
      "            )",
      "            tl.store(",
      "                p_hc_suffix, b_h.to(hc_suffix.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "            p_h_new = tl.make_block_ptr(",
      "                h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "            )",
      "            b_h_new = tl.load(p_h_new, boundary_check=(0, 1))",
      "            b_h = b_h + b_h_new - tl.dot(b_h.to(b_h_new.dtype), b_h_new)",
      "",
      "        p_hc_whole = tl.make_block_ptr(",
      "            hc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        tl.store(p_hc_whole, b_h.to(hc_whole.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/405.py",
    "header": "def chunk_cumprod_householder_fwd_kernel(q, q_new, k, k_new, h, hc_suffix, hc_prefix, hc_whole, cu_seqlens, split_indices, chunk_offsets, split_offsets, BT: tl.constexpr, K: tl.constexpr, G: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, BK: tl.constexpr, T: tl.constexpr, S: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_ss, i_hq = (tl.program_id(0), tl.program_id(1))\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_s = (tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(split_indices + i_ss * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NS = tl.cdiv(T, S)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    boh_large = tl.load(split_offsets + i_n).to(tl.int32)\nelse:\n    NS = tl.cdiv(T, S)\n    i_n, i_s = (i_ss // NS, i_ss % NS)\n    bos, eos = (i_n * T, i_n * T + T)\n    boh = i_n * tl.cdiv(T, BT)\n    boh_large = i_n * tl.cdiv(T, S)\nNT_small = tl.cdiv(min(S, T - i_s * S), BT)\nstride_h = H * K * K\nh += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\nhc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\nhc_prefix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K\nhc_whole += ((boh_large + i_s) * H + i_h) * K * K\nq += (bos * HQ + i_hq) * K\nq_new += (bos * HQ + i_hq) * K\nk += (bos * H + i_h) * K\nk_new += (bos * H + i_h) * K\np_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\nb_h = tl.zeros([BK, BK], dtype=tl.float32)\nb_h += tl.load(p_h, boundary_check=(0, 1))\np_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\np_q_new = tl.make_block_ptr(q_new, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0))\ntl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))\np_hc_prefix = tl.make_block_ptr(hc_prefix, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\ntl.store(p_hc_prefix, tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_prefix.dtype.element_ty), boundary_check=(0, 1))\nfor i_t_small in range(1, NT_small):\n    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q - tl.dot(b_q, b_h.to(b_q.dtype))).to(b_q.dtype)\n    p_q_new = tl.make_block_ptr(q_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))\n    if HQ % G == 0:\n        p_hc_prefix = tl.make_block_ptr(hc_prefix + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        tl.store(p_hc_prefix, b_h.to(hc_prefix.dtype.element_ty), boundary_check=(0, 1))\n    p_h_new = tl.make_block_ptr(h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_h_new = tl.load(p_h_new, boundary_check=(0, 1))\n    b_h = b_h + b_h_new - tl.dot(b_h_new, b_h.to(b_h_new.dtype))\ntl.debug_barrier()\nif HQ % G == 0:\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_s * S + (NT_small - 1) * BT, 0), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    p_k_new = tl.make_block_ptr(k_new, (T, K), (H * K, 1), (i_s * S + (NT_small - 1) * BT, 0), (BT, BK), (1, 0))\n    tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))\n    p_hc_suffix = tl.make_block_ptr(hc_suffix + (NT_small - 1) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    tl.store(p_hc_suffix, tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_suffix.dtype.element_ty), boundary_check=(0, 1))\n    p_h = tl.make_block_ptr(h + (NT_small - 1) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_h = tl.zeros([BK, BK], dtype=tl.float32)\n    b_h += tl.load(p_h, boundary_check=(0, 1))\n    for i_t_small in range(NT_small - 2, -1, -1):\n        p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_k = (b_k - tl.dot(b_k, tl.trans(b_h).to(b_k.dtype))).to(b_k.dtype)\n        p_k_new = tl.make_block_ptr(k_new, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0))\n        tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))\n        p_hc_suffix = tl.make_block_ptr(hc_suffix + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        tl.store(p_hc_suffix, b_h.to(hc_suffix.dtype.element_ty), boundary_check=(0, 1))\n        p_h_new = tl.make_block_ptr(h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        b_h_new = tl.load(p_h_new, boundary_check=(0, 1))\n        b_h = b_h + b_h_new - tl.dot(b_h.to(b_h_new.dtype), b_h_new)\n    p_hc_whole = tl.make_block_ptr(hc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    tl.store(p_hc_whole, b_h.to(hc_whole.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_ALPHA': lambda args: args['alpha'] is not None, 'HAS_BETA': lambda args: args['beta'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BM': 128, 'BK': 64, 'BN': 256, 'G': 4}, num_stages=3, num_warps=8), triton.Config({'BM': 64, 'BK': 32, 'BN': 256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 32, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 32, 'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 32, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=5, num_warps=2)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "c",
        "annotation": null
      },
      {
        "name": "input",
        "annotation": null
      },
      {
        "name": "alpha",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_ab",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cb",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_INPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ALPHA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BETA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ALLOW_TF32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "X_DIM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a,",
      "    b,",
      "    c,",
      "    input,",
      "    alpha,",
      "    beta,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_ab,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cb,",
      "    stride_cm,",
      "    stride_cn,",
      "    BM: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BN: tl.constexpr,",
      "    G: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_INPUT: tl.constexpr,",
      "    HAS_ALPHA: tl.constexpr,",
      "    HAS_BETA: tl.constexpr,",
      "    ALLOW_TF32: tl.constexpr,",
      "    X_DIM: tl.constexpr = 1,",
      "):",
      "",
      "    i_b, i_m, i_n = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    NM, NN = tl.num_programs(1), tl.num_programs(2)",
      "    i_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)",
      "",
      "    a_batch_ptr = a + i_b * stride_ab",
      "    o_am = (i_m * BM + tl.arange(0, BM)) % M",
      "    o_bn = (i_n * BN + tl.arange(0, BN)) % N",
      "    o_k = tl.arange(0, BK)",
      "",
      "    p_a = a_batch_ptr + (o_am[:, None] * stride_am + o_k[None, :] * stride_ak)",
      "    p_b = b + (o_k[:, None] * stride_bk + o_bn[None, :] * stride_bn)",
      "",
      "    b_acc = tl.zeros((BM, BN), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BK)):",
      "",
      "        b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)",
      "        b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)",
      "",
      "        b_acc = tl.dot(b_a, b_b, acc=b_acc, allow_tf32=ALLOW_TF32)",
      "",
      "        p_a += BK * stride_ak",
      "        p_b += BK * stride_bk",
      "",
      "    o_cm = i_m * BM + tl.arange(0, BM)",
      "    o_cn = i_n * BN + tl.arange(0, BN)",
      "    mask = (o_cm[:, None] < M) & (o_cn[None, :] < N)",
      "",
      "    b_c = b_acc",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        b_c = leaky_relu(b_c)",
      "    elif ACTIVATION == \"relu\":",
      "        b_c = relu(b_c)",
      "    elif ACTIVATION == \"sigmoid\":",
      "        b_c = sigmoid(b_c)",
      "    elif ACTIVATION == \"tanh\":",
      "        b_c = tanh(b_c)",
      "",
      "    if HAS_ALPHA:",
      "        b_c *= tl.load(alpha)",
      "",
      "    if HAS_INPUT:",
      "        p_i = (",
      "            input",
      "            + (stride_cm * o_cm[:, None] if X_DIM == 2 else 0)",
      "            + stride_cn * o_cn[None, :]",
      "        )",
      "        mask_p = (o_cn[None, :] < N) if X_DIM == 1 else mask",
      "        b_i = tl.load(p_i, mask=mask_p, other=0.0).to(tl.float32)",
      "        if HAS_BETA:",
      "            b_i *= tl.load(beta)",
      "        b_c += b_i",
      "",
      "    c_batch_ptr = c + i_b * stride_cb",
      "    p_c = c_batch_ptr + stride_cm * o_cm[:, None] + stride_cn * o_cn[None, :]",
      "    tl.store(p_c, b_c.to(c.dtype.element_ty), mask=mask)"
    ],
    "file": "codes/431.py",
    "header": "def matmul_kernel(a, b, c, input, alpha, beta, M, N, K, stride_ab, stride_am, stride_ak, stride_bk, stride_bn, stride_cb, stride_cm, stride_cn, BM: tl.constexpr, BK: tl.constexpr, BN: tl.constexpr, G: tl.constexpr, ACTIVATION: tl.constexpr, HAS_INPUT: tl.constexpr, HAS_ALPHA: tl.constexpr, HAS_BETA: tl.constexpr, ALLOW_TF32: tl.constexpr, X_DIM: tl.constexpr = 1):",
    "body": "i_b, i_m, i_n = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNM, NN = (tl.num_programs(1), tl.num_programs(2))\ni_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)\na_batch_ptr = a + i_b * stride_ab\no_am = (i_m * BM + tl.arange(0, BM)) % M\no_bn = (i_n * BN + tl.arange(0, BN)) % N\no_k = tl.arange(0, BK)\np_a = a_batch_ptr + (o_am[:, None] * stride_am + o_k[None, :] * stride_ak)\np_b = b + (o_k[:, None] * stride_bk + o_bn[None, :] * stride_bn)\nb_acc = tl.zeros((BM, BN), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BK)):\n    b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)\n    b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)\n    b_acc = tl.dot(b_a, b_b, acc=b_acc, allow_tf32=ALLOW_TF32)\n    p_a += BK * stride_ak\n    p_b += BK * stride_bk\no_cm = i_m * BM + tl.arange(0, BM)\no_cn = i_n * BN + tl.arange(0, BN)\nmask = (o_cm[:, None] < M) & (o_cn[None, :] < N)\nb_c = b_acc\nif ACTIVATION == 'leaky_relu':\n    b_c = leaky_relu(b_c)\nelif ACTIVATION == 'relu':\n    b_c = relu(b_c)\nelif ACTIVATION == 'sigmoid':\n    b_c = sigmoid(b_c)\nelif ACTIVATION == 'tanh':\n    b_c = tanh(b_c)\nif HAS_ALPHA:\n    b_c *= tl.load(alpha)\nif HAS_INPUT:\n    p_i = input + (stride_cm * o_cm[:, None] if X_DIM == 2 else 0) + stride_cn * o_cn[None, :]\n    mask_p = o_cn[None, :] < N if X_DIM == 1 else mask\n    b_i = tl.load(p_i, mask=mask_p, other=0.0).to(tl.float32)\n    if HAS_BETA:\n        b_i *= tl.load(beta)\n    b_c += b_i\nc_batch_ptr = c + i_b * stride_cb\np_c = c_batch_ptr + stride_cm * o_cm[:, None] + stride_cn * o_cn[None, :]\ntl.store(p_c, b_c.to(c.dtype.element_ty), mask=mask)"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=matmul_get_configs(), key=['M', 'N', 'K'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    start_m = pid_m * BLOCK_SIZE_M",
      "    start_n = pid_n * BLOCK_SIZE_N",
      "",
      "    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_am = tl.where(offs_am < M, offs_am, 0)",
      "    offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "",
      "    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if c_ptr.dtype.element_ty == tl.float8e4nv:",
      "        c = accumulator.to(tl.float8e4nv)",
      "    else:",
      "        c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/10.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\nstart_m = pid_m * BLOCK_SIZE_M\nstart_n = pid_n * BLOCK_SIZE_N\noffs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\noffs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\noffs_am = tl.where(offs_am < M, offs_am, 0)\noffs_bn = tl.where(offs_bn < N, offs_bn, 0)\noffs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\noffs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nif c_ptr.dtype.element_ty == tl.float8e4nv:\n    c = accumulator.to(tl.float8e4nv)\nelse:\n    c = accumulator.to(tl.float16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "matmul_kernel_tma",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=matmul_get_configs(pre_hook=matmul_tma_set_block_size_hook), key=['M', 'N', 'K', 'WARP_SPECIALIZE'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc",
        "annotation": null
      },
      {
        "name": "b_desc",
        "annotation": null
      },
      {
        "name": "c_desc",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WARP_SPECIALIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_tma(",
      "    a_desc,",
      "    b_desc,",
      "    c_desc,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    FP8_OUTPUT: tl.constexpr,",
      "    WARP_SPECIALIZE: tl.constexpr,",
      "):",
      "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M",
      "    offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in tl.range(k_tiles, warp_specialize=WARP_SPECIALIZE):",
      "        offs_k = k * BLOCK_SIZE_K",
      "        a = a_desc.load([offs_am, offs_k])",
      "        b = b_desc.load([offs_bn, offs_k])",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "    c = accumulator.to(dtype)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M",
      "    offs_cn = pid_n * BLOCK_SIZE_N",
      "    c_desc.store([offs_cm, offs_cn], c)"
    ],
    "file": "codes/10.py",
    "header": "def matmul_kernel_tma(a_desc, b_desc, c_desc, M, N, K, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, FP8_OUTPUT: tl.constexpr, WARP_SPECIALIZE: tl.constexpr):",
    "body": "dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\noffs_am = pid_m * BLOCK_SIZE_M\noffs_bn = pid_n * BLOCK_SIZE_N\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in tl.range(k_tiles, warp_specialize=WARP_SPECIALIZE):\n    offs_k = k * BLOCK_SIZE_K\n    a = a_desc.load([offs_am, offs_k])\n    b = b_desc.load([offs_bn, offs_k])\n    accumulator = tl.dot(a, b.T, accumulator)\nc = accumulator.to(dtype)\noffs_cm = pid_m * BLOCK_SIZE_M\noffs_cn = pid_n * BLOCK_SIZE_N\nc_desc.store([offs_cm, offs_cn], c)"
  },
  {
    "name": "matmul_kernel_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=matmul_get_configs(), key=['M', 'N', 'K'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tile_id_c = start_pid - NUM_SMS",
      "",
      "    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):",
      "        pid_m, pid_n = _compute_pid(",
      "            tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS",
      "        )",
      "        start_m = pid_m * BLOCK_SIZE_M",
      "        start_n = pid_n * BLOCK_SIZE_N",
      "        offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)",
      "        offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)",
      "        offs_am = tl.where(offs_am < M, offs_am, 0)",
      "        offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "        offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "        offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for ki in range(k_tiles):",
      "            offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "            a_ptrs = a_ptr + (",
      "                offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak",
      "            )",
      "            b_ptrs = b_ptr + (",
      "                offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn",
      "            )",
      "",
      "            a = tl.load(",
      "                a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0",
      "            )",
      "            b = tl.load(",
      "                b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0",
      "            )",
      "            accumulator = tl.dot(a, b, accumulator)",
      "",
      "        tile_id_c += NUM_SMS",
      "        pid_m, pid_n = _compute_pid(",
      "            tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS",
      "        )",
      "        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "        if c_ptr.dtype.element_ty == tl.float8e4nv:",
      "            c = accumulator.to(tl.float8e4nv)",
      "        else:",
      "            c = accumulator.to(tl.float16)",
      "        tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/10.py",
    "header": "def matmul_kernel_persistent(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, NUM_SMS: tl.constexpr):",
    "body": "start_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\ntile_id_c = start_pid - NUM_SMS\noffs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\nfor tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):\n    pid_m, pid_n = _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n    start_m = pid_m * BLOCK_SIZE_M\n    start_n = pid_n * BLOCK_SIZE_N\n    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\n    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\n    offs_am = tl.where(offs_am < M, offs_am, 0)\n    offs_bn = tl.where(offs_bn < N, offs_bn, 0)\n    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for ki in range(k_tiles):\n        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n        accumulator = tl.dot(a, b, accumulator)\n    tile_id_c += NUM_SMS\n    pid_m, pid_n = _compute_pid(tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n    if c_ptr.dtype.element_ty == tl.float8e4nv:\n        c = accumulator.to(tl.float8e4nv)\n    else:\n        c = accumulator.to(tl.float16)\n    tl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "matmul_kernel_tma_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=matmul_tma_persistent_get_configs(pre_hook=matmul_tma_set_block_size_hook), key=['M', 'N', 'K', 'WARP_SPECIALIZE'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc",
        "annotation": null
      },
      {
        "name": "b_desc",
        "annotation": null
      },
      {
        "name": "c_desc",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EPILOGUE_SUBTILE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WARP_SPECIALIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_tma_persistent(",
      "    a_desc,",
      "    b_desc,",
      "    c_desc,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    FP8_OUTPUT: tl.constexpr,",
      "    EPILOGUE_SUBTILE: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    WARP_SPECIALIZE: tl.constexpr,",
      "):",
      "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tile_id_c = start_pid - NUM_SMS",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    for tile_id in tl.range(",
      "        start_pid, num_tiles, NUM_SMS, flatten=True, warp_specialize=WARP_SPECIALIZE",
      "    ):",
      "        pid_m, pid_n = _compute_pid(",
      "            tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS",
      "        )",
      "        offs_am = pid_m * BLOCK_SIZE_M",
      "        offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for ki in range(k_tiles):",
      "            offs_k = ki * BLOCK_SIZE_K",
      "            a = a_desc.load([offs_am, offs_k])",
      "            b = b_desc.load([offs_bn, offs_k])",
      "            accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        tile_id_c += NUM_SMS",
      "        pid_m, pid_n = _compute_pid(",
      "            tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS",
      "        )",
      "        offs_am_c = pid_m * BLOCK_SIZE_M",
      "        offs_bn_c = pid_n * BLOCK_SIZE_N",
      "",
      "        if EPILOGUE_SUBTILE:",
      "            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))",
      "            acc = tl.permute(acc, (0, 2, 1))",
      "            acc0, acc1 = tl.split(acc)",
      "            c0 = acc0.to(dtype)",
      "            c_desc.store([offs_am_c, offs_bn_c], c0)",
      "            c1 = acc1.to(dtype)",
      "            c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)",
      "        else:",
      "            accumulator = accumulator.to(dtype)",
      "            c_desc.store([offs_am_c, offs_bn_c], accumulator)"
    ],
    "file": "codes/10.py",
    "header": "def matmul_kernel_tma_persistent(a_desc, b_desc, c_desc, M, N, K, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, FP8_OUTPUT: tl.constexpr, EPILOGUE_SUBTILE: tl.constexpr, NUM_SMS: tl.constexpr, WARP_SPECIALIZE: tl.constexpr):",
    "body": "dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\ntile_id_c = start_pid - NUM_SMS\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\nfor tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True, warp_specialize=WARP_SPECIALIZE):\n    pid_m, pid_n = _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n    offs_am = pid_m * BLOCK_SIZE_M\n    offs_bn = pid_n * BLOCK_SIZE_N\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for ki in range(k_tiles):\n        offs_k = ki * BLOCK_SIZE_K\n        a = a_desc.load([offs_am, offs_k])\n        b = b_desc.load([offs_bn, offs_k])\n        accumulator = tl.dot(a, b.T, accumulator)\n    tile_id_c += NUM_SMS\n    pid_m, pid_n = _compute_pid(tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n    offs_am_c = pid_m * BLOCK_SIZE_M\n    offs_bn_c = pid_n * BLOCK_SIZE_N\n    if EPILOGUE_SUBTILE:\n        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n        acc = tl.permute(acc, (0, 2, 1))\n        acc0, acc1 = tl.split(acc)\n        c0 = acc0.to(dtype)\n        c_desc.store([offs_am_c, offs_bn_c], c0)\n        c1 = acc1.to(dtype)\n        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)\n    else:\n        accumulator = accumulator.to(dtype)\n        c_desc.store([offs_am_c, offs_bn_c], accumulator)"
  },
  {
    "name": "matmul_kernel_descriptor_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=matmul_tma_persistent_get_configs(), key=['M', 'N', 'K', 'WARP_SPECIALIZE'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EPILOGUE_SUBTILE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WARP_SPECIALIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_descriptor_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    EPILOGUE_SUBTILE: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    WARP_SPECIALIZE: tl.constexpr,",
      "):",
      "",
      "    dtype = c_ptr.dtype.element_ty",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    a_desc = tl.make_tensor_descriptor(",
      "        a_ptr,",
      "        shape=[M, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],",
      "    )",
      "    b_desc = tl.make_tensor_descriptor(",
      "        b_ptr,",
      "        shape=[N, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],",
      "    )",
      "    c_desc = tl.make_tensor_descriptor(",
      "        c_ptr,",
      "        shape=[M, N],",
      "        strides=[N, 1],",
      "        block_shape=[",
      "            BLOCK_SIZE_M,",
      "            BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2,",
      "        ],",
      "    )",
      "",
      "    tile_id_c = start_pid - NUM_SMS",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    for tile_id in tl.range(",
      "        start_pid, num_tiles, NUM_SMS, flatten=True, warp_specialize=WARP_SPECIALIZE",
      "    ):",
      "        pid_m, pid_n = _compute_pid(",
      "            tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS",
      "        )",
      "        offs_am = pid_m * BLOCK_SIZE_M",
      "        offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for ki in range(k_tiles):",
      "            offs_k = ki * BLOCK_SIZE_K",
      "            a = a_desc.load([offs_am, offs_k])",
      "            b = b_desc.load([offs_bn, offs_k])",
      "            accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        tile_id_c += NUM_SMS",
      "        pid_m, pid_n = _compute_pid(",
      "            tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS",
      "        )",
      "        offs_cm = pid_m * BLOCK_SIZE_M",
      "        offs_cn = pid_n * BLOCK_SIZE_N",
      "",
      "        if EPILOGUE_SUBTILE:",
      "            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))",
      "            acc = tl.permute(acc, (0, 2, 1))",
      "            acc0, acc1 = tl.split(acc)",
      "            c0 = acc0.to(dtype)",
      "            c_desc.store([offs_cm, offs_cn], c0)",
      "            c1 = acc1.to(dtype)",
      "            c_desc.store([offs_cm, offs_cn + BLOCK_SIZE_N // 2], c1)",
      "        else:",
      "            c = accumulator.to(dtype)",
      "            c_desc.store([offs_cm, offs_cn], c)"
    ],
    "file": "codes/10.py",
    "header": "def matmul_kernel_descriptor_persistent(a_ptr, b_ptr, c_ptr, M, N, K, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, EPILOGUE_SUBTILE: tl.constexpr, NUM_SMS: tl.constexpr, WARP_SPECIALIZE: tl.constexpr):",
    "body": "dtype = c_ptr.dtype.element_ty\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\na_desc = tl.make_tensor_descriptor(a_ptr, shape=[M, K], strides=[K, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K])\nb_desc = tl.make_tensor_descriptor(b_ptr, shape=[N, K], strides=[K, 1], block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K])\nc_desc = tl.make_tensor_descriptor(c_ptr, shape=[M, N], strides=[N, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2])\ntile_id_c = start_pid - NUM_SMS\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\nfor tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True, warp_specialize=WARP_SPECIALIZE):\n    pid_m, pid_n = _compute_pid(tile_id, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n    offs_am = pid_m * BLOCK_SIZE_M\n    offs_bn = pid_n * BLOCK_SIZE_N\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for ki in range(k_tiles):\n        offs_k = ki * BLOCK_SIZE_K\n        a = a_desc.load([offs_am, offs_k])\n        b = b_desc.load([offs_bn, offs_k])\n        accumulator = tl.dot(a, b.T, accumulator)\n    tile_id_c += NUM_SMS\n    pid_m, pid_n = _compute_pid(tile_id_c, num_pid_in_group, num_pid_m, GROUP_SIZE_M, NUM_SMS)\n    offs_cm = pid_m * BLOCK_SIZE_M\n    offs_cn = pid_n * BLOCK_SIZE_N\n    if EPILOGUE_SUBTILE:\n        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n        acc = tl.permute(acc, (0, 2, 1))\n        acc0, acc1 = tl.split(acc)\n        c0 = acc0.to(dtype)\n        c_desc.store([offs_cm, offs_cn], c0)\n        c1 = acc1.to(dtype)\n        c_desc.store([offs_cm, offs_cn + BLOCK_SIZE_N // 2], c1)\n    else:\n        c = accumulator.to(dtype)\n        c_desc.store([offs_cm, offs_cn], c)"
  },
  {
    "name": "k_update_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size in [1024, 2048, 4096, 8192] for num_warps in [2, 4, 8, 16, 32]], key=['hidden_dim'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "ka",
        "annotation": null
      },
      {
        "name": "out",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def k_update_fwd_kernel(",
      "    k,",
      "    a,",
      "    ka,",
      "    out,",
      "    xnumel,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    xoffset = tl.program_id(0) * BLOCK_SIZE",
      "    xindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]",
      "",
      "    xmask = xindex < xnumel",
      "    x0 = xindex % hidden_dim",
      "",
      "    b_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_ka = tl.load(ka + x0, eviction_policy=\"evict_last\").to(tl.float32)",
      "",
      "    output = b_k * (1 + (b_a - 1) * b_ka)",
      "",
      "    tl.store(out + xindex, output.to(out.dtype.element_ty), xmask)"
    ],
    "file": "codes/421.py",
    "header": "def k_update_fwd_kernel(k, a, ka, out, xnumel, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "xoffset = tl.program_id(0) * BLOCK_SIZE\nxindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]\nxmask = xindex < xnumel\nx0 = xindex % hidden_dim\nb_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)\nb_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)\nb_ka = tl.load(ka + x0, eviction_policy='evict_last').to(tl.float32)\noutput = b_k * (1 + (b_a - 1) * b_ka)\ntl.store(out + xindex, output.to(out.dtype.element_ty), xmask)"
  },
  {
    "name": "k_update_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size in [1024, 2048, 4096, 8192] for num_warps in [2, 4, 8, 16, 32]], key=['hidden_dim'])"
    ],
    "args": [
      {
        "name": "grad_output",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "ka",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "da",
        "annotation": null
      },
      {
        "name": "dka",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def k_update_bwd_kernel(",
      "    grad_output,",
      "    k,",
      "    a,",
      "    ka,",
      "    dk,",
      "    da,",
      "    dka,",
      "    xnumel,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    xoffset = tl.program_id(0) * BLOCK_SIZE",
      "    xindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]",
      "",
      "    xmask = xindex < xnumel",
      "    x0 = xindex % hidden_dim",
      "",
      "    b_grad_output = tl.load(grad_output + xindex, xmask, other=0.0)",
      "    b_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_ka = tl.load(ka + x0, eviction_policy=\"evict_last\").to(tl.float32)",
      "",
      "    b_dk = b_grad_output * (1 + (b_a - 1) * b_ka)",
      "    b_da = b_grad_output * b_k * b_ka",
      "    b_dka = b_grad_output * b_k * (b_a - 1)",
      "",
      "    tl.store(dk + xindex, b_dk.to(dk.dtype.element_ty), xmask)",
      "    tl.store(da + xindex, b_da.to(da.dtype.element_ty), xmask)",
      "    tl.store(dka + xindex, b_dka.to(dka.dtype.element_ty), xmask)"
    ],
    "file": "codes/421.py",
    "header": "def k_update_bwd_kernel(grad_output, k, a, ka, dk, da, dka, xnumel, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "xoffset = tl.program_id(0) * BLOCK_SIZE\nxindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]\nxmask = xindex < xnumel\nx0 = xindex % hidden_dim\nb_grad_output = tl.load(grad_output + xindex, xmask, other=0.0)\nb_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)\nb_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)\nb_ka = tl.load(ka + x0, eviction_policy='evict_last').to(tl.float32)\nb_dk = b_grad_output * (1 + (b_a - 1) * b_ka)\nb_da = b_grad_output * b_k * b_ka\nb_dka = b_grad_output * b_k * (b_a - 1)\ntl.store(dk + xindex, b_dk.to(dk.dtype.element_ty), xmask)\ntl.store(da + xindex, b_da.to(da.dtype.element_ty), xmask)\ntl.store(dka + xindex, b_dka.to(dka.dtype.element_ty), xmask)"
  },
  {
    "name": "fused_chunk_retention_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_retention_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    h0,",
      "    ht,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "",
      "    o_i = tl.arange(0, BT)",
      "",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "",
      "    d_b, d_o, d_h = (",
      "        tl.math.exp2(BT * b_b),",
      "        tl.math.exp2((o_i + 1) * b_b),",
      "        tl.math.exp2((BT - o_i - 1) * b_b),",
      "    )",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_k * B * H + i_bh).to(tl.int64) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (0, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    NT = tl.cdiv(T, BT)",
      "    for i in range(0, NT):",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s",
      "",
      "        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "        if CHECK and i == 0:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]",
      "            b_h = d_b * b_h + tl.dot(",
      "                b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False",
      "            )",
      "        else:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]",
      "            if i == NT - 1 and (T % BT) != 0:",
      "                d_b = tl.math.exp2((T % BT) * b_b)",
      "                d_h = tl.math.exp2(((T % BT) - o_i - 1) * b_b)",
      "            b_h = d_b * b_h + tl.dot(",
      "                b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False",
      "            )",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        p_q = tl.advance(p_q, (BT, 0))",
      "        p_k = tl.advance(p_k, (0, BT))",
      "        p_v = tl.advance(p_v, (BT, 0))",
      "        p_o = tl.advance(p_o, (BT, 0))",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/415.py",
    "header": "def fused_chunk_retention_fwd_kernel(q, k, v, o, h0, ht, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, CHECK: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_h = i_bh % H\no_i = tl.arange(0, BT)\nb_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\nd_b, d_o, d_h = (tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b))\nm_s = o_i[:, None] >= o_i[None, :]\nd_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + (i_k * B * H + i_bh).to(tl.int64) * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0))\nif USE_INITIAL_STATE:\n    p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\nNT = tl.cdiv(T, BT)\nfor i in range(0, NT):\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s\n    b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n    if CHECK and i == 0:\n        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False)\n    else:\n        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]\n        if i == NT - 1 and T % BT != 0:\n            d_b = tl.math.exp2(T % BT * b_b)\n            d_h = tl.math.exp2((T % BT - o_i - 1) * b_b)\n        b_h = d_b * b_h + tl.dot(b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    p_q = tl.advance(p_q, (BT, 0))\n    p_k = tl.advance(p_k, (0, BT))\n    p_v = tl.advance(p_v, (BT, 0))\n    p_o = tl.advance(p_o, (BT, 0))\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_chunk_retention_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_retention_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "",
      "    o_i = tl.arange(0, BT)",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT - o_i - 1) * b_b)",
      "    d_b = tl.math.exp2(BT * b_b)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (i_bh + i_v * B * H).to(tl.int64) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)",
      "",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        b_ds = (b_ds * d_s).to(b_k.dtype)",
      "",
      "        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)",
      "",
      "        if CHECK and i == 0:",
      "            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)",
      "            b_h = d_b * b_h + tl.dot(",
      "                (b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False",
      "            )",
      "        else:",
      "            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)",
      "            b_h = d_b * b_h + tl.dot(",
      "                (b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False",
      "            )",
      "",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_h = None",
      "    tl.debug_barrier()",
      "    d_s = tl.trans(d_s)",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i in range(1, tl.cdiv(T, BT) + 1):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (i_bh + i_v * B * H).to(tl.int64) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (T - i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_bh + i_k * B * H).to(tl.int64) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (T - i * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)",
      "",
      "        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)",
      "        b_ds = (b_ds * d_s).to(b_k.dtype)",
      "",
      "        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s",
      "",
      "        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)",
      "",
      "        b_dv = tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)",
      "        if CHECK and i == 1:",
      "            b_dk += (",
      "                tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "                * d_k[:, None]",
      "            )",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]",
      "            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)",
      "        else:",
      "            b_dk += (",
      "                tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "                * d_k[:, None]",
      "            )",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]",
      "            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/415.py",
    "header": "def fused_chunk_retention_bwd_kernel(q, k, v, do, dq, dk, dv, h0, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, CHECK: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_h = i_bh % H\no_i = tl.arange(0, BT)\nb_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))\nd_q, d_k = (tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT - o_i - 1) * b_b))\nd_b = tl.math.exp2(BT * b_b)\nm_s = o_i[:, None] >= o_i[None, :]\nd_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\nfor i in range(0, tl.cdiv(T, BT)):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H).to(tl.int64) * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dd = (b_do * d_q[:, None]).to(b_do.dtype)\n    b_ds = tl.dot(b_do, b_v, allow_tf32=False)\n    b_ds = (b_ds * d_s).to(b_k.dtype)\n    b_dq = tl.dot(b_ds, b_k, allow_tf32=False)\n    if CHECK and i == 0:\n        b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)\n        b_h = d_b * b_h + tl.dot((b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False)\n    else:\n        b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)\n        b_h = d_b * b_h + tl.dot((b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False)\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\nb_h = None\ntl.debug_barrier()\nd_s = tl.trans(d_s)\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nfor i in range(1, tl.cdiv(T, BT) + 1):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H).to(tl.int64) * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H).to(tl.int64) * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dd = (b_do * d_q[:, None]).to(b_do.dtype)\n    b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)\n    b_ds = (b_ds * d_s).to(b_k.dtype)\n    b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s\n    b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)\n    b_dv = tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)\n    if CHECK and i == 1:\n        b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False) * d_k[:, None]\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]\n        b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n    else:\n        b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False) * d_k[:, None]\n        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]\n        b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "Lse",
        "annotation": null
      },
      {
        "name": "TMP",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_ob",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    Out,",
      "    Lse,",
      "    TMP,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_ob,",
      "    stride_oh,",
      "    stride_om,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    q_ptrs = (",
      "        Q",
      "        + off_b * stride_qb",
      "        + off_h * stride_qh",
      "        + (offs_m[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "    k_ptrs = (",
      "        K",
      "        + off_b * stride_kb",
      "        + off_h * stride_kh",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "    v_ptrs = (",
      "        V",
      "        + off_b * stride_vb",
      "        + off_h * stride_vh",
      "        + (offs_n[:, None] * stride_vn + offs_d[None, :])",
      "    )",
      "    if BIAS_TYPE == \"vector\":",
      "        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n",
      "    elif BIAS_TYPE == \"matrix\":",
      "        b_ptrs = (",
      "            Bias",
      "            + off_b * stride_bb",
      "            + off_h * stride_bh",
      "            + (offs_m[:, None] * stride_bm + offs_n[None, :])",
      "        )",
      "",
      "    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m",
      "    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    if EVEN_M & EVEN_N:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs)",
      "        else:",
      "            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)",
      "        else:",
      "            q = tl.load(",
      "                q_ptrs,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "                other=0.0,",
      "            )",
      "",
      "    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)",
      "    for start_n in range(0, end_n, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(k_ptrs + start_n * stride_kn)",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k, trans_b=True)",
      "",
      "        if not EVEN_N:",
      "            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float(\"-inf\"))",
      "        if IS_CAUSAL:",
      "            qk += tl.where(",
      "                offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float(\"-inf\")",
      "            )",
      "        if BIAS_TYPE != \"none\":",
      "            if BIAS_TYPE == \"vector\":",
      "                if EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n, mask=(start_n + offs_n) < seqlen_k, other=0.0",
      "                    ).to(tl.float32)",
      "                bias = bias[None, :]",
      "            elif BIAS_TYPE == \"matrix\":",
      "                if EVEN_M & EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n,",
      "                        mask=(offs_m[:, None] < seqlen_q)",
      "                        & ((start_n + offs_n)[None, :] < seqlen_k),",
      "                        other=0.0,",
      "                    ).to(tl.float32)",
      "",
      "            qk = qk * softmax_scale + bias",
      "            m_ij = tl.maximum(tl.max(qk, 1), lse_i)",
      "            p = tl.exp(qk - m_ij[:, None])",
      "        else:",
      "            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "            p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        tl.store(t_ptrs, acc_o_scale)",
      "        acc_o_scale = tl.load(t_ptrs)",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(v_ptrs + start_n * stride_vn)",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    o_scale = tl.exp(m_i - lse_i)",
      "",
      "    tl.store(t_ptrs, o_scale)",
      "    o_scale = tl.load(t_ptrs)",
      "    acc_o = acc_o * o_scale[:, None]",
      "",
      "    start_m = tl.program_id(0)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m",
      "    tl.store(lse_ptrs, lse_i)",
      "",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    out_ptrs = (",
      "        Out",
      "        + off_b * stride_ob",
      "        + off_h * stride_oh",
      "        + (offs_m[:, None] * stride_om + offs_d[None, :])",
      "    )",
      "    if EVEN_M:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o)",
      "        else:",
      "            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)",
      "        else:",
      "            tl.store(",
      "                out_ptrs,",
      "                acc_o,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "            )"
    ],
    "file": "codes/555.py",
    "header": "def _fwd_kernel(Q, K, V, Bias, Out, Lse, TMP, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.constexpr, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "start_m = tl.program_id(0)\noff_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nq_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] * stride_qm + offs_d[None, :])\nk_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] * stride_kn + offs_d[None, :])\nv_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] * stride_vn + offs_d[None, :])\nif BIAS_TYPE == 'vector':\n    b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\nelif BIAS_TYPE == 'matrix':\n    b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:, None] * stride_bm + offs_n[None, :])\nt_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m\nlse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nacc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\nif EVEN_M & EVEN_N:\n    if EVEN_HEADDIM:\n        q = tl.load(q_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\nelif EVEN_HEADDIM:\n    q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\nelse:\n    q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\nend_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)\nfor start_n in range(0, end_n, BLOCK_N):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n + offs_n)[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n + offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk += tl.dot(q, k, trans_b=True)\n    if not EVEN_N:\n        qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n    if IS_CAUSAL:\n        qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float('-inf'))\n    if BIAS_TYPE != 'none':\n        if BIAS_TYPE == 'vector':\n            if EVEN_N:\n                bias = tl.load(b_ptrs + start_n).to(tl.float32)\n            else:\n                bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n < seqlen_k, other=0.0).to(tl.float32)\n            bias = bias[None, :]\n        elif BIAS_TYPE == 'matrix':\n            if EVEN_M & EVEN_N:\n                bias = tl.load(b_ptrs + start_n).to(tl.float32)\n            else:\n                bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] < seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k), other=0.0).to(tl.float32)\n        qk = qk * softmax_scale + bias\n        m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n        p = tl.exp(qk - m_ij[:, None])\n    else:\n        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n        p = tl.exp(qk * softmax_scale - m_ij[:, None])\n    l_ij = tl.sum(p, 1)\n    acc_o_scale = tl.exp(m_i - m_ij)\n    tl.store(t_ptrs, acc_o_scale)\n    acc_o_scale = tl.load(t_ptrs)\n    acc_o = acc_o * acc_o_scale[:, None]\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n + offs_n)[:, None] < seqlen_k, other=0.0)\n    else:\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n + offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n    p = p.to(v.dtype)\n    acc_o += tl.dot(p, v)\n    m_i = m_ij\n    l_i_new = tl.exp(lse_i - m_ij) + l_ij\n    lse_i = m_ij + tl.log(l_i_new)\no_scale = tl.exp(m_i - lse_i)\ntl.store(t_ptrs, o_scale)\no_scale = tl.load(t_ptrs)\nacc_o = acc_o * o_scale[:, None]\nstart_m = tl.program_id(0)\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\nlse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\ntl.store(lse_ptrs, lse_i)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nout_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:, None] * stride_om + offs_d[None, :])\nif EVEN_M:\n    if EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o)\n    else:\n        tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\nelif EVEN_HEADDIM:\n    tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\nelse:\n    tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim))"
  },
  {
    "name": "_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K', 'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])",
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEQUENCE_PARALLEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    SEQUENCE_PARALLEL: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    K += off_b * stride_kb + off_h * stride_kh",
      "    V += off_b * stride_vb + off_h * stride_vh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "    DK += off_b * stride_dkb + off_h * stride_dkh",
      "    DV += off_b * stride_dvb + off_h * stride_dvh",
      "    if BIAS_TYPE != \"none\":",
      "        Bias += off_b * stride_bb + off_h * stride_bh",
      "",
      "    D += off_hb * seqlen_q_rounded",
      "    LSE += off_hb * seqlen_q_rounded",
      "    if not SEQUENCE_PARALLEL:",
      "        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)",
      "        for start_n in range(0, num_block_n):",
      "            _bwd_kernel_one_col_block(",
      "                start_n,",
      "                Q,",
      "                K,",
      "                V,",
      "                Bias,",
      "                DO,",
      "                DQ,",
      "                DK,",
      "                DV,",
      "                LSE,",
      "                D,",
      "                softmax_scale,",
      "                stride_qm,",
      "                stride_kn,",
      "                stride_vn,",
      "                stride_bm,",
      "                stride_dom,",
      "                stride_dqm,",
      "                stride_dkn,",
      "                stride_dvn,",
      "                seqlen_q,",
      "                seqlen_k,",
      "                headdim,",
      "                ATOMIC_ADD=False,",
      "                BIAS_TYPE=BIAS_TYPE,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "                EVEN_M=EVEN_M,",
      "                EVEN_N=EVEN_N,",
      "                EVEN_HEADDIM=EVEN_HEADDIM,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "    else:",
      "        start_n = tl.program_id(0)",
      "        _bwd_kernel_one_col_block(",
      "            start_n,",
      "            Q,",
      "            K,",
      "            V,",
      "            Bias,",
      "            DO,",
      "            DQ,",
      "            DK,",
      "            DV,",
      "            LSE,",
      "            D,",
      "            softmax_scale,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dqm,",
      "            stride_dkn,",
      "            stride_dvn,",
      "            seqlen_q,",
      "            seqlen_k,",
      "            headdim,",
      "            ATOMIC_ADD=True,",
      "            BIAS_TYPE=BIAS_TYPE,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "            EVEN_M=EVEN_M,",
      "            EVEN_N=EVEN_N,",
      "            EVEN_HEADDIM=EVEN_HEADDIM,",
      "            BLOCK_M=BLOCK_M,",
      "            BLOCK_N=BLOCK_N,",
      "        )"
    ],
    "file": "codes/555.py",
    "header": "def _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm, stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.constexpr, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, SEQUENCE_PARALLEL: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "off_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\nQ += off_b * stride_qb + off_h * stride_qh\nK += off_b * stride_kb + off_h * stride_kh\nV += off_b * stride_vb + off_h * stride_vh\nDO += off_b * stride_dob + off_h * stride_doh\nDQ += off_b * stride_dqb + off_h * stride_dqh\nDK += off_b * stride_dkb + off_h * stride_dkh\nDV += off_b * stride_dvb + off_h * stride_dvh\nif BIAS_TYPE != 'none':\n    Bias += off_b * stride_bb + off_h * stride_bh\nD += off_hb * seqlen_q_rounded\nLSE += off_hb * seqlen_q_rounded\nif not SEQUENCE_PARALLEL:\n    num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n    for start_n in range(0, num_block_n):\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\nelse:\n    start_n = tl.program_id(0)\n    _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)"
  },
  {
    "name": "kernel_consumer_gemm_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_ranks",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ready_ptr",
        "annotation": null
      },
      {
        "name": "comm_buf_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EPILOGUE_SUBTILE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ready_value",
        "annotation": "tl.constexpr"
      },
      {
        "name": "local_world_size",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_consumer_gemm_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    rank: tl.constexpr,",
      "    num_ranks: tl.constexpr,",
      "    ready_ptr,",
      "    comm_buf_ptr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    EPILOGUE_SUBTILE: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    ready_value: tl.constexpr = 1,",
      "    local_world_size: tl.constexpr = 8,",
      "):",
      "",
      "    dtype = c_ptr.dtype.element_ty",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "    node_id = rank // local_world_size",
      "    nnodes = num_ranks // local_world_size",
      "",
      "    a_desc = tl.make_tensor_descriptor(",
      "        a_ptr,",
      "        shape=[M, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],",
      "    )",
      "    b_desc = tl.make_tensor_descriptor(",
      "        b_ptr,",
      "        shape=[N, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],",
      "    )",
      "    c_desc = tl.make_tensor_descriptor(",
      "        c_ptr,",
      "        shape=[M, N],",
      "        strides=[N, 1],",
      "        block_shape=[",
      "            BLOCK_SIZE_M,",
      "            BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2,",
      "        ],",
      "    )",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    M_per_rank = M // num_ranks",
      "    pid_ms_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            if nnodes == 1:",
      "                alpha = 0",
      "                beta = 0",
      "                pid_m = (",
      "                    pid_m + ((((rank ^ alpha) + beta) % num_ranks) * pid_ms_per_rank)",
      "                ) % num_pid_m",
      "            else:",
      "                m_rank = pid_m // pid_ms_per_rank",
      "                pid_m_intra_rank = pid_m - m_rank * pid_ms_per_rank",
      "                m_node_id = m_rank // local_world_size",
      "                m_local_rank = m_rank % local_world_size",
      "                swizzle_m_node_id = (m_node_id + node_id) % nnodes",
      "                swizzle_m_local_rank = (m_local_rank + rank) % local_world_size",
      "                swizzle_m_rank = (",
      "                    swizzle_m_node_id * local_world_size + swizzle_m_local_rank",
      "                )",
      "",
      "                pid_m = swizzle_m_rank * pid_ms_per_rank + pid_m_intra_rank",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "            rank_beg = offs_am // M_per_rank",
      "            rank_end = (min(offs_am + BLOCK_SIZE_M, M) - 1) // M_per_rank",
      "",
      "            token = dl.wait(",
      "                ready_ptr + rank_beg,",
      "                rank_end - rank_beg + 1,",
      "                \"gpu\",",
      "                \"acquire\",",
      "                waitValue=ready_value,",
      "            )",
      "            a_desc = dl.consume_token(a_desc, token)",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = a_desc.load([offs_am, offs_k])",
      "        b = b_desc.load([offs_bn, offs_k])",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            if EPILOGUE_SUBTILE:",
      "                acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))",
      "                acc = tl.permute(acc, (0, 2, 1))",
      "                acc0, acc1 = tl.split(acc)",
      "                c0 = acc0.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c0)",
      "                c1 = acc1.to(dtype)",
      "                c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)",
      "            else:",
      "                c = accumulator.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c)",
      "",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/75.py",
    "header": "def kernel_consumer_gemm_persistent(a_ptr, b_ptr, c_ptr, M, N, K, rank: tl.constexpr, num_ranks: tl.constexpr, ready_ptr, comm_buf_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, EPILOGUE_SUBTILE: tl.constexpr, NUM_SMS: tl.constexpr, ready_value: tl.constexpr = 1, local_world_size: tl.constexpr = 8):",
    "body": "dtype = c_ptr.dtype.element_ty\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\nnode_id = rank // local_world_size\nnnodes = num_ranks // local_world_size\na_desc = tl.make_tensor_descriptor(a_ptr, shape=[M, K], strides=[K, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K])\nb_desc = tl.make_tensor_descriptor(b_ptr, shape=[N, K], strides=[K, 1], block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K])\nc_desc = tl.make_tensor_descriptor(c_ptr, shape=[M, N], strides=[N, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2])\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\npid_m = 0\npid_n = 0\noffs_am = 0\noffs_bn = 0\nM_per_rank = M // num_ranks\npid_ms_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        if nnodes == 1:\n            alpha = 0\n            beta = 0\n            pid_m = (pid_m + ((rank ^ alpha) + beta) % num_ranks * pid_ms_per_rank) % num_pid_m\n        else:\n            m_rank = pid_m // pid_ms_per_rank\n            pid_m_intra_rank = pid_m - m_rank * pid_ms_per_rank\n            m_node_id = m_rank // local_world_size\n            m_local_rank = m_rank % local_world_size\n            swizzle_m_node_id = (m_node_id + node_id) % nnodes\n            swizzle_m_local_rank = (m_local_rank + rank) % local_world_size\n            swizzle_m_rank = swizzle_m_node_id * local_world_size + swizzle_m_local_rank\n            pid_m = swizzle_m_rank * pid_ms_per_rank + pid_m_intra_rank\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n        rank_beg = offs_am // M_per_rank\n        rank_end = (min(offs_am + BLOCK_SIZE_M, M) - 1) // M_per_rank\n        token = dl.wait(ready_ptr + rank_beg, rank_end - rank_beg + 1, 'gpu', 'acquire', waitValue=ready_value)\n        a_desc = dl.consume_token(a_desc, token)\n    offs_k = ki * BLOCK_SIZE_K\n    a = a_desc.load([offs_am, offs_k])\n    b = b_desc.load([offs_bn, offs_k])\n    accumulator = tl.dot(a, b.T, accumulator)\n    if ki == k_tiles - 1:\n        if EPILOGUE_SUBTILE:\n            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n            acc = tl.permute(acc, (0, 2, 1))\n            acc0, acc1 = tl.split(acc)\n            c0 = acc0.to(dtype)\n            c_desc.store([offs_am, offs_bn], c0)\n            c1 = acc1.to(dtype)\n            c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)\n        else:\n            c = accumulator.to(dtype)\n            c_desc.store([offs_am, offs_bn], c)\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "        A_v = tl.load(A_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, mask=N_range < N, other=ignore_index)",
      "",
      "    reduction = tl.load(reduction_ptr)",
      "    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction",
      "",
      "    tl.store(",
      "        z_block_ptr,",
      "        (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))",
      "    lse = tl.where(",
      "        y != ignore_index,",
      "        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,",
      "        zero_lse_constant,",
      "    )",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None], boundary_check=(0,))"
    ],
    "file": "codes/179.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nnum_idx_N, num_idx_V_group = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_V_group = tl.swizzle2d(idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE)\nV_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    A_v = tl.load(A_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range, mask=N_range < N, other=ignore_index)\nreduction = tl.load(reduction_ptr)\nmask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]\nloss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction\ntl.store(z_block_ptr, (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty), boundary_check=(0, 1))\nm = tl.max(z_j_to_k, 1)\nzero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))\nlse = tl.where(y != ignore_index, tl.log(tl.sum(tl.exp(z_j_to_k - m[:, None]), axis=1)) + m, zero_lse_constant)\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\ntl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\ntl.store(lse_row_ptr, lse[:, None], boundary_check=(0,))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == V_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            A_v.type.element_ty",
      "        )",
      "",
      "        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
    ],
    "file": "codes/179.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(z_nv_ptr, y_ptr, A_t_ptr, x_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization: tl.constexpr, fp32_grad_accumulators: tl.constexpr, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0) // SPLIT_V\nidx_H = tl.program_id(axis=1)\nidx_V_tile = tl.program_id(axis=0) % SPLIT_V\nnum_idx_N, num_idx_H = (tl.num_programs(0) - triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N, tl.num_programs(1))\nidx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE)\nV_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, V_split_offset), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, V_split_offset), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range, eviction_policy='evict_last')\nlse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), eviction_policy='evict_last')\nreduction = tl.load(reduction_ptr)\nacc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)\nfor _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n    mask = y[:, None] == V_range[None, :]\n    A_v = tl.load(A_t_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    if z_regularization > 0:\n        softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n    z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)\n    valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(A_v.type.element_ty)\n    x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    V_range += V_BLOCK_SIZE\nif SPLIT_V == 1:\n    x_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))\nelse:\n    row_n = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    x_grad_simple_ptr = x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n    tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            x_ptr.type.element_ty",
      "        )",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
    ],
    "file": "codes/179.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(z_nv_ptr, y_ptr, x_ptr, A_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization: tl.constexpr, fp32_grad_accumulators: tl.constexpr, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\nidx_H = tl.program_id(axis=1)\nidx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\nnum_idx_V, num_idx_H = (tl.num_programs(0) - N_group // N_BLOCK_SIZE * SPLIT_V, tl.num_programs(1))\nidx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE)\nN_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(N_split_offset, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nreduction = tl.load(reduction_ptr)\nacc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty\nA_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)\nfor _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range, eviction_policy='evict_last')\n    lse = tl.load(lse_ptr + N_range, eviction_policy='evict_last')\n    mask = y[:, None] == V_range[None, :]\n    x_chunk = tl.load(x_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    if z_regularization > 0:\n        softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z\n    z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)\n    valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(x_ptr.type.element_ty)\n    A_grad_acc = tl.dot(x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n    N_range += N_BLOCK_SIZE\nif SPLIT_N == 1:\n    A_grad_T_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\n    if idx_N_group > 0:\n        tl.store(A_grad_T_block_ptr, tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty))\n    else:\n        tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))\nelse:\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    A_grad_T_simple_ptr = A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n    tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_dispatcher",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=bwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad",
        "annotation": null
      },
      {
        "name": "At_grad",
        "annotation": null
      },
      {
        "name": "lse_global",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_dispatcher(",
      "    logits_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_t_ptr,",
      "    x_grad,",
      "    At_grad,",
      "    lse_global,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 128,",
      "    N_BLOCK_SIZE: tl.constexpr = 128,",
      "    H_BLOCK_SIZE: tl.constexpr = 128,",
      "    GROUP_SIZE: tl.constexpr = 32,",
      "    SPLIT_N: tl.constexpr = 2,",
      "    SPLIT_V: tl.constexpr = 2,",
      "):",
      "    idx_NV = tl.program_id(axis=0)",
      "    if idx_NV < (N_group // N_BLOCK_SIZE * SPLIT_V):",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "            logits_ptr,",
      "            y_ptr,",
      "            A_t_ptr,",
      "            x_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            z_regularization,",
      "            fp32_grad_accumulators,",
      "            reduction_ptr,",
      "            ignore_index,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N,",
      "            SPLIT_V,",
      "        )",
      "    else:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "            logits_ptr,",
      "            y_ptr,",
      "            x_ptr,",
      "            At_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            z_regularization,",
      "            fp32_grad_accumulators,",
      "            reduction_ptr,",
      "            ignore_index,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N,",
      "            SPLIT_V,",
      "        )"
    ],
    "file": "codes/179.py",
    "header": "def linear_xent_bwd_dispatcher(logits_ptr, y_ptr, x_ptr, A_t_ptr, x_grad, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization: tl.constexpr, fp32_grad_accumulators: tl.constexpr, reduction_ptr, ignore_index: tl.constexpr, idx_N_group, N_group, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 128, N_BLOCK_SIZE: tl.constexpr = 128, H_BLOCK_SIZE: tl.constexpr = 128, GROUP_SIZE: tl.constexpr = 32, SPLIT_N: tl.constexpr = 2, SPLIT_V: tl.constexpr = 2):",
    "body": "idx_NV = tl.program_id(axis=0)\nif idx_NV < N_group // N_BLOCK_SIZE * SPLIT_V:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dx(logits_ptr, y_ptr, A_t_ptr, x_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization, fp32_grad_accumulators, reduction_ptr, ignore_index, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)\nelse:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dA(logits_ptr, y_ptr, x_ptr, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, z_regularization, fp32_grad_accumulators, reduction_ptr, ignore_index, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)"
  },
  {
    "name": "logsumexp_reduction_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'N_BLOCK_SIZE': 2}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 4}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=16, num_stages=5), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=1)], key=['N_group', 'V', 'V_BLOCK_SIZE'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "lse_local_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "lse_sum_ptr",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_reduction_kernel(",
      "    lse_local_ptr,",
      "    lse_global_ptr,",
      "    lse_sum_ptr,",
      "    reduction_ptr,",
      "    z_regularization: tl.constexpr,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr = 32,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_local_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_local = tl.load(lse_row_ptr)",
      "    m = tl.max(lse_local, 1)",
      "    lse = tl.log(tl.sum(tl.exp((lse_local - m[:, None])), axis=1)) + m",
      "",
      "    lse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(",
      "        reduction_ptr",
      "    )",
      "",
      "    tl.atomic_add(lse_sum_ptr, lse_reduction)",
      "    tl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)"
    ],
    "file": "codes/179.py",
    "header": "def logsumexp_reduction_kernel(lse_local_ptr, lse_global_ptr, lse_sum_ptr, reduction_ptr, z_regularization: tl.constexpr, stride_lse_N, stride_lse_B, N_group, V: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr = 32):",
    "body": "idx_N = tl.program_id(axis=0)\nlse_row_ptr = tl.make_block_ptr(base=lse_local_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE), order=(1, 0))\nlse_local = tl.load(lse_row_ptr)\nm = tl.max(lse_local, 1)\nlse = tl.log(tl.sum(tl.exp(lse_local - m[:, None]), axis=1)) + m\nlse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(reduction_ptr)\ntl.atomic_add(lse_sum_ptr, lse_reduction)\ntl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)"
  },
  {
    "name": "_bmm_chunk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_outm",
        "annotation": null
      },
      {
        "name": "stride_outn",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_fwd_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    out_ptr,",
      "    seq_idx_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_bk,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_outm,",
      "    stride_outn,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    IS_CAUSAL: tl.constexpr,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    if IS_CAUSAL:",
      "        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "            return",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + pid_h * stride_b_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit)",
      "            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)",
      "            & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    out = acc.to(out_ptr.dtype.element_ty)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)",
      "    tl.store(",
      "        out_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "codes/128.py",
    "header": "def _bmm_chunk_fwd_kernel(a_ptr, b_ptr, out_ptr, seq_idx_ptr, seqlen, chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak, stride_b_batch, stride_b_seqlen, stride_b_head, stride_bk, stride_out_batch, stride_out_chunk, stride_out_head, stride_outm, stride_outn, stride_seq_idx_batch, stride_seq_idx_seqlen, IS_CAUSAL: tl.constexpr, dot_dtype: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_ch = tl.program_id(axis=2)\npid_c = pid_ch // ngroups\npid_h = pid_ch - pid_c * ngroups\nnum_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nif IS_CAUSAL:\n    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n        return\na_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h * stride_b_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < K - k * BLOCK_SIZE_K), other=0.0).to(dot_dtype)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) & (offs_n[None, :] < chunk_size_limit), other=0.0).to(dot_dtype)\n    acc += tl.dot(a, b)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_SEQ_IDX:\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen, mask=offs_n < chunk_size_limit, other=-2)\n    acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\nout = acc.to(out_ptr.dtype.element_ty)\nout_ptr += pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\nout_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\ntl.store(out_ptrs, out, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))"
  },
  {
    "name": "_bmm_chunk_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "res_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_n",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_head",
        "annotation": null
      },
      {
        "name": "stride_db_k",
        "annotation": null
      },
      {
        "name": "stride_res_batch",
        "annotation": null
      },
      {
        "name": "stride_res_seqlen",
        "annotation": null
      },
      {
        "name": "stride_res_head",
        "annotation": null
      },
      {
        "name": "stride_res_k",
        "annotation": null
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_bwd_kernel(",
      "    a_ptr,",
      "    dout_ptr,",
      "    db_ptr,",
      "    res_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_csize_m,",
      "    stride_dout_csize_n,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_head,",
      "    stride_db_k,",
      "    stride_res_batch,",
      "    stride_res_seqlen,",
      "    stride_res_head,",
      "    stride_res_k,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_CS: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_cs = tl.arange(0, BLOCK_SIZE_CS)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m",
      "    )",
      "    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)",
      "            & (offs_n[None, :] < K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(dout, a)",
      "        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m",
      "        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_RESIDUAL:",
      "        res_ptr += (",
      "            pid_b * stride_res_batch",
      "            + pid_c * chunk_size * stride_res_seqlen",
      "            + pid_h * stride_res_head",
      "        )",
      "        res_ptrs = res_ptr + (",
      "            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k",
      "        )",
      "        res = tl.load(",
      "            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "        ).to(tl.float32)",
      "        acc += res",
      "    db = acc.to(db_ptr.dtype.element_ty)",
      "",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_h * stride_db_head",
      "    )",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k",
      "    )",
      "    tl.store(",
      "        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "    )"
    ],
    "file": "codes/128.py",
    "header": "def _bmm_chunk_bwd_kernel(a_ptr, dout_ptr, db_ptr, res_ptr, seqlen, chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak, stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_csize_m, stride_dout_csize_n, stride_db_batch, stride_db_seqlen, stride_db_head, stride_db_k, stride_res_batch, stride_res_seqlen, stride_res_head, stride_res_k, dot_dtype: tl.constexpr, HAS_RESIDUAL: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_CS: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_ch = tl.program_id(axis=2)\npid_c = pid_ch // ngroups\npid_h = pid_ch - pid_c * ngroups\nnum_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\na_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_cs = tl.arange(0, BLOCK_SIZE_CS)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m)\na_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS), other=0.0).to(dot_dtype)\n    a = tl.load(a_ptrs, mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS) & (offs_n[None, :] < K), other=0.0).to(dot_dtype)\n    acc += tl.dot(dout, a)\n    dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n    a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_RESIDUAL:\n    res_ptr += pid_b * stride_res_batch + pid_c * chunk_size * stride_res_seqlen + pid_h * stride_res_head\n    res_ptrs = res_ptr + (offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k)\n    res = tl.load(res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)).to(tl.float32)\n    acc += res\ndb = acc.to(db_ptr.dtype.element_ty)\ndb_ptr += pid_b * stride_db_batch + pid_c * chunk_size * stride_db_seqlen + pid_h * stride_db_head\ndb_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k)\ntl.store(db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K))"
  },
  {
    "name": "_angular_lsh_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen'] % args['BLOCK_M'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "in_mat",
        "annotation": null
      },
      {
        "name": "proj_dir",
        "annotation": null
      },
      {
        "name": "perm",
        "annotation": null
      },
      {
        "name": "enc_vec",
        "annotation": null
      },
      {
        "name": "buckets",
        "annotation": null
      },
      {
        "name": "stride_in_matb",
        "annotation": null
      },
      {
        "name": "stride_in_math",
        "annotation": null
      },
      {
        "name": "stride_in_matm",
        "annotation": null
      },
      {
        "name": "stride_proj_dirb",
        "annotation": null
      },
      {
        "name": "stride_proj_dirh",
        "annotation": null
      },
      {
        "name": "stride_proj_dird",
        "annotation": null
      },
      {
        "name": "stride_bucketsb",
        "annotation": null
      },
      {
        "name": "stride_bucketsh",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "seqlen_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "NUM_PROJ_ROUNDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_projs",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _angular_lsh_kernel(",
      "    in_mat,",
      "    proj_dir,",
      "    perm,",
      "    enc_vec,",
      "    buckets,",
      "    stride_in_matb,",
      "    stride_in_math,",
      "    stride_in_matm,",
      "    stride_proj_dirb,",
      "    stride_proj_dirh,",
      "    stride_proj_dird,",
      "    stride_bucketsb,",
      "    stride_bucketsh,",
      "    nheads,",
      "    seqlen,",
      "    seqlen_rounded,",
      "    headdim,",
      "    NUM_PROJ_ROUNDED: tl.constexpr,",
      "    num_projs: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, NUM_PROJ_ROUNDED)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    in_mat_ptrs = (",
      "        in_mat",
      "        + off_b * stride_in_matb",
      "        + off_h * stride_in_math",
      "        + (offs_m[:, None] * stride_in_matm + offs_d[None, :])",
      "    )",
      "    proj_dir_ptrs = (",
      "        proj_dir",
      "        + off_b * stride_proj_dirb",
      "        + off_h * stride_proj_dirh",
      "        + (offs_d[:, None] * stride_proj_dird + offs_n[None, :])",
      "    )",
      "",
      "    if EVEN_M:",
      "        if EVEN_HEADDIM:",
      "            mat = tl.load(in_mat_ptrs)",
      "        else:",
      "            mat = tl.load(in_mat_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            mat = tl.load(in_mat_ptrs, mask=offs_m[:, None] < seqlen, other=0.0)",
      "        else:",
      "            mat = tl.load(",
      "                in_mat_ptrs,",
      "                mask=(offs_m[:, None] < seqlen) & (offs_d[None, :] < headdim),",
      "                other=0.0,",
      "            )",
      "",
      "    if EVEN_HEADDIM:",
      "        proj_dir_block = tl.load(",
      "            proj_dir_ptrs, mask=offs_n[None, :] < num_projs, other=0.0",
      "        )",
      "    else:",
      "        proj_dir_block = tl.load(",
      "            proj_dir_ptrs,",
      "            mask=(offs_n[None, :] < num_projs)",
      "            & (offs_d[:, None] * stride_proj_dird < headdim),",
      "            other=0.0,",
      "        )",
      "",
      "    mask = tl.dot(mat, proj_dir_block)",
      "    mask = tl.where(mask > 0.0, 1.0, 0.0)",
      "",
      "    encoding_vectors = tl.load(enc_vec + offs_n, mask=offs_n < num_projs, other=0.0)",
      "",
      "    bin_ids = tl.sum(mask * encoding_vectors[None, :], 1).to(tl.int32)",
      "",
      "    hash_buckets = tl.load(perm + bin_ids)",
      "",
      "    buckets_ptrs = buckets + off_b * stride_bucketsb + off_h * stride_bucketsh + offs_m",
      "    if EVEN_M:",
      "        tl.store(buckets_ptrs, hash_buckets)",
      "    else:",
      "        tl.store(buckets_ptrs, hash_buckets, mask=offs_m < seqlen)"
    ],
    "file": "codes/299.py",
    "header": "def _angular_lsh_kernel(in_mat, proj_dir, perm, enc_vec, buckets, stride_in_matb, stride_in_math, stride_in_matm, stride_proj_dirb, stride_proj_dirh, stride_proj_dird, stride_bucketsb, stride_bucketsh, nheads, seqlen, seqlen_rounded, headdim, NUM_PROJ_ROUNDED: tl.constexpr, num_projs: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr):",
    "body": "start_m = tl.program_id(0)\noff_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, NUM_PROJ_ROUNDED)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nin_mat_ptrs = in_mat + off_b * stride_in_matb + off_h * stride_in_math + (offs_m[:, None] * stride_in_matm + offs_d[None, :])\nproj_dir_ptrs = proj_dir + off_b * stride_proj_dirb + off_h * stride_proj_dirh + (offs_d[:, None] * stride_proj_dird + offs_n[None, :])\nif EVEN_M:\n    if EVEN_HEADDIM:\n        mat = tl.load(in_mat_ptrs)\n    else:\n        mat = tl.load(in_mat_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\nelif EVEN_HEADDIM:\n    mat = tl.load(in_mat_ptrs, mask=offs_m[:, None] < seqlen, other=0.0)\nelse:\n    mat = tl.load(in_mat_ptrs, mask=(offs_m[:, None] < seqlen) & (offs_d[None, :] < headdim), other=0.0)\nif EVEN_HEADDIM:\n    proj_dir_block = tl.load(proj_dir_ptrs, mask=offs_n[None, :] < num_projs, other=0.0)\nelse:\n    proj_dir_block = tl.load(proj_dir_ptrs, mask=(offs_n[None, :] < num_projs) & (offs_d[:, None] * stride_proj_dird < headdim), other=0.0)\nmask = tl.dot(mat, proj_dir_block)\nmask = tl.where(mask > 0.0, 1.0, 0.0)\nencoding_vectors = tl.load(enc_vec + offs_n, mask=offs_n < num_projs, other=0.0)\nbin_ids = tl.sum(mask * encoding_vectors[None, :], 1).to(tl.int32)\nhash_buckets = tl.load(perm + bin_ids)\nbuckets_ptrs = buckets + off_b * stride_bucketsb + off_h * stride_bucketsh + offs_m\nif EVEN_M:\n    tl.store(buckets_ptrs, hash_buckets)\nelse:\n    tl.store(buckets_ptrs, hash_buckets, mask=offs_m < seqlen)"
  },
  {
    "name": "prepare_wy_repr_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config(triton_config, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "A_ak",
        "annotation": null
      },
      {
        "name": "ag",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv0",
        "annotation": null
      },
      {
        "name": "dag",
        "annotation": null
      },
      {
        "name": "dAak",
        "annotation": null
      },
      {
        "name": "dAab",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_bwd_kernel(",
      "    A_ab_inv,",
      "    A_ak,",
      "    ag,",
      "    v,",
      "    dw,",
      "    du,",
      "    dv,",
      "    dv0,",
      "    dag,",
      "    dAak,",
      "    dAab,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_Aak_t = tl.make_block_ptr(",
      "        A_ak + (bos * H + i_h) * BT,",
      "        (BT, T),",
      "        (1, H * BT),",
      "        (0, i_t * BT),",
      "        (BT, BT),",
      "        (0, 1),",
      "    )",
      "    p_Aab_inv_t = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (BT, T),",
      "        (1, H * BT),",
      "        (0, i_t * BT),",
      "        (BT, BT),",
      "        (0, 1),",
      "    )",
      "    p_dAak = tl.make_block_ptr(",
      "        dAak + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_dAab = tl.make_block_ptr(",
      "        dAab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A_ab_inv_t = tl.load(p_Aab_inv_t, boundary_check=(0, 1))",
      "    b_A_ak_t = tl.load(p_Aak_t, boundary_check=(0, 1))",
      "    b_A_ak_t = tl.where(",
      "        tl.arange(0, BT)[:, None] < tl.arange(0, BT)[None, :], b_A_ak_t, 0",
      "    )",
      "    b_A_ab_inv_t = tl.where(",
      "        tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A_ab_inv_t, 0",
      "    )",
      "    b_A_tmp_t = tl.dot(b_A_ak_t, b_A_ab_inv_t).to(v.dtype.element_ty)",
      "    b_dA_tmp = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv0 = tl.make_block_ptr(",
      "            dv0 + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_du = tl.make_block_ptr(",
      "            du + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_du = tl.load(p_du, boundary_check=(0, 1))",
      "        b_dA_tmp += tl.dot(b_du.to(b_v.dtype), tl.trans(b_v))",
      "        b_dv0 = tl.load(p_dv0, boundary_check=(0, 1))",
      "        b_dv = b_dv0 + tl.dot(b_A_tmp_t, b_du)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    m_i = tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :]",
      "    b_dA_tmp = tl.where(m_i, b_dA_tmp, 0)",
      "    b_dA_ak = tl.dot(b_A_ab_inv_t, b_dA_tmp)",
      "    b_dA_ak = tl.where(m_i, b_dA_ak, 0)",
      "    tl.store(p_dAak, b_dA_ak, boundary_check=(0, 1))",
      "    b_dA_ab_inv = tl.dot(b_dA_tmp, b_A_ak_t)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_ag = tl.make_block_ptr(",
      "            ag + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dag = tl.make_block_ptr(",
      "            dag + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_ag = tl.load(p_ag, boundary_check=(0, 1))",
      "        b_dw = tl.load(p_dw, boundary_check=(0, 1))",
      "        b_dA_ab_inv += tl.dot(b_dw, tl.trans(b_ag))",
      "        b_dag = tl.dot(b_A_ab_inv_t.to(b_dw.dtype), b_dw)",
      "        tl.store(p_dag, b_dag.to(p_dag.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA_ab_inv = tl.where(",
      "        tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dA_ab_inv, 0",
      "    )",
      "    b_dA_ab_inv = tl.dot(b_A_ab_inv_t, b_dA_ab_inv)",
      "    b_dA_ab_inv = tl.dot(b_dA_ab_inv, b_A_ab_inv_t)",
      "    b_dA_ab_inv = tl.where(m_i, b_dA_ab_inv, 0)",
      "    tl.store(p_dAab, b_dA_ab_inv, boundary_check=(0, 1))"
    ],
    "file": "codes/388.py",
    "header": "def prepare_wy_repr_bwd_kernel(A_ab_inv, A_ak, ag, v, dw, du, dv, dv0, dag, dAak, dAab, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_Aak_t = tl.make_block_ptr(A_ak + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\np_Aab_inv_t = tl.make_block_ptr(A_ab_inv + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\np_dAak = tl.make_block_ptr(dAak + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_dAab = tl.make_block_ptr(dAab + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_A_ab_inv_t = tl.load(p_Aab_inv_t, boundary_check=(0, 1))\nb_A_ak_t = tl.load(p_Aak_t, boundary_check=(0, 1))\nb_A_ak_t = tl.where(tl.arange(0, BT)[:, None] < tl.arange(0, BT)[None, :], b_A_ak_t, 0)\nb_A_ab_inv_t = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A_ab_inv_t, 0)\nb_A_tmp_t = tl.dot(b_A_ak_t, b_A_ab_inv_t).to(v.dtype.element_ty)\nb_dA_tmp = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv0 = tl.make_block_ptr(dv0 + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_du = tl.make_block_ptr(du + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_du = tl.load(p_du, boundary_check=(0, 1))\n    b_dA_tmp += tl.dot(b_du.to(b_v.dtype), tl.trans(b_v))\n    b_dv0 = tl.load(p_dv0, boundary_check=(0, 1))\n    b_dv = b_dv0 + tl.dot(b_A_tmp_t, b_du)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\nm_i = tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :]\nb_dA_tmp = tl.where(m_i, b_dA_tmp, 0)\nb_dA_ak = tl.dot(b_A_ab_inv_t, b_dA_tmp)\nb_dA_ak = tl.where(m_i, b_dA_ak, 0)\ntl.store(p_dAak, b_dA_ak, boundary_check=(0, 1))\nb_dA_ab_inv = tl.dot(b_dA_tmp, b_A_ak_t)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_ag = tl.make_block_ptr(ag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dag = tl.make_block_ptr(dag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dw = tl.make_block_ptr(dw + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_ag = tl.load(p_ag, boundary_check=(0, 1))\n    b_dw = tl.load(p_dw, boundary_check=(0, 1))\n    b_dA_ab_inv += tl.dot(b_dw, tl.trans(b_ag))\n    b_dag = tl.dot(b_A_ab_inv_t.to(b_dw.dtype), b_dw)\n    tl.store(p_dag, b_dag.to(p_dag.dtype.element_ty), boundary_check=(0, 1))\nb_dA_ab_inv = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dA_ab_inv, 0)\nb_dA_ab_inv = tl.dot(b_A_ab_inv_t, b_dA_ab_inv)\nb_dA_ab_inv = tl.dot(b_dA_ab_inv, b_A_ab_inv_t)\nb_dA_ab_inv = tl.where(m_i, b_dA_ab_inv, 0)\ntl.store(p_dAab, b_dA_ab_inv, boundary_check=(0, 1))"
  },
  {
    "name": "triton_red_fused_mv_0",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 8}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 4}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 8, 'RBLOCK': 512}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 8, 'RBLOCK': 256}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 64}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr1",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_mv_0(",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr1,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0).to(tl.int64) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None].to(tl.int64)",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :].to(tl.int64)",
      "    x0 = xindex",
      "",
      "    tmp0 = tl.load(in_ptr0 + (x0 // rnumel), None, eviction_policy=\"evict_last\")",
      "    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp7 = tl.load(in_ptr2 + (r1), None, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp1 = tmp0 + 8",
      "        tmp2 = tmp0 < 0",
      "        tmp3 = tl.where(tmp2, tmp1, tmp0)",
      "",
      "        tmp4 = tl.load(",
      "            in_ptr1 + (r1 + (rnumel * (x0 % rnumel)) + (rnumel * rnumel * tmp3)),",
      "            None,",
      "            eviction_policy=\"evict_first\",",
      "        )",
      "        tmp5 = tmp4.to(tl.float32)",
      "        tmp6 = tmp5.to(tl.float32)",
      "        tmp8 = tmp7.to(tl.float32)",
      "        tmp9 = tmp6 * tmp8",
      "        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])",
      "        tmp12 = _tmp11 + tmp10",
      "        _tmp11 = tmp12",
      "    tmp11 = tl.sum(_tmp11, 1)[:, None]",
      "    tmp13 = tmp11.to(tl.float32)",
      "    tl.store(out_ptr1 + (x0), tmp13, None)"
    ],
    "file": "codes/646.py",
    "header": "def triton_red_fused_mv_0(in_ptr0, in_ptr1, in_ptr2, out_ptr1, xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):",
    "body": "xoffset = tl.program_id(0).to(tl.int64) * XBLOCK\nxindex = xoffset + tl.arange(0, XBLOCK)[:, None].to(tl.int64)\nxmask = xindex < xnumel\nrbase = tl.arange(0, RBLOCK)[None, :].to(tl.int64)\nx0 = xindex\ntmp0 = tl.load(in_ptr0 + x0 // rnumel, None, eviction_policy='evict_last')\n_tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\nfor roffset in range(0, rnumel, RBLOCK):\n    rindex = roffset + rbase\n    rmask = rindex < rnumel\n    r1 = rindex\n    tmp7 = tl.load(in_ptr2 + r1, None, eviction_policy='evict_last').to(tl.float32)\n    tmp1 = tmp0 + 8\n    tmp2 = tmp0 < 0\n    tmp3 = tl.where(tmp2, tmp1, tmp0)\n    tmp4 = tl.load(in_ptr1 + (r1 + rnumel * (x0 % rnumel) + rnumel * rnumel * tmp3), None, eviction_policy='evict_first')\n    tmp5 = tmp4.to(tl.float32)\n    tmp6 = tmp5.to(tl.float32)\n    tmp8 = tmp7.to(tl.float32)\n    tmp9 = tmp6 * tmp8\n    tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])\n    tmp12 = _tmp11 + tmp10\n    _tmp11 = tmp12\ntmp11 = tl.sum(_tmp11, 1)[:, None]\ntmp13 = tmp11.to(tl.float32)\ntl.store(out_ptr1 + x0, tmp13, None)"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune([triton.Config({'BLOCK_SIZE_Q': BLOCK_SIZE_Q, 'BLOCK_SIZE_KV': BLOCK_SIZE_KV}, num_stages=num_stages, num_warps=num_warps) for BLOCK_SIZE_Q in [64, 128] for BLOCK_SIZE_KV in [32, 64] for num_stages in [3, 4, 7] for num_warps in [2, 4]], key=['SEQ_LEN', 'HEAD_DIM'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "stride_Q_batch",
        "annotation": null
      },
      {
        "name": "stride_Q_head",
        "annotation": null
      },
      {
        "name": "stride_Q_seq",
        "annotation": null
      },
      {
        "name": "stride_Q_dim",
        "annotation": null
      },
      {
        "name": "stride_K_batch",
        "annotation": null
      },
      {
        "name": "stride_K_head",
        "annotation": null
      },
      {
        "name": "stride_K_seq",
        "annotation": null
      },
      {
        "name": "stride_K_dim",
        "annotation": null
      },
      {
        "name": "stride_V_batch",
        "annotation": null
      },
      {
        "name": "stride_V_head",
        "annotation": null
      },
      {
        "name": "stride_V_seq",
        "annotation": null
      },
      {
        "name": "stride_V_dim",
        "annotation": null
      },
      {
        "name": "stride_O_batch",
        "annotation": null
      },
      {
        "name": "stride_O_head",
        "annotation": null
      },
      {
        "name": "stride_O_seq",
        "annotation": null
      },
      {
        "name": "stride_O_dim",
        "annotation": null
      },
      {
        "name": "BATCH_SIZE",
        "annotation": null
      },
      {
        "name": "NUM_HEADS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEQ_LEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_Q",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_KV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    softmax_scale,",
      "    M,",
      "    O,",
      "    stride_Q_batch,",
      "    stride_Q_head,",
      "    stride_Q_seq,",
      "    stride_Q_dim,",
      "    stride_K_batch,",
      "    stride_K_head,",
      "    stride_K_seq,",
      "    stride_K_dim,",
      "    stride_V_batch,",
      "    stride_V_head,",
      "    stride_V_seq,",
      "    stride_V_dim,",
      "    stride_O_batch,",
      "    stride_O_head,",
      "    stride_O_seq,",
      "    stride_O_dim,",
      "    BATCH_SIZE,",
      "    NUM_HEADS: tl.constexpr,",
      "    SEQ_LEN: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    BLOCK_SIZE_Q: tl.constexpr,",
      "    BLOCK_SIZE_KV: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)",
      "",
      "    block_index_q = tl.program_id(0)",
      "",
      "    index_batch_head = tl.program_id(1)",
      "",
      "    index_batch = index_batch_head // NUM_HEADS",
      "",
      "    index_head = index_batch_head % NUM_HEADS",
      "",
      "    qvk_offset = (",
      "        index_batch.to(tl.int64) * stride_Q_batch",
      "        + index_head.to(tl.int64) * stride_Q_head",
      "    )",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(SEQ_LEN, HEAD_DIM),",
      "        strides=(stride_Q_seq, stride_Q_dim),",
      "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),",
      "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(SEQ_LEN, HEAD_DIM),",
      "        strides=(stride_V_seq, stride_V_dim),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(HEAD_DIM, SEQ_LEN),",
      "        strides=(",
      "            stride_K_dim,",
      "            stride_K_seq,",
      "        ),",
      "        offsets=(0, 0),",
      "        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),",
      "        order=(0, 1),",
      "    )",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=O + qvk_offset,",
      "        shape=(SEQ_LEN, HEAD_DIM),",
      "        strides=(stride_O_seq, stride_O_dim),",
      "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),",
      "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)",
      "",
      "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)",
      "",
      "    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")",
      "",
      "    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0",
      "",
      "    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)",
      "",
      "    Q_block = tl.load(Q_block_ptr)",
      "",
      "    if STAGE == 1 or STAGE == 3:",
      "",
      "        O_block, l_i, m_i = _attn_fwd_inner(",
      "            O_block,",
      "            l_i,",
      "            m_i,",
      "            Q_block,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            block_index_q,",
      "            softmax_scale,",
      "            BLOCK_SIZE_Q,",
      "            BLOCK_SIZE_KV,",
      "            4 - STAGE,",
      "            offs_q,",
      "            offs_kv,",
      "            SEQ_LEN,",
      "        )",
      "",
      "    if STAGE == 3:",
      "",
      "        O_block, l_i, m_i = _attn_fwd_inner(",
      "            O_block,",
      "            l_i,",
      "            m_i,",
      "            Q_block,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            block_index_q,",
      "            softmax_scale,",
      "            BLOCK_SIZE_Q,",
      "            BLOCK_SIZE_KV,",
      "            2,",
      "            offs_q,",
      "            offs_kv,",
      "            SEQ_LEN,",
      "        )",
      "",
      "    m_i += tl.math.log(l_i)",
      "    O_block = O_block / l_i[:, None]",
      "    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, O_block.to(O.type.element_ty))"
    ],
    "file": "codes/464.py",
    "header": "def _attn_fwd(Q, K, V, softmax_scale, M, O, stride_Q_batch, stride_Q_head, stride_Q_seq, stride_Q_dim, stride_K_batch, stride_K_head, stride_K_seq, stride_K_dim, stride_V_batch, stride_V_head, stride_V_seq, stride_V_dim, stride_O_batch, stride_O_head, stride_O_seq, stride_O_dim, BATCH_SIZE, NUM_HEADS: tl.constexpr, SEQ_LEN: tl.constexpr, HEAD_DIM: tl.constexpr, BLOCK_SIZE_Q: tl.constexpr, BLOCK_SIZE_KV: tl.constexpr, STAGE: tl.constexpr):",
    "body": "tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)\nblock_index_q = tl.program_id(0)\nindex_batch_head = tl.program_id(1)\nindex_batch = index_batch_head // NUM_HEADS\nindex_head = index_batch_head % NUM_HEADS\nqvk_offset = index_batch.to(tl.int64) * stride_Q_batch + index_head.to(tl.int64) * stride_Q_head\nQ_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(SEQ_LEN, HEAD_DIM), strides=(stride_Q_seq, stride_Q_dim), offsets=(block_index_q * BLOCK_SIZE_Q, 0), block_shape=(BLOCK_SIZE_Q, HEAD_DIM), order=(1, 0))\nV_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(SEQ_LEN, HEAD_DIM), strides=(stride_V_seq, stride_V_dim), offsets=(0, 0), block_shape=(BLOCK_SIZE_KV, HEAD_DIM), order=(1, 0))\nK_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM, SEQ_LEN), strides=(stride_K_dim, stride_K_seq), offsets=(0, 0), block_shape=(HEAD_DIM, BLOCK_SIZE_KV), order=(0, 1))\nO_block_ptr = tl.make_block_ptr(base=O + qvk_offset, shape=(SEQ_LEN, HEAD_DIM), strides=(stride_O_seq, stride_O_dim), offsets=(block_index_q * BLOCK_SIZE_Q, 0), block_shape=(BLOCK_SIZE_Q, HEAD_DIM), order=(1, 0))\noffs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)\noffs_kv = tl.arange(0, BLOCK_SIZE_KV)\nm_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float('inf')\nl_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0\nO_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)\nQ_block = tl.load(Q_block_ptr)\nif STAGE == 1 or STAGE == 3:\n    O_block, l_i, m_i = _attn_fwd_inner(O_block, l_i, m_i, Q_block, K_block_ptr, V_block_ptr, block_index_q, softmax_scale, BLOCK_SIZE_Q, BLOCK_SIZE_KV, 4 - STAGE, offs_q, offs_kv, SEQ_LEN)\nif STAGE == 3:\n    O_block, l_i, m_i = _attn_fwd_inner(O_block, l_i, m_i, Q_block, K_block_ptr, V_block_ptr, block_index_q, softmax_scale, BLOCK_SIZE_Q, BLOCK_SIZE_KV, 2, offs_q, offs_kv, SEQ_LEN)\nm_i += tl.math.log(l_i)\nO_block = O_block / l_i[:, None]\nm_ptrs = M + index_batch_head * SEQ_LEN + offs_q\ntl.store(m_ptrs, m_i)\ntl.store(O_block_ptr, O_block.to(O.type.element_ty))"
  },
  {
    "name": "softmax_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "p",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_fwd_kernel(x, p, D: tl.constexpr, B: tl.constexpr):",
      "    i_n = tl.program_id(0)",
      "    o_d = tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))",
      "    b_m = tl.max(b_x, 0)",
      "    b_x = exp(b_x - b_m)",
      "    b_p = b_x / tl.sum(b_x, 0)",
      "",
      "    tl.store(p + i_n * D + o_d, b_p.to(p.dtype.element_ty), mask=m_d)"
    ],
    "file": "codes/435.py",
    "header": "def softmax_fwd_kernel(x, p, D: tl.constexpr, B: tl.constexpr):",
    "body": "i_n = tl.program_id(0)\no_d = tl.arange(0, B)\nm_d = o_d < D\nb_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float('inf'))\nb_m = tl.max(b_x, 0)\nb_x = exp(b_x - b_m)\nb_p = b_x / tl.sum(b_x, 0)\ntl.store(p + i_n * D + o_d, b_p.to(p.dtype.element_ty), mask=m_d)"
  },
  {
    "name": "softmax_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['D'])"
    ],
    "args": [
      {
        "name": "p",
        "annotation": null
      },
      {
        "name": "dp",
        "annotation": null
      },
      {
        "name": "ds",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_bwd_kernel(p, dp, ds, D: tl.constexpr, B: tl.constexpr):",
      "    i_n = tl.program_id(0)",
      "    o_d = tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_p = tl.load(p + i_n * D + o_d, mask=m_d, other=0.0)",
      "    b_dp = tl.load(dp + i_n * D + o_d, mask=m_d, other=0.0)",
      "    b_pp = tl.sum(b_p * b_dp, 0)",
      "    b_ds = b_p * b_dp - b_p * b_pp",
      "    tl.store(ds + i_n * D + o_d, b_ds.to(ds.dtype.element_ty), mask=m_d)"
    ],
    "file": "codes/435.py",
    "header": "def softmax_bwd_kernel(p, dp, ds, D: tl.constexpr, B: tl.constexpr):",
    "body": "i_n = tl.program_id(0)\no_d = tl.arange(0, B)\nm_d = o_d < D\nb_p = tl.load(p + i_n * D + o_d, mask=m_d, other=0.0)\nb_dp = tl.load(dp + i_n * D + o_d, mask=m_d, other=0.0)\nb_pp = tl.sum(b_p * b_dp, 0)\nb_ds = b_p * b_dp - b_p * b_pp\ntl.store(ds + i_n * D + o_d, b_ds.to(ds.dtype.element_ty), mask=m_d)"
  },
  {
    "name": "quantize_2d_bf16_to_int2",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=8), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=16)], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "bf16_ptr",
        "annotation": null
      },
      {
        "name": "int8_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_im",
        "annotation": null
      },
      {
        "name": "stride_in",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def quantize_2d_bf16_to_int2(",
      "    bf16_ptr,",
      "    int8_ptr,",
      "    M,",
      "    N,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_im,",
      "    stride_in,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_n = tl.program_id(1)",
      "",
      "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)",
      "",
      "    mask_m = offs_m < M",
      "    mask_n = offs_n < (N // 4)",
      "",
      "    packed_output = tl.zeros((BLOCK_M, BLOCK_N // 4), dtype=tl.int32)",
      "",
      "    for i in range(4):",
      "",
      "        column_idx = offs_n * 4 + i",
      "        bf16_vals = tl.load(",
      "            bf16_ptr + offs_m[:, None] * stride_bm + column_idx[None, :] * stride_bn,",
      "            mask=mask_m[:, None] & (column_idx[None, :] < N),",
      "            other=0.0,",
      "        )",
      "",
      "        int2_vals = tl.where(",
      "            bf16_vals == -2.0,",
      "            0,",
      "            tl.where(",
      "                bf16_vals == -1.0,",
      "                1,",
      "                tl.where(bf16_vals == 0.0, 2, 3),",
      "            ),",
      "        )",
      "",
      "        packed_output = packed_output | (int2_vals << (i * 2))",
      "",
      "    tl.store(",
      "        int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in,",
      "        packed_output.to(tl.int8),",
      "        mask=mask_m[:, None] & mask_n[None, :],",
      "    )"
    ],
    "file": "codes/672.py",
    "header": "def quantize_2d_bf16_to_int2(bf16_ptr, int8_ptr, M, N, stride_bm, stride_bn, stride_im, stride_in, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "pid_m = tl.program_id(0)\npid_n = tl.program_id(1)\noffs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)\nmask_m = offs_m < M\nmask_n = offs_n < N // 4\npacked_output = tl.zeros((BLOCK_M, BLOCK_N // 4), dtype=tl.int32)\nfor i in range(4):\n    column_idx = offs_n * 4 + i\n    bf16_vals = tl.load(bf16_ptr + offs_m[:, None] * stride_bm + column_idx[None, :] * stride_bn, mask=mask_m[:, None] & (column_idx[None, :] < N), other=0.0)\n    int2_vals = tl.where(bf16_vals == -2.0, 0, tl.where(bf16_vals == -1.0, 1, tl.where(bf16_vals == 0.0, 2, 3)))\n    packed_output = packed_output | int2_vals << i * 2\ntl.store(int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in, packed_output.to(tl.int8), mask=mask_m[:, None] & mask_n[None, :])"
  },
  {
    "name": "dequantize_2d_int2_to_bf16",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=8), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256}, num_warps=16)], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "int8_ptr",
        "annotation": null
      },
      {
        "name": "bf16_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_im",
        "annotation": null
      },
      {
        "name": "stride_in",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dequantize_2d_int2_to_bf16(",
      "    int8_ptr,",
      "    bf16_ptr,",
      "    M,",
      "    N,",
      "    stride_im,",
      "    stride_in,",
      "    stride_bm,",
      "    stride_bn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_n = tl.program_id(1)",
      "",
      "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)",
      "",
      "    mask_m = offs_m < M",
      "    mask_n = offs_n < (N // 4)",
      "",
      "    packed_vals = tl.load(",
      "        int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in,",
      "        mask=mask_m[:, None] & mask_n[None, :],",
      "        other=0,",
      "    ).to(tl.int32)",
      "",
      "    for i in range(4):",
      "        shift = i * 2",
      "        mask = 0b11 << shift",
      "        int2_vals = (packed_vals & mask) >> shift",
      "",
      "        bf16_vals = tl.where(",
      "            int2_vals == 0b00,",
      "            tl.full(int2_vals.shape, -2.0, dtype=tl.float32),",
      "            tl.where(",
      "                int2_vals == 0b01,",
      "                tl.full(int2_vals.shape, -1.0, dtype=tl.float32),",
      "                tl.where(",
      "                    int2_vals == 0b10,",
      "                    tl.full(int2_vals.shape, 0.0, dtype=tl.float32),",
      "                    tl.full(int2_vals.shape, 1.0, dtype=tl.float32),",
      "                ),",
      "            ),",
      "        )",
      "",
      "        output_idx = offs_n * 4 + i",
      "",
      "        tl.store(",
      "            bf16_ptr + offs_m[:, None] * stride_bm + output_idx[None, :] * stride_bn,",
      "            bf16_vals.to(tl.bfloat16),",
      "            mask=mask_m[:, None] & (output_idx[None, :] < N),",
      "        )"
    ],
    "file": "codes/672.py",
    "header": "def dequantize_2d_int2_to_bf16(int8_ptr, bf16_ptr, M, N, stride_im, stride_in, stride_bm, stride_bn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "pid_m = tl.program_id(0)\npid_n = tl.program_id(1)\noffs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)\nmask_m = offs_m < M\nmask_n = offs_n < N // 4\npacked_vals = tl.load(int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in, mask=mask_m[:, None] & mask_n[None, :], other=0).to(tl.int32)\nfor i in range(4):\n    shift = i * 2\n    mask = 3 << shift\n    int2_vals = (packed_vals & mask) >> shift\n    bf16_vals = tl.where(int2_vals == 0, tl.full(int2_vals.shape, -2.0, dtype=tl.float32), tl.where(int2_vals == 1, tl.full(int2_vals.shape, -1.0, dtype=tl.float32), tl.where(int2_vals == 2, tl.full(int2_vals.shape, 0.0, dtype=tl.float32), tl.full(int2_vals.shape, 1.0, dtype=tl.float32))))\n    output_idx = offs_n * 4 + i\n    tl.store(bf16_ptr + offs_m[:, None] * stride_bm + output_idx[None, :] * stride_bn, bf16_vals.to(tl.bfloat16), mask=mask_m[:, None] & (output_idx[None, :] < N))"
  },
  {
    "name": "parallel_nsa_compression_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    cu_seqlens,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (TC, V),",
      "            (H * V, 1),",
      "            (i_c, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "        b_mp = b_m",
      "    if NC == 0:",
      "        b_lse = tl.zeros([G], dtype=tl.float32)",
      "    else:",
      "        b_o = b_o / b_acc[:, None]",
      "        b_lse = b_m + log(b_acc)",
      "",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    if i_v == 0:",
      "        tl.store(",
      "            lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G),",
      "            b_lse.to(lse.dtype.element_ty),",
      "        )"
    ],
    "file": "codes/401.py",
    "header": "def parallel_nsa_compression_fwd_kernel(q, k, v, o, lse, scale, cu_seqlens, token_indices, chunk_offsets, T, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\np_q = tl.make_block_ptr(q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nTC = tl.cdiv(T, BS)\nNC = (i_t + 1) // BS\np_o = tl.make_block_ptr(o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\nb_o = tl.zeros([G, BV], dtype=tl.float32)\nb_m = tl.full([G], float('-inf'), dtype=tl.float32)\nb_acc = tl.zeros([G], dtype=tl.float32)\nfor i_c in range(0, NC, BC):\n    o_c = i_c + tl.arange(0, BC)\n    p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n    p_v = tl.make_block_ptr(v + (boc * H + i_h) * V, (TC, V), (H * V, 1), (i_c, i_v * BV), (BC, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_s = tl.where((o_c < NC)[None, :], b_s, float('-inf'))\n    b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n    b_r = exp(b_mp - b_m)\n    b_p = exp(b_s - b_m[:, None])\n    b_acc = b_acc * b_r + tl.sum(b_p, 1)\n    b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n    b_mp = b_m\nif NC == 0:\n    b_lse = tl.zeros([G], dtype=tl.float32)\nelse:\n    b_o = b_o / b_acc[:, None]\n    b_lse = b_m + log(b_acc)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\nif i_v == 0:\n    tl.store(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G), b_lse.to(lse.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    cu_seqlens,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (V, TC),",
      "            (1, H * V),",
      "            (i_v * BV, i_c),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_p = exp(b_s - b_lse[:, None])",
      "        b_p = tl.where((o_c < NC)[None, :], b_p, 0)",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/401.py",
    "header": "def parallel_nsa_compression_bwd_kernel_dq(q, k, v, lse, delta, do, dq, scale, cu_seqlens, token_indices, chunk_offsets, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\nq += (bos + i_t) * HQ * K\ndo += (bos + i_t) * HQ * V\nlse += (bos + i_t) * HQ\ndelta += (bos + i_t) * HQ\ndq += (i_v * B * T + bos + i_t) * HQ * K\np_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\np_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\np_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\np_lse = lse + i_h * G + tl.arange(0, G)\np_delta = delta + i_h * G + tl.arange(0, G)\nTC = tl.cdiv(T, BS)\nNC = (i_t + 1) // BS\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_lse = tl.load(p_lse)\nb_delta = tl.load(p_delta)\nb_dq = tl.zeros([G, BK], dtype=tl.float32)\nfor i_c in range(0, NC, BC):\n    o_c = i_c + tl.arange(0, BC)\n    p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n    p_v = tl.make_block_ptr(v + (boc * H + i_h) * V, (V, TC), (1, H * V), (i_v * BV, i_c), (BV, BC), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_p = exp(b_s - b_lse[:, None])\n    b_p = tl.where((o_c < NC)[None, :], b_p, 0)\n    b_dp = tl.dot(b_do, b_v)\n    b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n    b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\nb_dq *= scale\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_c = tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_c * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + boc * H + i_h) * K,",
      "        (TC, K),",
      "        (H * K, 1),",
      "        (i_c * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (i_v * B * T * H + boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_c * BC * BS, T):",
      "        o_c = i_c * BC + tl.arange(0, BC)",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos + i) * HQ * V,",
      "            (HQ, V),",
      "            (V, 1),",
      "            (i_h * G, i_v * BV),",
      "            (G, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse)",
      "        b_delta = tl.load(p_delta)",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        b_p = exp(b_s - b_lse[None, :])",
      "        b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/401.py",
    "header": "def parallel_nsa_compression_bwd_kernel_dkv(q, k, v, lse, delta, do, dk, dv, cu_seqlens, chunk_indices, chunk_offsets, scale, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_c = (tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(chunk_indices + i_c * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\nTC = tl.cdiv(T, BS)\np_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0))\np_v = tl.make_block_ptr(v + (boc * H + i_h) * V, (TC, V), (H * V, 1), (i_c * BC, i_v * BV), (BC, BV), (1, 0))\np_dk = tl.make_block_ptr(dk + (i_v * B * T * H + boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0))\np_dv = tl.make_block_ptr(dv + (i_v * B * T * H + boc * H + i_h) * V, (TC, V), (H * V, 1), (i_c * BC, i_v * BV), (BC, BV), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dk = tl.zeros([BC, BK], dtype=tl.float32)\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BC, BV], dtype=tl.float32)\nfor i in range(i_c * BC * BS, T):\n    o_c = i_c * BC + tl.arange(0, BC)\n    p_q = tl.make_block_ptr(q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    p_do = tl.make_block_ptr(do + (bos + i) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n    p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n    p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_lse = tl.load(p_lse)\n    b_delta = tl.load(p_delta)\n    b_s = tl.dot(b_k, tl.trans(b_q))\n    b_p = exp(b_s - b_lse[None, :])\n    b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)\n    b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n    b_dp = tl.dot(b_v, tl.trans(b_do))\n    b_ds = b_p * (b_dp - b_delta[None, :])\n    b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    output = language.Mean.forward(",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "    tl.store(output_block_ptr, output)"
    ],
    "file": "codes/510.py",
    "header": "def forward(output_ptr: tl.tensor, input_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, y_stride: tl.int32, x_stride: tl.int32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\noutput = language.Mean.forward(input_ptr, y_size, x_size, y_stride, x_stride, y_offset, dtype, x_block_size, require_x_boundary_check)\ntl.store(output_block_ptr, output)"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    y_offset = pid // num_x_blocks",
      "    x = pid % num_x_blocks",
      "    x_offset = x * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, x_offset),",
      "        block_shape=(1, x_block_size),",
      "        order=(0, 1),",
      "    )",
      "",
      "    grad_input = language.Mean.backward(",
      "        grad_output_ptr, y_size, x_size, y_offset, dtype, x_block_size",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input)"
    ],
    "file": "codes/510.py",
    "header": "def backward(grad_input_ptr: tl.tensor, grad_output_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, y_stride: tl.int32, x_stride: tl.int32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_x_blocks = tl.cdiv(x_size, x_block_size)\ny_offset = pid // num_x_blocks\nx = pid % num_x_blocks\nx_offset = x * x_block_size\ngrad_input_block_ptr = tl.make_block_ptr(grad_input_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, x_offset), block_shape=(1, x_block_size), order=(0, 1))\ngrad_input = language.Mean.backward(grad_output_ptr, y_size, x_size, y_offset, dtype, x_block_size)\nif require_x_boundary_check:\n    tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))\nelse:\n    tl.store(grad_input_block_ptr, grad_input)"
  },
  {
    "name": "parallel_nsa_compression_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    offsets,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (TC, V),",
      "            (H * V, 1),",
      "            (i_c, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = tl.exp(b_mp - b_m)",
      "",
      "        b_p = tl.exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "        b_mp = b_m",
      "    if NC == 0:",
      "        b_lse = tl.zeros([G], dtype=tl.float32)",
      "    else:",
      "        b_o = b_o / b_acc[:, None]",
      "        b_lse = b_m + tl.log(b_acc)",
      "",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    if i_v == 0:",
      "        tl.store(",
      "            lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G),",
      "            b_lse.to(lse.dtype.element_ty),",
      "        )"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_compression_fwd_kernel(q, k, v, o, lse, scale, offsets, token_indices, chunk_offsets, T, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_OFFSETS: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\np_q = tl.make_block_ptr(q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nTC = tl.cdiv(T, BS)\nNC = (i_t + 1) // BS\np_o = tl.make_block_ptr(o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\nb_o = tl.zeros([G, BV], dtype=tl.float32)\nb_m = tl.full([G], float('-inf'), dtype=tl.float32)\nb_acc = tl.zeros([G], dtype=tl.float32)\nfor i_c in range(0, NC, BC):\n    o_c = i_c + tl.arange(0, BC)\n    p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n    p_v = tl.make_block_ptr(v + (boc * H + i_h) * V, (TC, V), (H * V, 1), (i_c, i_v * BV), (BC, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_s = tl.where((o_c < NC)[None, :], b_s, float('-inf'))\n    b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n    b_r = tl.exp(b_mp - b_m)\n    b_p = tl.exp(b_s - b_m[:, None])\n    b_acc = b_acc * b_r + tl.sum(b_p, 1)\n    b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n    b_mp = b_m\nif NC == 0:\n    b_lse = tl.zeros([G], dtype=tl.float32)\nelse:\n    b_o = b_o / b_acc[:, None]\n    b_lse = b_m + tl.log(b_acc)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\nif i_v == 0:\n    tl.store(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G), b_lse.to(lse.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    offsets,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (V, TC),",
      "            (1, H * V),",
      "            (i_v * BV, i_c),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_p = tl.exp(b_s - b_lse[:, None])",
      "        b_p = tl.where((o_c < NC)[None, :], b_p, 0)",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_compression_bwd_kernel_dq(q, k, v, lse, delta, do, dq, scale, offsets, token_indices, chunk_offsets, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_OFFSETS: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\nq += (bos + i_t) * HQ * K\ndo += (bos + i_t) * HQ * V\nlse += (bos + i_t) * HQ\ndelta += (bos + i_t) * HQ\ndq += (i_v * B * T + bos + i_t) * HQ * K\np_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\np_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\np_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\np_lse = lse + i_h * G + tl.arange(0, G)\np_delta = delta + i_h * G + tl.arange(0, G)\nTC = tl.cdiv(T, BS)\nNC = (i_t + 1) // BS\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_lse = tl.load(p_lse)\nb_delta = tl.load(p_delta)\nb_dq = tl.zeros([G, BK], dtype=tl.float32)\nfor i_c in range(0, NC, BC):\n    o_c = i_c + tl.arange(0, BC)\n    p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n    p_v = tl.make_block_ptr(v + (boc * H + i_h) * V, (V, TC), (1, H * V), (i_v * BV, i_c), (BV, BC), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_p = tl.exp(b_s - b_lse[:, None])\n    b_p = tl.where((o_c < NC)[None, :], b_p, 0)\n    b_dp = tl.dot(b_do, b_v)\n    b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n    b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\nb_dq *= scale\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    offsets,",
      "    chunk_indices,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_c = tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_c * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + boc * H + i_h) * K,",
      "        (TC, K),",
      "        (H * K, 1),",
      "        (i_c * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (i_v * B * T * H + boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_c * BC * BS, T):",
      "        o_c = i_c * BC + tl.arange(0, BC)",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos + i) * HQ * V,",
      "            (HQ, V),",
      "            (V, 1),",
      "            (i_h * G, i_v * BV),",
      "            (G, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse)",
      "        b_delta = tl.load(p_delta)",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        b_p = tl.exp(b_s - b_lse[None, :])",
      "        b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_compression_bwd_kernel_dkv(q, k, v, lse, delta, do, dk, dv, offsets, chunk_indices, chunk_offsets, scale, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_OFFSETS: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_c = (tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(chunk_indices + i_c * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\nTC = tl.cdiv(T, BS)\np_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0))\np_v = tl.make_block_ptr(v + (boc * H + i_h) * V, (TC, V), (H * V, 1), (i_c * BC, i_v * BV), (BC, BV), (1, 0))\np_dk = tl.make_block_ptr(dk + (i_v * B * T * H + boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0))\np_dv = tl.make_block_ptr(dv + (i_v * B * T * H + boc * H + i_h) * V, (TC, V), (H * V, 1), (i_c * BC, i_v * BV), (BC, BV), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dk = tl.zeros([BC, BK], dtype=tl.float32)\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BC, BV], dtype=tl.float32)\nfor i in range(i_c * BC * BS, T):\n    o_c = i_c * BC + tl.arange(0, BC)\n    p_q = tl.make_block_ptr(q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    p_do = tl.make_block_ptr(do + (bos + i) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n    p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n    p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_lse = tl.load(p_lse)\n    b_delta = tl.load(p_delta)\n    b_s = tl.dot(b_k, tl.trans(b_q))\n    b_p = tl.exp(b_s - b_lse[None, :])\n    b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)\n    b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n    b_dp = tl.dot(b_v, tl.trans(b_do))\n    b_ds = b_p * (b_dp - b_delta[None, :])\n    b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_nsa_kernel_topk",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_topk(",
      "    q,",
      "    k,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    offsets,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    if lse is not None:",
      "        b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))",
      "    else:",
      "",
      "        b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "        b_acc = tl.zeros([G], dtype=tl.float32)",
      "        for i_c in range(0, NC, BC):",
      "            o_c = i_c + tl.arange(0, BC)",
      "",
      "            p_k = tl.make_block_ptr(",
      "                k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = tl.exp(b_mp - b_m)",
      "",
      "            b_p = tl.exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_mp = b_m",
      "        if NC == 0:",
      "            b_lse = tl.zeros([G], dtype=tl.float32)",
      "        else:",
      "            b_lse = b_m + tl.log(b_acc)",
      "",
      "    b_i = tl.full([BC], -1, dtype=tl.float32)",
      "    o_i = tl.zeros([BC], dtype=tl.int32)",
      "    m_i = tl.arange(0, BC) < BC // 2",
      "    for i_c in range(0, i_t // BS + 1, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_p = tl.where(",
      "            (i_t // BS == o_c)[None, :], float(1.0), tl.exp(b_s - b_lse[:, None])",
      "        )",
      "",
      "        b_i, b_ip = tl.sum(b_p, 0), b_i",
      "        o_i, o_ip = tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i",
      "",
      "        n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])",
      "        for i in tl.static_range(1, n_dims):",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)",
      "",
      "        if i_c != 0:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)",
      "            b_i_new = b_ip * m_i + b_i * (1 - m_i)",
      "            o_i_new = o_ip * m_i + o_i * (1 - m_i)",
      "            b_i, o_i = _bitonic_merge(",
      "                b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims",
      "            )",
      "        else:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)",
      "",
      "    m_top = tl.arange(0, BC // S) == 0",
      "    b_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)",
      "",
      "    p_b = tl.make_block_ptr(",
      "        block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,)",
      "    )",
      "    tl.store(p_b, b_top.to(p_b.dtype.element_ty))"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_kernel_topk(q, k, lse, scale, block_indices, offsets, token_indices, chunk_offsets, T, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, S: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, USE_OFFSETS: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\np_q = tl.make_block_ptr(q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nTC = tl.cdiv(T, BS)\nNC = (i_t + 1) // BS\nif lse is not None:\n    b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))\nelse:\n    b_m = tl.full([G], float('-inf'), dtype=tl.float32)\n    b_acc = tl.zeros([G], dtype=tl.float32)\n    for i_c in range(0, NC, BC):\n        o_c = i_c + tl.arange(0, BC)\n        p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((o_c < NC)[None, :], b_s, float('-inf'))\n        b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n        b_r = tl.exp(b_mp - b_m)\n        b_p = tl.exp(b_s - b_m[:, None])\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n        b_mp = b_m\n    if NC == 0:\n        b_lse = tl.zeros([G], dtype=tl.float32)\n    else:\n        b_lse = b_m + tl.log(b_acc)\nb_i = tl.full([BC], -1, dtype=tl.float32)\no_i = tl.zeros([BC], dtype=tl.int32)\nm_i = tl.arange(0, BC) < BC // 2\nfor i_c in range(0, i_t // BS + 1, BC):\n    o_c = i_c + tl.arange(0, BC)\n    p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float('-inf'))\n    b_p = tl.where((i_t // BS == o_c)[None, :], float(1.0), tl.exp(b_s - b_lse[:, None]))\n    b_i, b_ip = (tl.sum(b_p, 0), b_i)\n    o_i, o_ip = (tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i)\n    n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])\n    for i in tl.static_range(1, n_dims):\n        b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)\n    if i_c != 0:\n        b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)\n        b_i_new = b_ip * m_i + b_i * (1 - m_i)\n        o_i_new = o_ip * m_i + o_i * (1 - m_i)\n        b_i, o_i = _bitonic_merge(b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims)\n    else:\n        b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)\nm_top = tl.arange(0, BC // S) == 0\nb_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)\np_b = tl.make_block_ptr(block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,))\ntl.store(p_b, b_top.to(p_b.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    offsets,",
      "    token_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "    p_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where(",
      "                (i_t >= (i_s + tl.arange(0, BS)))[None, :], b_s, float(\"-inf\")",
      "            )",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = tl.exp(b_mp - b_m)",
      "",
      "            b_p = tl.exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "            b_mp = b_m",
      "    b_o = b_o / b_acc[:, None]",
      "    b_m += tl.log(b_acc)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty))"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_fwd_kernel(q, k, v, o, lse, scale, block_indices, block_counts, offsets, token_indices, T, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, S: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_OFFSETS: tl.constexpr, USE_BLOCK_COUNTS: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\nblock_indices += (bos + i_t) * H * S + i_h * S\nif USE_BLOCK_COUNTS:\n    NS = tl.load(block_counts + (bos + i_t) * H + i_h)\nelse:\n    NS = S\np_q = tl.make_block_ptr(q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\np_o = tl.make_block_ptr(o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\np_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)\nb_o = tl.zeros([G, BV], dtype=tl.float32)\nb_m = tl.full([G], float('-inf'), dtype=tl.float32)\nb_acc = tl.zeros([G], dtype=tl.float32)\nfor i in range(NS):\n    i_s = tl.load(block_indices + i).to(tl.int32) * BS\n    if i_s <= i_t and i_s >= 0:\n        p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((i_t >= i_s + tl.arange(0, BS))[None, :], b_s, float('-inf'))\n        b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n        b_r = tl.exp(b_mp - b_m)\n        b_p = tl.exp(b_s - b_m[:, None])\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n        b_mp = b_m\nb_o = b_o / b_acc[:, None]\nb_m += tl.log(b_acc)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_lse, b_m.to(p_lse.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_kernel_mask",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})"
    ],
    "args": [
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_mask(",
      "    block_indices,",
      "    block_counts,",
      "    block_mask,",
      "    T: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_b, i_hs = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h, i_s = i_hs // S, i_hs % S",
      "",
      "    b_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)",
      "    if USE_BLOCK_COUNTS:",
      "        b_m = b_i * BS <= i_t and i_s < tl.load(",
      "            block_counts + i_b * T * H + i_t * H + i_h",
      "        )",
      "    else:",
      "        b_m = b_i * BS <= i_t",
      "",
      "    if b_i < NS and b_i >= 0:",
      "        tl.store(",
      "            block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i,",
      "            b_m.to(block_mask.dtype.element_ty),",
      "        )"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_kernel_mask(block_indices, block_counts, block_mask, T: tl.constexpr, H: tl.constexpr, S: tl.constexpr, BS: tl.constexpr, NS: tl.constexpr, USE_BLOCK_COUNTS: tl.constexpr):",
    "body": "i_t, i_b, i_hs = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_h, i_s = (i_hs // S, i_hs % S)\nb_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)\nif USE_BLOCK_COUNTS:\n    b_m = b_i * BS <= i_t and i_s < tl.load(block_counts + i_b * T * H + i_t * H + i_h)\nelse:\n    b_m = b_i * BS <= i_t\nif b_i < NS and b_i >= 0:\n    tl.store(block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i, b_m.to(block_mask.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    offsets,",
      "    token_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_p = tl.exp(b_s - b_lse[:, None])",
      "            b_p = tl.where((i_t >= (i_s + tl.arange(0, BS)))[None, :], b_p, 0)",
      "",
      "            b_dp = tl.dot(b_do, b_v)",
      "            b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "            b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_bwd_kernel_dq(q, k, v, lse, delta, do, dq, scale, block_indices, block_counts, offsets, token_indices, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, S: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_OFFSETS: tl.constexpr, USE_BLOCK_COUNTS: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos + i_t) * HQ * K\ndo += (bos + i_t) * HQ * V\nlse += (bos + i_t) * HQ\ndelta += (bos + i_t) * HQ\ndq += (i_v * B * T + bos + i_t) * HQ * K\nblock_indices += (bos + i_t) * H * S + i_h * S\nif USE_BLOCK_COUNTS:\n    NS = tl.load(block_counts + (bos + i_t) * H + i_h)\nelse:\n    NS = S\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\np_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\np_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\np_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\np_lse = lse + i_h * G + tl.arange(0, G)\np_delta = delta + i_h * G + tl.arange(0, G)\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_lse = tl.load(p_lse)\nb_delta = tl.load(p_delta)\nb_dq = tl.zeros([G, BK], dtype=tl.float32)\nfor i in range(NS):\n    i_s = tl.load(block_indices + i).to(tl.int32) * BS\n    if i_s <= i_t and i_s >= 0:\n        p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        b_p = tl.exp(b_s - b_lse[:, None])\n        b_p = tl.where((i_t >= i_s + tl.arange(0, BS))[None, :], b_p, 0)\n        b_dp = tl.dot(b_do, b_v)\n        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\nb_dq *= scale\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    block_mask,",
      "    offsets,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    M: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_v, i_s, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_s = tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_s * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_s * BS, 0),",
      "        (BS, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BS, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BS, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_s * BS, T):",
      "        b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)",
      "        if b_m:",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "            p_do = tl.make_block_ptr(",
      "                do + (bos + i) * HQ * V,",
      "                (HQ, V),",
      "                (V, 1),",
      "                (i_h * G, i_v * BV),",
      "                (G, BV),",
      "                (1, 0),",
      "            )",
      "            p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "            p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "            b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "            b_lse = tl.load(p_lse)",
      "            b_delta = tl.load(p_delta)",
      "",
      "            b_s = tl.dot(b_k, tl.trans(b_q))",
      "            b_p = tl.exp(b_s - b_lse[None, :])",
      "            b_p = tl.where((i >= (i_s * BS + tl.arange(0, BS)))[:, None], b_p, 0)",
      "",
      "            b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "            b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "            b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "            b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/440.py",
    "header": "def parallel_nsa_bwd_kernel_dkv(q, k, v, lse, delta, do, dk, dv, block_mask, offsets, chunk_indices, scale, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, M: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_OFFSETS: tl.constexpr):",
    "body": "i_v, i_s, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_OFFSETS:\n    i_n, i_s = (tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(chunk_indices + i_s * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s * BS, i_v * BV), (BS, BV), (1, 0))\np_dk = tl.make_block_ptr(dk + (i_v * B * T * H + bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0))\np_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s * BS, i_v * BV), (BS, BV), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dk = tl.zeros([BS, BK], dtype=tl.float32)\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BS, BV], dtype=tl.float32)\nfor i in range(i_s * BS, T):\n    b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)\n    if b_m:\n        p_q = tl.make_block_ptr(q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        p_do = tl.make_block_ptr(do + (bos + i) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_lse = tl.load(p_lse)\n        b_delta = tl.load(p_delta)\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        b_p = tl.exp(b_s - b_lse[None, :])\n        b_p = tl.where((i >= i_s * BS + tl.arange(0, BS))[:, None], b_p, 0)\n        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n        b_dp = tl.dot(b_v, tl.trans(b_do))\n        b_ds = b_p * (b_dp - b_delta[None, :])\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_path_bwd_intra_chunk_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dq_new",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_bwd_intra_chunk_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    h,",
      "    L,",
      "    D,",
      "    dq,",
      "    dq_new,",
      "    dk,",
      "    dv,",
      "    dh,",
      "    do,",
      "    dg_cumsum,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    T,",
      "    scale,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh = i_n * tl.cdiv(T, BT)",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    h += (boh * H + i_h) * K * K",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    dq += (bos * HQ + i_hq) * K",
      "    dq_new += (bos * HQ + i_hq) * K",
      "    dk += (bos * HQ + i_hq) * K",
      "    dv += (bos * HQ + i_hq) * V",
      "    do += (bos * HQ + i_hq) * V",
      "    dh += (boh * HQ + i_hq) * K * K",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "",
      "    sm_scale = scale * 1.44269504",
      "",
      "    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    p_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_delta = tl.load(p_delta, boundary_check=(0,))",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_l, boundary_check=(0,))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    p_dq = tl.make_block_ptr(dq, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_dq += tl.load(p_dq, boundary_check=(0, 1))",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "    if USE_GATE:",
      "        p_gq_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        b_gq_cumsum = tl.load(p_gq_cumsum, boundary_check=(0,))",
      "        b_dgq = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_dgq = None",
      "",
      "    curr_start = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)",
      "",
      "    for offset in range(curr_start, i_t * BT, BT):",
      "        p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (offset, 0), (BT, BK), (1, 0))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_q_tmp = tl.zeros([BT, BK], dtype=tl.float32)",
      "        b_q_tmp += b_q",
      "",
      "        for i_t_small in range(i_t * BT - BT, offset, -BT):",
      "            p_h = tl.make_block_ptr(",
      "                h + tl.cdiv(i_t_small, BT) * H * K * K,",
      "                (K, K),",
      "                (K, 1),",
      "                (0, 0),",
      "                (BK, BK),",
      "                (1, 0),",
      "            )",
      "            b_h = tl.load(p_h, boundary_check=(0, 1))",
      "            b_q_tmp -= tl.dot(b_q_tmp.to(b_h.dtype), b_h)",
      "",
      "        b_q2 = b_q_tmp.to(b_k.dtype)",
      "        b_dh = -tl.dot(tl.trans(b_q2), b_dq.to(b_q2.dtype))",
      "        tl.atomic_add(",
      "            dh",
      "            + tl.cdiv(offset, BT) * HQ * K * K",
      "            + tl.arange(0, K)[:, None] * K",
      "            + tl.arange(0, K)[None, :],",
      "            b_dh,",
      "            sem=\"relaxed\",",
      "        )",
      "",
      "        b_A = tl.dot(b_q2, tl.trans(b_k))",
      "        if USE_GATE:",
      "            p_gk_cumsum = tl.make_block_ptr(",
      "                g_cumsum, (T,), (HQ,), (offset,), (BT,), (0,)",
      "            )",
      "            b_gk_cumsum = tl.load(p_gk_cumsum, boundary_check=(0,))",
      "            b_A = b_A + b_gq_cumsum[:, None] - b_gk_cumsum[None, :]",
      "            b_A = tl.where(",
      "                (i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\")",
      "            )",
      "",
      "        b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "        b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "",
      "        tl.atomic_add(",
      "            dv",
      "            + ((offset + tl.arange(0, BT)) * HQ * V)[:, None]",
      "            + tl.arange(0, BV)[None, :],",
      "            b_dv.to(dv.dtype.element_ty),",
      "            sem=\"relaxed\",",
      "        )",
      "",
      "        p_v = tl.make_block_ptr(v, (T, V), (V * H, 1), (offset, 0), (BT, BV), (1, 0))",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_dp = tl.dot(b_do, tl.trans(b_v))",
      "        b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "        if USE_GATE:",
      "            b_dgk = -tl.sum(b_dA, axis=0)",
      "            tl.atomic_add(",
      "                dg_cumsum + (offset + tl.arange(0, BT)) * HQ, b_dgk, sem=\"relaxed\"",
      "            )",
      "            b_dgq += tl.sum(b_dA, axis=1)",
      "",
      "        b_dA = b_dA.to(b_k.dtype)",
      "        p_h = tl.make_block_ptr(",
      "            h + tl.cdiv(offset, BT) * H * K * K,",
      "            (K, K),",
      "            (1, K),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (0, 1),",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dk = tl.dot(tl.trans(b_dA), b_q2)",
      "        tl.atomic_add(",
      "            dk",
      "            + (offset + tl.arange(0, BT))[:, None] * HQ * K",
      "            + tl.arange(0, BK)[None, :],",
      "            b_dk,",
      "            sem=\"relaxed\",",
      "        )",
      "        b_dq -= tl.dot(b_dq.to(b_h.dtype), b_h)",
      "        b_dq += tl.dot(b_dA, b_k)",
      "",
      "    p_dq_new = tl.make_block_ptr(",
      "        dq_new, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dq_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_GATE:",
      "        tl.atomic_add(",
      "            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dgq, sem=\"relaxed\"",
      "        )"
    ],
    "file": "codes/411.py",
    "header": "def parallel_path_bwd_intra_chunk_kernel(q, k, v, g_cumsum, h, L, D, dq, dq_new, dk, dv, dh, do, dg_cumsum, offsets, indices, chunk_offsets, T, scale, G: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, BT: tl.constexpr, S: tl.constexpr, IS_VARLEN: tl.constexpr, USE_GATE: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\n    boh = i_n * tl.cdiv(T, BT)\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\nh += (boh * H + i_h) * K * K\nq += (bos * HQ + i_hq) * K\ndq += (bos * HQ + i_hq) * K\ndq_new += (bos * HQ + i_hq) * K\ndk += (bos * HQ + i_hq) * K\ndv += (bos * HQ + i_hq) * V\ndo += (bos * HQ + i_hq) * V\ndh += (boh * HQ + i_hq) * K * K\nL += bos * HQ + i_hq\nD += bos * HQ + i_hq\nif USE_GATE:\n    g_cumsum += bos * HQ + i_hq\n    dg_cumsum += bos * HQ + i_hq\nsm_scale = scale * 1.44269504\np_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_do = tl.load(p_do, boundary_check=(0, 1))\np_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_delta = tl.load(p_delta, boundary_check=(0,))\np_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_l = tl.load(p_l, boundary_check=(0,))\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\np_dq = tl.make_block_ptr(dq, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_dq += tl.load(p_dq, boundary_check=(0, 1))\np_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nif USE_GATE:\n    p_gq_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_gq_cumsum = tl.load(p_gq_cumsum, boundary_check=(0,))\n    b_dgq = tl.zeros([BT], dtype=tl.float32)\nelse:\n    b_dgq = None\ncurr_start = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)\nfor offset in range(curr_start, i_t * BT, BT):\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (offset, 0), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_q_tmp = tl.zeros([BT, BK], dtype=tl.float32)\n    b_q_tmp += b_q\n    for i_t_small in range(i_t * BT - BT, offset, -BT):\n        p_h = tl.make_block_ptr(h + tl.cdiv(i_t_small, BT) * H * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_q_tmp -= tl.dot(b_q_tmp.to(b_h.dtype), b_h)\n    b_q2 = b_q_tmp.to(b_k.dtype)\n    b_dh = -tl.dot(tl.trans(b_q2), b_dq.to(b_q2.dtype))\n    tl.atomic_add(dh + tl.cdiv(offset, BT) * HQ * K * K + tl.arange(0, K)[:, None] * K + tl.arange(0, K)[None, :], b_dh, sem='relaxed')\n    b_A = tl.dot(b_q2, tl.trans(b_k))\n    if USE_GATE:\n        p_gk_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (offset,), (BT,), (0,))\n        b_gk_cumsum = tl.load(p_gk_cumsum, boundary_check=(0,))\n        b_A = b_A + b_gq_cumsum[:, None] - b_gk_cumsum[None, :]\n        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float('-inf'))\n    b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n    b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n    tl.atomic_add(dv + ((offset + tl.arange(0, BT)) * HQ * V)[:, None] + tl.arange(0, BV)[None, :], b_dv.to(dv.dtype.element_ty), sem='relaxed')\n    p_v = tl.make_block_ptr(v, (T, V), (V * H, 1), (offset, 0), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_dp = tl.dot(b_do, tl.trans(b_v))\n    b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n    if USE_GATE:\n        b_dgk = -tl.sum(b_dA, axis=0)\n        tl.atomic_add(dg_cumsum + (offset + tl.arange(0, BT)) * HQ, b_dgk, sem='relaxed')\n        b_dgq += tl.sum(b_dA, axis=1)\n    b_dA = b_dA.to(b_k.dtype)\n    p_h = tl.make_block_ptr(h + tl.cdiv(offset, BT) * H * K * K, (K, K), (1, K), (0, 0), (BK, BK), (0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dk = tl.dot(tl.trans(b_dA), b_q2)\n    tl.atomic_add(dk + (offset + tl.arange(0, BT))[:, None] * HQ * K + tl.arange(0, BK)[None, :], b_dk, sem='relaxed')\n    b_dq -= tl.dot(b_dq.to(b_h.dtype), b_h)\n    b_dq += tl.dot(b_dA, b_k)\np_dq_new = tl.make_block_ptr(dq_new, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_dq_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))\nif USE_GATE:\n    tl.atomic_add(dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dgq, sem='relaxed')"
  },
  {
    "name": "all_gather_push_1d_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_value'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_value",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def all_gather_push_1d_kernel(",
      "    symm_ptr, bytes_per_rank, symm_flag, WORLD_SIZE: tl.constexpr, rank, signal_value",
      "):",
      "    pid = tl.program_id(0)",
      "    thread_idx = tid(0)",
      "",
      "    if pid == rank:",
      "        peer = thread_idx",
      "        if peer < WORLD_SIZE and peer != rank:",
      "",
      "            libshmem_device.signal_wait_until(",
      "                symm_flag + peer,",
      "                libshmem_device.NVSHMEM_CMP_EQ,",
      "                signal_value,",
      "            )",
      "",
      "        __syncthreads()",
      "    else:",
      "        peer = pid",
      "        segment = rank",
      "",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_value,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/71.py",
    "header": "def all_gather_push_1d_kernel(symm_ptr, bytes_per_rank, symm_flag, WORLD_SIZE: tl.constexpr, rank, signal_value):",
    "body": "pid = tl.program_id(0)\nthread_idx = tid(0)\nif pid == rank:\n    peer = thread_idx\n    if peer < WORLD_SIZE and peer != rank:\n        libshmem_device.signal_wait_until(symm_flag + peer, libshmem_device.NVSHMEM_CMP_EQ, signal_value)\n    __syncthreads()\nelse:\n    peer = pid\n    segment = rank\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_value, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "all_gather_push_2d_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_value'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "NNODES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_value",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def all_gather_push_2d_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_flag,",
      "    NNODES: tl.constexpr,",
      "    WORLD_SIZE: tl.constexpr,",
      "    rank,",
      "    signal_value,",
      "):",
      "    pid = tl.program_id(0)",
      "    thread_idx = tid(0)",
      "",
      "    LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES",
      "    node_id = rank // LOCAL_WORLD_SIZE",
      "    local_rank = rank % LOCAL_WORLD_SIZE",
      "",
      "    peer_rank = pid",
      "    peer_node_id = peer_rank // LOCAL_WORLD_SIZE",
      "    peer_local_rank = peer_rank % LOCAL_WORLD_SIZE",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "",
      "    if peer_local_rank == local_rank:",
      "        if peer_rank != rank:",
      "",
      "            peer = peer_node_id * LOCAL_WORLD_SIZE + local_rank",
      "            segment = rank",
      "            libshmem_device.putmem_signal_nbi_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                symm_ptr + segment * bytes_per_rank,",
      "                bytes_per_rank,",
      "                symm_flag + segment,",
      "                signal_value,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                peer,",
      "            )",
      "        else:",
      "            if thread_idx < WORLD_SIZE and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_value,",
      "                )",
      "            __syncthreads()",
      "    else:",
      "        peer = node_id * LOCAL_WORLD_SIZE + peer_local_rank",
      "        segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank",
      "",
      "        if peer_node_id != node_id:",
      "            if thread_idx == 0:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + segment,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_value,",
      "                )",
      "            __syncthreads()",
      "",
      "        libshmem_device.putmem_signal_block(",
      "            symm_ptr + segment * bytes_per_rank,",
      "            symm_ptr + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_value,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/71.py",
    "header": "def all_gather_push_2d_kernel(symm_ptr, bytes_per_rank, symm_flag, NNODES: tl.constexpr, WORLD_SIZE: tl.constexpr, rank, signal_value):",
    "body": "pid = tl.program_id(0)\nthread_idx = tid(0)\nLOCAL_WORLD_SIZE = WORLD_SIZE // NNODES\nnode_id = rank // LOCAL_WORLD_SIZE\nlocal_rank = rank % LOCAL_WORLD_SIZE\npeer_rank = pid\npeer_node_id = peer_rank // LOCAL_WORLD_SIZE\npeer_local_rank = peer_rank % LOCAL_WORLD_SIZE\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nif peer_local_rank == local_rank:\n    if peer_rank != rank:\n        peer = peer_node_id * LOCAL_WORLD_SIZE + local_rank\n        segment = rank\n        libshmem_device.putmem_signal_nbi_block(symm_ptr + segment * bytes_per_rank, symm_ptr + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_value, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\n    else:\n        if thread_idx < WORLD_SIZE and thread_idx != rank:\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_value)\n        __syncthreads()\nelse:\n    peer = node_id * LOCAL_WORLD_SIZE + peer_local_rank\n    segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank\n    if peer_node_id != node_id:\n        if thread_idx == 0:\n            libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_value)\n        __syncthreads()\n    libshmem_device.putmem_signal_block(symm_ptr + segment * bytes_per_rank, symm_ptr + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_value, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "triton_jagged_softmax_kernel_simple_fused_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_softmax_kernel_simple_fused_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "",
      "    buffer_max_all = tl.full(",
      "        (BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float(\"-inf\"), dtype=tl.float32",
      "    )",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer_max_all = tl.maximum(buffer_max_all, input)",
      "",
      "    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer += tl.exp(input - buffer_max)",
      "",
      "    buffer_exp_sum = tl.sum(buffer, axis=0)",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)",
      "",
      "        tl.store(output_ptr + idxs, output, mask=mask)"
    ],
    "file": "codes/662.py",
    "header": "def triton_jagged_softmax_kernel_simple_fused_buffer_then_sum(input_ptr_values, input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_b), tl.load(input_ptr_offsets + (pid_b + 1)))\nbuffer_max_all = tl.full((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float('-inf'), dtype=tl.float32)\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf'))\n    buffer_max_all = tl.maximum(buffer_max_all, input)\nbuffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf'))\n    buffer += tl.exp(input - buffer_max)\nbuffer_exp_sum = tl.sum(buffer, axis=0)\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf'))\n    output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)\n    tl.store(output_ptr + idxs, output, mask=mask)"
  },
  {
    "name": "triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "",
      "    buffer_max_all = tl.full(",
      "        (BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float(\"-inf\"), dtype=tl.float32",
      "    )",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer_max_all = tl.maximum(buffer_max_all, input)",
      "",
      "    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer += tl.exp(input - buffer_max)",
      "",
      "    buffer_exp_sum = tl.sum(buffer, axis=0)",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)",
      "",
      "        tl.store(output_ptr + idxs, output, mask=mask)"
    ],
    "file": "codes/662.py",
    "header": "def triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum(input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_b), tl.load(input_ptr_offsets + (pid_b + 1)))\nbuffer_max_all = tl.full((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float('-inf'), dtype=tl.float32)\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf'))\n    buffer_max_all = tl.maximum(buffer_max_all, input)\nbuffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf'))\n    buffer += tl.exp(input - buffer_max)\nbuffer_exp_sum = tl.sum(buffer, axis=0)\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=float('-inf'))\n    output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)\n    tl.store(output_ptr + idxs, output, mask=mask)"
  },
  {
    "name": "tile_swizzling",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": null
      },
      {
        "name": "BLOCK_K",
        "annotation": null
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tile_swizzling(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = tile_id // width",
      "    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (tile_id % group_size)",
      "    pid_n = (tile_id % width) // group_size",
      "    return pid_m, pid_n"
    ],
    "file": "codes/565.py",
    "header": "def tile_swizzling(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):",
    "body": "grid_m = tl.cdiv(M, BLOCK_M)\ngrid_n = tl.cdiv(N, BLOCK_N)\nwidth = GROUP_M * grid_n\ngroup_id = tile_id // width\ngroup_size = min(grid_m - group_id * GROUP_M, GROUP_M)\npid_m = group_id * GROUP_M + tile_id % group_size\npid_n = tile_id % width // group_size\nreturn (pid_m, pid_n)"
  },
  {
    "name": "tile_classic",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": null
      },
      {
        "name": "BLOCK_K",
        "annotation": null
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tile_classic(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):",
      "    pid_m = tile_id // tl.cdiv(N, BLOCK_N)",
      "    pid_n = tile_id % tl.cdiv(N, BLOCK_N)",
      "    return pid_m, pid_n"
    ],
    "file": "codes/565.py",
    "header": "def tile_classic(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):",
    "body": "pid_m = tile_id // tl.cdiv(N, BLOCK_N)\npid_n = tile_id % tl.cdiv(N, BLOCK_N)\nreturn (pid_m, pid_n)"
  },
  {
    "name": "mac_loop",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "locks",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "total_programs_streamk",
        "annotation": null
      },
      {
        "name": "total_iters_streamk",
        "annotation": null
      },
      {
        "name": "total_tiles_streamk",
        "annotation": null
      },
      {
        "name": "iters_per_tile",
        "annotation": null
      },
      {
        "name": "start_iter",
        "annotation": null
      },
      {
        "name": "end_iter",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SWIZZLING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mac_loop(",
      "    A,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    locks,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    total_programs_streamk,",
      "    total_iters_streamk,",
      "    total_tiles_streamk,",
      "    iters_per_tile,",
      "    start_iter,",
      "    end_iter,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    SWIZZLING: tl.constexpr,",
      "):",
      "",
      "    if end_iter == start_iter:",
      "        return",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)",
      "",
      "    tile_id = start_iter // iters_per_tile",
      "    if SWIZZLING:",
      "        pid_m, pid_n = tile_swizzling(",
      "            tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M",
      "        )",
      "    else:",
      "        pid_m, pid_n = tile_classic(",
      "            tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M",
      "        )",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    ram = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "    rbn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "    rk = tl.arange(0, BLOCK_K)",
      "    A = (",
      "        A",
      "        + (ram[:, None] * stride_am + rk[None, :] * stride_ak)",
      "        + BLOCK_K * stride_ak * (start_iter % iters_per_tile)",
      "    )",
      "    B = (",
      "        B",
      "        + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)",
      "        + BLOCK_K * stride_bk * (start_iter % iters_per_tile)",
      "    )",
      "",
      "    for current_iter in range(start_iter, end_iter):",
      "        a = tl.load(A)",
      "        b = tl.load(B)",
      "        acc += tl.dot(a, b)",
      "        A += BLOCK_K * stride_ak",
      "        B += BLOCK_K * stride_bk",
      "",
      "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "    if end_iter % iters_per_tile == 0:",
      "        tl.store(C, acc)",
      "        if start_iter % iters_per_tile != 0:",
      "            tl.store(locks + tile_id, 1)",
      "    else:",
      "        while tl.atomic_min(locks + tile_id, 1) != 1:",
      "            pass",
      "        tl.atomic_add(C, acc)"
    ],
    "file": "codes/565.py",
    "header": "def mac_loop(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_programs_streamk, total_iters_streamk, total_tiles_streamk, iters_per_tile, start_iter, end_iter, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr, SWIZZLING: tl.constexpr):",
    "body": "if end_iter == start_iter:\n    return\nacc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\ntile_id = start_iter // iters_per_tile\nif SWIZZLING:\n    pid_m, pid_n = tile_swizzling(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nelse:\n    pid_m, pid_n = tile_classic(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nrm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nrn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nram = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\nrbn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\nrk = tl.arange(0, BLOCK_K)\nA = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak) + BLOCK_K * stride_ak * (start_iter % iters_per_tile)\nB = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn) + BLOCK_K * stride_bk * (start_iter % iters_per_tile)\nfor current_iter in range(start_iter, end_iter):\n    a = tl.load(A)\n    b = tl.load(B)\n    acc += tl.dot(a, b)\n    A += BLOCK_K * stride_ak\n    B += BLOCK_K * stride_bk\nC = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\nif end_iter % iters_per_tile == 0:\n    tl.store(C, acc)\n    if start_iter % iters_per_tile != 0:\n        tl.store(locks + tile_id, 1)\nelse:\n    while tl.atomic_min(locks + tile_id, 1) != 1:\n        pass\n    tl.atomic_add(C, acc)"
  },
  {
    "name": "first_wave",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "locks",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "total_programs_streamk",
        "annotation": null
      },
      {
        "name": "total_iters_streamk",
        "annotation": null
      },
      {
        "name": "total_tiles_streamk",
        "annotation": null
      },
      {
        "name": "iters_per_tile",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SWIZZLING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def first_wave(",
      "    A,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    locks,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    total_programs_streamk,",
      "    total_iters_streamk,",
      "    total_tiles_streamk,",
      "    iters_per_tile,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    SWIZZLING: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    full = total_iters_streamk // total_programs_streamk",
      "    remaining = total_iters_streamk % total_programs_streamk",
      "    start_iter_1 = pid * full + tl.minimum(pid, remaining)",
      "    end_iter = (pid + 1) * full + tl.minimum(pid + 1, remaining)",
      "    start_iter_2 = start_iter_1 + (iters_per_tile - start_iter_1 % iters_per_tile)",
      "    start_iter_2 = tl.minimum(start_iter_2, end_iter)",
      "    start_iter_3 = start_iter_2 + (iters_per_tile - start_iter_2 % iters_per_tile)",
      "    start_iter_3 = tl.minimum(start_iter_3, end_iter)",
      "",
      "    mac_loop(",
      "        A,",
      "        B,",
      "        C,",
      "        M,",
      "        N,",
      "        K,",
      "        locks,",
      "        stride_am,",
      "        stride_ak,",
      "        stride_bk,",
      "        stride_bn,",
      "        stride_cm,",
      "        stride_cn,",
      "        total_programs_streamk,",
      "        total_iters_streamk,",
      "        total_tiles_streamk,",
      "        iters_per_tile,",
      "        start_iter_1,",
      "        start_iter_2,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        ACC_TYPE,",
      "        GROUP_M,",
      "        SWIZZLING,",
      "    )",
      "",
      "    mac_loop(",
      "        A,",
      "        B,",
      "        C,",
      "        M,",
      "        N,",
      "        K,",
      "        locks,",
      "        stride_am,",
      "        stride_ak,",
      "        stride_bk,",
      "        stride_bn,",
      "        stride_cm,",
      "        stride_cn,",
      "        total_programs_streamk,",
      "        total_iters_streamk,",
      "        total_tiles_streamk,",
      "        iters_per_tile,",
      "        start_iter_2,",
      "        start_iter_3,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        ACC_TYPE,",
      "        GROUP_M,",
      "        SWIZZLING,",
      "    )",
      "",
      "    mac_loop(",
      "        A,",
      "        B,",
      "        C,",
      "        M,",
      "        N,",
      "        K,",
      "        locks,",
      "        stride_am,",
      "        stride_ak,",
      "        stride_bk,",
      "        stride_bn,",
      "        stride_cm,",
      "        stride_cn,",
      "        total_programs_streamk,",
      "        total_iters_streamk,",
      "        total_tiles_streamk,",
      "        iters_per_tile,",
      "        start_iter_3,",
      "        end_iter,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        BLOCK_K,",
      "        ACC_TYPE,",
      "        GROUP_M,",
      "        SWIZZLING,",
      "    )"
    ],
    "file": "codes/565.py",
    "header": "def first_wave(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_programs_streamk, total_iters_streamk, total_tiles_streamk, iters_per_tile, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr, SWIZZLING: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nfull = total_iters_streamk // total_programs_streamk\nremaining = total_iters_streamk % total_programs_streamk\nstart_iter_1 = pid * full + tl.minimum(pid, remaining)\nend_iter = (pid + 1) * full + tl.minimum(pid + 1, remaining)\nstart_iter_2 = start_iter_1 + (iters_per_tile - start_iter_1 % iters_per_tile)\nstart_iter_2 = tl.minimum(start_iter_2, end_iter)\nstart_iter_3 = start_iter_2 + (iters_per_tile - start_iter_2 % iters_per_tile)\nstart_iter_3 = tl.minimum(start_iter_3, end_iter)\nmac_loop(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_programs_streamk, total_iters_streamk, total_tiles_streamk, iters_per_tile, start_iter_1, start_iter_2, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE, GROUP_M, SWIZZLING)\nmac_loop(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_programs_streamk, total_iters_streamk, total_tiles_streamk, iters_per_tile, start_iter_2, start_iter_3, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE, GROUP_M, SWIZZLING)\nmac_loop(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_programs_streamk, total_iters_streamk, total_tiles_streamk, iters_per_tile, start_iter_3, end_iter, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE, GROUP_M, SWIZZLING)"
  },
  {
    "name": "full_tiles",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "total_programs_streamk",
        "annotation": null
      },
      {
        "name": "total_iters_streamk",
        "annotation": null
      },
      {
        "name": "total_tiles_streamk",
        "annotation": null
      },
      {
        "name": "iters_per_tile",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SWIZZLING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def full_tiles(",
      "    A,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    total_programs_streamk,",
      "    total_iters_streamk,",
      "    total_tiles_streamk,",
      "    iters_per_tile,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    SWIZZLING: tl.constexpr,",
      "):",
      "",
      "    tile_id = tl.program_id(0) + total_tiles_streamk",
      "    if SWIZZLING:",
      "        pid_m, pid_n = tile_swizzling(",
      "            tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M",
      "        )",
      "    else:",
      "        pid_m, pid_n = tile_classic(",
      "            tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M",
      "        )",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    ram = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "    rbn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "    rk = tl.arange(0, BLOCK_K)",
      "",
      "    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)",
      "    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)",
      "    for k in range(K, 0, -BLOCK_K):",
      "        a = tl.load(A)",
      "        b = tl.load(B)",
      "        acc += tl.dot(a, b)",
      "        A += BLOCK_K * stride_ak",
      "        B += BLOCK_K * stride_bk",
      "    acc = acc.to(tl.float16)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "    tl.store(C, acc)"
    ],
    "file": "codes/565.py",
    "header": "def full_tiles(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_programs_streamk, total_iters_streamk, total_tiles_streamk, iters_per_tile, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr, SWIZZLING: tl.constexpr):",
    "body": "tile_id = tl.program_id(0) + total_tiles_streamk\nif SWIZZLING:\n    pid_m, pid_n = tile_swizzling(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nelse:\n    pid_m, pid_n = tile_classic(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nrm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nrn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nram = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\nrbn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\nrk = tl.arange(0, BLOCK_K)\nA = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)\nB = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)\nacc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\nfor k in range(K, 0, -BLOCK_K):\n    a = tl.load(A)\n    b = tl.load(B)\n    acc += tl.dot(a, b)\n    A += BLOCK_K * stride_ak\n    B += BLOCK_K * stride_bk\nacc = acc.to(tl.float16)\nrm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nrn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nC = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\ntl.store(C, acc)"
  },
  {
    "name": "chunk_ttt_linear_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hb",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "hbt",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    v_new,",
      "    eta,",
      "    w,",
      "    b,",
      "    eps,",
      "    h,",
      "    hb,",
      "    h0,",
      "    hb0,",
      "    ht,",
      "    hbt,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    offs = tl.arange(0, BV)",
      "    b_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_hb = tl.make_block_ptr(",
      "            hb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_hb, b_hb.to(p_hb.dtype.element_ty), boundary_check=(0,))",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_eta_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "        b_eta_last = tl.load(p_eta_last)",
      "        b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/425.py",
    "header": "def chunk_ttt_linear_fwd_kernel_h(k, v, v_new, eta, w, b, eps, h, hb, h0, hb0, ht, hbt, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_INITIAL_STATE_B: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nb_hb = tl.zeros([BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option='zero').to(tl.float32)\nif USE_INITIAL_STATE_B:\n    p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option='zero').to(tl.float32)\noffs = tl.arange(0, BV)\nb_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)\nb_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)\nfor i_t in range(NT):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_hb = tl.make_block_ptr(hb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_hb, b_hb.to(p_hb.dtype.element_ty), boundary_check=(0,))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_eta_last = eta + bos * H + i_h + (T - 1) * H if i_t == NT - 1 else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\n    b_kh = tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32) + b_hb[None, :]\n    b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)\n    mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n    xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n    rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n    b_kh_hat = (b_kh - mean) * rstd\n    b_v = b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype) + b_b[None, :].to(b_k.dtype) - b_v.to(b_k.dtype) + tl.trans(b_k)\n    b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n    b_v2 = rstd * (V * b_v - tl.sum(b_v, axis=1, keep_dims=True) - b_kh_hat.to(b_k.dtype) * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)) / V\n    tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n    b_eta_last = tl.load(p_eta_last)\n    b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n    b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_ttt_linear_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hb",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    eta,",
      "    h,",
      "    hb,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    eta += bos * H + i_h",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    hb += (i_tg * H + i_h) * V",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_eta = H",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (0, i_t * BT), (BK, BT), (0, 1))",
      "    p_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))",
      "    p_h = tl.make_block_ptr(h, (K, V), (V, 1), (0, i_v * BV), (BK, BV), (1, 0))",
      "    p_hb = tl.make_block_ptr(hb, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    b_eta = tl.load(p_eta, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    b_hb = tl.load(p_hb, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    b_o = tl.dot(b_q, b_h, allow_tf32=False)",
      "",
      "    b_A = tl.dot(b_q, b_k, allow_tf32=False)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_A = tl.where(m_A, b_A, 0)",
      "    b_Ae = tl.where(m_A, b_eta[:, None], 0.0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "    b_o = (",
      "        b_o - tl.dot(b_eta[:, None] * b_A.to(b_v.dtype), b_v, allow_tf32=False)",
      "    ) * scale",
      "    b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v.dtype), b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/425.py",
    "header": "def chunk_ttt_linear_fwd_kernel_o(q, k, v, eta, h, hb, o, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\neta += bos * H + i_h\no += (bos * H + i_h) * V\nh += (i_tg * H + i_h) * K * V\nhb += (i_tg * H + i_h) * V\nstride_qk = H * K\nstride_vo = H * V\nstride_eta = H\np_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (0, i_t * BT), (BK, BT), (0, 1))\np_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))\np_h = tl.make_block_ptr(h, (K, V), (V, 1), (0, i_v * BV), (BK, BV), (1, 0))\np_hb = tl.make_block_ptr(hb, (V,), (1,), (i_v * BV,), (BV,), (0,))\nb_q = tl.load(p_q, boundary_check=(0, 1), padding_option='zero')\nb_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\nb_eta = tl.load(p_eta, boundary_check=(0,), padding_option='zero')\nb_h = tl.load(p_h, boundary_check=(0, 1), padding_option='zero')\nb_hb = tl.load(p_hb, boundary_check=(0,), padding_option='zero')\nb_o = tl.dot(b_q, b_h, allow_tf32=False)\nb_A = tl.dot(b_q, b_k, allow_tf32=False)\no_i = tl.arange(0, BT)\nm_A = o_i[:, None] >= o_i[None, :]\nb_A = tl.where(m_A, b_A, 0)\nb_Ae = tl.where(m_A, b_eta[:, None], 0.0)\np_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\nb_o = (b_o - tl.dot(b_eta[:, None] * b_A.to(b_v.dtype), b_v, allow_tf32=False)) * scale\nb_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v.dtype), b_v, allow_tf32=False)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_ttt_linear_bwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_bwd_kernel_h(",
      "    k,",
      "    v,",
      "    v_new,",
      "    eta,",
      "    w,",
      "    b,",
      "    eps,",
      "    h,",
      "    h0,",
      "    hb0,",
      "    x,",
      "    y,",
      "    r,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    offs = tl.arange(0, BV)",
      "    b_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_eta_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "        b_eta_last = tl.load(p_eta_last)",
      "        b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)"
    ],
    "file": "codes/425.py",
    "header": "def chunk_ttt_linear_bwd_kernel_h(k, v, v_new, eta, w, b, eps, h, h0, hb0, x, y, r, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_INITIAL_STATE_B: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nb_hb = tl.zeros([BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option='zero').to(tl.float32)\nif USE_INITIAL_STATE_B:\n    p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option='zero').to(tl.float32)\noffs = tl.arange(0, BV)\nb_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)\nb_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)\nfor i_t in range(NT):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_x = tl.make_block_ptr(x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_y = tl.make_block_ptr(y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_r = tl.make_block_ptr(r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0))\n    p_eta_last = eta + bos * H + i_h + (T - 1) * H if i_t == NT - 1 else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\n    b_kh = tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32) + b_hb[None, :]\n    b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)\n    mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n    xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n    rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n    b_kh_hat = (b_kh - mean) * rstd\n    b_v = b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype) + b_b[None, :].to(b_k.dtype) - b_v.to(b_k.dtype) + tl.trans(b_k)\n    b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n    b_v2 = rstd * (V * b_v - tl.sum(b_v, axis=1, keep_dims=True) - b_kh_hat.to(b_k.dtype) * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)) / V\n    tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n    b_eta_last = tl.load(p_eta_last)\n    b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n    b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)"
  },
  {
    "name": "chunk_ttt_linear_bwd_kernel_dv_local",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [4]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_bwd_kernel_dv_local(",
      "    q,",
      "    k,",
      "    eta,",
      "    do,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    eta += bos * H + i_h",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_eta = H",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_k, b_q)",
      "",
      "    p_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))",
      "    b_eta = tl.load(p_eta, boundary_check=(0,))",
      "    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]",
      "    b_A = -tl.where(mask, b_A * scale * b_eta[None, :], 0).to(do.dtype.element_ty)",
      "    b_Ae = -tl.where(mask, b_eta[None, :], 0).to(do.dtype.element_ty)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dv = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/425.py",
    "header": "def chunk_ttt_linear_bwd_kernel_dv_local(q, k, eta, do, dv, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\neta += bos * H + i_h\ndo += (bos * H + i_h) * V\ndv += (bos * H + i_h) * V\nstride_qk = H * K\nstride_vo = H * V\nstride_eta = H\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A += tl.dot(b_k, b_q)\np_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))\nb_eta = tl.load(p_eta, boundary_check=(0,))\nmask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\nb_A = -tl.where(mask, b_A * scale * b_eta[None, :], 0).to(do.dtype.element_ty)\nb_Ae = -tl.where(mask, b_eta[None, :], 0).to(do.dtype.element_ty)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_ttt_linear_bwd_kernel_norm",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'USE_FINAL_STATE_GRADIENT_B': lambda args: args['dhbt'] is not None, 'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['dhb0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8, 16]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dhbt",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "dhb0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dhb",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv_new",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_bwd_kernel_norm(",
      "    q,",
      "    k,",
      "    v,",
      "    v_new,",
      "    x,",
      "    y,",
      "    r,",
      "    w,",
      "    b,",
      "    eta,",
      "    h,",
      "    dht,",
      "    dhbt,",
      "    dh0,",
      "    dhb0,",
      "    do,",
      "    dh,",
      "    dhb,",
      "    dv,",
      "    dv_new,",
      "    dk,",
      "    dw,",
      "    db,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT_B: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_dhb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option=\"zero\")",
      "    if USE_FINAL_STATE_GRADIENT_B:",
      "        p_dhbt = tl.make_block_ptr(",
      "            dhbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    offs_v = tl.arange(0, BV)",
      "    offs_t = tl.arange(0, BT)",
      "    b_w = tl.load(w + i_h * V + offs_v, mask=offs_v < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + offs_v, mask=offs_v < V, other=0.0)",
      "    b_dw = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_w.dtype,",
      "    )",
      "    b_db = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_b.dtype,",
      "    )",
      "    p_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "    p_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dhb = tl.make_block_ptr(",
      "            dhb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dhb, b_dhb.to(p_dhb.dtype.element_ty), boundary_check=(0,))",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv_new = tl.make_block_ptr(",
      "            dv_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_eta_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_dv_new = tl.load(p_dv_new, boundary_check=(0, 1), padding_option=\"zero\").to(",
      "            b_k.dtype",
      "        )",
      "        b_eta_last = tl.load(p_eta_last)",
      "        b_dv_new -= tl.dot(b_eta_last * b_k, b_dh.to(b_k.dtype))",
      "        b_dv_new -= b_eta_last * b_dhb.to(b_k.dtype)[None, :]",
      "",
      "        b_v_new = tl.load(p_v_new, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_x = tl.load(p_x, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_y = tl.load(p_y, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option=\"zero\").to(",
      "            tl.float32",
      "        )",
      "        b_dy = (",
      "            b_rstd",
      "            * (",
      "                b_dv_new * V",
      "                - tl.sum(b_dv_new, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dx = (",
      "            -b_rstd",
      "            * (",
      "                b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True)",
      "                + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_drstd = tl.sum(",
      "            b_dv_new.to(b_rstd.dtype) * b_v_new.to(b_rstd.dtype) / b_rstd,",
      "            axis=1,",
      "            keep_dims=True,",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_w = b_w.to(b_k.dtype)",
      "        b_b = b_b.to(b_k.dtype)",
      "        b_dv = -b_w * b_dy.to(b_k.dtype)",
      "        b_dk = b_w * b_dy.to(b_k.dtype)",
      "        b_dw += tl.sum(",
      "            2 * b_w * b_x * b_dy.to(b_k.dtype)",
      "            + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype),",
      "            axis=0,",
      "        ).to(b_dw.dtype)",
      "        b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)",
      "        b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_dkh = (",
      "            b_rstd",
      "            * (",
      "                V * b_dx",
      "                - tl.sum(b_dx, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V",
      "        b_dkh = tl.where(",
      "            (offs_v < V)[None, :] * (offs_t < T - i_t * BT)[:, None], b_dkh, 0.0",
      "        )",
      "        b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)",
      "        b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(",
      "            tl.trans(b_k).to(b_dkh.dtype), b_dkh",
      "        )",
      "        b_dhb += tl.sum(b_do + b_dkh, axis=0)",
      "        b_dh = tl.where((offs_v < V)[None, :], b_dh, 0.0)",
      "        b_dhb = tl.where((offs_v < V), b_dhb, 0.0)",
      "",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_INITIAL_STATE_B:",
      "        p_dhb0 = tl.make_block_ptr(",
      "            dhb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/425.py",
    "header": "def chunk_ttt_linear_bwd_kernel_norm(q, k, v, v_new, x, y, r, w, b, eta, h, dht, dhbt, dh0, dhb0, do, dh, dhb, dv, dv_new, dk, dw, db, cu_seqlens, chunk_offsets, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT_B: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_INITIAL_STATE_B: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nb_dhb = tl.zeros([BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option='zero')\nif USE_FINAL_STATE_GRADIENT_B:\n    p_dhbt = tl.make_block_ptr(dhbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option='zero')\noffs_v = tl.arange(0, BV)\noffs_t = tl.arange(0, BT)\nb_w = tl.load(w + i_h * V + offs_v, mask=offs_v < V, other=0.0)\nb_b = tl.load(b + i_h * V + offs_v, mask=offs_v < V, other=0.0)\nb_dw = tl.zeros([BV], dtype=b_w.dtype)\nb_db = tl.zeros([BV], dtype=b_b.dtype)\np_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\np_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\nfor i_t in range(NT - 1, -1, -1):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_dhb = tl.make_block_ptr(dhb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dhb, b_dhb.to(p_dhb.dtype.element_ty), boundary_check=(0,))\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_x = tl.make_block_ptr(x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_y = tl.make_block_ptr(y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv_new = tl.make_block_ptr(dv_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_r = tl.make_block_ptr(r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0))\n    p_eta_last = eta + bos * H + i_h + (T - 1) * H if i_t == NT - 1 else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\n    b_dv_new = tl.load(p_dv_new, boundary_check=(0, 1), padding_option='zero').to(b_k.dtype)\n    b_eta_last = tl.load(p_eta_last)\n    b_dv_new -= tl.dot(b_eta_last * b_k, b_dh.to(b_k.dtype))\n    b_dv_new -= b_eta_last * b_dhb.to(b_k.dtype)[None, :]\n    b_v_new = tl.load(p_v_new, boundary_check=(0, 1), padding_option='zero')\n    b_x = tl.load(p_x, boundary_check=(0, 1), padding_option='zero').to(b_k.dtype)\n    b_y = tl.load(p_y, boundary_check=(0, 1), padding_option='zero').to(b_k.dtype)\n    b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option='zero').to(tl.float32)\n    b_dy = b_rstd * (b_dv_new * V - tl.sum(b_dv_new, axis=1, keep_dims=True) - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)) / V\n    b_dx = -b_rstd * (b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True) + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)) / V\n    b_drstd = tl.sum(b_dv_new.to(b_rstd.dtype) * b_v_new.to(b_rstd.dtype) / b_rstd, axis=1, keep_dims=True)\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\n    b_w = b_w.to(b_k.dtype)\n    b_b = b_b.to(b_k.dtype)\n    b_dv = -b_w * b_dy.to(b_k.dtype)\n    b_dk = b_w * b_dy.to(b_k.dtype)\n    b_dw += tl.sum(2 * b_w * b_x * b_dy.to(b_k.dtype) + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype), axis=0).to(b_dw.dtype)\n    b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)\n    b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)\n    b_q = tl.load(p_q, boundary_check=(0, 1), padding_option='zero')\n    b_h = tl.load(p_h, boundary_check=(0, 1), padding_option='zero')\n    b_do = tl.load(p_do, boundary_check=(0, 1), padding_option='zero')\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dkh = b_rstd * (V * b_dx - tl.sum(b_dx, axis=1, keep_dims=True) - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)) / V\n    b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V\n    b_dkh = tl.where((offs_v < V)[None, :] * (offs_t < T - i_t * BT)[:, None], b_dkh, 0.0)\n    b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)\n    b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(tl.trans(b_k).to(b_dkh.dtype), b_dkh)\n    b_dhb += tl.sum(b_do + b_dkh, axis=0)\n    b_dh = tl.where((offs_v < V)[None, :], b_dh, 0.0)\n    b_dhb = tl.where(offs_v < V, b_dhb, 0.0)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))\ntl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))\nif USE_INITIAL_STATE:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\nif USE_INITIAL_STATE_B:\n    p_dhb0 = tl.make_block_ptr(dhb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_bwd_kernel_dqke",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "e",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dhb",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "de",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dqke(",
      "    q,",
      "    k,",
      "    v,",
      "    e,",
      "    h,",
      "    do,",
      "    dh,",
      "    dhb,",
      "    dq,",
      "    dk,",
      "    de,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    dh += (i_tg * H + i_h) * K * V",
      "    dhb += (i_tg * H + i_h) * V",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    e += bos * H + i_h",
      "    de += bos * H + i_h",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_e = H",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_de = tl.zeros(",
      "        [",
      "            BT,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    p_e_last = (",
      "        (e + (i_t * BT + BT - 1) * stride_e)",
      "        if (i_t * BT + BT) <= T",
      "        else (e + (T - 1) * stride_e)",
      "    )",
      "    i_last = (BT - 1) if (i_t * BT + BT) <= T else (T % BT - 1)",
      "    mask = tl.arange(0, BT) == i_last",
      "    b_e_last = tl.load(p_e_last)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dhb = tl.make_block_ptr(dhb, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dhb = tl.load(p_dhb, boundary_check=(0,))",
      "",
      "        b_ds += tl.dot(b_do, tl.trans(b_v))",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "",
      "        b_dk -= b_e_last * tl.dot(b_v, b_dh.to(b_v.dtype))",
      "        b_de -= mask * tl.sum(tl.trans(b_dh) * tl.dot(tl.trans(b_k), b_v.to(b_k.dtype)))",
      "        b_de -= mask * tl.sum(b_dhb * tl.sum(b_v, axis=0).to(b_k.dtype))",
      "",
      "    o_i = tl.arange(0, BT)",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_e = tl.make_block_ptr(e, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_e = tl.load(p_e, boundary_check=(0,))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_de = tl.make_block_ptr(de, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))",
      "",
      "    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)",
      "    b_ds = b_ds.to(b_k.dtype)",
      "    b_dq -= tl.dot(b_ds, b_k) * b_e[:, None]",
      "    b_dk -= tl.dot(tl.trans(b_ds), b_q * b_e[:, None]) * scale",
      "    b_de -= tl.sum(scale * tl.dot(b_ds, b_k) * b_q, axis=1)",
      "    b_de -= tl.sum(b_ds, axis=1)",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/425.py",
    "header": "def chunk_bwd_kernel_dqke(q, k, v, e, h, do, dh, dhb, dq, dk, de, cu_seqlens, chunk_indices, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nv += (bos * H + i_h) * V\ndo += (bos * H + i_h) * V\nh += (i_tg * H + i_h) * K * V\ndh += (i_tg * H + i_h) * K * V\ndhb += (i_tg * H + i_h) * V\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\ndq += (bos * H + i_h) * K\ndk += (bos * H + i_h) * K\ne += bos * H + i_h\nde += bos * H + i_h\nstride_qk = H * K\nstride_vo = H * V\nstride_e = H\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_ds = tl.zeros([BT, BT], dtype=tl.float32)\nb_de = tl.zeros([BT], dtype=tl.float32)\np_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\np_e_last = e + (i_t * BT + BT - 1) * stride_e if i_t * BT + BT <= T else e + (T - 1) * stride_e\ni_last = BT - 1 if i_t * BT + BT <= T else T % BT - 1\nmask = tl.arange(0, BT) == i_last\nb_e_last = tl.load(p_e_last)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dhb = tl.make_block_ptr(dhb, (V,), (1,), (i_v * BV,), (BV,), (0,))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dhb = tl.load(p_dhb, boundary_check=(0,))\n    b_ds += tl.dot(b_do, tl.trans(b_v))\n    b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n    b_dk -= b_e_last * tl.dot(b_v, b_dh.to(b_v.dtype))\n    b_de -= mask * tl.sum(tl.trans(b_dh) * tl.dot(tl.trans(b_k), b_v.to(b_k.dtype)))\n    b_de -= mask * tl.sum(b_dhb * tl.sum(b_v, axis=0).to(b_k.dtype))\no_i = tl.arange(0, BT)\np_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_e = tl.make_block_ptr(e, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_e = tl.load(p_e, boundary_check=(0,))\np_dq = tl.make_block_ptr(dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_de = tl.make_block_ptr(de, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))\nb_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)\nb_ds = b_ds.to(b_k.dtype)\nb_dq -= tl.dot(b_ds, b_k) * b_e[:, None]\nb_dk -= tl.dot(tl.trans(b_ds), b_q * b_e[:, None]) * scale\nb_de -= tl.sum(scale * tl.dot(b_ds, b_k) * b_q, axis=1)\nb_de -= tl.sum(b_ds, axis=1)\nb_dq *= scale\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "linear_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[linear_forward_config(32, 32, 32, n_warps=2, n_stages=2), linear_forward_config(64, 32, 32, n_warps=2, n_stages=5), linear_forward_config(64, 32, 128, n_warps=4, n_stages=4), linear_forward_config(64, 32, 256, n_warps=4, n_stages=4), linear_forward_config(128, 32, 32, n_warps=4, n_stages=4), linear_forward_config(128, 32, 64, n_warps=4, n_stages=4), linear_forward_config(128, 32, 128, n_warps=4, n_stages=4), linear_forward_config(128, 64, 256, n_warps=8, n_stages=3)], key=['batch_dim', 'in_feat_dim', 'out_feat_dim', 'fp16'])",
      "@triton.heuristics({'tf32': lambda _: allow_tf32()})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "bias_pointer",
        "annotation": null
      },
      {
        "name": "pre_act_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "in_feat_dim",
        "annotation": null
      },
      {
        "name": "out_feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_in_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_in_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_out_feat_stride",
        "annotation": null
      },
      {
        "name": "pre_act_batch_stride",
        "annotation": null
      },
      {
        "name": "pre_act_out_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_out_feat_stride",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "add_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_pre_act",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp16",
        "annotation": "tl.constexpr"
      },
      {
        "name": "tf32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_IN_FEAT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_OUT_FEAT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_BATCH",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    bias_pointer,",
      "    pre_act_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    in_feat_dim,",
      "    out_feat_dim,",
      "    input_batch_stride,",
      "    input_in_feat_stride,",
      "    weight_in_feat_stride,",
      "    weight_out_feat_stride,",
      "    pre_act_batch_stride,",
      "    pre_act_out_feat_stride,",
      "    output_batch_stride,",
      "    output_out_feat_stride,",
      "    param,",
      "    add_bias: tl.constexpr,",
      "    act_func: tl.constexpr,",
      "    save_pre_act: tl.constexpr,",
      "    fp16: tl.constexpr,",
      "    tf32: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_IN_FEAT: tl.constexpr,",
      "    BLOCK_SIZE_OUT_FEAT: tl.constexpr,",
      "    GROUP_SIZE_BATCH: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    n_batch_pids = tl.cdiv(batch_dim, BLOCK_SIZE_BATCH)",
      "    n_out_feat_pids = tl.cdiv(out_feat_dim, BLOCK_SIZE_OUT_FEAT)",
      "    pids_per_group = GROUP_SIZE_BATCH * n_out_feat_pids",
      "    group_id = pid // pids_per_group",
      "    first_batch_pid = group_id * GROUP_SIZE_BATCH",
      "    GROUP_SIZE_BATCH = min(n_batch_pids - first_batch_pid, GROUP_SIZE_BATCH)",
      "    batch_pid = first_batch_pid + (pid % GROUP_SIZE_BATCH)",
      "    out_feat_pid = (pid % pids_per_group) // GROUP_SIZE_BATCH",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    out_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(",
      "        0, BLOCK_SIZE_OUT_FEAT",
      "    )",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    out_feat_mask = out_feat_offset < out_feat_dim",
      "",
      "    input_pointer += input_batch_stride * batch_offset[:, None]",
      "    weight_pointer += weight_out_feat_stride * out_feat_offset[None, :]",
      "",
      "    accum = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32)",
      "",
      "    for block_ind in range(0, tl.cdiv(in_feat_dim, BLOCK_SIZE_IN_FEAT)):",
      "        in_feat_offset = block_ind * BLOCK_SIZE_IN_FEAT + tl.arange(",
      "            0, BLOCK_SIZE_IN_FEAT",
      "        )",
      "        in_feat_mask = in_feat_offset < in_feat_dim",
      "",
      "        curr_input_pointer = (",
      "            input_pointer + input_in_feat_stride * in_feat_offset[None, :]",
      "        )",
      "        curr_weight_pointer = (",
      "            weight_pointer + weight_in_feat_stride * in_feat_offset[:, None]",
      "        )",
      "",
      "        input_block = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & in_feat_mask[None, :]",
      "        )",
      "        weight_block = tl.load(",
      "            curr_weight_pointer, mask=out_feat_mask[None, :] & in_feat_mask[:, None]",
      "        )",
      "",
      "        if fp16:",
      "            input_block = input_block.to(tl.float16)",
      "            weight_block = weight_block.to(tl.float16)",
      "",
      "        accum += tl.dot(input_block, weight_block, allow_tf32=tf32)",
      "",
      "    if add_bias:",
      "        bias = tl.load(bias_pointer + out_feat_offset, mask=out_feat_mask)",
      "",
      "        if fp16:",
      "            bias = bias.to(tl.float16)",
      "",
      "        accum += bias[None, :]",
      "",
      "    if act_func is not None:",
      "        if save_pre_act:",
      "            pre_act_pointer += (",
      "                pre_act_batch_stride * batch_offset[:, None]",
      "                + pre_act_out_feat_stride * out_feat_offset[None, :]",
      "            )",
      "            tl.store(",
      "                pre_act_pointer,",
      "                accum,",
      "                mask=batch_mask[:, None] & out_feat_mask[None, :],",
      "            )",
      "",
      "        accum = apply_act_func(accum, None, None, None, param, act_func, False)",
      "",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_out_feat_stride * out_feat_offset[None, :]",
      "    )",
      "    tl.store(output_pointer, accum, mask=batch_mask[:, None] & out_feat_mask[None, :])"
    ],
    "file": "codes/14.py",
    "header": "def linear_forward_kernel(input_pointer, weight_pointer, bias_pointer, pre_act_pointer, output_pointer, batch_dim, in_feat_dim, out_feat_dim, input_batch_stride, input_in_feat_stride, weight_in_feat_stride, weight_out_feat_stride, pre_act_batch_stride, pre_act_out_feat_stride, output_batch_stride, output_out_feat_stride, param, add_bias: tl.constexpr, act_func: tl.constexpr, save_pre_act: tl.constexpr, fp16: tl.constexpr, tf32: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_IN_FEAT: tl.constexpr, BLOCK_SIZE_OUT_FEAT: tl.constexpr, GROUP_SIZE_BATCH: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nn_batch_pids = tl.cdiv(batch_dim, BLOCK_SIZE_BATCH)\nn_out_feat_pids = tl.cdiv(out_feat_dim, BLOCK_SIZE_OUT_FEAT)\npids_per_group = GROUP_SIZE_BATCH * n_out_feat_pids\ngroup_id = pid // pids_per_group\nfirst_batch_pid = group_id * GROUP_SIZE_BATCH\nGROUP_SIZE_BATCH = min(n_batch_pids - first_batch_pid, GROUP_SIZE_BATCH)\nbatch_pid = first_batch_pid + pid % GROUP_SIZE_BATCH\nout_feat_pid = pid % pids_per_group // GROUP_SIZE_BATCH\nbatch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\nout_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(0, BLOCK_SIZE_OUT_FEAT)\nbatch_mask = batch_offset < batch_dim\nout_feat_mask = out_feat_offset < out_feat_dim\ninput_pointer += input_batch_stride * batch_offset[:, None]\nweight_pointer += weight_out_feat_stride * out_feat_offset[None, :]\naccum = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32)\nfor block_ind in range(0, tl.cdiv(in_feat_dim, BLOCK_SIZE_IN_FEAT)):\n    in_feat_offset = block_ind * BLOCK_SIZE_IN_FEAT + tl.arange(0, BLOCK_SIZE_IN_FEAT)\n    in_feat_mask = in_feat_offset < in_feat_dim\n    curr_input_pointer = input_pointer + input_in_feat_stride * in_feat_offset[None, :]\n    curr_weight_pointer = weight_pointer + weight_in_feat_stride * in_feat_offset[:, None]\n    input_block = tl.load(curr_input_pointer, mask=batch_mask[:, None] & in_feat_mask[None, :])\n    weight_block = tl.load(curr_weight_pointer, mask=out_feat_mask[None, :] & in_feat_mask[:, None])\n    if fp16:\n        input_block = input_block.to(tl.float16)\n        weight_block = weight_block.to(tl.float16)\n    accum += tl.dot(input_block, weight_block, allow_tf32=tf32)\nif add_bias:\n    bias = tl.load(bias_pointer + out_feat_offset, mask=out_feat_mask)\n    if fp16:\n        bias = bias.to(tl.float16)\n    accum += bias[None, :]\nif act_func is not None:\n    if save_pre_act:\n        pre_act_pointer += pre_act_batch_stride * batch_offset[:, None] + pre_act_out_feat_stride * out_feat_offset[None, :]\n        tl.store(pre_act_pointer, accum, mask=batch_mask[:, None] & out_feat_mask[None, :])\n    accum = apply_act_func(accum, None, None, None, param, act_func, False)\noutput_pointer += output_batch_stride * batch_offset[:, None] + output_out_feat_stride * out_feat_offset[None, :]\ntl.store(output_pointer, accum, mask=batch_mask[:, None] & out_feat_mask[None, :])"
  },
  {
    "name": "fused_recurrent_hgrn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_hgrn_fwd_kernel(",
      "    x,",
      "    g,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_n = tl.program_id(0), tl.program_id(1)",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_x = x + bos * D + o_d",
      "    p_g = g + bos * D + o_d",
      "    p_o = o + bos * D + o_d",
      "",
      "    b_h = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_n * D + o_d",
      "        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)",
      "    for _ in range(0, T):",
      "        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_h = exp(b_g) * b_h + b_x",
      "        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)",
      "",
      "        p_x += D",
      "        p_g += D",
      "        p_o += D",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_n * D + o_d",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)"
    ],
    "file": "codes/398.py",
    "header": "def fused_recurrent_hgrn_fwd_kernel(x, g, o, h0, ht, cu_seqlens, T, D: tl.constexpr, BD: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_d, i_n = (tl.program_id(0), tl.program_id(1))\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\np_x = x + bos * D + o_d\np_g = g + bos * D + o_d\np_o = o + bos * D + o_d\nb_h = tl.zeros([BD], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_n * D + o_d\n    b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)\n    b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n    b_h = exp(b_g) * b_h + b_x\n    tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)\n    p_x += D\n    p_g += D\n    p_o += D\nif STORE_FINAL_STATE:\n    p_ht = ht + i_n * D + o_d\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)"
  },
  {
    "name": "fused_recurrent_hgrn_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_hgrn_bwd_kernel(",
      "    g,",
      "    o,",
      "    h0,",
      "    dx,",
      "    dg,",
      "    do,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_n = tl.program_id(0), tl.program_id(1)",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_g = g + (bos + T - 1) * D + o_d",
      "    p_o = o + (bos + T - 2) * D + o_d",
      "    p_dx = dx + (bos + T - 1) * D + o_d",
      "    p_dg = dg + (bos + T - 1) * D + o_d",
      "    p_do = do + (bos + T - 1) * D + o_d",
      "",
      "    b_dh = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = dht + i_n * D + o_d",
      "        b_dh += tl.load(p_dht, mask=mask, other=0).to(tl.float32)",
      "",
      "    for i in range(T - 1, -1, -1):",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)",
      "        if i > 0:",
      "            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)",
      "        elif USE_INITIAL_STATE:",
      "            b_o = tl.load(h0 + i_n * D + o_d, mask=mask, other=0).to(tl.float32)",
      "        else:",
      "            b_o = tl.zeros([BD], dtype=tl.float32)",
      "",
      "        b_dh = b_dh + b_do",
      "        b_dx = b_dh",
      "        b_dh = b_dh * exp(b_g)",
      "        b_dg = b_dh * b_o",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)",
      "",
      "        p_g -= D",
      "        p_o -= D",
      "        p_dx -= D",
      "        p_dg -= D",
      "        p_do -= D",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = dh0 + i_n * D + o_d",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask)"
    ],
    "file": "codes/398.py",
    "header": "def fused_recurrent_hgrn_bwd_kernel(g, o, h0, dx, dg, do, dht, dh0, cu_seqlens, T, D: tl.constexpr, BD: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_d, i_n = (tl.program_id(0), tl.program_id(1))\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\np_g = g + (bos + T - 1) * D + o_d\np_o = o + (bos + T - 2) * D + o_d\np_dx = dx + (bos + T - 1) * D + o_d\np_dg = dg + (bos + T - 1) * D + o_d\np_do = do + (bos + T - 1) * D + o_d\nb_dh = tl.zeros([BD], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = dht + i_n * D + o_d\n    b_dh += tl.load(p_dht, mask=mask, other=0).to(tl.float32)\nfor i in range(T - 1, -1, -1):\n    b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n    if i > 0:\n        b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)\n    elif USE_INITIAL_STATE:\n        b_o = tl.load(h0 + i_n * D + o_d, mask=mask, other=0).to(tl.float32)\n    else:\n        b_o = tl.zeros([BD], dtype=tl.float32)\n    b_dh = b_dh + b_do\n    b_dx = b_dh\n    b_dh = b_dh * exp(b_g)\n    b_dg = b_dh * b_o\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)\n    p_g -= D\n    p_o -= D\n    p_dx -= D\n    p_dg -= D\n    p_do -= D\nif USE_INITIAL_STATE:\n    p_dh0 = dh0 + i_n * D + o_d\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask)"
  },
  {
    "name": "_bmm_chunk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_outm",
        "annotation": null
      },
      {
        "name": "stride_outn",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_fwd_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    out_ptr,",
      "    seq_idx_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_bk,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_outm,",
      "    stride_outn,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    IS_CAUSAL: tl.constexpr,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    if IS_CAUSAL:",
      "        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "            return",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + pid_h * stride_b_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit)",
      "            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)",
      "            & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    out = acc.to(out_ptr.dtype.element_ty)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)",
      "    tl.store(",
      "        out_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "codes/118.py",
    "header": "def _bmm_chunk_fwd_kernel(a_ptr, b_ptr, out_ptr, seq_idx_ptr, seqlen, chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak, stride_b_batch, stride_b_seqlen, stride_b_head, stride_bk, stride_out_batch, stride_out_chunk, stride_out_head, stride_outm, stride_outn, stride_seq_idx_batch, stride_seq_idx_seqlen, IS_CAUSAL: tl.constexpr, dot_dtype: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_ch = tl.program_id(axis=2)\npid_c = pid_ch // ngroups\npid_h = pid_ch - pid_c * ngroups\nnum_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nif IS_CAUSAL:\n    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:\n        return\na_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h * stride_b_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < K - k * BLOCK_SIZE_K), other=0.0).to(dot_dtype)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K) & (offs_n[None, :] < chunk_size_limit), other=0.0).to(dot_dtype)\n    acc += tl.dot(a, b)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_SEQ_IDX:\n    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_n = tl.load(seq_idx_ptr + offs_n * stride_seq_idx_seqlen, mask=offs_n < chunk_size_limit, other=-2)\n    acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)\nout = acc.to(out_ptr.dtype.element_ty)\nout_ptr += pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head\nout_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)\ntl.store(out_ptrs, out, mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size))"
  },
  {
    "name": "_bmm_chunk_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "res_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_n",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_head",
        "annotation": null
      },
      {
        "name": "stride_db_k",
        "annotation": null
      },
      {
        "name": "stride_res_batch",
        "annotation": null
      },
      {
        "name": "stride_res_seqlen",
        "annotation": null
      },
      {
        "name": "stride_res_head",
        "annotation": null
      },
      {
        "name": "stride_res_k",
        "annotation": null
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_bwd_kernel(",
      "    a_ptr,",
      "    dout_ptr,",
      "    db_ptr,",
      "    res_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_csize_m,",
      "    stride_dout_csize_n,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_head,",
      "    stride_db_k,",
      "    stride_res_batch,",
      "    stride_res_seqlen,",
      "    stride_res_head,",
      "    stride_res_k,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_CS: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_cs = tl.arange(0, BLOCK_SIZE_CS)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m",
      "    )",
      "    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)",
      "            & (offs_n[None, :] < K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(dout, a)",
      "        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m",
      "        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_RESIDUAL:",
      "        res_ptr += (",
      "            pid_b * stride_res_batch",
      "            + pid_c * chunk_size * stride_res_seqlen",
      "            + pid_h * stride_res_head",
      "        )",
      "        res_ptrs = res_ptr + (",
      "            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k",
      "        )",
      "        res = tl.load(",
      "            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "        ).to(tl.float32)",
      "        acc += res",
      "    db = acc.to(db_ptr.dtype.element_ty)",
      "",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_h * stride_db_head",
      "    )",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k",
      "    )",
      "    tl.store(",
      "        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "    )"
    ],
    "file": "codes/118.py",
    "header": "def _bmm_chunk_bwd_kernel(a_ptr, dout_ptr, db_ptr, res_ptr, seqlen, chunk_size, K, ngroups, stride_a_batch, stride_a_seqlen, stride_a_head, stride_ak, stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_csize_m, stride_dout_csize_n, stride_db_batch, stride_db_seqlen, stride_db_head, stride_db_k, stride_res_batch, stride_res_seqlen, stride_res_head, stride_res_k, dot_dtype: tl.constexpr, HAS_RESIDUAL: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_CS: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_ch = tl.program_id(axis=2)\npid_c = pid_ch // ngroups\npid_h = pid_ch - pid_c * ngroups\nnum_pid_n = tl.cdiv(K, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\na_ptr += pid_b * stride_a_batch + pid_c * chunk_size * stride_a_seqlen + pid_h * stride_a_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_cs = tl.arange(0, BLOCK_SIZE_CS)\ndout_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m)\na_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):\n    dout = tl.load(dout_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS), other=0.0).to(dot_dtype)\n    a = tl.load(a_ptrs, mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS) & (offs_n[None, :] < K), other=0.0).to(dot_dtype)\n    acc += tl.dot(dout, a)\n    dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m\n    a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nif HAS_RESIDUAL:\n    res_ptr += pid_b * stride_res_batch + pid_c * chunk_size * stride_res_seqlen + pid_h * stride_res_head\n    res_ptrs = res_ptr + (offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k)\n    res = tl.load(res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)).to(tl.float32)\n    acc += res\ndb = acc.to(db_ptr.dtype.element_ty)\ndb_ptr += pid_b * stride_db_batch + pid_c * chunk_size * stride_db_seqlen + pid_h * stride_db_head\ndb_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k)\ntl.store(db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K))"
  },
  {
    "name": "chunk_dplr_bwd_kernel_dhu",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV', 'V'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "bg",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_dhu(",
      "    qg,",
      "    bg,",
      "    w,",
      "    gk,",
      "    dht,",
      "    dh0,",
      "    do,",
      "    dh,",
      "    dv,",
      "    dv2,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1))",
      "",
      "    mask_k = tl.arange(0, BK) < K",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)",
      "        for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):",
      "            p_qg = tl.make_block_ptr(",
      "                qg + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_bg = tl.make_block_ptr(",
      "                bg + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_dv = tl.make_block_ptr(",
      "                dv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_do = tl.make_block_ptr(",
      "                do + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_dv2 = tl.make_block_ptr(",
      "                dv2 + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "",
      "            b_qg = tl.load(p_qg, boundary_check=(0, 1))",
      "",
      "            b_bg = tl.load(p_bg, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "",
      "            b_do = tl.load(p_do, boundary_check=(0, 1))",
      "            b_dv = tl.load(p_dv, boundary_check=(0, 1))",
      "            b_dv2 = b_dv + tl.dot(b_bg, b_dh.to(b_bg.dtype))",
      "            tl.store(p_dv2, b_dv2.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "            b_dh_tmp += tl.dot(b_qg, b_do.to(b_qg.dtype))",
      "            b_dh_tmp += tl.dot(b_w, b_dv2.to(b_qg.dtype))",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "        bg_last = tl.load(",
      "            gk + ((bos + last_idx) * H + i_h) * K + tl.arange(0, BK), mask=mask_k",
      "        )",
      "        b_dh *= exp(bg_last)[:, None]",
      "        b_dh += b_dh_tmp",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/383.py",
    "header": "def chunk_dplr_bwd_kernel_dhu(qg, bg, w, gk, dht, dh0, do, dh, dv, dv2, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dht, boundary_check=(0, 1))\nmask_k = tl.arange(0, BK) < K\nfor i_t in range(NT - 1, -1, -1):\n    p_dh = tl.make_block_ptr(dh + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):\n        p_qg = tl.make_block_ptr(qg + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n        p_bg = tl.make_block_ptr(bg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n        p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n        p_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        p_dv2 = tl.make_block_ptr(dv2 + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        b_qg = tl.load(p_qg, boundary_check=(0, 1))\n        b_bg = tl.load(p_bg, boundary_check=(0, 1))\n        b_w = tl.load(p_w, boundary_check=(0, 1))\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dv2 = b_dv + tl.dot(b_bg, b_dh.to(b_bg.dtype))\n        tl.store(p_dv2, b_dv2.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n        b_dh_tmp += tl.dot(b_qg, b_do.to(b_qg.dtype))\n        b_dh_tmp += tl.dot(b_w, b_dv2.to(b_qg.dtype))\n    last_idx = min((i_t + 1) * BT, T) - 1\n    bg_last = tl.load(gk + ((bos + last_idx) * H + i_h) * K + tl.arange(0, BK), mask=mask_k)\n    b_dh *= exp(bg_last)[:, None]\n    b_dh += b_dh_tmp\nif USE_INITIAL_STATE:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_fwd_kernel_h_parallel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_parallel(",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    gk,",
      "    gv,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    NV = tl.cdiv(V, BV)",
      "",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        i_n, i_tg = i_b, i_b * NT + i_t",
      "    i_nh = i_n * H + i_h",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (K, T),",
      "        (1, H * K),",
      "        (i_k * BK, i_t * BT),",
      "        (BK, BT),",
      "        (0, 1),",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_h = tl.make_block_ptr(",
      "        h + (i_tg * H + i_h) * K * V,",
      "        (K, V),",
      "        (V, 1),",
      "        (i_k * BK, i_v * BV),",
      "        (BK, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if i_t == 0:",
      "        if USE_INITIAL_STATE:",
      "            p_h0 = tl.make_block_ptr(",
      "                h0 + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "        else:",
      "            b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "    if USE_G:",
      "        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "        b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)",
      "",
      "    if USE_GK:",
      "        p_gk = tl.make_block_ptr(",
      "            gk + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_gk_last = (",
      "            gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "        )",
      "        b_gk_last = tl.load(",
      "            p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "        )",
      "",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)",
      "",
      "    if USE_GV:",
      "        p_gv = tl.make_block_ptr(",
      "            gv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_gv_last = (",
      "            gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "        )",
      "",
      "        b_gv_last = tl.load(",
      "            p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "        )",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "    b_h = tl.dot(b_k, b_v)",
      "    if i_t < NT - 1:",
      "        p_h = tl.make_block_ptr(",
      "            h + ((i_tg + 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/370.py",
    "header": "def chunk_fwd_kernel_h_parallel(k, v, h, g, gk, gv, h0, ht, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_kv, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNV = tl.cdiv(V, BV)\ni_k, i_v = (i_kv // NV, i_kv % NV)\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    NT = tl.cdiv(T, BT)\n    i_n, i_tg = (i_b, i_b * NT + i_t)\ni_nh = i_n * H + i_h\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\nif i_t == 0:\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    else:\n        b_h = tl.zeros([BK, BV], dtype=tl.float32)\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nlast_idx = min(i_t * BT + BT, T) - 1\nif USE_G:\n    b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n    p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n    b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n    b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\nif USE_GK:\n    p_gk = tl.make_block_ptr(gk + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n    b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\nif USE_GV:\n    p_gv = tl.make_block_ptr(gv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n    b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\nb_h = tl.dot(b_k, b_v)\nif i_t < NT - 1:\n    p_h = tl.make_block_ptr(h + ((i_tg + 1) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\nelif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_fwd_kernel_h_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "kvt",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_reduction(",
      "    h,",
      "    g,",
      "    gk,",
      "    gv,",
      "    kvt,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "        if i_t > 0:",
      "            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        if USE_G:",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            b_h *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_h *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_h *= exp(b_gv_last)[None, :]",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_kvt = tl.make_block_ptr(",
      "            kvt + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h += tl.load(p_kvt, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/370.py",
    "header": "def chunk_fwd_kernel_h_reduction(h, g, gk, gv, kvt, ht, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nfor i_t in range(NT):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\n    if i_t > 0:\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    last_idx = min(i_t * BT + BT, T) - 1\n    if USE_G:\n        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n        b_h *= exp(b_g_last)\n    if USE_GK:\n        p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n        b_h *= exp(b_gk_last)[:, None]\n    if USE_GV:\n        p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n        b_h *= exp(b_gv_last)[None, :]\nif STORE_FINAL_STATE:\n    p_kvt = tl.make_block_ptr(kvt + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h += tl.load(p_kvt, boundary_check=(0, 1)).to(tl.float32)\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dh_parallel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_parallel(",
      "    q,",
      "    g,",
      "    gk,",
      "    gv,",
      "    do,",
      "    dh,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    NV = tl.cdiv(V, BV)",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        i_n, i_tg = i_b, i_b * NT + i_t",
      "    i_nh = i_n * HQ + i_hq",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K,",
      "        (K, T),",
      "        (1, HQ * K),",
      "        (i_k * BK, i_t * BT),",
      "        (BK, BT),",
      "        (0, 1),",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_dh = tl.make_block_ptr(",
      "        dh + (i_tg * H + i_h) * K * V,",
      "        (K, V),",
      "        (V, 1),",
      "        (i_k * BK, i_v * BV),",
      "        (BK, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if i_t == NT - 1:",
      "        if USE_FINAL_STATE_GRADIENT:",
      "            p_dht = tl.make_block_ptr(",
      "                dht + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_dh = tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "        else:",
      "            b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    if USE_G:",
      "        p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h",
      "        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "        b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)",
      "",
      "    if USE_GK:",
      "        p_gk = tl.make_block_ptr(",
      "            gk + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_q = (b_q * exp(b_gk)).to(b_q.dtype)",
      "",
      "    if USE_GV:",
      "        p_gv = tl.make_block_ptr(",
      "            gv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_gv)).to(b_do.dtype)",
      "",
      "    b_dh = tl.dot(b_q, b_do)",
      "    if i_t > 0:",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((i_tg - 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/370.py",
    "header": "def chunk_bwd_kernel_dh_parallel(q, g, gk, gv, do, dh, dht, dh0, cu_seqlens, chunk_indices, scale, T, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_kv, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNV = tl.cdiv(V, BV)\ni_k, i_v = (i_kv // NV, i_kv % NV)\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    NT = tl.cdiv(T, BT)\n    i_n, i_tg = (i_b, i_b * NT + i_t)\ni_nh = i_n * HQ + i_hq\np_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (K, T), (1, HQ * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\np_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_dh = tl.make_block_ptr(dh + (i_tg * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\nif i_t == NT - 1:\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh = tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n    else:\n        b_dh = tl.zeros([BK, BV], dtype=tl.float32)\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nb_do = tl.load(p_do, boundary_check=(0, 1))\nif USE_G:\n    p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h\n    b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n    b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)\nif USE_GK:\n    p_gk = tl.make_block_ptr(gk + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_q = (b_q * exp(b_gk)).to(b_q.dtype)\nif USE_GV:\n    p_gv = tl.make_block_ptr(gv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_do = (b_do * exp(b_gv)).to(b_do.dtype)\nb_dh = tl.dot(b_q, b_do)\nif i_t > 0:\n    p_dh = tl.make_block_ptr(dh + ((i_tg - 1) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\nelif STORE_INITIAL_STATE_GRADIENT:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dh_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "doq0",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_reduction(",
      "    g,",
      "    gk,",
      "    gv,",
      "    dh,",
      "    doq0,",
      "    dh0,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_dh += tl.load(p_dh, boundary_check=(0, 1)).to(tl.float32)",
      "        if i_t < NT - 1:",
      "            tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "        if USE_G:",
      "            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            b_dh *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_dh *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_dh *= exp(b_gv_last)[None, :]",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_doq0 = tl.make_block_ptr(",
      "            doq0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_doq0, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/370.py",
    "header": "def chunk_bwd_kernel_dh_reduction(g, gk, gv, dh, doq0, dh0, cu_seqlens, chunk_offsets, T, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nfor i_t in range(NT - 1, -1, -1):\n    p_dh = tl.make_block_ptr(dh + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dh, boundary_check=(0, 1)).to(tl.float32)\n    if i_t < NT - 1:\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    last_idx = min(i_t * BT + BT, T) - 1\n    if USE_G:\n        b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n        b_dh *= exp(b_g_last)\n    if USE_GK:\n        p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n        b_dh *= exp(b_gk_last)[:, None]\n    if USE_GV:\n        p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n        b_dh *= exp(b_gv_last)[None, :]\nif STORE_INITIAL_STATE_GRADIENT:\n    p_doq0 = tl.make_block_ptr(doq0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_doq0, boundary_check=(0, 1)).to(tl.float32)\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['QSeq'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['KSeq'] % args['BLOCK_N'] == 0})"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "dropout_prob",
        "annotation": null
      },
      {
        "name": "dropout_seed",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "nheads_q",
        "annotation": null
      },
      {
        "name": "num_repeats",
        "annotation": null
      },
      {
        "name": "QSeq",
        "annotation": null
      },
      {
        "name": "cum_seqlens_q",
        "annotation": null
      },
      {
        "name": "KSeq",
        "annotation": null
      },
      {
        "name": "max_seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CQSeq",
        "annotation": null
      },
      {
        "name": "CKSeq",
        "annotation": null
      },
      {
        "name": "DRuntime",
        "annotation": null
      },
      {
        "name": "Po",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_ON",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEADS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    q,",
      "    k,",
      "    v,",
      "    B,",
      "    softmax_scale,",
      "    dropout_prob,",
      "    dropout_seed,",
      "    stride_qz,",
      "    stride_qm,",
      "    stride_qh,",
      "    stride_kz,",
      "    stride_kn,",
      "    stride_kh,",
      "    stride_vz,",
      "    stride_vn,",
      "    stride_vh,",
      "    stride_oz,",
      "    stride_om,",
      "    stride_oh,",
      "    stride_bz,",
      "    stride_bm,",
      "    stride_bh,",
      "    nheads_q,",
      "    num_repeats,",
      "    QSeq,",
      "    cum_seqlens_q,",
      "    KSeq,",
      "    max_seqlen_q_rounded,",
      "    headdim,",
      "    CQSeq,",
      "    CKSeq,",
      "    DRuntime,",
      "    Po,",
      "    M,",
      "    VARLEN: tl.constexpr,",
      "    USE_DROPOUT: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BIAS_ON: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    PADDED_HEADS: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    LN2: tl.constexpr = 1.44269504089",
      "    i_start_m = tl.program_id(0)",
      "    off_zh = tl.program_id(1)",
      "    off_head_q = off_zh % nheads_q",
      "    off_head_kv = off_head_q // num_repeats",
      "    off_z = off_zh // nheads_q",
      "",
      "    if VARLEN:",
      "        cu_seq_start_q = tl.load(cum_seqlens_q + off_z)",
      "        actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q",
      "        if i_start_m * BLOCK_M >= actual_seqlen_q:",
      "            return",
      "        actual_seqlen_k = actual_seqlen_q",
      "        cu_seq_start_k = cu_seq_start_q",
      "        off_z = 0",
      "    else:",
      "        actual_seqlen_q = QSeq",
      "        actual_seqlen_k = KSeq",
      "        cu_seq_start_q = 0",
      "        cu_seq_start_k = 0",
      "",
      "    softmax_scale = softmax_scale * LN2",
      "",
      "    offs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    fully_masked_lines = actual_seqlen_q - actual_seqlen_k if IS_CAUSAL else 0",
      "    if fully_masked_lines >= (i_start_m + 1) * BLOCK_M:",
      "        return",
      "",
      "    q_ptrs = (",
      "        q",
      "        + off_z * stride_qz",
      "        + off_head_q * stride_qh",
      "        + cu_seq_start_q * stride_qm",
      "        + (offs_m[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "",
      "    k_ptrs = (",
      "        k",
      "        + off_z * stride_kz",
      "        + off_head_kv * stride_kh",
      "        + cu_seq_start_k * stride_kn",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "",
      "    v_ptrs = (",
      "        v",
      "        + off_z * stride_vz",
      "        + off_head_kv * stride_vh",
      "        + cu_seq_start_k * stride_vn",
      "        + (offs_n[:, None] * stride_vn + offs_d[None, :])",
      "    )",
      "",
      "    if BIAS_ON:",
      "        bias_ptrs = (",
      "            B",
      "            + off_z * stride_bz",
      "            + off_head_kv * stride_bh",
      "            + cu_seq_start_q * stride_bm",
      "            + (offs_m[:, None] * stride_bm + offs_n[None, :])",
      "        )",
      "    else:",
      "        bias_ptrs = None",
      "    if USE_DROPOUT:",
      "        dropout_off = actual_seqlen_k * (",
      "            cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z)",
      "        )",
      "        dropout_offs = dropout_off + offs_m[:, None] * actual_seqlen_k + offs_n[None, :]",
      "    else:",
      "        dropout_offs = None",
      "",
      "    me_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    pad_rows = (not EVEN_M) or (VARLEN and (i_start_m * BLOCK_M > actual_seqlen_q))",
      "    q = padded_load(",
      "        q_ptrs,",
      "        offs_m,",
      "        offs_d,",
      "        PA0=pad_rows,",
      "        PA1=PADDED_HEADS,",
      "        LA0=actual_seqlen_q,",
      "        LA1=headdim,",
      "    )",
      "    if IS_CAUSAL:",
      "        end_n = min(",
      "            actual_seqlen_k - actual_seqlen_q + (i_start_m + 1) * BLOCK_M,",
      "            actual_seqlen_k,",
      "        )",
      "        if end_n < 0:",
      "            return",
      "    else:",
      "        end_n = actual_seqlen_k",
      "",
      "    uneven_n = actual_seqlen_k % BLOCK_N != 0",
      "    attention_padding = VARLEN & uneven_n",
      "    if IS_CAUSAL:",
      "        first_masked_col = i_start_m * BLOCK_M + 1 + actual_seqlen_k - actual_seqlen_q",
      "    elif attention_padding:",
      "        first_masked_col = actual_seqlen_k",
      "    else:",
      "        first_masked_col = end_n",
      "    nb_full_blocks = first_masked_col // BLOCK_N",
      "",
      "    next_start_n = 0",
      "    if nb_full_blocks > 0:",
      "        for _ in range(0, nb_full_blocks):",
      "            m_i, me_i, acc_o = _attn_fwd_inner(",
      "                q,",
      "                m_i,",
      "                me_i,",
      "                k_ptrs,",
      "                v_ptrs,",
      "                bias_ptrs,",
      "                acc_o,",
      "                offs_m,",
      "                offs_n,",
      "                offs_d,",
      "                softmax_scale,",
      "                dropout_prob,",
      "                dropout_seed,",
      "                dropout_offs,",
      "                stride_kn,",
      "                stride_vn,",
      "                next_start_n,",
      "                actual_seqlen_q,",
      "                actual_seqlen_k,",
      "                headdim,",
      "                USE_DROPOUT=USE_DROPOUT,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BIAS_ON=BIAS_ON,",
      "                MASKED=False,",
      "                PADDED_COLS=False,",
      "                PADDED_HEADS=PADDED_HEADS,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "            next_start_n += BLOCK_N",
      "    if next_start_n < end_n:",
      "        for index_start_n in range(next_start_n, end_n, BLOCK_N):",
      "            pad_cols = (not EVEN_N) or VARLEN",
      "            m_i, me_i, acc_o = _attn_fwd_inner(",
      "                q,",
      "                m_i,",
      "                me_i,",
      "                k_ptrs,",
      "                v_ptrs,",
      "                bias_ptrs,",
      "                acc_o,",
      "                offs_m,",
      "                offs_n,",
      "                offs_d,",
      "                softmax_scale,",
      "                dropout_prob,",
      "                dropout_seed,",
      "                dropout_offs,",
      "                stride_kn,",
      "                stride_vn,",
      "                index_start_n,",
      "                actual_seqlen_q,",
      "                actual_seqlen_k,",
      "                headdim,",
      "                USE_DROPOUT=USE_DROPOUT,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BIAS_ON=BIAS_ON,",
      "                MASKED=True,",
      "                PADDED_COLS=pad_cols,",
      "                PADDED_HEADS=PADDED_HEADS,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "",
      "    if USE_DROPOUT:",
      "        o_scale = tl.exp2((m_i - me_i) - tl.log2(1 - dropout_prob))",
      "    else:",
      "        o_scale = tl.exp2(m_i - me_i)",
      "    acc_o = acc_o * o_scale[:, None]",
      "    if fully_masked_lines > i_start_m * BLOCK_M:",
      "        acc_o = tl.where(offs_m[:, None] < fully_masked_lines, 0, acc_o)",
      "    i_start_m = tl.program_id(0)",
      "    offs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    lse_ptrs = M + off_zh * max_seqlen_q_rounded + offs_m",
      "    tl.store(lse_ptrs, me_i)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    out_ptrs = (",
      "        Po",
      "        + off_z * stride_oz",
      "        + off_head_q * stride_oh",
      "        + cu_seq_start_q * stride_om",
      "        + (offs_m[:, None] * stride_om + offs_d[None, :])",
      "    )",
      "",
      "    tl.store(",
      "        out_ptrs,",
      "        acc_o,",
      "        mask=(offs_m[:, None] < actual_seqlen_q) & (offs_d[None, :] < headdim),",
      "    )"
    ],
    "file": "codes/321.py",
    "header": "def _attn_fwd(q, k, v, B, softmax_scale, dropout_prob, dropout_seed, stride_qz, stride_qm, stride_qh, stride_kz, stride_kn, stride_kh, stride_vz, stride_vn, stride_vh, stride_oz, stride_om, stride_oh, stride_bz, stride_bm, stride_bh, nheads_q, num_repeats, QSeq, cum_seqlens_q, KSeq, max_seqlen_q_rounded, headdim, CQSeq, CKSeq, DRuntime, Po, M, VARLEN: tl.constexpr, USE_DROPOUT: tl.constexpr, IS_CAUSAL: tl.constexpr, BIAS_ON: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, PADDED_HEADS: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "LN2: tl.constexpr = 1.44269504089\ni_start_m = tl.program_id(0)\noff_zh = tl.program_id(1)\noff_head_q = off_zh % nheads_q\noff_head_kv = off_head_q // num_repeats\noff_z = off_zh // nheads_q\nif VARLEN:\n    cu_seq_start_q = tl.load(cum_seqlens_q + off_z)\n    actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q\n    if i_start_m * BLOCK_M >= actual_seqlen_q:\n        return\n    actual_seqlen_k = actual_seqlen_q\n    cu_seq_start_k = cu_seq_start_q\n    off_z = 0\nelse:\n    actual_seqlen_q = QSeq\n    actual_seqlen_k = KSeq\n    cu_seq_start_q = 0\n    cu_seq_start_k = 0\nsoftmax_scale = softmax_scale * LN2\noffs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nfully_masked_lines = actual_seqlen_q - actual_seqlen_k if IS_CAUSAL else 0\nif fully_masked_lines >= (i_start_m + 1) * BLOCK_M:\n    return\nq_ptrs = q + off_z * stride_qz + off_head_q * stride_qh + cu_seq_start_q * stride_qm + (offs_m[:, None] * stride_qm + offs_d[None, :])\nk_ptrs = k + off_z * stride_kz + off_head_kv * stride_kh + cu_seq_start_k * stride_kn + (offs_n[:, None] * stride_kn + offs_d[None, :])\nv_ptrs = v + off_z * stride_vz + off_head_kv * stride_vh + cu_seq_start_k * stride_vn + (offs_n[:, None] * stride_vn + offs_d[None, :])\nif BIAS_ON:\n    bias_ptrs = B + off_z * stride_bz + off_head_kv * stride_bh + cu_seq_start_q * stride_bm + (offs_m[:, None] * stride_bm + offs_n[None, :])\nelse:\n    bias_ptrs = None\nif USE_DROPOUT:\n    dropout_off = actual_seqlen_k * (cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z))\n    dropout_offs = dropout_off + offs_m[:, None] * actual_seqlen_k + offs_n[None, :]\nelse:\n    dropout_offs = None\nme_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nacc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\npad_rows = not EVEN_M or (VARLEN and i_start_m * BLOCK_M > actual_seqlen_q)\nq = padded_load(q_ptrs, offs_m, offs_d, PA0=pad_rows, PA1=PADDED_HEADS, LA0=actual_seqlen_q, LA1=headdim)\nif IS_CAUSAL:\n    end_n = min(actual_seqlen_k - actual_seqlen_q + (i_start_m + 1) * BLOCK_M, actual_seqlen_k)\n    if end_n < 0:\n        return\nelse:\n    end_n = actual_seqlen_k\nuneven_n = actual_seqlen_k % BLOCK_N != 0\nattention_padding = VARLEN & uneven_n\nif IS_CAUSAL:\n    first_masked_col = i_start_m * BLOCK_M + 1 + actual_seqlen_k - actual_seqlen_q\nelif attention_padding:\n    first_masked_col = actual_seqlen_k\nelse:\n    first_masked_col = end_n\nnb_full_blocks = first_masked_col // BLOCK_N\nnext_start_n = 0\nif nb_full_blocks > 0:\n    for _ in range(0, nb_full_blocks):\n        m_i, me_i, acc_o = _attn_fwd_inner(q, m_i, me_i, k_ptrs, v_ptrs, bias_ptrs, acc_o, offs_m, offs_n, offs_d, softmax_scale, dropout_prob, dropout_seed, dropout_offs, stride_kn, stride_vn, next_start_n, actual_seqlen_q, actual_seqlen_k, headdim, USE_DROPOUT=USE_DROPOUT, IS_CAUSAL=IS_CAUSAL, BIAS_ON=BIAS_ON, MASKED=False, PADDED_COLS=False, PADDED_HEADS=PADDED_HEADS, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\n        next_start_n += BLOCK_N\nif next_start_n < end_n:\n    for index_start_n in range(next_start_n, end_n, BLOCK_N):\n        pad_cols = not EVEN_N or VARLEN\n        m_i, me_i, acc_o = _attn_fwd_inner(q, m_i, me_i, k_ptrs, v_ptrs, bias_ptrs, acc_o, offs_m, offs_n, offs_d, softmax_scale, dropout_prob, dropout_seed, dropout_offs, stride_kn, stride_vn, index_start_n, actual_seqlen_q, actual_seqlen_k, headdim, USE_DROPOUT=USE_DROPOUT, IS_CAUSAL=IS_CAUSAL, BIAS_ON=BIAS_ON, MASKED=True, PADDED_COLS=pad_cols, PADDED_HEADS=PADDED_HEADS, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\nif USE_DROPOUT:\n    o_scale = tl.exp2(m_i - me_i - tl.log2(1 - dropout_prob))\nelse:\n    o_scale = tl.exp2(m_i - me_i)\nacc_o = acc_o * o_scale[:, None]\nif fully_masked_lines > i_start_m * BLOCK_M:\n    acc_o = tl.where(offs_m[:, None] < fully_masked_lines, 0, acc_o)\ni_start_m = tl.program_id(0)\noffs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)\nlse_ptrs = M + off_zh * max_seqlen_q_rounded + offs_m\ntl.store(lse_ptrs, me_i)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nout_ptrs = Po + off_z * stride_oz + off_head_q * stride_oh + cu_seq_start_q * stride_om + (offs_m[:, None] * stride_om + offs_d[None, :])\ntl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < actual_seqlen_q) & (offs_d[None, :] < headdim))"
  },
  {
    "name": "fused_fwd_kernel_s_km",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64, 128] for BK in [64] for BV in [64] for num_warps in [2, 4, 8] for num_stages in [2]], key=['L'])"
    ],
    "args": [
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "KM",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "stride_km_bh",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_fwd_kernel_s_km(",
      "    K,",
      "    V,",
      "    S,",
      "    KM,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    stride_km_bh,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + off_bs_head * stride_qk_bh,",
      "        shape=(DK, L),",
      "        strides=(stride_qk_d, stride_qk_l),",
      "        offsets=(start_k * BK, 0),",
      "        block_shape=(BK, BL),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    s = tl.zeros([BK, BV], dtype=tl.float32)",
      "    km = tl.zeros([BK], dtype=tl.float32)",
      "",
      "    for _ in range(0, L, BL):",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1))",
      "        v = tl.load(V_block_ptr, boundary_check=(0, 1))",
      "",
      "        v = (v * scale).to(v.dtype)",
      "        s += tl.dot(k, v, allow_tf32=False)",
      "        km += tl.sum(k, axis=1) / L",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BL))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))",
      "",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(start_k * BK, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    KM_block_ptr = KM + off_bs_head * stride_km_bh + start_k * BK + tl.arange(0, BK)",
      "    tl.store(",
      "        KM_block_ptr,",
      "        km.to(KM.dtype.element_ty),",
      "        mask=((start_k * BK + tl.arange(0, BK)) < DK),",
      "    )"
    ],
    "file": "codes/331.py",
    "header": "def fused_fwd_kernel_s_km(K, V, S, KM, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, stride_km_bh, scale, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_v, start_k, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nK_block_ptr = tl.make_block_ptr(base=K + off_bs_head * stride_qk_bh, shape=(DK, L), strides=(stride_qk_d, stride_qk_l), offsets=(start_k * BK, 0), block_shape=(BK, BL), order=(0, 1))\nV_block_ptr = tl.make_block_ptr(base=V + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(0, start_v * BV), block_shape=(BL, BV), order=(1, 0))\ns = tl.zeros([BK, BV], dtype=tl.float32)\nkm = tl.zeros([BK], dtype=tl.float32)\nfor _ in range(0, L, BL):\n    k = tl.load(K_block_ptr, boundary_check=(0, 1))\n    v = tl.load(V_block_ptr, boundary_check=(0, 1))\n    v = (v * scale).to(v.dtype)\n    s += tl.dot(k, v, allow_tf32=False)\n    km += tl.sum(k, axis=1) / L\n    K_block_ptr = tl.advance(K_block_ptr, (0, BL))\n    V_block_ptr = tl.advance(V_block_ptr, (BL, 0))\nS_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh, shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(start_k * BK, start_v * BV), block_shape=(BK, BV), order=(1, 0))\ntl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))\nKM_block_ptr = KM + off_bs_head * stride_km_bh + start_k * BK + tl.arange(0, BK)\ntl.store(KM_block_ptr, km.to(KM.dtype.element_ty), mask=start_k * BK + tl.arange(0, BK) < DK)"
  },
  {
    "name": "fused_fwd_kernel_o",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64, 128] for BK in [64] for BV in [64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "KM",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "stride_km_bh",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_fwd_kernel_o(",
      "    Q,",
      "    S,",
      "    O,",
      "    KM,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    stride_km_bh,",
      "    eps,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    start_v, start_l, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_l * BL, 0),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    KM_block_ptr = KM + off_bs_head * stride_km_bh + tl.arange(0, BK)",
      "",
      "    o = tl.zeros([BL, BV], dtype=tl.float32)",
      "    z = tl.zeros([BL], dtype=tl.float32)",
      "",
      "    for offset_k in range(0, DK, BK):",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1))",
      "        s = tl.load(S_block_ptr, boundary_check=(0, 1))",
      "        km = tl.load(KM_block_ptr, mask=((offset_k + tl.arange(0, BK)) < DK))",
      "",
      "        z += tl.sum(q * km[None, :], axis=1, keep_dims=False)",
      "        o += tl.dot(q, s, allow_tf32=False)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))",
      "        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))",
      "        KM_block_ptr = KM_block_ptr + tl.arange(0, BK)",
      "",
      "    o = o / (z[:, None] + eps)",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=O + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_l * BL, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/331.py",
    "header": "def fused_fwd_kernel_o(Q, S, O, KM, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, stride_km_bh, eps, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_v, start_l, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nQ_block_ptr = tl.make_block_ptr(base=Q + off_bs_head * stride_qk_bh, shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_l * BL, 0), block_shape=(BL, BK), order=(1, 0))\nS_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh, shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(0, start_v * BV), block_shape=(BK, BV), order=(1, 0))\nKM_block_ptr = KM + off_bs_head * stride_km_bh + tl.arange(0, BK)\no = tl.zeros([BL, BV], dtype=tl.float32)\nz = tl.zeros([BL], dtype=tl.float32)\nfor offset_k in range(0, DK, BK):\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    s = tl.load(S_block_ptr, boundary_check=(0, 1))\n    km = tl.load(KM_block_ptr, mask=offset_k + tl.arange(0, BK) < DK)\n    z += tl.sum(q * km[None, :], axis=1, keep_dims=False)\n    o += tl.dot(q, s, allow_tf32=False)\n    Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))\n    S_block_ptr = tl.advance(S_block_ptr, (BK, 0))\n    KM_block_ptr = KM_block_ptr + tl.arange(0, BK)\no = o / (z[:, None] + eps)\nO_block_ptr = tl.make_block_ptr(base=O + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_l * BL, start_v * BV), block_shape=(BL, BV), order=(1, 0))\ntl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "layer_norm_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None, 'HAS_Z': lambda args: args['Z'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    Z,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "codes/360.py",
    "header": "def layer_norm_fwd_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row, stride_y_row, stride_z_row, M, N, eps, BLOCK_N: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.constexpr):",
    "body": "row = tl.program_id(0)\ngroup = tl.program_id(1)\nX += row * stride_x_row + group * N\nY += row * stride_y_row + group * N\nif HAS_Z:\n    Z += row * stride_z_row + group * N\nif not IS_RMS_NORM:\n    Mean += group * M\nRstd += group * M\nW += group * N\nif HAS_BIAS:\n    B += group * N\ncols = tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\nif HAS_Z and (not NORM_BEFORE_GATE):\n    z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n    x *= z * tl.sigmoid(z)\nif not IS_RMS_NORM:\n    mean = tl.sum(x, axis=0) / N\n    tl.store(Mean + row, mean)\n    xbar = tl.where(cols < N, x - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nelse:\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nmask = cols < N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif HAS_BIAS:\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\nx_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\ny = x_hat * w + b if HAS_BIAS else x_hat * w\nif HAS_Z and NORM_BEFORE_GATE:\n    z = tl.load(Z + cols, mask=mask).to(tl.float32)\n    y *= z * tl.sigmoid(z)\ntl.store(Y + cols, y, mask=mask)"
  },
  {
    "name": "layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None, 'HAS_Z': lambda args: args['Z'] is not None, 'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DZ",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dz_row",
        "annotation": null
      },
      {
        "name": "stride_dw_row",
        "annotation": null
      },
      {
        "name": "stride_db_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Z,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DZ,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_z_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dz_row,",
      "    stride_dw_row,",
      "    stride_db_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row + group * N",
      "    if HAS_Z:",
      "        Z += row_start * stride_z_row + group * N",
      "        DZ += row_start * stride_dz_row + group * N",
      "    DY += row_start * stride_dy_row + group * N",
      "    DX += row_start * stride_dx_row + group * N",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:",
      "        B += group * N",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            x_og = x",
      "            x = x_og * z * tl.sigmoid(z)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if HAS_Z and NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            z_sigmoid = tl.sigmoid(z)",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            if RECOMPUTE_OUTPUT:",
      "                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)",
      "            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dy *= z * z_sigmoid",
      "        else:",
      "            if RECOMPUTE_OUTPUT:",
      "                y = xhat * w + b if HAS_BIAS else xhat * w",
      "                tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        c1 = tl.sum(xhat * wdy, axis=0) / N",
      "        if not IS_RMS_NORM:",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            dx = (wdy - xhat * c1) * rstd",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z_sigmoid = tl.sigmoid(z)",
      "            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dx *= z * z_sigmoid",
      "",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_Z:",
      "            Z += stride_z_row",
      "            DZ += stride_dz_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
    ],
    "file": "codes/360.py",
    "header": "def layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd, stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dz_row, stride_dw_row, stride_db_row, M, N, eps, rows_per_program, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\ngroup = tl.program_id(1)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, BLOCK_N)\nmask = cols < N\nX += row_start * stride_x_row + group * N\nif HAS_Z:\n    Z += row_start * stride_z_row + group * N\n    DZ += row_start * stride_dz_row + group * N\nDY += row_start * stride_dy_row + group * N\nDX += row_start * stride_dx_row + group * N\nif RECOMPUTE_OUTPUT:\n    Y += row_start * stride_y_row + group * N\nif not IS_RMS_NORM:\n    Mean += group * M\nRstd += group * M\nW += group * N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n    B += group * N\n    b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\ndw = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_BIAS:\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\nrow_end = min((row_block_id + 1) * rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        mean = tl.load(Mean + row)\n    if HAS_Z and (not NORM_BEFORE_GATE):\n        z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n        x_og = x\n        x = x_og * z * tl.sigmoid(z)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n        z_sigmoid = tl.sigmoid(z)\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        if RECOMPUTE_OUTPUT:\n            tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n        dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n        tl.store(DZ + cols, dz, mask=mask)\n        dy *= z * z_sigmoid\n    elif RECOMPUTE_OUTPUT:\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        tl.store(Y + cols, y, mask=mask)\n    wdy = w * dy\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    if not IS_RMS_NORM:\n        c2 = tl.sum(wdy, axis=0) / N\n        dx = (wdy - (xhat * c1 + c2)) * rstd\n    else:\n        dx = (wdy - xhat * c1) * rstd\n    dw += dy * xhat\n    if HAS_BIAS:\n        db += dy\n    if HAS_Z and (not NORM_BEFORE_GATE):\n        z_sigmoid = tl.sigmoid(z)\n        dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n        tl.store(DZ + cols, dz, mask=mask)\n        dx *= z * z_sigmoid\n    tl.store(DX + cols, dx, mask=mask)\n    X += stride_x_row\n    if HAS_Z:\n        Z += stride_z_row\n        DZ += stride_dz_row\n    if RECOMPUTE_OUTPUT:\n        Y += stride_y_row\n    DY += stride_dy_row\n    DX += stride_dx_row\ntl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)\nif HAS_BIAS:\n    tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BC'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_inter(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_i, i_j = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "    if i_i <= i_j:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_qg = b_q * exp(b_g - b_gn[None, :]) * scale",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_kg = b_k * exp(b_gn[:, None] - b_gk)",
      "",
      "        b_A += tl.dot(b_qg, b_kg)",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_fwd_A_kernel_intra_sub_inter(q, k, g, A, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_i, i_j = (i_c // NC, i_c % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nif i_t * BT + i_i * BC >= T:\n    return\nif i_i <= i_j:\n    return\nb_A = tl.zeros([BC, BC], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n    p_gk = tl.make_block_ptr(g + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n    p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_qg = b_q * exp(b_g - b_gn[None, :]) * scale\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_kg = b_k * exp(b_gn[:, None] - b_gk)\n    b_A += tl.dot(b_qg, b_kg)\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\ntl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BK', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i >= j, b_A * scale, 0.0)",
      "",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_k += H * K",
      "        p_gk += H * K"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_fwd_A_kernel_intra_sub_intra(q, k, g, A, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_i, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_j = i_i\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nif i_t * BT + i_i * BC >= T:\n    return\no_i = tl.arange(0, BC)\no_k = tl.arange(0, BK)\nm_k = o_k < K\nm_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\no_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\np_g = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\np_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\np_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_g = tl.load(p_g, boundary_check=(0, 1))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n    b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n    b_A = tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)\n    b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n    tl.store(A + o_A + j, b_A, mask=m_A)\n    p_k += H * K\n    p_gk += H * K"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC', 'BK'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra_split(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_tc // NC, i_tc % NC",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "",
      "    o_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_A = tl.zeros([BC], dtype=tl.float32)",
      "        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A += tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i >= j, b_A * scale, 0.0)",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_k += H * K",
      "        p_gk += H * K"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_fwd_A_kernel_intra_sub_intra_split(q, k, g, A, cu_seqlens, chunk_indices, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_tc, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_t, i_i = (i_tc // NC, i_tc % NC)\ni_j = i_i\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    all = B * T\nif i_t * BT + i_i * BC >= T:\n    return\no_i = tl.arange(0, BC)\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\nm_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\no_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_g = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\np_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_g = tl.load(p_g, boundary_check=(0, 1))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_A = tl.zeros([BC], dtype=tl.float32)\n    b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)\n    b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n    b_A += tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)\n    b_A = tl.where(o_i >= j, b_A * scale, 0.0)\n    tl.store(A + o_A + j, b_A, mask=m_A)\n    p_k += H * K\n    p_gk += H * K"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra_merge",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra_merge(",
      "    A,",
      "    A2,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    NK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_c * BC >= T:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(0, NK):",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_k * all + bos) * H * BC + i_h * BC,",
      "            (T, BC),",
      "            (H * BC, 1),",
      "            (i_t * BT + i_c * BC, 0),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        b_A += tl.load(p_A, boundary_check=(0, 1))",
      "    p_A2 = tl.make_block_ptr(",
      "        A2 + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_c * BC, i_c * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_fwd_A_kernel_intra_sub_intra_merge(A, A2, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, NK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    all = B * T\nif i_t * BT + i_c * BC >= T:\n    return\nb_A = tl.zeros([BC, BC], dtype=tl.float32)\nfor i_k in range(0, NK):\n    p_A = tl.make_block_ptr(A + (i_k * all + bos) * H * BC + i_h * BC, (T, BC), (H * BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n    b_A += tl.load(p_A, boundary_check=(0, 1))\np_A2 = tl.make_block_ptr(A2 + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\ntl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gla_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in [32, 64] for BV in [64, 128] for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_kernel_o(",
      "    q,",
      "    v,",
      "    g,",
      "    h,",
      "    o,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "",
      "        b_qg = (b_q * exp(b_g)).to(b_q.dtype)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        if i_k >= 0:",
      "            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)",
      "    b_o += tl.dot(b_A, b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_fwd_kernel_o(q, v, g, h, o, A, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nm_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_g = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_qg = (b_q * exp(b_g)).to(b_q.dtype)\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    if i_k >= 0:\n        b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)\nb_o += tl.dot(b_A, b_v, allow_tf32=False)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gla_bwd_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BK', 'NC', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_intra(",
      "    q,",
      "    k,",
      "    g,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    if i_i > 0:",
      "        p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(0, i_i):",
      "            p_k = tl.make_block_ptr(",
      "                k + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gk = tl.make_block_ptr(",
      "                g + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (T, BT),",
      "                (H * BT, 1),",
      "                (i_t * BT + i_i * BC, i_j * BC),",
      "                (BC, BC),",
      "                (1, 0),",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_kg = b_k * exp(b_gn[None, :] - b_gk)",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dq += tl.dot(b_dA, b_kg)",
      "        b_dq *= exp(b_g - b_gn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "    )",
      "    p_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gkj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)",
      "",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] >= j",
      "",
      "        b_dq += tl.where(",
      "            m_i, b_dA[:, None] * b_kj[None, :] * exp(b_g - b_gkj[None, :]), 0.0",
      "        )",
      "        p_kj += H * K",
      "        p_gkj += H * K",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_gk = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    NC = min(NC, tl.cdiv(T - i_t * BT, BC))",
      "    if i_i < NC - 1:",
      "        p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(i_i + 1, NC):",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gq = tl.make_block_ptr(",
      "                g + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (BT, T),",
      "                (1, H * BT),",
      "                (i_i * BC, i_t * BT + i_j * BC),",
      "                (BC, BC),",
      "                (0, 1),",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_gq = tl.load(p_gq, boundary_check=(0, 1))",
      "            b_qg = b_q * safe_exp(b_gq - b_gn[None, :])",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dk += tl.dot(b_dA, b_qg)",
      "        b_dk *= exp(b_gn[None, :] - b_gk)",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "        + tl.arange(0, BC)",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gqj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j * H * BT)",
      "",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dk += tl.where(",
      "            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0",
      "        )",
      "        p_qj += H * K",
      "        p_gqj += H * K",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_bwd_kernel_intra(q, k, g, dA, dq, dk, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_t, i_i = (i_c // NC, i_c % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nT = eos - bos\nif i_t * BT + i_i * BC >= T:\n    return\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\np_g = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nb_g = tl.load(p_g, boundary_check=(0, 1))\nb_dq = tl.zeros([BC, BK], dtype=tl.float32)\nif i_i > 0:\n    p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * exp(b_gn[None, :] - b_gk)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kg)\n    b_dq *= exp(b_g - b_gn[None, :])\no_i = tl.arange(0, BC)\nm_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\no_dA = bos * H * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_i * BC\np_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_gkj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_dq = tl.make_block_ptr(dq + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n    b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n    b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)\n    m_i = o_i[:, None] >= j\n    b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * exp(b_g - b_gkj[None, :]), 0.0)\n    p_kj += H * K\n    p_gkj += H * K\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.debug_barrier()\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_gk = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_gk = tl.load(p_gk, boundary_check=(0, 1))\nb_dk = tl.zeros([BC, BK], dtype=tl.float32)\nNC = min(NC, tl.cdiv(T - i_t * BT, BC))\nif i_i < NC - 1:\n    p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    for i_j in range(i_i + 1, NC):\n        p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gq = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.load(p_gq, boundary_check=(0, 1))\n        b_qg = b_q * safe_exp(b_gq - b_gn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(b_dA, b_qg)\n    b_dk *= exp(b_gn[None, :] - b_gk)\no_dA = bos * H * BT + (i_t * BT + i_i * BC) * H * BT + i_h * BT + i_i * BC + tl.arange(0, BC)\np_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_gqj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_dA = tl.load(dA + o_dA + j * H * BT)\n    b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n    b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)\n    m_i = o_i[:, None] <= j\n    b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0)\n    p_qj += H * K\n    p_gqj += H * K\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gla_bwd_kernel_dA",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BV', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_dA(",
      "    v,",
      "    do,",
      "    dA,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_do, b_v)",
      "    p_dA = tl.make_block_ptr(",
      "        dA + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "    b_dA = tl.where(m_s, b_dA * scale, 0.0)",
      "    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_bwd_kernel_dA(v, do, dA, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nT = eos - bos\nb_dA = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_t * BT), (BV, BT), (0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dA += tl.dot(b_do, b_v)\np_dA = tl.make_block_ptr(dA + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nm_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\nb_dA = tl.where(m_s, b_dA * scale, 0.0)\ntl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gla_bwd_kernel_dv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in BK_LIST for BV in BV_LIST for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_dv(",
      "    k,",
      "    g,",
      "    A,",
      "    do,",
      "    dh,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A, 0.0)",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_dv = tl.dot(b_A, b_do.to(b_A.dtype), allow_tf32=False)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_gn = g + (bos + min(i_t * BT + BT, T) - 1) * H * K + i_h * K + o_k",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_gn = exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)",
      "        b_k = (b_k * b_gn).to(b_k.dtype)",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_bwd_kernel_dv(k, g, A, do, dh, dv, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\np_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A, 0.0)\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_dv = tl.dot(b_A, b_do.to(b_A.dtype), allow_tf32=False)\nfor i_k in range(tl.cdiv(K, BK)):\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_gk = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_gn = g + (bos + min(i_t * BT + BT, T) - 1) * H * K + i_h * K + o_k\n    p_dh = tl.make_block_ptr(dh + (i_tg * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_gn = exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)\n    b_k = (b_k * b_gn).to(b_k.dtype)\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gla_bwd_kernel_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in BK_LIST for BV in BV_LIST for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dq2",
        "annotation": null
      },
      {
        "name": "dk2",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_inter(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dq2,",
      "    dk2,",
      "    dg,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_gk = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_gn = g + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k",
      "    b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dgk = tl.zeros(",
      "        [",
      "            BK,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dgk += tl.sum(b_h * b_dh, axis=0)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "    b_dgk *= exp(b_gn)",
      "    b_dq *= scale",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_dq = b_dq * exp(b_gk)",
      "    b_dk = b_dk * exp(b_gn[None, :] - b_gk)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dgk += tl.sum(b_dk * b_k, axis=0)",
      "    b_dq += tl.load(p_dq, boundary_check=(0, 1))",
      "    b_dk += tl.load(p_dk, boundary_check=(0, 1))",
      "    b_dg = b_q * b_dq - b_k * b_dk",
      "",
      "    b_dg = (",
      "        b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :] + b_dgk[None, :]",
      "    )",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/393.py",
    "header": "def chunk_gla_bwd_kernel_inter(q, k, v, h, g, do, dh, dq, dk, dq2, dk2, dg, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\np_gk = tl.make_block_ptr(g + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_gn = g + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k\nb_gn = tl.load(p_gn, mask=m_k, other=0)\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_dgk = tl.zeros([BK], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh + (i_tg * H + i_h) * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_h * b_dh, axis=0)\n    b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\nb_dgk *= exp(b_gn)\nb_dq *= scale\nb_gk = tl.load(p_gk, boundary_check=(0, 1))\nb_dq = b_dq * exp(b_gk)\nb_dk = b_dk * exp(b_gn[None, :] - b_gk)\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dq = tl.make_block_ptr(dq + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dgk += tl.sum(b_dk * b_k, axis=0)\nb_dq += tl.load(p_dq, boundary_check=(0, 1))\nb_dk += tl.load(p_dk, boundary_check=(0, 1))\nb_dg = b_q * b_dq - b_k * b_dk\nb_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :] + b_dgk[None, :]\np_dq = tl.make_block_ptr(dq2 + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk2 + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dg = tl.make_block_ptr(dg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_swiglu_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_fwd_kernel(",
      "    X,",
      "    Y,",
      "    OUT,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_out_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    OUT += row * stride_out_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    out = x * tl.sigmoid(x) * y",
      "    tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "codes/113.py",
    "header": "def _swiglu_fwd_kernel(X, Y, OUT, stride_x_row, stride_y_row, stride_out_row, ncols, BLOCK_N: tl.constexpr):",
    "body": "row = tl.program_id(0)\nstart_col = tl.program_id(1) * BLOCK_N\nX += row * stride_x_row\nY += row * stride_y_row\nOUT += row * stride_out_row\ncols = start_col + tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\ny = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\nout = x * tl.sigmoid(x) * y\ntl.store(OUT + cols, out, mask=cols < ncols)"
  },
  {
    "name": "_swiglu_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DOUT",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dout_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_bwd_kernel(",
      "    X,",
      "    Y,",
      "    DOUT,",
      "    OUT,",
      "    DX,",
      "    DY,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dout_row,",
      "    stride_out_row,",
      "    stride_dx_row,",
      "    stride_dy_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    DOUT += row * stride_dout_row",
      "    if RECOMPUTE_OUTPUT:",
      "        OUT += row * stride_out_row",
      "    DX += row * stride_dx_row",
      "    DY += row * stride_dy_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    x_sigmoid = tl.sigmoid(x)",
      "    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout",
      "    dy = x * x_sigmoid * dout",
      "    tl.store(DX + cols, dx, mask=cols < ncols)",
      "    tl.store(DY + cols, dy, mask=cols < ncols)",
      "    if RECOMPUTE_OUTPUT:",
      "        out = x * x_sigmoid * y",
      "        tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "codes/113.py",
    "header": "def _swiglu_bwd_kernel(X, Y, DOUT, OUT, DX, DY, stride_x_row, stride_y_row, stride_dout_row, stride_out_row, stride_dx_row, stride_dy_row, ncols, BLOCK_N: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "row = tl.program_id(0)\nstart_col = tl.program_id(1) * BLOCK_N\nX += row * stride_x_row\nY += row * stride_y_row\nDOUT += row * stride_dout_row\nif RECOMPUTE_OUTPUT:\n    OUT += row * stride_out_row\nDX += row * stride_dx_row\nDY += row * stride_dy_row\ncols = start_col + tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\ny = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\ndout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)\nx_sigmoid = tl.sigmoid(x)\ndx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\ndy = x * x_sigmoid * dout\ntl.store(DX + cols, dx, mask=cols < ncols)\ntl.store(DY + cols, dy, mask=cols < ncols)\nif RECOMPUTE_OUTPUT:\n    out = x * x_sigmoid * y\n    tl.store(OUT + cols, out, mask=cols < ncols)"
  },
  {
    "name": "_linear_fwd",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_linear_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "bias_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _linear_fwd(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    bias_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    HAS_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    start_m = pid_m * BLOCK_SIZE_M",
      "    start_n = pid_n * BLOCK_SIZE_N",
      "",
      "    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_am = tl.where(offs_am < M, offs_am, 0)",
      "    offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "",
      "    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if HAS_BIAS:",
      "        bias_ptrs = bias_ptr + offs_bn[None, :]",
      "        bias = tl.load(bias_ptrs).to(tl.float32)",
      "        bias = tl.broadcast_to(bias, [BLOCK_SIZE_M, BLOCK_SIZE_N])",
      "        accumulator += bias",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    if ACTIVATION == \"GELU\":",
      "        c = gelu(c)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/282.py",
    "header": "def _linear_fwd(a_ptr, b_ptr, c_ptr, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, HAS_BIAS: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, ACTIVATION: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\nstart_m = pid_m * BLOCK_SIZE_M\nstart_n = pid_n * BLOCK_SIZE_N\noffs_am = start_m + tl.arange(0, BLOCK_SIZE_M)\noffs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)\noffs_am = tl.where(offs_am < M, offs_am, 0)\noffs_bn = tl.where(offs_bn < N, offs_bn, 0)\noffs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\noffs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nif HAS_BIAS:\n    bias_ptrs = bias_ptr + offs_bn[None, :]\n    bias = tl.load(bias_ptrs).to(tl.float32)\n    bias = tl.broadcast_to(bias, [BLOCK_SIZE_M, BLOCK_SIZE_N])\n    accumulator += bias\nc = accumulator.to(tl.bfloat16)\nif ACTIVATION == 'GELU':\n    c = gelu(c)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "unfold_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size, num_warps in itertools.product([32, 64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=['length', 'kernel_size', 'stride', 'n_frames'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "length",
        "annotation": null
      },
      {
        "name": "kernel_size",
        "annotation": null
      },
      {
        "name": "stride",
        "annotation": null
      },
      {
        "name": "n_frames",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def unfold_kernel(",
      "    input_ptr,",
      "    output_ptr,",
      "    length,",
      "    kernel_size,",
      "    stride,",
      "    n_frames,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    batch_idx = tl.program_id(0)",
      "",
      "    frame_idx = tl.program_id(1) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "",
      "    mask = frame_idx < n_frames",
      "",
      "    input_pos = frame_idx * stride",
      "",
      "    for i in range(kernel_size):",
      "        in_bounds = mask & ((input_pos + i) < length)",
      "",
      "        val = tl.where(",
      "            in_bounds,",
      "            tl.load(input_ptr + batch_idx * length + input_pos + i, mask=in_bounds),",
      "            0,",
      "        )",
      "",
      "        out_idx = batch_idx * n_frames * kernel_size + frame_idx * kernel_size + i",
      "        tl.store(output_ptr + out_idx, val, mask=in_bounds)"
    ],
    "file": "codes/185.py",
    "header": "def unfold_kernel(input_ptr, output_ptr, length, kernel_size, stride, n_frames, BLOCK_SIZE: tl.constexpr):",
    "body": "batch_idx = tl.program_id(0)\nframe_idx = tl.program_id(1) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = frame_idx < n_frames\ninput_pos = frame_idx * stride\nfor i in range(kernel_size):\n    in_bounds = mask & (input_pos + i < length)\n    val = tl.where(in_bounds, tl.load(input_ptr + batch_idx * length + input_pos + i, mask=in_bounds), 0)\n    out_idx = batch_idx * n_frames * kernel_size + frame_idx * kernel_size + i\n    tl.store(output_ptr + out_idx, val, mask=in_bounds)"
  },
  {
    "name": "complex_mul_conjugate_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['num_batches', 'num_frames', 'fft_size'])"
    ],
    "args": [
      {
        "name": "a_real_ptr",
        "annotation": null
      },
      {
        "name": "b_real_ptr",
        "annotation": null
      },
      {
        "name": "a_imag_ptr",
        "annotation": null
      },
      {
        "name": "b_imag_ptr",
        "annotation": null
      },
      {
        "name": "output1_ptr",
        "annotation": null
      },
      {
        "name": "output2_ptr",
        "annotation": null
      },
      {
        "name": "num_batches",
        "annotation": null
      },
      {
        "name": "num_frames",
        "annotation": null
      },
      {
        "name": "fft_size",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def complex_mul_conjugate_kernel(",
      "    a_real_ptr,",
      "    b_real_ptr,",
      "    a_imag_ptr,",
      "    b_imag_ptr,",
      "    output1_ptr,",
      "    output2_ptr,",
      "    num_batches,",
      "    num_frames,",
      "    fft_size,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    batch_idx = tl.program_id(0)",
      "",
      "    if batch_idx >= num_batches:",
      "        return",
      "",
      "    fft_idx = tl.arange(0, BLOCK_SIZE)",
      "    fft_mask = fft_idx < fft_size",
      "",
      "    batch_by_fft = batch_idx * fft_size",
      "",
      "    b_real_val = tl.load(b_real_ptr + batch_by_fft + fft_idx, mask=fft_mask)",
      "    b_imag_val = tl.load(b_imag_ptr + batch_by_fft + fft_idx, mask=fft_mask)",
      "",
      "    for frame_idx in range(num_frames):",
      "        global_idx = num_frames * batch_by_fft + frame_idx * fft_size + fft_idx",
      "",
      "        a_real_val = tl.load(a_real_ptr + global_idx, mask=fft_mask)",
      "        a_imag_val = tl.load(a_imag_ptr + global_idx, mask=fft_mask)",
      "",
      "        result1 = a_real_val * b_real_val + a_imag_val * b_imag_val",
      "        result2 = a_imag_val * b_real_val - a_real_val * b_imag_val",
      "",
      "        tl.store(output1_ptr + global_idx, result1, mask=fft_mask)",
      "        tl.store(output2_ptr + global_idx, result2, mask=fft_mask)"
    ],
    "file": "codes/185.py",
    "header": "def complex_mul_conjugate_kernel(a_real_ptr, b_real_ptr, a_imag_ptr, b_imag_ptr, output1_ptr, output2_ptr, num_batches, num_frames, fft_size, BLOCK_SIZE: tl.constexpr):",
    "body": "batch_idx = tl.program_id(0)\nif batch_idx >= num_batches:\n    return\nfft_idx = tl.arange(0, BLOCK_SIZE)\nfft_mask = fft_idx < fft_size\nbatch_by_fft = batch_idx * fft_size\nb_real_val = tl.load(b_real_ptr + batch_by_fft + fft_idx, mask=fft_mask)\nb_imag_val = tl.load(b_imag_ptr + batch_by_fft + fft_idx, mask=fft_mask)\nfor frame_idx in range(num_frames):\n    global_idx = num_frames * batch_by_fft + frame_idx * fft_size + fft_idx\n    a_real_val = tl.load(a_real_ptr + global_idx, mask=fft_mask)\n    a_imag_val = tl.load(a_imag_ptr + global_idx, mask=fft_mask)\n    result1 = a_real_val * b_real_val + a_imag_val * b_imag_val\n    result2 = a_imag_val * b_real_val - a_real_val * b_imag_val\n    tl.store(output1_ptr + global_idx, result1, mask=fft_mask)\n    tl.store(output2_ptr + global_idx, result2, mask=fft_mask)"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "codes/176.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr):",
    "body": "idx = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\noffsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + offsets)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    local_x_block_ptr = x_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == v_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    m = m_new\n    A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n    v_range = v_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\nloss += tl.sum(lse) / N\ntl.store(losses_ptr + idx, loss)\ntl.store(lse_ptr + offsets, lse)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N",
      "            tl.store(",
      "                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)",
      "            )",
      "",
      "            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "codes/176.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dA(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_V = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nN_offsets = tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nA_grad_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(0 * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nfor idx_N in range(N // N_BLOCK_SIZE):\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n    A_block_ptr = tl.advance(A_block_ptr, [-H, 0])\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n        temp_AgradT = temp_Agrad.trans() / N\n        tl.store(A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr))\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])\n    A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])\n    N_offsets += N_BLOCK_SIZE"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 4})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 1,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "    x_grad_slice = tl.zeros((N_BLOCK_SIZE, H), tl.float16)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "",
      "        x_block_ptr = tl.make_block_ptr(",
      "            base=x_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "            block_shape=(N_BLOCK_SIZE, H),",
      "            order=(1, 0),",
      "        )",
      "        A_slice_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(0, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v_full = tl.load(A_slice_ptr)",
      "",
      "        z_j_to_k = tl.sum(x_chunk[:, :, None] * A_v_full[None, :, :], axis=1).to(",
      "            tl.float32",
      "        )",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        temp_xgrad = (",
      "            tl.sum(softmax_z[:, :, None] * A_v_full.trans()[None, :, :], axis=1) / N",
      "        )",
      "        temp_xgrad -= (",
      "            tl.sum(tl.where(mask, A_v_full.trans()[None, :, :], 0.0), axis=1) / N",
      "        )",
      "        temp_xgrad = temp_xgrad.to(tl.float16)",
      "",
      "        x_grad_slice += temp_xgrad",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(x_grad_block_ptr, x_grad_slice)"
    ],
    "file": "codes/176.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 1, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_N = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_offsets)\nlse = tl.load(lse_global_ptr + N_offsets)\nx_grad_slice = tl.zeros((N_BLOCK_SIZE, H), tl.float16)\nfor idx_V in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H), order=(1, 0))\n    A_slice_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H, V_BLOCK_SIZE), order=(1, 0))\n    x_chunk = tl.load(x_block_ptr)\n    A_v_full = tl.load(A_slice_ptr)\n    z_j_to_k = tl.sum(x_chunk[:, :, None] * A_v_full[None, :, :], axis=1).to(tl.float32)\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n    temp_xgrad = tl.sum(softmax_z[:, :, None] * A_v_full.trans()[None, :, :], axis=1) / N\n    temp_xgrad -= tl.sum(tl.where(mask, A_v_full.trans()[None, :, :], 0.0), axis=1) / N\n    temp_xgrad = temp_xgrad.to(tl.float16)\n    x_grad_slice += temp_xgrad\n    V_offsets += V_BLOCK_SIZE\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H), order=(1, 0))\ntl.store(x_grad_block_ptr, x_grad_slice)"
  },
  {
    "name": "block_scaled_matmul_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc",
        "annotation": null
      },
      {
        "name": "a_scale",
        "annotation": null
      },
      {
        "name": "b_desc",
        "annotation": null
      },
      {
        "name": "b_scale",
        "annotation": null
      },
      {
        "name": "c_desc",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_sk",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_sb",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_sc",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_sd",
        "annotation": "tl.constexpr"
      },
      {
        "name": "output_type",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ELEM_PER_BYTE_A",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ELEM_PER_BYTE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "VEC_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_STAGES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_2D_SCALE_LOAD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def block_scaled_matmul_kernel(",
      "    a_desc,",
      "    a_scale,",
      "    b_desc,",
      "    b_scale,",
      "    c_desc,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    stride_sk: tl.constexpr,",
      "    stride_sb: tl.constexpr,",
      "    stride_sc: tl.constexpr,",
      "    stride_sd: tl.constexpr,",
      "    output_type: tl.constexpr,",
      "    ELEM_PER_BYTE_A: tl.constexpr,",
      "    ELEM_PER_BYTE_B: tl.constexpr,",
      "    VEC_SIZE: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    NUM_STAGES: tl.constexpr,",
      "    USE_2D_SCALE_LOAD: tl.constexpr,",
      "):",
      "",
      "    if output_type == 0:",
      "        output_dtype = tl.float32",
      "    elif output_type == 1:",
      "        output_dtype = tl.float16",
      "    elif output_type == 2:",
      "        output_dtype = tl.float8e4nv",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    pid_m = pid % num_pid_m",
      "    pid_n = pid // num_pid_m",
      "    offs_am = pid_m * BLOCK_M",
      "    offs_bn = pid_n * BLOCK_N",
      "    offs_k_a = 0",
      "    offs_k_b = 0",
      "",
      "    offs_sm = (pid_m * (BLOCK_M // 128) + tl.arange(0, BLOCK_M // 128)) % M",
      "    offs_sn = (pid_n * (BLOCK_N // 128) + tl.arange(0, BLOCK_N // 128)) % N",
      "",
      "    MIXED_PREC: tl.constexpr = ELEM_PER_BYTE_A == 1 and ELEM_PER_BYTE_B == 2",
      "",
      "    if USE_2D_SCALE_LOAD:",
      "        offs_inner = tl.arange(0, (BLOCK_K // VEC_SIZE // 4) * 32 * 4 * 4)",
      "        a_scale_ptr = a_scale + offs_sm[:, None] * stride_sk + offs_inner[None, :]",
      "        b_scale_ptr = b_scale + offs_sn[:, None] * stride_sk + offs_inner[None, :]",
      "    else:",
      "        offs_sk = tl.arange(0, (BLOCK_K // VEC_SIZE // 4))",
      "",
      "        offs_sc = tl.arange(0, 32)",
      "",
      "        offs_sd = tl.arange(0, 4)",
      "        a_scale_ptr = a_scale + (",
      "            offs_sm[:, None, None, None, None] * stride_sk",
      "            + offs_sk[None, :, None, None, None] * stride_sb",
      "            + offs_sc[None, None, :, None, None] * stride_sc",
      "            + offs_sd[None, None, None, :, None] * stride_sd",
      "            + offs_sd[None, None, None, None, :]",
      "        )",
      "        b_scale_ptr = b_scale + (",
      "            offs_sn[:, None, None, None, None] * stride_sk",
      "            + offs_sk[None, :, None, None, None] * stride_sb",
      "            + offs_sc[None, None, :, None, None] * stride_sc",
      "            + offs_sd[None, None, None, :, None] * stride_sd",
      "            + offs_sd[None, None, None, None, :]",
      "        )",
      "",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "    for k in tl.range(0, tl.cdiv(K, BLOCK_K), num_stages=NUM_STAGES):",
      "        a = a_desc.load([offs_am, offs_k_a])",
      "        b = b_desc.load([offs_bn, offs_k_b])",
      "        scale_a = tl.load(a_scale_ptr)",
      "        scale_b = tl.load(b_scale_ptr)",
      "        if USE_2D_SCALE_LOAD:",
      "            scale_a = scale_a.reshape(",
      "                BLOCK_M // 128, BLOCK_K // VEC_SIZE // 4, 32, 4, 4",
      "            )",
      "            scale_b = scale_b.reshape(",
      "                BLOCK_N // 128, BLOCK_K // VEC_SIZE // 4, 32, 4, 4",
      "            )",
      "        scale_a = scale_a.trans(0, 3, 2, 1, 4).reshape(BLOCK_M, BLOCK_K // VEC_SIZE)",
      "        scale_b = scale_b.trans(0, 3, 2, 1, 4).reshape(BLOCK_N, BLOCK_K // VEC_SIZE)",
      "",
      "        if MIXED_PREC:",
      "            accumulator = tl.dot_scaled(",
      "                a, scale_a, \"e4m3\", b.T, scale_b, \"e2m1\", accumulator",
      "            )",
      "        elif ELEM_PER_BYTE_A == 2 and ELEM_PER_BYTE_B == 2:",
      "            accumulator = tl.dot_scaled(",
      "                a, scale_a, \"e2m1\", b.T, scale_b, \"e2m1\", accumulator",
      "            )",
      "        else:",
      "            accumulator = tl.dot_scaled(",
      "                a, scale_a, \"e4m3\", b.T, scale_b, \"e4m3\", accumulator",
      "            )",
      "",
      "        offs_k_a += BLOCK_K // ELEM_PER_BYTE_A",
      "        offs_k_b += BLOCK_K // ELEM_PER_BYTE_B",
      "        a_scale_ptr += (BLOCK_K // VEC_SIZE // 4) * stride_sb",
      "        b_scale_ptr += (BLOCK_K // VEC_SIZE // 4) * stride_sb",
      "",
      "    c_desc.store([offs_am, offs_bn], accumulator.to(output_dtype))"
    ],
    "file": "codes/2.py",
    "header": "def block_scaled_matmul_kernel(a_desc, a_scale, b_desc, b_scale, c_desc, M: tl.constexpr, N: tl.constexpr, K: tl.constexpr, stride_sk: tl.constexpr, stride_sb: tl.constexpr, stride_sc: tl.constexpr, stride_sd: tl.constexpr, output_type: tl.constexpr, ELEM_PER_BYTE_A: tl.constexpr, ELEM_PER_BYTE_B: tl.constexpr, VEC_SIZE: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, NUM_STAGES: tl.constexpr, USE_2D_SCALE_LOAD: tl.constexpr):",
    "body": "if output_type == 0:\n    output_dtype = tl.float32\nelif output_type == 1:\n    output_dtype = tl.float16\nelif output_type == 2:\n    output_dtype = tl.float8e4nv\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_M)\npid_m = pid % num_pid_m\npid_n = pid // num_pid_m\noffs_am = pid_m * BLOCK_M\noffs_bn = pid_n * BLOCK_N\noffs_k_a = 0\noffs_k_b = 0\noffs_sm = (pid_m * (BLOCK_M // 128) + tl.arange(0, BLOCK_M // 128)) % M\noffs_sn = (pid_n * (BLOCK_N // 128) + tl.arange(0, BLOCK_N // 128)) % N\nMIXED_PREC: tl.constexpr = ELEM_PER_BYTE_A == 1 and ELEM_PER_BYTE_B == 2\nif USE_2D_SCALE_LOAD:\n    offs_inner = tl.arange(0, BLOCK_K // VEC_SIZE // 4 * 32 * 4 * 4)\n    a_scale_ptr = a_scale + offs_sm[:, None] * stride_sk + offs_inner[None, :]\n    b_scale_ptr = b_scale + offs_sn[:, None] * stride_sk + offs_inner[None, :]\nelse:\n    offs_sk = tl.arange(0, BLOCK_K // VEC_SIZE // 4)\n    offs_sc = tl.arange(0, 32)\n    offs_sd = tl.arange(0, 4)\n    a_scale_ptr = a_scale + (offs_sm[:, None, None, None, None] * stride_sk + offs_sk[None, :, None, None, None] * stride_sb + offs_sc[None, None, :, None, None] * stride_sc + offs_sd[None, None, None, :, None] * stride_sd + offs_sd[None, None, None, None, :])\n    b_scale_ptr = b_scale + (offs_sn[:, None, None, None, None] * stride_sk + offs_sk[None, :, None, None, None] * stride_sb + offs_sc[None, None, :, None, None] * stride_sc + offs_sd[None, None, None, :, None] * stride_sd + offs_sd[None, None, None, None, :])\naccumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\nfor k in tl.range(0, tl.cdiv(K, BLOCK_K), num_stages=NUM_STAGES):\n    a = a_desc.load([offs_am, offs_k_a])\n    b = b_desc.load([offs_bn, offs_k_b])\n    scale_a = tl.load(a_scale_ptr)\n    scale_b = tl.load(b_scale_ptr)\n    if USE_2D_SCALE_LOAD:\n        scale_a = scale_a.reshape(BLOCK_M // 128, BLOCK_K // VEC_SIZE // 4, 32, 4, 4)\n        scale_b = scale_b.reshape(BLOCK_N // 128, BLOCK_K // VEC_SIZE // 4, 32, 4, 4)\n    scale_a = scale_a.trans(0, 3, 2, 1, 4).reshape(BLOCK_M, BLOCK_K // VEC_SIZE)\n    scale_b = scale_b.trans(0, 3, 2, 1, 4).reshape(BLOCK_N, BLOCK_K // VEC_SIZE)\n    if MIXED_PREC:\n        accumulator = tl.dot_scaled(a, scale_a, 'e4m3', b.T, scale_b, 'e2m1', accumulator)\n    elif ELEM_PER_BYTE_A == 2 and ELEM_PER_BYTE_B == 2:\n        accumulator = tl.dot_scaled(a, scale_a, 'e2m1', b.T, scale_b, 'e2m1', accumulator)\n    else:\n        accumulator = tl.dot_scaled(a, scale_a, 'e4m3', b.T, scale_b, 'e4m3', accumulator)\n    offs_k_a += BLOCK_K // ELEM_PER_BYTE_A\n    offs_k_b += BLOCK_K // ELEM_PER_BYTE_B\n    a_scale_ptr += BLOCK_K // VEC_SIZE // 4 * stride_sb\n    b_scale_ptr += BLOCK_K // VEC_SIZE // 4 * stride_sb\nc_desc.store([offs_am, offs_bn], accumulator.to(output_dtype))"
  },
  {
    "name": "matmul_kernel_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=tuning_configs, key=['M', 'N', 'K'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_BUFFER_OPS_ASSUMES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,",
      "):",
      "    if ENABLE_BUFFER_OPS_ASSUMES:",
      "        tl.assume(M >= 0)",
      "        tl.assume(N >= 0)",
      "        tl.assume(K >= 0)",
      "        tl.assume(stride_am >= 0)",
      "        tl.assume(stride_ak >= 0)",
      "        tl.assume(stride_bn >= 0)",
      "        tl.assume(stride_bk >= 0)",
      "        tl.assume(stride_cm >= 0)",
      "        tl.assume(stride_cn >= 0)",
      "",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    offs_k_for_mask = tl.arange(0, BLOCK_K)",
      "",
      "    num_pid_in_group = GROUP_M * num_pid_n",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = tl.arange(0, BLOCK_M)",
      "    offs_bn = tl.arange(0, BLOCK_N)",
      "",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            start_m = pid_m * BLOCK_M",
      "            start_n = pid_n * BLOCK_N",
      "            offs_am = tl.arange(0, BLOCK_M)",
      "            offs_bn = tl.arange(0, BLOCK_N)",
      "            offs_am = tl.where(offs_am < M - start_m, offs_am, 0)",
      "            offs_bn = tl.where(offs_bn < N - start_n, offs_bn, 0)",
      "            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_M), BLOCK_M)",
      "            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_N), BLOCK_N)",
      "        offs_k = ki * BLOCK_K + tl.arange(0, BLOCK_K)",
      "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "            offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "            if c_ptr.dtype == tl.float8e4nv:",
      "                c = accumulator.to(tl.float8e4nv)",
      "            else:",
      "                c = accumulator.to(tl.float16)",
      "            tl.store(c_ptrs, c, mask=c_mask)",
      "            accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)"
    ],
    "file": "codes/649.py",
    "header": "def matmul_kernel_persistent(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr, NUM_SMS: tl.constexpr, ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr):",
    "body": "if ENABLE_BUFFER_OPS_ASSUMES:\n    tl.assume(M >= 0)\n    tl.assume(N >= 0)\n    tl.assume(K >= 0)\n    tl.assume(stride_am >= 0)\n    tl.assume(stride_ak >= 0)\n    tl.assume(stride_bn >= 0)\n    tl.assume(stride_bk >= 0)\n    tl.assume(stride_cm >= 0)\n    tl.assume(stride_cn >= 0)\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_M)\nnum_pid_n = tl.cdiv(N, BLOCK_N)\nk_tiles = tl.cdiv(K, BLOCK_K)\nnum_tiles = num_pid_m * num_pid_n\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\noffs_k_for_mask = tl.arange(0, BLOCK_K)\nnum_pid_in_group = GROUP_M * num_pid_n\npid_m = 0\npid_n = 0\noffs_am = tl.arange(0, BLOCK_M)\noffs_bn = tl.arange(0, BLOCK_N)\naccumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        start_m = pid_m * BLOCK_M\n        start_n = pid_n * BLOCK_N\n        offs_am = tl.arange(0, BLOCK_M)\n        offs_bn = tl.arange(0, BLOCK_N)\n        offs_am = tl.where(offs_am < M - start_m, offs_am, 0)\n        offs_bn = tl.where(offs_bn < N - start_n, offs_bn, 0)\n        offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_M), BLOCK_M)\n        offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_N), BLOCK_N)\n    offs_k = ki * BLOCK_K + tl.arange(0, BLOCK_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    if ki == k_tiles - 1:\n        offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n        offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n        if c_ptr.dtype == tl.float8e4nv:\n            c = accumulator.to(tl.float8e4nv)\n        else:\n            c = accumulator.to(tl.float16)\n        tl.store(c_ptrs, c, mask=c_mask)\n        accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)"
  },
  {
    "name": "matmul_kernel_tma_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc_ptr",
        "annotation": null
      },
      {
        "name": "b_desc_ptr",
        "annotation": null
      },
      {
        "name": "c_desc_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_tma_persistent(",
      "    a_desc_ptr,",
      "    b_desc_ptr,",
      "    c_desc_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    FP8_OUTPUT: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = tl._experimental_descriptor_load(",
      "            a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype",
      "        )",
      "        b = tl._experimental_descriptor_load(",
      "            b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype",
      "        )",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            c = accumulator.to(dtype)",
      "",
      "            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/649.py",
    "header": "def matmul_kernel_tma_persistent(a_desc_ptr, b_desc_ptr, c_desc_ptr, M, N, K, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, FP8_OUTPUT: tl.constexpr, NUM_SMS: tl.constexpr):",
    "body": "dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\npid_m = 0\npid_n = 0\noffs_am = 0\noffs_bn = 0\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n    offs_k = ki * BLOCK_SIZE_K\n    a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype)\n    b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype)\n    accumulator = tl.dot(a, b.T, accumulator)\n    if ki == k_tiles - 1:\n        c = accumulator.to(dtype)\n        tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "fused_recurrent_dplr_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BV in [16, 32, 64] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_dplr_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    a,",
      "    b,",
      "    gk,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_k = tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_q = q + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_a = a + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_b = b + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_gk = gk + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_o = o + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "",
      "        tmp = tl.sum(b_h * b_a[None, :], axis=1)",
      "        b_h = exp(b_gk)[None, :] * b_h + (",
      "            tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "        )",
      "        b_o = tl.sum(b_h * b_q[None, :], axis=1)",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        p_q += (-1 if REVERSE else 1) * H * K",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_a += (-1 if REVERSE else 1) * H * K",
      "        p_b += (-1 if REVERSE else 1) * H * K",
      "        p_gk += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_o += (-1 if REVERSE else 1) * H * V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/387.py",
    "header": "def fused_recurrent_dplr_delta_rule_fwd_kernel(q, k, v, a, b, gk, o, h0, ht, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\no_k = tl.arange(0, BK)\no_v = i_v * BV + tl.arange(0, BV)\np_q = q + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_k = k + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_a = a + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_b = b + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_gk = gk + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_v = v + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\np_o = o + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\nmask_k = o_k < K\nmask_v = o_v < V\nmask_h = mask_k[None, :] & mask_v[:, None]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n    b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n    b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    tmp = tl.sum(b_h * b_a[None, :], axis=1)\n    b_h = exp(b_gk)[None, :] * b_h + (tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None])\n    b_o = tl.sum(b_h * b_q[None, :], axis=1)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    p_q += (-1 if REVERSE else 1) * H * K\n    p_k += (-1 if REVERSE else 1) * H * K\n    p_a += (-1 if REVERSE else 1) * H * K\n    p_b += (-1 if REVERSE else 1) * H * K\n    p_gk += (-1 if REVERSE else 1) * H * K\n    p_v += (-1 if REVERSE else 1) * H * V\n    p_o += (-1 if REVERSE else 1) * H * V\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configs)), key=['N_CTX', 'HEAD_DIM'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "mask",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "stride_mask_z",
        "annotation": null
      },
      {
        "name": "stride_mask_h",
        "annotation": null
      },
      {
        "name": "stride_mask_m",
        "annotation": null
      },
      {
        "name": "stride_mask_n",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_MASK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    mask,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    stride_mask_z,",
      "    stride_mask_h,",
      "    stride_mask_m,",
      "    stride_mask_n,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    HEAD_DIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    USE_MASK: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "",
      "    if USE_MASK:",
      "        mask_offset = (",
      "            off_z.to(tl.int64) * stride_mask_z + off_h.to(tl.int64) * stride_mask_h",
      "        )",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, HEAD_DIM),",
      "        order=v_order,",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(HEAD_DIM, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(HEAD_DIM, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    mask_block_ptr = (",
      "        None",
      "        if not USE_MASK",
      "        else tl.make_block_ptr(",
      "            base=mask + mask_offset,",
      "            shape=(N_CTX, N_CTX),",
      "            strides=(stride_mask_m, stride_mask_n),",
      "            offsets=(start_m * BLOCK_M, 0),",
      "            block_shape=(BLOCK_M, BLOCK_N),",
      "            order=(0, 1),",
      "        )",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0",
      "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)",
      "",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.44269504",
      "",
      "    q = tl.load(Q_block_ptr)",
      "",
      "    if USE_MASK:",
      "        acc, l_i, m_i = _attn_fwd_inner(",
      "            acc,",
      "            l_i,",
      "            m_i,",
      "            q,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            mask_block_ptr,",
      "            start_m,",
      "            qk_scale,",
      "            BLOCK_M,",
      "            HEAD_DIM,",
      "            BLOCK_N,",
      "            4 - STAGE,",
      "            offs_m,",
      "            offs_n,",
      "            N_CTX,",
      "            V.dtype.element_ty == tl.float8e5,",
      "            USE_MASK,",
      "        )",
      "    else:",
      "        acc, l_i, m_i = _attn_fwd_inner(",
      "            acc,",
      "            l_i,",
      "            m_i,",
      "            q,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            None,",
      "            start_m,",
      "            qk_scale,",
      "            BLOCK_M,",
      "            HEAD_DIM,",
      "            BLOCK_N,",
      "            2,",
      "            offs_m,",
      "            offs_n,",
      "            N_CTX,",
      "            V.dtype.element_ty == tl.float8e5,",
      "            USE_MASK,",
      "        )",
      "",
      "    m_i += tl.math.log2(l_i)",
      "    acc = acc / l_i[:, None]",
      "    m_ptrs = M + off_hz * N_CTX + offs_m",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))"
    ],
    "file": "codes/296.py",
    "header": "def _attn_fwd(Q, K, V, mask, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, stride_mask_z, stride_mask_h, stride_mask_m, stride_mask_n, Z, H, N_CTX, HEAD_DIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, STAGE: tl.constexpr, USE_MASK: tl.constexpr):",
    "body": "tl.static_assert(BLOCK_N <= HEAD_DIM)\nstart_m = tl.program_id(0)\noff_hz = tl.program_id(1)\noff_z = off_hz // H\noff_h = off_hz % H\nqvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\nif USE_MASK:\n    mask_offset = off_z.to(tl.int64) * stride_mask_z + off_h.to(tl.int64) * stride_mask_h\nQ_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\nv_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)\nV_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0), block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\nK_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape=(HEAD_DIM, BLOCK_N), order=(0, 1))\nO_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\nmask_block_ptr = None if not USE_MASK else tl.make_block_ptr(base=mask + mask_offset, shape=(N_CTX, N_CTX), strides=(stride_mask_m, stride_mask_n), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_N), order=(0, 1))\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nl_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\nacc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\nqk_scale = sm_scale\nqk_scale *= 1.44269504\nq = tl.load(Q_block_ptr)\nif USE_MASK:\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, mask_block_ptr, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 4 - STAGE, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5, USE_MASK)\nelse:\n    acc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, None, start_m, qk_scale, BLOCK_M, HEAD_DIM, BLOCK_N, 2, offs_m, offs_n, N_CTX, V.dtype.element_ty == tl.float8e5, USE_MASK)\nm_i += tl.math.log2(l_i)\nacc = acc / l_i[:, None]\nm_ptrs = M + off_hz * N_CTX + offs_m\ntl.store(m_ptrs, m_i)\ntl.store(O_block_ptr, acc.to(Out.type.element_ty))"
  },
  {
    "name": "fused_recurrent_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BK', 'BV', 'USE_GK', 'USE_GV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    gk,",
      "    gv,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    p_q = (",
      "        q",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_k = (",
      "        k",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_v = (",
      "        v",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_o = (",
      "        o",
      "        + ((i_k * all + bos) + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    if USE_G:",
      "        p_g = g + (bos + ((T - 1) if REVERSE else 0)) * H + i_h",
      "    if USE_GK:",
      "        p_gk = (",
      "            gk",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "            + i_h * K",
      "            + i_k * BK",
      "            + tl.arange(0, BK)",
      "        )",
      "    if USE_GV:",
      "        p_gv = (",
      "            gv",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "            + i_h * V",
      "            + i_v * BV",
      "            + tl.arange(0, BV)",
      "        )",
      "",
      "    mask_k = (i_k * BK + tl.arange(0, BK)) < K",
      "    mask_v = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        if USE_GK:",
      "            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gk[None, :])",
      "        if USE_GV:",
      "            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gv[:, None])",
      "        if USE_G:",
      "            b_g = tl.load(p_g).to(tl.float32)",
      "            b_h = b_h * exp(b_g)",
      "        b_h += b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        p_q += (-1 if REVERSE else 1) * H * K",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_o += (-1 if REVERSE else 1) * H * V",
      "        if USE_GK:",
      "            p_gk += (-1 if REVERSE else 1) * H * K",
      "        if USE_GV:",
      "            p_gv += (-1 if REVERSE else 1) * H * V",
      "        if USE_G:",
      "            p_g += (-1 if REVERSE else 1) * H",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/374.py",
    "header": "def fused_recurrent_fwd_kernel(q, k, v, g, gk, gv, o, h0, ht, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64), tl.program_id(2).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\np_q = q + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\np_k = k + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\np_v = v + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\np_o = o + (i_k * all + bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\nif USE_G:\n    p_g = g + (bos + (T - 1 if REVERSE else 0)) * H + i_h\nif USE_GK:\n    p_gk = gk + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\nif USE_GV:\n    p_gv = gv + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\nmask_k = i_k * BK + tl.arange(0, BK) < K\nmask_v = i_v * BV + tl.arange(0, BV) < V\nmask_h = mask_k[None, :] & mask_v[:, None]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    if USE_GK:\n        b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n        b_h = b_h * exp(b_gk[None, :])\n    if USE_GV:\n        b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n        b_h = b_h * exp(b_gv[:, None])\n    if USE_G:\n        b_g = tl.load(p_g).to(tl.float32)\n        b_h = b_h * exp(b_g)\n    b_h += b_k[None, :] * b_v[:, None]\n    b_o = b_h * b_q[None, :]\n    b_o = tl.sum(b_o, axis=1)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    p_q += (-1 if REVERSE else 1) * H * K\n    p_k += (-1 if REVERSE else 1) * H * K\n    p_v += (-1 if REVERSE else 1) * H * V\n    p_o += (-1 if REVERSE else 1) * H * V\n    if USE_GK:\n        p_gk += (-1 if REVERSE else 1) * H * K\n    if USE_GV:\n        p_gv += (-1 if REVERSE else 1) * H * V\n    if USE_G:\n        p_g += (-1 if REVERSE else 1) * H\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "fused_recurrent_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BK', 'BV', 'USE_GK', 'USE_GV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    gk,",
      "    gv,",
      "    h0,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    p_k = (",
      "        k",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_v = (",
      "        v",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_do = (",
      "        do",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_dq = (",
      "        dq",
      "        + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    if USE_G:",
      "        p_g = g + (bos + ((T - 1) if REVERSE else 0)) * H + i_h",
      "    if USE_GK:",
      "        p_gk = (",
      "            gk",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "            + i_h * K",
      "            + i_k * BK",
      "            + tl.arange(0, BK)",
      "        )",
      "    if USE_GV:",
      "        p_gv = (",
      "            gv",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "            + i_h * V",
      "            + i_v * BV",
      "            + tl.arange(0, BV)",
      "        )",
      "",
      "    mask_k = i_k * BK + tl.arange(0, BK) < K",
      "    mask_v = i_v * BV + tl.arange(0, BV) < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        if USE_G:",
      "            b_g = tl.load(p_g).to(tl.float32)",
      "            b_h = b_h * exp(b_g)",
      "        if USE_GK:",
      "            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gk[:, None])",
      "        if USE_GV:",
      "            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gv[None, :])",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        b_dq = b_h * b_do[None, :]",
      "        b_dq = tl.sum(b_dq, axis=1) * scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_do += (-1 if REVERSE else 1) * H * V",
      "        p_dq += (-1 if REVERSE else 1) * H * K",
      "        if USE_G:",
      "            p_g += (-1 if REVERSE else 1) * H",
      "        if USE_GK:",
      "            p_gk += (-1 if REVERSE else 1) * H * K",
      "        if USE_GV:",
      "            p_gv += (-1 if REVERSE else 1) * H * V",
      "",
      "    tl.debug_barrier()",
      "",
      "    p_q = (",
      "        q",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_k = (",
      "        k",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_v = (",
      "        v",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_do = (",
      "        do",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_dk = (",
      "        dk",
      "        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_dv = (",
      "        dv",
      "        + ((i_k * all + bos) + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    if USE_G:",
      "        p_g = g + (bos + ((T - 1) if not REVERSE else 0)) * H + i_h",
      "    if USE_GK:",
      "        p_gk = (",
      "            gk",
      "            + (bos + ((T - 1) if not REVERSE else 0)) * H * K",
      "            + i_h * K",
      "            + i_k * BK",
      "            + tl.arange(0, BK)",
      "        )",
      "    if USE_GV:",
      "        p_gv = (",
      "            gv",
      "            + (bos + ((T - 1) if not REVERSE else 0)) * H * V",
      "            + i_h * V",
      "            + i_v * BV",
      "            + tl.arange(0, BV)",
      "        )",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = (",
      "            dht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_dh += tl.load(p_dht, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_dh += b_q[:, None] * b_do[None, :]",
      "        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)",
      "        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)",
      "        if USE_G:",
      "            b_g = tl.load(p_g).to(tl.float32)",
      "            b_dh *= exp(b_g)",
      "        if USE_GK:",
      "            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "            b_dh *= exp(b_gk)[:, None]",
      "        if USE_GV:",
      "            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)",
      "            b_dh *= exp(b_gv)[None, :]",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)",
      "",
      "        p_q += (1 if REVERSE else -1) * H * K",
      "        p_k += (1 if REVERSE else -1) * H * K",
      "        p_v += (1 if REVERSE else -1) * H * V",
      "        p_do += (1 if REVERSE else -1) * H * V",
      "        p_dk += (1 if REVERSE else -1) * H * K",
      "        p_dv += (1 if REVERSE else -1) * H * V",
      "        if USE_G:",
      "            p_g += (1 if REVERSE else -1) * H",
      "        if USE_GK:",
      "            p_gk += (1 if REVERSE else -1) * H * K",
      "        if USE_GV:",
      "            p_gv += (1 if REVERSE else -1) * H * V",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = (",
      "            dh0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/374.py",
    "header": "def fused_recurrent_bwd_kernel(q, k, v, g, gk, gv, h0, do, dq, dk, dv, dht, dh0, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64), tl.program_id(2).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\np_k = k + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\np_v = v + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\np_do = do + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\np_dq = dq + (i_v * all + bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\nif USE_G:\n    p_g = g + (bos + (T - 1 if REVERSE else 0)) * H + i_h\nif USE_GK:\n    p_gk = gk + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\nif USE_GV:\n    p_gv = gv + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\nmask_k = i_k * BK + tl.arange(0, BK) < K\nmask_v = i_v * BV + tl.arange(0, BV) < V\nmask_h = mask_k[:, None] & mask_v[None, :]\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    if USE_G:\n        b_g = tl.load(p_g).to(tl.float32)\n        b_h = b_h * exp(b_g)\n    if USE_GK:\n        b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n        b_h = b_h * exp(b_gk[:, None])\n    if USE_GV:\n        b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n        b_h = b_h * exp(b_gv[None, :])\n    b_h += b_k[:, None] * b_v[None, :]\n    b_dq = b_h * b_do[None, :]\n    b_dq = tl.sum(b_dq, axis=1) * scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)\n    p_k += (-1 if REVERSE else 1) * H * K\n    p_v += (-1 if REVERSE else 1) * H * V\n    p_do += (-1 if REVERSE else 1) * H * V\n    p_dq += (-1 if REVERSE else 1) * H * K\n    if USE_G:\n        p_g += (-1 if REVERSE else 1) * H\n    if USE_GK:\n        p_gk += (-1 if REVERSE else 1) * H * K\n    if USE_GV:\n        p_gv += (-1 if REVERSE else 1) * H * V\ntl.debug_barrier()\np_q = q + (bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\np_k = k + (bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\np_v = v + (bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\np_do = do + (bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\np_dk = dk + (i_v * all + bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\np_dv = dv + (i_k * all + bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\nif USE_G:\n    p_g = g + (bos + (T - 1 if not REVERSE else 0)) * H + i_h\nif USE_GK:\n    p_gk = gk + (bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\nif USE_GV:\n    p_gv = gv + (bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = dht + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    b_dh += tl.load(p_dht, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    b_dh += b_q[:, None] * b_do[None, :]\n    b_dk = tl.sum(b_dh * b_v[None, :], axis=1)\n    b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n    if USE_G:\n        b_g = tl.load(p_g).to(tl.float32)\n        b_dh *= exp(b_g)\n    if USE_GK:\n        b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)\n        b_dh *= exp(b_gk)[:, None]\n    if USE_GV:\n        b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)\n        b_dh *= exp(b_gv)[None, :]\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n    p_q += (1 if REVERSE else -1) * H * K\n    p_k += (1 if REVERSE else -1) * H * K\n    p_v += (1 if REVERSE else -1) * H * V\n    p_do += (1 if REVERSE else -1) * H * V\n    p_dk += (1 if REVERSE else -1) * H * K\n    p_dv += (1 if REVERSE else -1) * H * V\n    if USE_G:\n        p_g += (1 if REVERSE else -1) * H\n    if USE_GK:\n        p_gk += (1 if REVERSE else -1) * H * K\n    if USE_GV:\n        p_gv += (1 if REVERSE else -1) * H * V\nif STORE_INITIAL_STATE_GRADIENT:\n    p_dh0 = dh0 + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "_fwd_hyper_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] == args['V_BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "q_sort_idx",
        "annotation": null
      },
      {
        "name": "k_sort_idx",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "Lse",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxm",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxn",
        "annotation": null
      },
      {
        "name": "stride_ob",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "block_size",
        "annotation": null
      },
      {
        "name": "sample_size",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "v_headdim",
        "annotation": null
      },
      {
        "name": "smooth_block",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_V_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_hyper_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    q_sort_idx,",
      "    k_sort_idx,",
      "    Out,",
      "    Lse,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_q_sort_idxb,",
      "    stride_q_sort_idxh,",
      "    stride_q_sort_idxm,",
      "    stride_k_sort_idxb,",
      "    stride_k_sort_idxh,",
      "    stride_k_sort_idxn,",
      "    stride_ob,",
      "    stride_oh,",
      "    stride_om,",
      "    nheads,",
      "    block_size,",
      "    sample_size,",
      "    seqlen_k,",
      "    seqlen_q,",
      "    headdim,",
      "    v_headdim,",
      "    smooth_block,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    V_BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    EVEN_V_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)",
      "",
      "    q_idx_ptrs = (",
      "        q_sort_idx",
      "        + off_b * stride_q_sort_idxb",
      "        + off_h * stride_q_sort_idxh",
      "        + offs_m * stride_q_sort_idxm",
      "    )",
      "    q_idx = tl.load(q_idx_ptrs).to(tl.int32)",
      "",
      "    k_sort_idx += off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh",
      "",
      "    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, V_BLOCK_HEADDIM], dtype=tl.float32)",
      "    q_ptrs = (",
      "        Q",
      "        + off_b * stride_qb",
      "        + off_h * stride_qh",
      "        + (q_idx[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "    if EVEN_HEADDIM:",
      "        q = tl.load(q_ptrs)",
      "    else:",
      "        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "",
      "    block_id = start_m // block_size",
      "    block_offs = (",
      "        seqlen_k + (start_m % block_size) * BLOCK_N - (block_size - 1) * BLOCK_N // 2",
      "    )",
      "    end_n = tl.minimum((block_id + 1) * BLOCK_N * block_size, seqlen_k)",
      "    for start_n in range(block_id * BLOCK_N * block_size, end_n, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        if smooth_block:",
      "            k_idx_ptrs = (",
      "                (start_n + block_offs + offs_n) * stride_k_sort_idxn",
      "            ) % seqlen_k",
      "        else:",
      "            k_idx_ptrs = (start_n + offs_n) * stride_k_sort_idxn",
      "",
      "        k_idx = tl.load(k_sort_idx + k_idx_ptrs).to(tl.int32)",
      "        k_ptrs = (",
      "            K",
      "            + off_b * stride_kb",
      "            + off_h * stride_kh",
      "            + (k_idx[:, None] * stride_kn + offs_d[None, :])",
      "        )",
      "",
      "        if EVEN_HEADDIM:",
      "            k = tl.load(k_ptrs)",
      "        else:",
      "            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, tl.trans(k))",
      "        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "        p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        v_ptrs = (",
      "            V",
      "            + off_b * stride_vb",
      "            + off_h * stride_vh",
      "            + (k_idx[:, None] * stride_vn + offs_vd[None, :])",
      "        )",
      "        if EVEN_V_HEADDIM:",
      "            v = tl.load(v_ptrs)",
      "        else:",
      "            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    for col_block in range(0, sample_size):",
      "        curr_offs_n = col_block * BLOCK_N * stride_kn + offs_n",
      "        k_ptrs = (",
      "            K",
      "            + off_b * stride_kb",
      "            + off_h * stride_kh",
      "            + (curr_offs_n[:, None] * stride_kn + offs_d[None, :])",
      "        )",
      "",
      "        if EVEN_HEADDIM:",
      "            k = tl.load(k_ptrs)",
      "        else:",
      "            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, tl.trans(k))",
      "        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "        p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        v_ptrs = (",
      "            V",
      "            + off_b * stride_vb",
      "            + off_h * stride_vh",
      "            + (curr_offs_n[:, None] * stride_vn + offs_vd[None, :])",
      "        )",
      "        if EVEN_V_HEADDIM:",
      "            v = tl.load(v_ptrs)",
      "        else:",
      "            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    o_scale = tl.exp(m_i - lse_i)",
      "    acc_o = acc_o * o_scale[:, None]",
      "",
      "    lse_ptrs = Lse + off_hb * seqlen_q + q_idx",
      "    out_ptrs = (",
      "        Out",
      "        + off_b * stride_ob",
      "        + off_h * stride_oh",
      "        + (q_idx[:, None] * stride_om + offs_vd[None, :])",
      "    )",
      "",
      "    tl.store(lse_ptrs, lse_i)",
      "    if EVEN_V_HEADDIM:",
      "        tl.store(out_ptrs, acc_o)",
      "    else:",
      "        tl.store(out_ptrs, acc_o, mask=offs_vd[None, :] < v_headdim)"
    ],
    "file": "codes/301.py",
    "header": "def _fwd_hyper_kernel(Q, K, V, q_sort_idx, k_sort_idx, Out, Lse, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_q_sort_idxb, stride_q_sort_idxh, stride_q_sort_idxm, stride_k_sort_idxb, stride_k_sort_idxh, stride_k_sort_idxn, stride_ob, stride_oh, stride_om, nheads, block_size, sample_size, seqlen_k, seqlen_q, headdim, v_headdim, smooth_block, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BLOCK_HEADDIM: tl.constexpr, V_BLOCK_HEADDIM: tl.constexpr, EVEN_HEADDIM: tl.constexpr, EVEN_V_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "start_m = tl.program_id(0)\noff_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\noffs_vd = tl.arange(0, V_BLOCK_HEADDIM)\nq_idx_ptrs = q_sort_idx + off_b * stride_q_sort_idxb + off_h * stride_q_sort_idxh + offs_m * stride_q_sort_idxm\nq_idx = tl.load(q_idx_ptrs).to(tl.int32)\nk_sort_idx += off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh\nlse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nacc_o = tl.zeros([BLOCK_M, V_BLOCK_HEADDIM], dtype=tl.float32)\nq_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (q_idx[:, None] * stride_qm + offs_d[None, :])\nif EVEN_HEADDIM:\n    q = tl.load(q_ptrs)\nelse:\n    q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\nblock_id = start_m // block_size\nblock_offs = seqlen_k + start_m % block_size * BLOCK_N - (block_size - 1) * BLOCK_N // 2\nend_n = tl.minimum((block_id + 1) * BLOCK_N * block_size, seqlen_k)\nfor start_n in range(block_id * BLOCK_N * block_size, end_n, BLOCK_N):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    if smooth_block:\n        k_idx_ptrs = (start_n + block_offs + offs_n) * stride_k_sort_idxn % seqlen_k\n    else:\n        k_idx_ptrs = (start_n + offs_n) * stride_k_sort_idxn\n    k_idx = tl.load(k_sort_idx + k_idx_ptrs).to(tl.int32)\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (k_idx[:, None] * stride_kn + offs_d[None, :])\n    if EVEN_HEADDIM:\n        k = tl.load(k_ptrs)\n    else:\n        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk += tl.dot(q, tl.trans(k))\n    m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n    p = tl.exp(qk * softmax_scale - m_ij[:, None])\n    l_ij = tl.sum(p, 1)\n    acc_o_scale = tl.exp(m_i - m_ij)\n    acc_o = acc_o * acc_o_scale[:, None]\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (k_idx[:, None] * stride_vn + offs_vd[None, :])\n    if EVEN_V_HEADDIM:\n        v = tl.load(v_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n    p = p.to(v.dtype)\n    acc_o += tl.dot(p, v)\n    m_i = m_ij\n    l_i_new = tl.exp(lse_i - m_ij) + l_ij\n    lse_i = m_ij + tl.log(l_i_new)\nfor col_block in range(0, sample_size):\n    curr_offs_n = col_block * BLOCK_N * stride_kn + offs_n\n    k_ptrs = K + off_b * stride_kb + off_h * stride_kh + (curr_offs_n[:, None] * stride_kn + offs_d[None, :])\n    if EVEN_HEADDIM:\n        k = tl.load(k_ptrs)\n    else:\n        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk += tl.dot(q, tl.trans(k))\n    m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n    p = tl.exp(qk * softmax_scale - m_ij[:, None])\n    l_ij = tl.sum(p, 1)\n    acc_o_scale = tl.exp(m_i - m_ij)\n    acc_o = acc_o * acc_o_scale[:, None]\n    v_ptrs = V + off_b * stride_vb + off_h * stride_vh + (curr_offs_n[:, None] * stride_vn + offs_vd[None, :])\n    if EVEN_V_HEADDIM:\n        v = tl.load(v_ptrs)\n    else:\n        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n    p = p.to(v.dtype)\n    acc_o += tl.dot(p, v)\n    m_i = m_ij\n    l_i_new = tl.exp(lse_i - m_ij) + l_ij\n    lse_i = m_ij + tl.log(l_i_new)\no_scale = tl.exp(m_i - lse_i)\nacc_o = acc_o * o_scale[:, None]\nlse_ptrs = Lse + off_hb * seqlen_q + q_idx\nout_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (q_idx[:, None] * stride_om + offs_vd[None, :])\ntl.store(lse_ptrs, lse_i)\nif EVEN_V_HEADDIM:\n    tl.store(out_ptrs, acc_o)\nelse:\n    tl.store(out_ptrs, acc_o, mask=offs_vd[None, :] < v_headdim)"
  },
  {
    "name": "_bwd_permuted_block_diagonal_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] == args['V_BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "q_sort_idx",
        "annotation": null
      },
      {
        "name": "k_sort_idx",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxm",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxn",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "block_size",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "v_headdim",
        "annotation": null
      },
      {
        "name": "smooth_block",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_V_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_permuted_block_diagonal_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    q_sort_idx,",
      "    k_sort_idx,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_q_sort_idxb,",
      "    stride_q_sort_idxh,",
      "    stride_q_sort_idxm,",
      "    stride_k_sort_idxb,",
      "    stride_k_sort_idxh,",
      "    stride_k_sort_idxn,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    block_size,",
      "    headdim,",
      "    v_headdim,",
      "    smooth_block,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    V_BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    EVEN_V_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    K += off_b * stride_kb + off_h * stride_kh",
      "    V += off_b * stride_vb + off_h * stride_vh",
      "    Q_idx = q_sort_idx + off_b * stride_q_sort_idxb + off_h * stride_q_sort_idxh",
      "    K_idx = k_sort_idx + off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "    DK += off_b * stride_dkb + off_h * stride_dkh",
      "    DV += off_b * stride_dvb + off_h * stride_dvh",
      "",
      "    D += off_hb * seqlen_q",
      "    LSE += off_hb * seqlen_q",
      "",
      "    start_n = tl.program_id(0)",
      "    _bwd_blocked_kernel_one_col(",
      "        start_n=start_n,",
      "        Q=Q,",
      "        K=K,",
      "        V=V,",
      "        Q_idx=Q_idx,",
      "        K_idx=K_idx,",
      "        DO=DO,",
      "        DQ=DQ,",
      "        DK=DK,",
      "        DV=DV,",
      "        LSE=LSE,",
      "        D=D,",
      "        softmax_scale=softmax_scale,",
      "        stride_qm=stride_qm,",
      "        stride_kn=stride_kn,",
      "        stride_vn=stride_vn,",
      "        stride_dom=stride_dom,",
      "        stride_dqm=stride_dqm,",
      "        stride_dkn=stride_dkn,",
      "        stride_dvn=stride_dvn,",
      "        stride_q_idxm=stride_q_sort_idxm,",
      "        stride_k_idxn=stride_k_sort_idxn,",
      "        seqlen_q=seqlen_q,",
      "        block_size=block_size // BLOCK_N,",
      "        headdim=headdim,",
      "        v_headdim=v_headdim,",
      "        smooth_block=smooth_block,",
      "        BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "        V_BLOCK_HEADDIM=V_BLOCK_HEADDIM,",
      "        EVEN_HEADDIM=EVEN_HEADDIM,",
      "        EVEN_V_HEADDIM=EVEN_V_HEADDIM,",
      "        BLOCK_M=BLOCK_M,",
      "        BLOCK_N=BLOCK_N,",
      "    )"
    ],
    "file": "codes/301.py",
    "header": "def _bwd_permuted_block_diagonal_kernel(Q, K, V, q_sort_idx, k_sort_idx, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_q_sort_idxb, stride_q_sort_idxh, stride_q_sort_idxm, stride_k_sort_idxb, stride_k_sort_idxh, stride_k_sort_idxn, stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm, stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn, nheads, seqlen_q, block_size, headdim, v_headdim, smooth_block, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BLOCK_HEADDIM: tl.constexpr, V_BLOCK_HEADDIM: tl.constexpr, EVEN_HEADDIM: tl.constexpr, EVEN_V_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "off_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\nQ += off_b * stride_qb + off_h * stride_qh\nK += off_b * stride_kb + off_h * stride_kh\nV += off_b * stride_vb + off_h * stride_vh\nQ_idx = q_sort_idx + off_b * stride_q_sort_idxb + off_h * stride_q_sort_idxh\nK_idx = k_sort_idx + off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh\nDO += off_b * stride_dob + off_h * stride_doh\nDQ += off_b * stride_dqb + off_h * stride_dqh\nDK += off_b * stride_dkb + off_h * stride_dkh\nDV += off_b * stride_dvb + off_h * stride_dvh\nD += off_hb * seqlen_q\nLSE += off_hb * seqlen_q\nstart_n = tl.program_id(0)\n_bwd_blocked_kernel_one_col(start_n=start_n, Q=Q, K=K, V=V, Q_idx=Q_idx, K_idx=K_idx, DO=DO, DQ=DQ, DK=DK, DV=DV, LSE=LSE, D=D, softmax_scale=softmax_scale, stride_qm=stride_qm, stride_kn=stride_kn, stride_vn=stride_vn, stride_dom=stride_dom, stride_dqm=stride_dqm, stride_dkn=stride_dkn, stride_dvn=stride_dvn, stride_q_idxm=stride_q_sort_idxm, stride_k_idxn=stride_k_sort_idxn, seqlen_q=seqlen_q, block_size=block_size // BLOCK_N, headdim=headdim, v_headdim=v_headdim, smooth_block=smooth_block, BLOCK_HEADDIM=BLOCK_HEADDIM, V_BLOCK_HEADDIM=V_BLOCK_HEADDIM, EVEN_HEADDIM=EVEN_HEADDIM, EVEN_V_HEADDIM=EVEN_V_HEADDIM, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)"
  },
  {
    "name": "_bwd_sampled_col_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] == args['V_BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "v_headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_V_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_sampled_col_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    headdim,",
      "    v_headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    V_BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    EVEN_V_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "",
      "    D += off_hb * seqlen_q",
      "    LSE += off_hb * seqlen_q",
      "",
      "    start_n = tl.program_id(0)",
      "",
      "    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    offs_m = tl.arange(0, BLOCK_M)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)",
      "",
      "    k_ptrs = (",
      "        K",
      "        + off_b * stride_kb",
      "        + off_h * stride_kh",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "    v_ptrs = (",
      "        V",
      "        + off_b * stride_vb",
      "        + off_h * stride_vh",
      "        + (offs_n[:, None] * stride_vn + offs_vd[None, :])",
      "    )",
      "",
      "    dv = tl.zeros([BLOCK_N, V_BLOCK_HEADDIM], dtype=tl.float32)",
      "    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    if EVEN_HEADDIM:",
      "        k = tl.load(k_ptrs)",
      "    else:",
      "        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    if EVEN_V_HEADDIM:",
      "        v = tl.load(v_ptrs)",
      "    else:",
      "        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "",
      "    for start_m in range(0, seqlen_q, BLOCK_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_M)",
      "        offs_m_curr = start_m + offs_m",
      "        q_ptrs = Q + (offs_m_curr[:, None] * stride_qm + offs_d[None, :])",
      "",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs)",
      "        else:",
      "            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "",
      "        qk = tl.dot(q, tl.trans(k))",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "        lse_i = tl.load(LSE + offs_m_curr)",
      "        p = tl.exp(qk * softmax_scale - lse_i[:, None])",
      "",
      "        do_ptrs = DO + (offs_m_curr[:, None] * stride_dom + offs_vd[None, :])",
      "        if EVEN_V_HEADDIM:",
      "            do = tl.load(do_ptrs)",
      "        else:",
      "            do = tl.load(do_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "        dv += tl.dot(tl.trans(p.to(do.dtype)), do)",
      "",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "        dp = tl.dot(do, tl.trans(v))",
      "",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "",
      "        Di = tl.load(D + offs_m_curr)",
      "",
      "        ds = (p * (dp - Di[:, None]) * softmax_scale).to(q.dtype)",
      "",
      "        dk += tl.dot(tl.trans(ds), q)",
      "",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "",
      "        dq_ptrs = DQ + (offs_m_curr[:, None] * stride_dqm + offs_d[None, :])",
      "        dq = tl.dot(ds, k)",
      "        if EVEN_HEADDIM:",
      "            tl.atomic_add(dq_ptrs, dq)",
      "        else:",
      "            tl.atomic_add(dq_ptrs, dq, mask=offs_d[None, :] < headdim)",
      "",
      "    dv_ptrs = (",
      "        DV",
      "        + off_b * stride_dvb",
      "        + off_h * stride_dvh",
      "        + (offs_n[:, None] * stride_dvn + offs_vd[None, :])",
      "    )",
      "    dk_ptrs = (",
      "        DK",
      "        + off_b * stride_dkb",
      "        + off_h * stride_dkh",
      "        + (offs_n[:, None] * stride_dkn + offs_d[None, :])",
      "    )",
      "    dk += tl.load(dk_ptrs)",
      "    dv += tl.load(dv_ptrs)",
      "    _bwd_store_dx(",
      "        dk_ptrs,",
      "        dk,",
      "        offs_d,",
      "        headdim,",
      "        even_headdim=EVEN_HEADDIM,",
      "    )",
      "    _bwd_store_dx(",
      "        dv_ptrs,",
      "        dv,",
      "        offs_vd,",
      "        v_headdim,",
      "        even_headdim=EVEN_V_HEADDIM,",
      "    )",
      "",
      "    return"
    ],
    "file": "codes/301.py",
    "header": "def _bwd_sampled_col_kernel(Q, K, V, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm, stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn, nheads, seqlen_q, headdim, v_headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BLOCK_HEADDIM: tl.constexpr, V_BLOCK_HEADDIM: tl.constexpr, EVEN_HEADDIM: tl.constexpr, EVEN_V_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "off_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\nQ += off_b * stride_qb + off_h * stride_qh\nDO += off_b * stride_dob + off_h * stride_doh\nDQ += off_b * stride_dqb + off_h * stride_dqh\nD += off_hb * seqlen_q\nLSE += off_hb * seqlen_q\nstart_n = tl.program_id(0)\noffs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)\noffs_m = tl.arange(0, BLOCK_M)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\noffs_vd = tl.arange(0, V_BLOCK_HEADDIM)\nk_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] * stride_kn + offs_d[None, :])\nv_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] * stride_vn + offs_vd[None, :])\ndv = tl.zeros([BLOCK_N, V_BLOCK_HEADDIM], dtype=tl.float32)\ndk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)\nif EVEN_HEADDIM:\n    k = tl.load(k_ptrs)\nelse:\n    k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\nif EVEN_V_HEADDIM:\n    v = tl.load(v_ptrs)\nelse:\n    v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\nfor start_m in range(0, seqlen_q, BLOCK_M):\n    start_m = tl.multiple_of(start_m, BLOCK_M)\n    offs_m_curr = start_m + offs_m\n    q_ptrs = Q + (offs_m_curr[:, None] * stride_qm + offs_d[None, :])\n    if EVEN_HEADDIM:\n        q = tl.load(q_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\n    qk = tl.dot(q, tl.trans(k))\n    if not EVEN_HEADDIM:\n        tl.debug_barrier()\n    lse_i = tl.load(LSE + offs_m_curr)\n    p = tl.exp(qk * softmax_scale - lse_i[:, None])\n    do_ptrs = DO + (offs_m_curr[:, None] * stride_dom + offs_vd[None, :])\n    if EVEN_V_HEADDIM:\n        do = tl.load(do_ptrs)\n    else:\n        do = tl.load(do_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)\n    dv += tl.dot(tl.trans(p.to(do.dtype)), do)\n    if not EVEN_HEADDIM:\n        tl.debug_barrier()\n    dp = tl.dot(do, tl.trans(v))\n    if not EVEN_HEADDIM:\n        tl.debug_barrier()\n    Di = tl.load(D + offs_m_curr)\n    ds = (p * (dp - Di[:, None]) * softmax_scale).to(q.dtype)\n    dk += tl.dot(tl.trans(ds), q)\n    if not EVEN_HEADDIM:\n        tl.debug_barrier()\n    dq_ptrs = DQ + (offs_m_curr[:, None] * stride_dqm + offs_d[None, :])\n    dq = tl.dot(ds, k)\n    if EVEN_HEADDIM:\n        tl.atomic_add(dq_ptrs, dq)\n    else:\n        tl.atomic_add(dq_ptrs, dq, mask=offs_d[None, :] < headdim)\ndv_ptrs = DV + off_b * stride_dvb + off_h * stride_dvh + (offs_n[:, None] * stride_dvn + offs_vd[None, :])\ndk_ptrs = DK + off_b * stride_dkb + off_h * stride_dkh + (offs_n[:, None] * stride_dkn + offs_d[None, :])\ndk += tl.load(dk_ptrs)\ndv += tl.load(dv_ptrs)\n_bwd_store_dx(dk_ptrs, dk, offs_d, headdim, even_headdim=EVEN_HEADDIM)\n_bwd_store_dx(dv_ptrs, dv, offs_vd, v_headdim, even_headdim=EVEN_V_HEADDIM)\nreturn"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 2}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 4}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=2)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "codes/181.py",
    "header": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nnum_idx_N, num_idx_V_group = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_V_group = tl.swizzle2d(idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE)\nV_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nm = tl.max(z_j_to_k, 1)\ns = tl.sum(tl.exp(z_j_to_k - m[:, None]), axis=1)\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nmask = y[:, None] == V_range[None, :]\nloss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\ntl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\nlse = m + tl.log(s)\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\ntl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\ntl.store(lse_row_ptr, lse[:, None])"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (V // V_BLOCK_SIZE), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
    ],
    "file": "codes/181.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(z_nv_ptr, y_ptr, A_t_ptr, x_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_H = tl.program_id(axis=1)\nidx_V = 0\nnum_idx_N, num_idx_H = (tl.num_programs(0) - V // V_BLOCK_SIZE, tl.num_programs(1))\nidx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = 0 + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range, eviction_policy='evict_last')\nlse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), eviction_policy='evict_last')\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\nfor _ in range(V // V_BLOCK_SIZE):\n    mask = y[:, None] == v_range[None, :]\n    A_v = tl.load(A_t_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n    x_grad_acc = tl.dot(z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    v_range += V_BLOCK_SIZE\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\ntl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE",
      "    idx_H = tl.program_id(axis=1)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(N_group // N_BLOCK_SIZE):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    A_grad_T_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "    if idx_N_group > 0:",
      "        tl.store(",
      "            A_grad_T_block_ptr,",
      "            tl.load(A_grad_T_block_ptr)",
      "            + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "        )",
      "    else:",
      "        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
    ],
    "file": "codes/181.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(z_nv_ptr, y_ptr, x_ptr, A_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE\nidx_H = tl.program_id(axis=1)\nnum_idx_V, num_idx_H = (tl.num_programs(0) - N_group // N_BLOCK_SIZE, tl.num_programs(1))\nidx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\nfor _ in range(N_group // N_BLOCK_SIZE):\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range, eviction_policy='evict_last')\n    lse = tl.load(lse_ptr + N_range, eviction_policy='evict_last')\n    mask = y[:, None] == V_range[None, :]\n    x_chunk = tl.load(x_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n    A_grad_acc = tl.dot(x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n    N_range += N_BLOCK_SIZE\nA_grad_T_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nif idx_N_group > 0:\n    tl.store(A_grad_T_block_ptr, tl.load(A_grad_T_block_ptr) + (A_grad_acc / N).to(A_grad_ptr.type.element_ty))\nelse:\n    tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_dispatcher",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3)], key=['V', 'N', 'H'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "logits",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "At",
        "annotation": null
      },
      {
        "name": "x_grad",
        "annotation": null
      },
      {
        "name": "At_grad",
        "annotation": null
      },
      {
        "name": "lse_global",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_dispatcher(",
      "    logits,",
      "    y,",
      "    x,",
      "    At,",
      "    x_grad,",
      "    At_grad,",
      "    lse_global,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_NV = tl.program_id(axis=0)",
      "    if idx_NV < N_group // N_BLOCK_SIZE:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "            logits,",
      "            y,",
      "            At,",
      "            x_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "        )",
      "    else:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "            logits,",
      "            y,",
      "            x,",
      "            At_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "        )"
    ],
    "file": "codes/181.py",
    "header": "def linear_xent_bwd_dispatcher(logits, y, x, At, x_grad, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_NV = tl.program_id(axis=0)\nif idx_NV < N_group // N_BLOCK_SIZE:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dx(logits, y, At, x_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)\nelse:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dA(logits, y, x, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "codes/172.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr):",
    "body": "idx = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\noffsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + offsets)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    local_x_block_ptr = x_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == v_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    m = m_new\n    A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n    v_range = v_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\nloss += tl.sum(lse) / N\ntl.store(losses_ptr + idx, loss)\ntl.store(lse_ptr + offsets, lse)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "        x_block_ptr = tl.make_block_ptr(",
      "            base=x_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(local_A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            A_grad_block_ptr = tl.make_block_ptr(",
      "                base=A_grad_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "",
      "            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)",
      "            A_v = tl.load(local_A_block_ptr).to(tl.float32)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N + tl.load(A_grad_block_ptr)",
      "            tl.store(A_grad_block_ptr, temp_AgradT, boundary_check=(0, 1))",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "codes/172.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dA(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_V = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nN_offsets = tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nfor idx_N in range(N // N_BLOCK_SIZE):\n    x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n    local_x_block_ptr = x_block_ptr\n    local_A_block_ptr = A_block_ptr\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(local_A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    local_x_block_ptr = x_block_ptr\n    local_A_block_ptr = A_block_ptr\n    for idx_H in range(H // H_BLOCK_SIZE):\n        A_grad_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n        x_chunk = tl.load(local_x_block_ptr).to(tl.float32)\n        A_v = tl.load(local_A_block_ptr).to(tl.float32)\n        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n        temp_AgradT = temp_Agrad.trans() / N + tl.load(A_grad_block_ptr)\n        tl.store(A_grad_block_ptr, temp_AgradT, boundary_check=(0, 1))\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n    N_offsets += N_BLOCK_SIZE"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(0, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(local_A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "        local_x_grad_block_ptr = x_grad_block_ptr",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "",
      "            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)",
      "            A_v = tl.load(local_A_block_ptr).to(tl.float32)",
      "",
      "            temp_xgrad = tl.dot(softmax_z, A_v.trans()) / N",
      "            temp_xgrad -= (",
      "                tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1) / N",
      "            )",
      "",
      "            temp_xgrad += tl.load(local_x_grad_block_ptr)",
      "            tl.store(local_x_grad_block_ptr, temp_xgrad, boundary_check=(0, 1))",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_x_grad_block_ptr = tl.advance(",
      "                local_x_grad_block_ptr, [0, H_BLOCK_SIZE]",
      "            )",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        V_offsets += V_BLOCK_SIZE"
    ],
    "file": "codes/172.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_N = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = tl.arange(0, V_BLOCK_SIZE)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\ny = tl.load(y_ptr + N_offsets)\nlse = tl.load(lse_global_ptr + N_offsets)\nfor idx_V in range(V // V_BLOCK_SIZE):\n    A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n    local_x_block_ptr = x_block_ptr\n    local_A_block_ptr = A_block_ptr\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(local_A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    local_x_block_ptr = x_block_ptr\n    local_A_block_ptr = A_block_ptr\n    local_x_grad_block_ptr = x_grad_block_ptr\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr).to(tl.float32)\n        A_v = tl.load(local_A_block_ptr).to(tl.float32)\n        temp_xgrad = tl.dot(softmax_z, A_v.trans()) / N\n        temp_xgrad -= tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1) / N\n        temp_xgrad += tl.load(local_x_grad_block_ptr)\n        tl.store(local_x_grad_block_ptr, temp_xgrad, boundary_check=(0, 1))\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        local_x_grad_block_ptr = tl.advance(local_x_grad_block_ptr, [0, H_BLOCK_SIZE])\n        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])\n    V_offsets += V_BLOCK_SIZE"
  },
  {
    "name": "_swiglu_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_fwd_kernel(",
      "    X,",
      "    Y,",
      "    OUT,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_out_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    OUT += row * stride_out_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    out = x * tl.sigmoid(x) * y",
      "    tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "codes/123.py",
    "header": "def _swiglu_fwd_kernel(X, Y, OUT, stride_x_row, stride_y_row, stride_out_row, ncols, BLOCK_N: tl.constexpr):",
    "body": "row = tl.program_id(0)\nstart_col = tl.program_id(1) * BLOCK_N\nX += row * stride_x_row\nY += row * stride_y_row\nOUT += row * stride_out_row\ncols = start_col + tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\ny = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\nout = x * tl.sigmoid(x) * y\ntl.store(OUT + cols, out, mask=cols < ncols)"
  },
  {
    "name": "_swiglu_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DOUT",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dout_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_bwd_kernel(",
      "    X,",
      "    Y,",
      "    DOUT,",
      "    OUT,",
      "    DX,",
      "    DY,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dout_row,",
      "    stride_out_row,",
      "    stride_dx_row,",
      "    stride_dy_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    DOUT += row * stride_dout_row",
      "    if RECOMPUTE_OUTPUT:",
      "        OUT += row * stride_out_row",
      "    DX += row * stride_dx_row",
      "    DY += row * stride_dy_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    x_sigmoid = tl.sigmoid(x)",
      "    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout",
      "    dy = x * x_sigmoid * dout",
      "    tl.store(DX + cols, dx, mask=cols < ncols)",
      "    tl.store(DY + cols, dy, mask=cols < ncols)",
      "    if RECOMPUTE_OUTPUT:",
      "        out = x * x_sigmoid * y",
      "        tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "codes/123.py",
    "header": "def _swiglu_bwd_kernel(X, Y, DOUT, OUT, DX, DY, stride_x_row, stride_y_row, stride_dout_row, stride_out_row, stride_dx_row, stride_dy_row, ncols, BLOCK_N: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "row = tl.program_id(0)\nstart_col = tl.program_id(1) * BLOCK_N\nX += row * stride_x_row\nY += row * stride_y_row\nDOUT += row * stride_dout_row\nif RECOMPUTE_OUTPUT:\n    OUT += row * stride_out_row\nDX += row * stride_dx_row\nDY += row * stride_dy_row\ncols = start_col + tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)\ny = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)\ndout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)\nx_sigmoid = tl.sigmoid(x)\ndx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout\ndy = x * x_sigmoid * dout\ntl.store(DX + cols, dx, mask=cols < ncols)\ntl.store(DY + cols, dy, mask=cols < ncols)\nif RECOMPUTE_OUTPUT:\n    out = x * x_sigmoid * y\n    tl.store(OUT + cols, out, mask=cols < ncols)"
  },
  {
    "name": "naive_attn_decoding_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([] if check_shared_mem('hopper') else [8]) for num_stages in [2, 3, 4, 5]], key=['H', 'G', 'K', 'V', 'BK', 'BV', 'USE_G'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "gate_scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def naive_attn_decoding_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    g_cumsum,",
      "    scale,",
      "    gate_scale,",
      "    cu_seqlens,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_v, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    bos, eos = tl.load(cu_seqlens + i_b).to(tl.int32), tl.load(cu_seqlens + i_b + 1).to(",
      "        tl.int32",
      "    )",
      "    T = eos - bos",
      "",
      "    p_q = tl.make_block_ptr(q + i_bh * K, (K,), (1,), (0,), (BK,), (0,))",
      "    p_o = tl.make_block_ptr(o + i_bh * V, (V,), (1,), (0,), (BV,), (0,))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0,))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_o = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    b_m = tl.full(",
      "        [",
      "            1,",
      "        ],",
      "        float(\"-inf\"),",
      "        dtype=tl.float32,",
      "    )",
      "    b_acc = tl.zeros(",
      "        [",
      "            1,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (T - 1,), (1,), (0,)",
      "        )",
      "        b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)",
      "    else:",
      "        b_gq = None",
      "",
      "    for i_s in range(0, T, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.sum(b_q[None, :] * b_k, 1)",
      "",
      "        mask = i_s + tl.arange(0, BS) < T",
      "        b_s = tl.where(mask, b_s, float(\"-inf\"))",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += (b_gq - b_gk) * gate_scale",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = exp(b_s - b_m)",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 0)",
      "",
      "        b_o = b_o * b_r + tl.sum(b_p[:, None] * b_v, 0)",
      "        b_mp = b_m",
      "    b_o = b_o / b_acc",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/364.py",
    "header": "def naive_attn_decoding_kernel(q, k, v, o, g_cumsum, scale, gate_scale, cu_seqlens, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr):",
    "body": "i_v, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nbos, eos = (tl.load(cu_seqlens + i_b).to(tl.int32), tl.load(cu_seqlens + i_b + 1).to(tl.int32))\nT = eos - bos\np_q = tl.make_block_ptr(q + i_bh * K, (K,), (1,), (0,), (BK,), (0,))\np_o = tl.make_block_ptr(o + i_bh * V, (V,), (1,), (0,), (BV,), (0,))\nb_q = tl.load(p_q, boundary_check=(0,))\nb_q = (b_q * scale).to(b_q.dtype)\nb_o = tl.zeros([BV], dtype=tl.float32)\nb_m = tl.full([1], float('-inf'), dtype=tl.float32)\nb_acc = tl.zeros([1], dtype=tl.float32)\nif USE_G:\n    p_g = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (T - 1,), (1,), (0,))\n    b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)\nelse:\n    b_gq = None\nfor i_s in range(0, T, BS):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s, 0), (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.sum(b_q[None, :] * b_k, 1)\n    mask = i_s + tl.arange(0, BS) < T\n    b_s = tl.where(mask, b_s, float('-inf'))\n    if USE_G:\n        p_gk = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_s += (b_gq - b_gk) * gate_scale\n    b_m, b_mp = (tl.maximum(b_m, tl.max(b_s)), b_m)\n    b_r = exp(b_mp - b_m)\n    b_p = exp(b_s - b_m)\n    b_acc = b_acc * b_r + tl.sum(b_p, 0)\n    b_o = b_o * b_r + tl.sum(b_p[:, None] * b_v, 0)\n    b_mp = b_m\nb_o = b_o / b_acc\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_hgrn_fwd_kernel_h",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4), triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64}, num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton.Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps=2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128}, num_warps=8)], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_fwd_kernel_h(",
      "    x,",
      "    g,",
      "    gc,",
      "    o,",
      "    h0,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_x = x + i_b * T * D + i_t * BT * D + o_d",
      "    p_g = g + i_b * T * D + i_t * BT * D + o_d",
      "    p_gc = gc + i_b * T * D + i_t * BT * D + o_d",
      "    p_o = o + i_b * T * D + i_t * BT * D + o_d",
      "",
      "    b_h = tl.zeros([BD], dtype=tl.float32)",
      "    b_gc = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        if i_t == 0:",
      "            b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0).to(tl.float32)",
      "    for i in range(0, BT):",
      "        mask_t = mask & ((i_t * BT + i) < T)",
      "        b_x = tl.load(p_x, mask=mask_t, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g, mask=mask_t, other=0).to(tl.float32)",
      "        b_h = exp(b_g) * b_h + b_x",
      "        b_gc = b_gc + b_g",
      "        tl.store(p_gc, b_gc.to(p_o.dtype.element_ty), mask=mask_t)",
      "        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask_t)",
      "",
      "        p_x += D",
      "        p_g += D",
      "        p_gc += D",
      "        p_o += D"
    ],
    "file": "codes/397.py",
    "header": "def chunk_hgrn_fwd_kernel_h(x, g, gc, o, h0, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, USE_INITIAL_STATE: tl.constexpr):",
    "body": "i_d, i_t, i_b = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\np_x = x + i_b * T * D + i_t * BT * D + o_d\np_g = g + i_b * T * D + i_t * BT * D + o_d\np_gc = gc + i_b * T * D + i_t * BT * D + o_d\np_o = o + i_b * T * D + i_t * BT * D + o_d\nb_h = tl.zeros([BD], dtype=tl.float32)\nb_gc = tl.zeros([BD], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    if i_t == 0:\n        b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0).to(tl.float32)\nfor i in range(0, BT):\n    mask_t = mask & (i_t * BT + i < T)\n    b_x = tl.load(p_x, mask=mask_t, other=0).to(tl.float32)\n    b_g = tl.load(p_g, mask=mask_t, other=0).to(tl.float32)\n    b_h = exp(b_g) * b_h + b_x\n    b_gc = b_gc + b_g\n    tl.store(p_gc, b_gc.to(p_o.dtype.element_ty), mask=mask_t)\n    tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask_t)\n    p_x += D\n    p_g += D\n    p_gc += D\n    p_o += D"
  },
  {
    "name": "chunk_hgrn_fwd_kernel_o",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "s_b",
        "annotation": null
      },
      {
        "name": "s_t",
        "annotation": null
      },
      {
        "name": "s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_fwd_kernel_o(",
      "    gc, o, s_b, s_t, s_d, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr",
      "):",
      "    i_d, i_b = tl.program_id(0), tl.program_id(1)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    for i_t in range(1, tl.cdiv(T, BT)):",
      "        p_gc = tl.make_block_ptr(",
      "            gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask, other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)",
      "        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)",
      "        b_o = b_o + exp(b_gc) * b_h0[None, :]",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/397.py",
    "header": "def chunk_hgrn_fwd_kernel_o(gc, o, s_b, s_t, s_d, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr):",
    "body": "i_d, i_b = (tl.program_id(0), tl.program_id(1))\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\nfor i_t in range(1, tl.cdiv(T, BT)):\n    p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask, other=0).to(tl.float32)\n    b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n    b_o = b_o + exp(b_gc) * b_h0[None, :]\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_hgrn_bwd_kernel_h",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_bwd_kernel_h(",
      "    g, gc, dx, do, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "    BC = min(BT, T - i_t * BT)",
      "    NT = tl.num_programs(1)",
      "",
      "    p_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "    p_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "    p_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "    p_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "",
      "    if i_t == NT - 1:",
      "        b_gc = tl.zeros([BD], dtype=tl.float32)",
      "    else:",
      "        b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask, other=0).to(",
      "            tl.float32",
      "        )",
      "    b_dh = tl.zeros([BD], dtype=tl.float32)",
      "    for _ in range(BC - 1, -1, -1):",
      "        tl.store(p_gc, b_gc.to(p_gc.dtype.element_ty), mask=mask)",
      "",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)",
      "",
      "        b_gc = b_gc + b_g",
      "        b_dh = b_dh + b_do",
      "        b_dx = b_dh",
      "        b_dh = b_dh * exp(b_g)",
      "",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)",
      "",
      "        p_g -= D",
      "        p_gc -= D",
      "        p_dx -= D",
      "        p_do -= D"
    ],
    "file": "codes/397.py",
    "header": "def chunk_hgrn_bwd_kernel_h(g, gc, dx, do, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr):",
    "body": "i_d, i_t, i_b = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\nBC = min(BT, T - i_t * BT)\nNT = tl.num_programs(1)\np_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d\np_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d\np_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d\np_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d\nif i_t == NT - 1:\n    b_gc = tl.zeros([BD], dtype=tl.float32)\nelse:\n    b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask, other=0).to(tl.float32)\nb_dh = tl.zeros([BD], dtype=tl.float32)\nfor _ in range(BC - 1, -1, -1):\n    tl.store(p_gc, b_gc.to(p_gc.dtype.element_ty), mask=mask)\n    b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)\n    b_gc = b_gc + b_g\n    b_dh = b_dh + b_do\n    b_dx = b_dh\n    b_dh = b_dh * exp(b_g)\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)\n    p_g -= D\n    p_gc -= D\n    p_dx -= D\n    p_do -= D"
  },
  {
    "name": "chunk_hgrn_bwd_kernel_o",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "s_b",
        "annotation": null
      },
      {
        "name": "s_t",
        "annotation": null
      },
      {
        "name": "s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_bwd_kernel_o(",
      "    g,",
      "    gc,",
      "    o,",
      "    dx,",
      "    dg,",
      "    s_b,",
      "    s_t,",
      "    s_d,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_d, i_b = tl.program_id(0), tl.program_id(1)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):",
      "        p_g = tl.make_block_ptr(",
      "            g + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_gc = tl.make_block_ptr(",
      "            gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + i_b * s_b,",
      "            (T, D),",
      "            (s_t, s_d),",
      "            (i_t * BT - 1, i_d * BD),",
      "            (BT, BD),",
      "            (1, 0),",
      "        )",
      "        p_dx = tl.make_block_ptr(",
      "            dx + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_dg = tl.make_block_ptr(",
      "            dg + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        mask_t = mask & ((i_t + 1) * BT < T)",
      "        b_ht = tl.load(",
      "            dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=mask_t, other=0",
      "        ).to(tl.float32)",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)",
      "        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)",
      "        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dx = tl.load(p_dx, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        b_dx = b_dx + exp(b_gc) * b_ht[None, :]",
      "        b_dg = b_o * b_dx * exp(b_g)",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/397.py",
    "header": "def chunk_hgrn_bwd_kernel_o(g, gc, o, dx, dg, s_b, s_t, s_d, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr):",
    "body": "i_d, i_b = (tl.program_id(0), tl.program_id(1))\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\nfor i_t in range(tl.cdiv(T, BT) - 1, -1, -1):\n    p_g = tl.make_block_ptr(g + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    p_gc = tl.make_block_ptr(gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    p_o = tl.make_block_ptr(o + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT - 1, i_d * BD), (BT, BD), (1, 0))\n    p_dx = tl.make_block_ptr(dx + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    p_dg = tl.make_block_ptr(dg + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    mask_t = mask & ((i_t + 1) * BT < T)\n    b_ht = tl.load(dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=mask_t, other=0).to(tl.float32)\n    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n    b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)\n    b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\n    b_dx = tl.load(p_dx, boundary_check=(0, 1)).to(tl.float32)\n    b_dx = b_dx + exp(b_gc) * b_ht[None, :]\n    b_dg = b_o * b_dx * exp(b_g)\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_matmul_partition_k",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_mm_configs(), key=['M', 'N', 'K', 'PK'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_buf_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "PK",
        "annotation": null
      },
      {
        "name": "PK_SIZE",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cb_m",
        "annotation": null
      },
      {
        "name": "stride_cb_n",
        "annotation": null
      },
      {
        "name": "stride_cb_k",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _matmul_partition_k(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_buf_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    PK,",
      "    PK_SIZE,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cb_m,",
      "    stride_cb_n,",
      "    stride_cb_k,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_pk = PK",
      "    num_pid_nk = num_pid_n * num_pid_pk",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_nk",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_nk = (pid % num_pid_in_group) // group_size_m",
      "    pid_n = pid_nk // num_pid_pk",
      "    pid_pk = pid_nk % num_pid_pk",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = (pid_pk * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(PK_SIZE, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "        accumulator += tl.dot(a, b)",
      "        a_ptrs += PK_SIZE * stride_ak",
      "        b_ptrs += PK_SIZE * stride_bk",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_ck = pid_pk",
      "    c_buf_ptrs = (",
      "        c_buf_ptr",
      "        + stride_cb_m * offs_cm[:, None, None]",
      "        + stride_cb_n * offs_cn[None, :, None]",
      "        + stride_cb_k * offs_ck[None, None, :]",
      "    )",
      "    tl.store(c_buf_ptrs, accumulator[:, :, None])"
    ],
    "file": "codes/648.py",
    "header": "def _matmul_partition_k(a_ptr, b_ptr, c_buf_ptr, M, N, K, PK, PK_SIZE, stride_am, stride_ak, stride_bk, stride_bn, stride_cb_m, stride_cb_n, stride_cb_k, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_pk = PK\nnum_pid_nk = num_pid_n * num_pid_pk\nnum_pid_in_group = GROUP_SIZE_M * num_pid_nk\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_nk = pid % num_pid_in_group // group_size_m\npid_n = pid_nk // num_pid_pk\npid_pk = pid_nk % num_pid_pk\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = (pid_pk * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(PK_SIZE, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs)\n    b = tl.load(b_ptrs)\n    accumulator += tl.dot(a, b)\n    a_ptrs += PK_SIZE * stride_ak\n    b_ptrs += PK_SIZE * stride_bk\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_ck = pid_pk\nc_buf_ptrs = c_buf_ptr + stride_cb_m * offs_cm[:, None, None] + stride_cb_n * offs_cn[None, :, None] + stride_cb_k * offs_ck[None, None, :]\ntl.store(c_buf_ptrs, accumulator[:, :, None])"
  },
  {
    "name": "chunk_dplr_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BK_LIST for BV in BK_LIST for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "A_qk",
        "annotation": null
      },
      {
        "name": "A_qb",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_fwd_kernel_o(",
      "    qg,",
      "    v,",
      "    v_new,",
      "    A_qk,",
      "    A_qb,",
      "    h,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_qg = tl.make_block_ptr(",
      "            qg + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_qg = tl.load(p_qg, boundary_check=(0, 1))",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_qg, b_h)",
      "",
      "    p_Aqk = tl.make_block_ptr(",
      "        A_qk + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_Aqb = tl.make_block_ptr(",
      "        A_qb + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_v_new = tl.make_block_ptr(",
      "        v_new + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "    b_Aqk = tl.load(p_Aqk, boundary_check=(0, 1))",
      "    b_Aqb = tl.load(p_Aqb, boundary_check=(0, 1))",
      "    b_Aqk = tl.where(m_s, b_Aqk, 0)",
      "    b_Aqb = tl.where(m_s, b_Aqb, 0)",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_v_new = tl.load(p_v_new, boundary_check=(0, 1))",
      "    b_o = (",
      "        b_o",
      "        + tl.dot(b_Aqk.to(b_v.dtype), b_v)",
      "        + tl.dot(b_Aqb.to(b_v_new.dtype), b_v_new)",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/386.py",
    "header": "def chunk_dplr_fwd_kernel_o(qg, v, v_new, A_qk, A_qb, h, o, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_qg = tl.make_block_ptr(qg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_qg = tl.load(p_qg, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_o += tl.dot(b_qg, b_h)\np_Aqk = tl.make_block_ptr(A_qk + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_Aqb = tl.make_block_ptr(A_qb + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nm_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\nb_Aqk = tl.load(p_Aqk, boundary_check=(0, 1))\nb_Aqb = tl.load(p_Aqb, boundary_check=(0, 1))\nb_Aqk = tl.where(m_s, b_Aqk, 0)\nb_Aqb = tl.where(m_s, b_Aqb, 0)\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_v_new = tl.load(p_v_new, boundary_check=(0, 1))\nb_o = b_o + tl.dot(b_Aqk.to(b_v.dtype), b_v) + tl.dot(b_Aqb.to(b_v_new.dtype), b_v_new)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_recurrent_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_BETA_HEADWISE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    u,",
      "    beta,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_BETA_HEADWISE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_u = u + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    else:",
      "        p_beta = beta + bos * H + i_h",
      "    p_o = o + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    mask_k = (i_k * BK + tl.arange(0, BK)) < K",
      "    mask_v = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_v_minus = tl.sum(b_h * b_k[None, :], axis=1)",
      "        b_v -= b_v_minus",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        tl.store(p_u, b_v.to(p_v.dtype.element_ty), mask=mask_v)",
      "        b_v *= b_beta",
      "        b_h += b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "",
      "        p_q += H * K",
      "        p_k += H * K",
      "        p_o += H * V",
      "        p_v += H * V",
      "        p_u += H * V",
      "        p_beta += H * (V if IS_BETA_HEADWISE else 1)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/375.py",
    "header": "def fused_recurrent_delta_rule_fwd_kernel(q, k, v, u, beta, o, h0, ht, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_BETA_HEADWISE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\np_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\np_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\np_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\np_u = u + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nif IS_BETA_HEADWISE:\n    p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nelse:\n    p_beta = beta + bos * H + i_h\np_o = o + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nmask_k = i_k * BK + tl.arange(0, BK) < K\nmask_v = i_v * BV + tl.arange(0, BV) < V\nmask_h = mask_k[None, :] & mask_v[:, None]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_v_minus = tl.sum(b_h * b_k[None, :], axis=1)\n    b_v -= b_v_minus\n    if IS_BETA_HEADWISE:\n        b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n    else:\n        b_beta = tl.load(p_beta).to(tl.float32)\n    tl.store(p_u, b_v.to(p_v.dtype.element_ty), mask=mask_v)\n    b_v *= b_beta\n    b_h += b_k[None, :] * b_v[:, None]\n    b_o = b_h * b_q[None, :]\n    b_o = tl.sum(b_o, axis=1)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    p_q += H * K\n    p_k += H * K\n    p_o += H * V\n    p_v += H * V\n    p_u += H * V\n    p_beta += H * (V if IS_BETA_HEADWISE else 1)\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[None, :]) * V + (i_v * BV + tl.arange(0, BV)[:, None])\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "fused_recurrent_delta_rule_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_BETA_HEADWISE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_delta_rule_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    beta,",
      "    h0,",
      "    dh0,",
      "    dht,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    db,",
      "    cu_seqlens,",
      "    scale,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NK: tl.constexpr,",
      "    IS_BETA_HEADWISE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    mask_k = i_k * BK + tl.arange(0, BK) < K",
      "    mask_v = i_v * BV + tl.arange(0, BV) < V",
      "",
      "    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_dk = (",
      "        dk",
      "        + ((i_v * all + bos) * H + i_h) * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "        + (T - 1) * H * K",
      "    )",
      "    p_dv = (",
      "        dv",
      "        + ((i_k * all + bos) * H + i_h) * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "        + (T - 1) * H * V",
      "    )",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos + T - 1) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "        p_dbeta = (",
      "            db",
      "            + ((i_v * NK + i_k) * all + bos + T - 1) * H * V",
      "            + i_h * V",
      "            + tl.arange(0, BV)",
      "        )",
      "    else:",
      "        p_beta = beta + (bos + T - 1) * H + i_h",
      "        p_dbeta = db + (i_v * all + bos + T - 1) * H + i_h",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_ht = (",
      "            dht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "    for _ in range(T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_dh += b_q[:, None] * b_do[None, :]",
      "        b_dk = tl.sum(b_dh * (b_v * b_beta)[None, :], axis=1)",
      "        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)",
      "",
      "        b_db = b_dv * b_v if IS_BETA_HEADWISE else tl.sum(b_dv * b_v)",
      "        b_dv = b_dv * b_beta",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)",
      "        if IS_BETA_HEADWISE:",
      "            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty), mask=mask_v)",
      "        else:",
      "            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty))",
      "",
      "        b_dh -= b_k[:, None] * b_dv[None, :]",
      "",
      "        p_q -= H * K",
      "        p_k -= H * K",
      "        p_v -= H * V",
      "        p_do -= H * V",
      "        p_dk -= H * K",
      "        p_dv -= H * V",
      "        p_dbeta -= H * (V if IS_BETA_HEADWISE else 1)",
      "        p_beta -= H * (V if IS_BETA_HEADWISE else 1)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = (",
      "            dh0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        tl.store(",
      "            p_dh0,",
      "            b_dh.to(p_dh0.dtype.element_ty),",
      "            mask=mask_k[:, None] & mask_v[None, :],",
      "        )",
      "",
      "    tl.debug_barrier()",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    else:",
      "        p_beta = beta + bos * H + i_h",
      "    p_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_dq = dq + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_dk = dk + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_dv = dv + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    if USE_INITIAL_STATE:",
      "        mask_h = mask_k[:, None] & mask_v[None, :]",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_dk = tl.load(p_dk, mask=mask_k, other=0).to(tl.float32)",
      "        b_dv = tl.load(p_dv, mask=mask_v, other=0).to(tl.float32)",
      "        b_dk -= tl.sum(b_dv[None, :] * b_h, axis=1)",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_v *= b_beta",
      "",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        b_dq = b_h * b_do[None, :]",
      "        d_q = tl.sum(b_dq, axis=1) * scale",
      "        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += H * K",
      "        p_v += H * V",
      "        p_do += H * V",
      "        p_dq += H * K",
      "        p_dk += H * K",
      "        p_dv += H * V",
      "        p_beta += H * (V if IS_BETA_HEADWISE else 1)"
    ],
    "file": "codes/375.py",
    "header": "def fused_recurrent_delta_rule_bwd_kernel(q, k, v, beta, h0, dh0, dht, do, dq, dk, dv, db, cu_seqlens, scale, B: tl.constexpr, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NK: tl.constexpr, IS_BETA_HEADWISE: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\nmask_k = i_k * BK + tl.arange(0, BK) < K\nmask_v = i_v * BV + tl.arange(0, BV) < V\np_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K\np_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K\np_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V\np_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V\np_dk = dk + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K\np_dv = dv + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V\nif IS_BETA_HEADWISE:\n    p_beta = beta + (bos + T - 1) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n    p_dbeta = db + ((i_v * NK + i_k) * all + bos + T - 1) * H * V + i_h * V + tl.arange(0, BV)\nelse:\n    p_beta = beta + (bos + T - 1) * H + i_h\n    p_dbeta = db + (i_v * all + bos + T - 1) * H + i_h\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_ht = dht + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(tl.float32)\nfor _ in range(T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    if IS_BETA_HEADWISE:\n        b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n    else:\n        b_beta = tl.load(p_beta).to(tl.float32)\n    b_dh += b_q[:, None] * b_do[None, :]\n    b_dk = tl.sum(b_dh * (b_v * b_beta)[None, :], axis=1)\n    b_dv = tl.sum(b_dh * b_k[:, None], axis=0)\n    b_db = b_dv * b_v if IS_BETA_HEADWISE else tl.sum(b_dv * b_v)\n    b_dv = b_dv * b_beta\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n    if IS_BETA_HEADWISE:\n        tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty), mask=mask_v)\n    else:\n        tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty))\n    b_dh -= b_k[:, None] * b_dv[None, :]\n    p_q -= H * K\n    p_k -= H * K\n    p_v -= H * V\n    p_do -= H * V\n    p_dk -= H * K\n    p_dv -= H * V\n    p_dbeta -= H * (V if IS_BETA_HEADWISE else 1)\n    p_beta -= H * (V if IS_BETA_HEADWISE else 1)\nif USE_INITIAL_STATE:\n    p_dh0 = dh0 + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_k[:, None] & mask_v[None, :])\ntl.debug_barrier()\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\np_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\np_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)\np_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nif IS_BETA_HEADWISE:\n    p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nelse:\n    p_beta = beta + bos * H + i_h\np_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\np_dq = dq + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)\np_dk = dk + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)\np_dv = dv + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nif USE_INITIAL_STATE:\n    mask_h = mask_k[:, None] & mask_v[None, :]\n    p_h0 = h0 + i_nh * K * V + (i_k * BK + tl.arange(0, BK)[:, None]) * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_dk = tl.load(p_dk, mask=mask_k, other=0).to(tl.float32)\n    b_dv = tl.load(p_dv, mask=mask_v, other=0).to(tl.float32)\n    b_dk -= tl.sum(b_dv[None, :] * b_h, axis=1)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    if IS_BETA_HEADWISE:\n        b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n    else:\n        b_beta = tl.load(p_beta).to(tl.float32)\n    b_v *= b_beta\n    b_h += b_k[:, None] * b_v[None, :]\n    b_dq = b_h * b_do[None, :]\n    d_q = tl.sum(b_dq, axis=1) * scale\n    tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)\n    p_k += H * K\n    p_v += H * V\n    p_do += H * V\n    p_dq += H * K\n    p_dk += H * K\n    p_dv += H * V\n    p_beta += H * (V if IS_BETA_HEADWISE else 1)"
  },
  {
    "name": "_state_passing_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "final_states_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "initstates_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_final_states_batch",
        "annotation": null
      },
      {
        "name": "stride_final_states_head",
        "annotation": null
      },
      {
        "name": "stride_final_states_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_initstates_batch",
        "annotation": null
      },
      {
        "name": "stride_initstates_head",
        "annotation": null
      },
      {
        "name": "stride_initstates_dim",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "HAS_INITSTATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_fwd_kernel(",
      "    states_ptr,",
      "    out_ptr,",
      "    final_states_ptr,",
      "    dA_cs_ptr,",
      "    initstates_ptr,",
      "    seq_idx_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_final_states_batch,",
      "    stride_final_states_head,",
      "    stride_final_states_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_initstates_batch,",
      "    stride_initstates_head,",
      "    stride_initstates_dim,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    HAS_INITSTATES: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    final_states_ptr += (",
      "        pid_b * stride_final_states_batch + pid_h * stride_final_states_head",
      "    )",
      "    if HAS_INITSTATES:",
      "        initstates_ptr += (",
      "            pid_b * stride_initstates_batch + pid_h * stride_initstates_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    states_ptrs = states_ptr + offs_m * stride_states_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim",
      "",
      "    if not HAS_INITSTATES:",
      "        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "    else:",
      "        initstates_ptrs = initstates_ptr + offs_m * stride_initstates_dim",
      "        states = tl.load(initstates_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    tl.store(out_ptrs, states, mask=offs_m < dim)",
      "    out_ptrs += stride_out_chunk",
      "    seq_idx = 0",
      "    for c in range(nchunks):",
      "        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_new = tl.load(",
      "                seq_idx_ptr",
      "                + (min((c + 1) * chunk_size, seqlen) - 1) * stride_seq_idx_seqlen",
      "            )",
      "            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)",
      "            seq_idx = seq_idx_new",
      "        states = scale * states + new_states",
      "        if c < nchunks - 1:",
      "            tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        else:",
      "            tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "        states_ptrs += stride_states_chunk",
      "        dA_cs_ptr += stride_dA_cs_chunk",
      "        out_ptrs += stride_out_chunk"
    ],
    "file": "codes/132.py",
    "header": "def _state_passing_fwd_kernel(states_ptr, out_ptr, final_states_ptr, dA_cs_ptr, initstates_ptr, seq_idx_ptr, dim, nchunks, seqlen, chunk_size, stride_states_batch, stride_states_chunk, stride_states_head, stride_states_dim, stride_out_batch, stride_out_chunk, stride_out_head, stride_out_dim, stride_final_states_batch, stride_final_states_head, stride_final_states_dim, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_initstates_batch, stride_initstates_head, stride_initstates_dim, stride_seq_idx_batch, stride_seq_idx_seqlen, HAS_INITSTATES: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nstates_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\ndA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\nout_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\nfinal_states_ptr += pid_b * stride_final_states_batch + pid_h * stride_final_states_head\nif HAS_INITSTATES:\n    initstates_ptr += pid_b * stride_initstates_batch + pid_h * stride_initstates_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch\noffs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nstates_ptrs = states_ptr + offs_m * stride_states_dim\nout_ptrs = out_ptr + offs_m * stride_out_dim\nfinal_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim\nif not HAS_INITSTATES:\n    states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\nelse:\n    initstates_ptrs = initstates_ptr + offs_m * stride_initstates_dim\n    states = tl.load(initstates_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\ntl.store(out_ptrs, states, mask=offs_m < dim)\nout_ptrs += stride_out_chunk\nseq_idx = 0\nfor c in range(nchunks):\n    new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n    scale = tl.exp(dA_cs)\n    if HAS_SEQ_IDX:\n        seq_idx_new = tl.load(seq_idx_ptr + (min((c + 1) * chunk_size, seqlen) - 1) * stride_seq_idx_seqlen)\n        scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n        seq_idx = seq_idx_new\n    states = scale * states + new_states\n    if c < nchunks - 1:\n        tl.store(out_ptrs, states, mask=offs_m < dim)\n    else:\n        tl.store(final_states_ptrs, states, mask=offs_m < dim)\n    states_ptrs += stride_states_chunk\n    dA_cs_ptr += stride_dA_cs_chunk\n    out_ptrs += stride_out_chunk"
  },
  {
    "name": "_state_passing_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dfinal_states_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dinitstates_ptr",
        "annotation": null
      },
      {
        "name": "states_converted_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dfinal_states_batch",
        "annotation": null
      },
      {
        "name": "stride_dfinal_states_head",
        "annotation": null
      },
      {
        "name": "stride_dfinal_states_dim",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_dim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dinitstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dinitstates_head",
        "annotation": null
      },
      {
        "name": "stride_dinitstates_dim",
        "annotation": null
      },
      {
        "name": "CONVERT_STATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DFINAL_STATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DINITSTATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_bwd_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dA_cs_ptr,",
      "    dfinal_states_ptr,",
      "    seq_idx_ptr,",
      "    dstates_ptr,",
      "    ddA_cs_ptr,",
      "    dinitstates_ptr,",
      "    states_converted_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dfinal_states_batch,",
      "    stride_dfinal_states_head,",
      "    stride_dfinal_states_dim,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_dstates_head,",
      "    stride_dstates_dim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_dinitstates_batch,",
      "    stride_dinitstates_head,",
      "    stride_dinitstates_dim,",
      "    CONVERT_STATES: tl.constexpr,",
      "    HAS_DFINAL_STATES: tl.constexpr,",
      "    HAS_DINITSTATES: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_h * stride_dstates_head",
      "        + (nchunks - 1) * stride_dstates_chunk",
      "    )",
      "    dA_cs_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_h * stride_dA_cs_head",
      "        + (nchunks - 1) * stride_dA_cs_chunk",
      "    )",
      "    ddA_cs_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_h * stride_ddA_cs_head",
      "        + (nchunks - 1) * stride_ddA_cs_chunk",
      "        + pid_m",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_h * stride_out_head",
      "        + (nchunks - 1) * stride_out_chunk",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_h * stride_dout_head",
      "        + (nchunks - 1) * stride_dout_chunk",
      "    )",
      "    if CONVERT_STATES:",
      "        states_converted_ptr += (",
      "            pid_b * stride_out_batch",
      "            + pid_h * stride_out_head",
      "            + (nchunks - 1) * stride_out_chunk",
      "        )",
      "    if HAS_DFINAL_STATES:",
      "        dfinal_states_ptr += (",
      "            pid_b * stride_dfinal_states_batch + pid_h * stride_dfinal_states_head",
      "        )",
      "    if HAS_DINITSTATES:",
      "        dinitstates_ptr += (",
      "            pid_b * stride_dinitstates_batch + pid_h * stride_dinitstates_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    dout_ptrs = dout_ptr + offs_m * stride_dout_dim",
      "    if CONVERT_STATES:",
      "        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim",
      "",
      "    if HAS_DFINAL_STATES:",
      "        dstates = tl.load(",
      "            dfinal_states_ptr + offs_m * stride_dfinal_states_dim,",
      "            mask=offs_m < dim,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    else:",
      "        dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx = tl.load(seq_idx_ptr + (seqlen - 1) * stride_seq_idx_seqlen)",
      "    dstates_ptrs -= stride_dstates_chunk",
      "    for c in range(nchunks - 1):",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_new = tl.load(",
      "                seq_idx_ptr",
      "                + (((nchunks - c - 1) * chunk_size - 1) * stride_seq_idx_seqlen)",
      "            )",
      "            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)",
      "            seq_idx = seq_idx_new",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if CONVERT_STATES:",
      "            tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "        ddA = tl.sum(out * dstates) * scale",
      "        tl.store(ddA_cs_ptr, ddA)",
      "        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dstates = scale * dstates + dout",
      "        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "        dout_ptrs -= stride_dout_chunk",
      "        dstates_ptrs -= stride_dstates_chunk",
      "        dA_cs_ptr -= stride_dA_cs_chunk",
      "        ddA_cs_ptr -= stride_ddA_cs_chunk",
      "        out_ptrs -= stride_out_chunk",
      "        if CONVERT_STATES:",
      "            states_converted_ptrs -= stride_out_chunk",
      "    if CONVERT_STATES:",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "    if not HAS_DINITSTATES:",
      "        tl.store(ddA_cs_ptr, 0.0)",
      "    else:",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        if HAS_SEQ_IDX:",
      "            scale = tl.where(seq_idx == 0, scale, 0.0)",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        ddA = tl.sum(out * dstates) * scale",
      "        tl.store(ddA_cs_ptr, ddA)",
      "        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dstates = scale * dstates + dout",
      "        tl.store(",
      "            dinitstates_ptr + offs_m * stride_dinitstates_dim,",
      "            dstates,",
      "            mask=offs_m < dim,",
      "        )"
    ],
    "file": "codes/132.py",
    "header": "def _state_passing_bwd_kernel(dout_ptr, out_ptr, dA_cs_ptr, dfinal_states_ptr, seq_idx_ptr, dstates_ptr, ddA_cs_ptr, dinitstates_ptr, states_converted_ptr, dim, nchunks, seqlen, chunk_size, stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_dim, stride_out_batch, stride_out_chunk, stride_out_head, stride_out_dim, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dfinal_states_batch, stride_dfinal_states_head, stride_dfinal_states_dim, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_dstates_batch, stride_dstates_chunk, stride_dstates_head, stride_dstates_dim, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_dinitstates_batch, stride_dinitstates_head, stride_dinitstates_dim, CONVERT_STATES: tl.constexpr, HAS_DFINAL_STATES: tl.constexpr, HAS_DINITSTATES: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\ndstates_ptr += pid_b * stride_dstates_batch + pid_h * stride_dstates_head + (nchunks - 1) * stride_dstates_chunk\ndA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head + (nchunks - 1) * stride_dA_cs_chunk\nddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head + (nchunks - 1) * stride_ddA_cs_chunk + pid_m\nout_ptr += pid_b * stride_out_batch + pid_h * stride_out_head + (nchunks - 1) * stride_out_chunk\ndout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head + (nchunks - 1) * stride_dout_chunk\nif CONVERT_STATES:\n    states_converted_ptr += pid_b * stride_out_batch + pid_h * stride_out_head + (nchunks - 1) * stride_out_chunk\nif HAS_DFINAL_STATES:\n    dfinal_states_ptr += pid_b * stride_dfinal_states_batch + pid_h * stride_dfinal_states_head\nif HAS_DINITSTATES:\n    dinitstates_ptr += pid_b * stride_dinitstates_batch + pid_h * stride_dinitstates_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch\noffs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\ndstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim\nout_ptrs = out_ptr + offs_m * stride_out_dim\ndout_ptrs = dout_ptr + offs_m * stride_dout_dim\nif CONVERT_STATES:\n    states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim\nif HAS_DFINAL_STATES:\n    dstates = tl.load(dfinal_states_ptr + offs_m * stride_dfinal_states_dim, mask=offs_m < dim, other=0.0).to(tl.float32)\nelse:\n    dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\ntl.store(dstates_ptrs, dstates, mask=offs_m < dim)\nif HAS_SEQ_IDX:\n    seq_idx = tl.load(seq_idx_ptr + (seqlen - 1) * stride_seq_idx_seqlen)\ndstates_ptrs -= stride_dstates_chunk\nfor c in range(nchunks - 1):\n    dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n    scale = tl.exp(dA_cs)\n    if HAS_SEQ_IDX:\n        seq_idx_new = tl.load(seq_idx_ptr + ((nchunks - c - 1) * chunk_size - 1) * stride_seq_idx_seqlen)\n        scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)\n        seq_idx = seq_idx_new\n    out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if CONVERT_STATES:\n        tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n    ddA = tl.sum(out * dstates) * scale\n    tl.store(ddA_cs_ptr, ddA)\n    dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    dstates = scale * dstates + dout\n    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n    dout_ptrs -= stride_dout_chunk\n    dstates_ptrs -= stride_dstates_chunk\n    dA_cs_ptr -= stride_dA_cs_chunk\n    ddA_cs_ptr -= stride_ddA_cs_chunk\n    out_ptrs -= stride_out_chunk\n    if CONVERT_STATES:\n        states_converted_ptrs -= stride_out_chunk\nif CONVERT_STATES:\n    out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    tl.store(states_converted_ptrs, out, mask=offs_m < dim)\nif not HAS_DINITSTATES:\n    tl.store(ddA_cs_ptr, 0.0)\nelse:\n    dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n    scale = tl.exp(dA_cs)\n    if HAS_SEQ_IDX:\n        scale = tl.where(seq_idx == 0, scale, 0.0)\n    out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    ddA = tl.sum(out * dstates) * scale\n    tl.store(ddA_cs_ptr, ddA)\n    dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    dstates = scale * dstates + dout\n    tl.store(dinitstates_ptr + offs_m * stride_dinitstates_dim, dstates, mask=offs_m < dim)"
  },
  {
    "name": "_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "Lse",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_ob",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    Out,",
      "    Lse,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_ob,",
      "    stride_oh,",
      "    stride_om,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    q_ptrs = (",
      "        Q",
      "        + off_b * stride_qb",
      "        + off_h * stride_qh",
      "        + (offs_m[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "    k_ptrs = (",
      "        K",
      "        + off_b * stride_kb",
      "        + off_h * stride_kh",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "    v_ptrs = (",
      "        V",
      "        + off_b * stride_vb",
      "        + off_h * stride_vh",
      "        + (offs_n[:, None] * stride_vn + offs_d[None, :])",
      "    )",
      "    if BIAS_TYPE == \"vector\":",
      "        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n",
      "    elif BIAS_TYPE == \"matrix\":",
      "        b_ptrs = (",
      "            Bias",
      "            + off_b * stride_bb",
      "            + off_h * stride_bh",
      "            + (offs_m[:, None] * stride_bm + offs_n[None, :])",
      "        )",
      "",
      "    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    if EVEN_M & EVEN_N:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs)",
      "        else:",
      "            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)",
      "        else:",
      "            q = tl.load(",
      "                q_ptrs,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "                other=0.0,",
      "            )",
      "",
      "    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)",
      "    for start_n in range(0, end_n, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(k_ptrs + start_n * stride_kn)",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, tl.trans(k))",
      "",
      "        if not EVEN_N:",
      "            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float(\"-inf\"))",
      "        if IS_CAUSAL:",
      "            qk += tl.where(",
      "                offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float(\"-inf\")",
      "            )",
      "        if BIAS_TYPE != \"none\":",
      "            if BIAS_TYPE == \"vector\":",
      "                if EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n, mask=(start_n + offs_n) < seqlen_k, other=0.0",
      "                    ).to(tl.float32)",
      "                bias = bias[None, :]",
      "            elif BIAS_TYPE == \"matrix\":",
      "                if EVEN_M & EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n,",
      "                        mask=(offs_m[:, None] < seqlen_q)",
      "                        & ((start_n + offs_n)[None, :] < seqlen_k),",
      "                        other=0.0,",
      "                    ).to(tl.float32)",
      "",
      "            qk = qk * softmax_scale + bias",
      "            m_ij = tl.maximum(tl.max(qk, 1), lse_i)",
      "            p = tl.exp(qk - m_ij[:, None])",
      "        else:",
      "            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "            p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(v_ptrs + start_n * stride_vn)",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    o_scale = tl.exp(m_i - lse_i)",
      "    acc_o = acc_o * o_scale[:, None]",
      "",
      "    start_m = tl.program_id(0)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m",
      "    tl.store(lse_ptrs, lse_i)",
      "",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    out_ptrs = (",
      "        Out",
      "        + off_b * stride_ob",
      "        + off_h * stride_oh",
      "        + (offs_m[:, None] * stride_om + offs_d[None, :])",
      "    )",
      "    if EVEN_M:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o)",
      "        else:",
      "            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)",
      "        else:",
      "            tl.store(",
      "                out_ptrs,",
      "                acc_o,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "            )"
    ],
    "file": "codes/300.py",
    "header": "def _fwd_kernel(Q, K, V, Bias, Out, Lse, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_ob, stride_oh, stride_om, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.constexpr, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "start_m = tl.program_id(0)\noff_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nq_ptrs = Q + off_b * stride_qb + off_h * stride_qh + (offs_m[:, None] * stride_qm + offs_d[None, :])\nk_ptrs = K + off_b * stride_kb + off_h * stride_kh + (offs_n[:, None] * stride_kn + offs_d[None, :])\nv_ptrs = V + off_b * stride_vb + off_h * stride_vh + (offs_n[:, None] * stride_vn + offs_d[None, :])\nif BIAS_TYPE == 'vector':\n    b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n\nelif BIAS_TYPE == 'matrix':\n    b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + (offs_m[:, None] * stride_bm + offs_n[None, :])\nlse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nacc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)\nif EVEN_M & EVEN_N:\n    if EVEN_HEADDIM:\n        q = tl.load(q_ptrs)\n    else:\n        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)\nelif EVEN_HEADDIM:\n    q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)\nelse:\n    q = tl.load(q_ptrs, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim), other=0.0)\nend_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)\nfor start_n in range(0, end_n, BLOCK_N):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            k = tl.load(k_ptrs + start_n * stride_kn)\n        else:\n            k = tl.load(k_ptrs + start_n * stride_kn, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=(start_n + offs_n)[:, None] < seqlen_k, other=0.0)\n    else:\n        k = tl.load(k_ptrs + start_n * stride_kn, mask=((start_n + offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk += tl.dot(q, tl.trans(k))\n    if not EVEN_N:\n        qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float('-inf'))\n    if IS_CAUSAL:\n        qk += tl.where(offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float('-inf'))\n    if BIAS_TYPE != 'none':\n        if BIAS_TYPE == 'vector':\n            if EVEN_N:\n                bias = tl.load(b_ptrs + start_n).to(tl.float32)\n            else:\n                bias = tl.load(b_ptrs + start_n, mask=start_n + offs_n < seqlen_k, other=0.0).to(tl.float32)\n            bias = bias[None, :]\n        elif BIAS_TYPE == 'matrix':\n            if EVEN_M & EVEN_N:\n                bias = tl.load(b_ptrs + start_n).to(tl.float32)\n            else:\n                bias = tl.load(b_ptrs + start_n, mask=(offs_m[:, None] < seqlen_q) & ((start_n + offs_n)[None, :] < seqlen_k), other=0.0).to(tl.float32)\n        qk = qk * softmax_scale + bias\n        m_ij = tl.maximum(tl.max(qk, 1), lse_i)\n        p = tl.exp(qk - m_ij[:, None])\n    else:\n        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)\n        p = tl.exp(qk * softmax_scale - m_ij[:, None])\n    l_ij = tl.sum(p, 1)\n    acc_o_scale = tl.exp(m_i - m_ij)\n    acc_o = acc_o * acc_o_scale[:, None]\n    if EVEN_N & EVEN_M:\n        if EVEN_HEADDIM:\n            v = tl.load(v_ptrs + start_n * stride_vn)\n        else:\n            v = tl.load(v_ptrs + start_n * stride_vn, mask=offs_d[None, :] < headdim, other=0.0)\n    elif EVEN_HEADDIM:\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=(start_n + offs_n)[:, None] < seqlen_k, other=0.0)\n    else:\n        v = tl.load(v_ptrs + start_n * stride_vn, mask=((start_n + offs_n)[:, None] < seqlen_k) & (offs_d[None, :] < headdim), other=0.0)\n    p = p.to(v.dtype)\n    acc_o += tl.dot(p, v)\n    m_i = m_ij\n    l_i_new = tl.exp(lse_i - m_ij) + l_ij\n    lse_i = m_ij + tl.log(l_i_new)\no_scale = tl.exp(m_i - lse_i)\nacc_o = acc_o * o_scale[:, None]\nstart_m = tl.program_id(0)\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\nlse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m\ntl.store(lse_ptrs, lse_i)\noffs_d = tl.arange(0, BLOCK_HEADDIM)\nout_ptrs = Out + off_b * stride_ob + off_h * stride_oh + (offs_m[:, None] * stride_om + offs_d[None, :])\nif EVEN_M:\n    if EVEN_HEADDIM:\n        tl.store(out_ptrs, acc_o)\n    else:\n        tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)\nelif EVEN_HEADDIM:\n    tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)\nelse:\n    tl.store(out_ptrs, acc_o, mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim))"
  },
  {
    "name": "_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K', 'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])",
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEQUENCE_PARALLEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    SEQUENCE_PARALLEL: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    K += off_b * stride_kb + off_h * stride_kh",
      "    V += off_b * stride_vb + off_h * stride_vh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "    DK += off_b * stride_dkb + off_h * stride_dkh",
      "    DV += off_b * stride_dvb + off_h * stride_dvh",
      "    if BIAS_TYPE != \"none\":",
      "        Bias += off_b * stride_bb + off_h * stride_bh",
      "",
      "    D += off_hb * seqlen_q_rounded",
      "    LSE += off_hb * seqlen_q_rounded",
      "    if not SEQUENCE_PARALLEL:",
      "        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)",
      "        for start_n in range(0, num_block_n):",
      "            _bwd_kernel_one_col_block(",
      "                start_n,",
      "                Q,",
      "                K,",
      "                V,",
      "                Bias,",
      "                DO,",
      "                DQ,",
      "                DK,",
      "                DV,",
      "                LSE,",
      "                D,",
      "                softmax_scale,",
      "                stride_qm,",
      "                stride_kn,",
      "                stride_vn,",
      "                stride_bm,",
      "                stride_dom,",
      "                stride_dqm,",
      "                stride_dkn,",
      "                stride_dvn,",
      "                seqlen_q,",
      "                seqlen_k,",
      "                headdim,",
      "                ATOMIC_ADD=False,",
      "                BIAS_TYPE=BIAS_TYPE,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "                EVEN_M=EVEN_M,",
      "                EVEN_N=EVEN_N,",
      "                EVEN_HEADDIM=EVEN_HEADDIM,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "    else:",
      "        start_n = tl.program_id(0)",
      "        _bwd_kernel_one_col_block(",
      "            start_n,",
      "            Q,",
      "            K,",
      "            V,",
      "            Bias,",
      "            DO,",
      "            DQ,",
      "            DK,",
      "            DV,",
      "            LSE,",
      "            D,",
      "            softmax_scale,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dqm,",
      "            stride_dkn,",
      "            stride_dvn,",
      "            seqlen_q,",
      "            seqlen_k,",
      "            headdim,",
      "            ATOMIC_ADD=True,",
      "            BIAS_TYPE=BIAS_TYPE,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "            EVEN_M=EVEN_M,",
      "            EVEN_N=EVEN_N,",
      "            EVEN_HEADDIM=EVEN_HEADDIM,",
      "            BLOCK_M=BLOCK_M,",
      "            BLOCK_N=BLOCK_N,",
      "        )"
    ],
    "file": "codes/300.py",
    "header": "def _bwd_kernel(Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qb, stride_qh, stride_qm, stride_kb, stride_kh, stride_kn, stride_vb, stride_vh, stride_vn, stride_bb, stride_bh, stride_bm, stride_dob, stride_doh, stride_dom, stride_dqb, stride_dqh, stride_dqm, stride_dkb, stride_dkh, stride_dkn, stride_dvb, stride_dvh, stride_dvn, nheads, seqlen_q, seqlen_k, seqlen_q_rounded, headdim, CACHE_KEY_SEQLEN_Q, CACHE_KEY_SEQLEN_K, BIAS_TYPE: tl.constexpr, IS_CAUSAL: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, SEQUENCE_PARALLEL: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_HEADDIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "off_hb = tl.program_id(1)\noff_b = off_hb // nheads\noff_h = off_hb % nheads\nQ += off_b * stride_qb + off_h * stride_qh\nK += off_b * stride_kb + off_h * stride_kh\nV += off_b * stride_vb + off_h * stride_vh\nDO += off_b * stride_dob + off_h * stride_doh\nDQ += off_b * stride_dqb + off_h * stride_dqh\nDK += off_b * stride_dkb + off_h * stride_dkh\nDV += off_b * stride_dvb + off_h * stride_dvh\nif BIAS_TYPE != 'none':\n    Bias += off_b * stride_bb + off_h * stride_bh\nD += off_hb * seqlen_q_rounded\nLSE += off_hb * seqlen_q_rounded\nif not SEQUENCE_PARALLEL:\n    num_block_n = tl.cdiv(seqlen_k, BLOCK_N)\n    for start_n in range(0, num_block_n):\n        _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k, headdim, ATOMIC_ADD=False, BIAS_TYPE=BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)\nelse:\n    start_n = tl.program_id(0)\n    _bwd_kernel_one_col_block(start_n, Q, K, V, Bias, DO, DQ, DK, DV, LSE, D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm, stride_dom, stride_dqm, stride_dkn, stride_dvn, seqlen_q, seqlen_k, headdim, ATOMIC_ADD=True, BIAS_TYPE=BIAS_TYPE, IS_CAUSAL=IS_CAUSAL, BLOCK_HEADDIM=BLOCK_HEADDIM, EVEN_M=EVEN_M, EVEN_N=EVEN_N, EVEN_HEADDIM=EVEN_HEADDIM, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)"
  },
  {
    "name": "causal_conv1d_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_WEIGHT': lambda args: args['weight'] is not None, 'HAS_BIAS': lambda args: args['bias'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [4, 8, 16, 32]], key=['B', 'D', 'W', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "weight",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "W",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def causal_conv1d_fwd_kernel(",
      "    x,",
      "    y,",
      "    weight,",
      "    bias,",
      "    residual,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    D: tl.constexpr,",
      "    W: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    o_w = tl.arange(0, W)",
      "    m_d = o_d < D",
      "",
      "    if HAS_WEIGHT:",
      "",
      "        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)",
      "",
      "    b_y = tl.zeros((BT, BD), dtype=tl.float32)",
      "    for i_w in tl.static_range(-W + 1, 1):",
      "        p_yi = tl.make_block_ptr(",
      "            x + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_yi = tl.load(p_yi, boundary_check=(0, 1))",
      "        if HAS_WEIGHT:",
      "            b_yi *= tl.sum(b_w * (o_w == (i_w + W - 1)), 1)",
      "        b_y += b_yi",
      "    if HAS_BIAS:",
      "        b_y += tl.load(bias + o_d, mask=m_d)",
      "",
      "    if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":",
      "        b_y = b_y * tl.sigmoid(b_y)",
      "",
      "    if HAS_RESIDUAL:",
      "        p_residual = tl.make_block_ptr(",
      "            residual + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        b_residual = tl.load(p_residual, boundary_check=(0, 1))",
      "        b_y += b_residual",
      "",
      "    p_y = tl.make_block_ptr(",
      "        y + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_y,",
      "        tl.cast(b_y, dtype=p_y.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/351.py",
    "header": "def causal_conv1d_fwd_kernel(x, y, weight, bias, residual, cu_seqlens, chunk_indices, T, B: tl.constexpr, D: tl.constexpr, W: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, NB: tl.constexpr, ACTIVATION: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_RESIDUAL: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_d, i_t, i_b = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_b * T, i_b * T + T)\no_d = i_d * BD + tl.arange(0, BD)\no_w = tl.arange(0, W)\nm_d = o_d < D\nif HAS_WEIGHT:\n    b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)\nb_y = tl.zeros((BT, BD), dtype=tl.float32)\nfor i_w in tl.static_range(-W + 1, 1):\n    p_yi = tl.make_block_ptr(x + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n    b_yi = tl.load(p_yi, boundary_check=(0, 1))\n    if HAS_WEIGHT:\n        b_yi *= tl.sum(b_w * (o_w == i_w + W - 1), 1)\n    b_y += b_yi\nif HAS_BIAS:\n    b_y += tl.load(bias + o_d, mask=m_d)\nif ACTIVATION == 'swish' or ACTIVATION == 'silu':\n    b_y = b_y * tl.sigmoid(b_y)\nif HAS_RESIDUAL:\n    p_residual = tl.make_block_ptr(residual + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    b_residual = tl.load(p_residual, boundary_check=(0, 1))\n    b_y += b_residual\np_y = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\ntl.store(p_y, tl.cast(b_y, dtype=p_y.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "causal_conv1d_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_WEIGHT': lambda args: args['dw'] is not None, 'HAS_BIAS': lambda args: args['db'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [4, 8, 16, 32]], key=['B', 'D', 'W', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "weight",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "W",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def causal_conv1d_bwd_kernel(",
      "    x,",
      "    y,",
      "    weight,",
      "    bias,",
      "    residual,",
      "    dy,",
      "    dx,",
      "    dw,",
      "    db,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    D: tl.constexpr,",
      "    W: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)",
      "        T = eos - bos",
      "    else:",
      "        i_tg = i_b * tl.num_programs(1) + i_t",
      "        i_n = i_b",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    o_w = tl.arange(0, W)",
      "    m_d = o_d < D",
      "",
      "    if HAS_WEIGHT:",
      "        p_x = tl.make_block_ptr(",
      "            x + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        b_x = tl.load(p_x, boundary_check=(0, 1))",
      "",
      "        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)",
      "",
      "    b_dx = tl.zeros((BT, BD), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_db = tl.zeros((BD,), dtype=tl.float32)",
      "    for i_w in tl.static_range(0, W):",
      "        p_dy = tl.make_block_ptr(",
      "            dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_dy = tl.load(p_dy, boundary_check=(0, 1))",
      "",
      "        if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":",
      "            p_y = tl.make_block_ptr(",
      "                y + bos * D,",
      "                (T, D),",
      "                (D, 1),",
      "                (i_t * BT + i_w, i_d * BD),",
      "                (BT, BD),",
      "                (1, 0),",
      "            )",
      "",
      "            b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)",
      "            b_ys = tl.sigmoid(b_y)",
      "            b_dy = b_dy * b_ys * (1 + b_y * (1 - b_ys))",
      "",
      "        b_wdy = b_dy",
      "        if HAS_WEIGHT:",
      "",
      "            b_wdy = b_wdy * tl.sum(b_w * (o_w == (W - i_w - 1)), 1)",
      "",
      "            b_dw = tl.sum(b_dy * b_x, 0)",
      "            tl.store(",
      "                dw + i_tg * D * W + o_d * W + W - i_w - 1,",
      "                b_dw.to(dw.dtype.element_ty),",
      "                mask=m_d,",
      "            )",
      "        if HAS_BIAS and i_w == 0:",
      "            b_db += tl.sum(b_dy, 0)",
      "        b_dx += b_wdy",
      "    if HAS_BIAS:",
      "        b_db = tl.cast(b_db, dtype=db.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "        tl.store(db + i_tg * D + o_d, b_db, mask=m_d)",
      "",
      "    p_dx = tl.make_block_ptr(",
      "        dx + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_dx,",
      "        tl.cast(b_dx, dtype=p_dx.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/351.py",
    "header": "def causal_conv1d_bwd_kernel(x, y, weight, bias, residual, dy, dx, dw, db, cu_seqlens, chunk_indices, T, B: tl.constexpr, D: tl.constexpr, W: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, NB: tl.constexpr, ACTIVATION: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_RESIDUAL: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_d, i_t, i_b = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1))\n    T = eos - bos\nelse:\n    i_tg = i_b * tl.num_programs(1) + i_t\n    i_n = i_b\n    bos, eos = (i_b * T, i_b * T + T)\no_d = i_d * BD + tl.arange(0, BD)\no_w = tl.arange(0, W)\nm_d = o_d < D\nif HAS_WEIGHT:\n    p_x = tl.make_block_ptr(x + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1))\n    b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)\nb_dx = tl.zeros((BT, BD), dtype=tl.float32)\nif HAS_BIAS:\n    b_db = tl.zeros((BD,), dtype=tl.float32)\nfor i_w in tl.static_range(0, W):\n    p_dy = tl.make_block_ptr(dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n    b_dy = tl.load(p_dy, boundary_check=(0, 1))\n    if ACTIVATION == 'swish' or ACTIVATION == 'silu':\n        p_y = tl.make_block_ptr(y + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0))\n        b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)\n        b_ys = tl.sigmoid(b_y)\n        b_dy = b_dy * b_ys * (1 + b_y * (1 - b_ys))\n    b_wdy = b_dy\n    if HAS_WEIGHT:\n        b_wdy = b_wdy * tl.sum(b_w * (o_w == W - i_w - 1), 1)\n        b_dw = tl.sum(b_dy * b_x, 0)\n        tl.store(dw + i_tg * D * W + o_d * W + W - i_w - 1, b_dw.to(dw.dtype.element_ty), mask=m_d)\n    if HAS_BIAS and i_w == 0:\n        b_db += tl.sum(b_dy, 0)\n    b_dx += b_wdy\nif HAS_BIAS:\n    b_db = tl.cast(b_db, dtype=db.dtype.element_ty, fp_downcast_rounding='rtne')\n    tl.store(db + i_tg * D + o_d, b_db, mask=m_d)\np_dx = tl.make_block_ptr(dx + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0))\ntl.store(p_dx, tl.cast(b_dx, dtype=p_dx.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "codes/173.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr):",
    "body": "idx = tl.program_id(axis=0)\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\noffsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + offsets)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    local_x_block_ptr = x_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == v_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    m = m_new\n    A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n    v_range = v_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\nloss += tl.sum(lse) / N\ntl.store(losses_ptr + idx, loss)\ntl.store(lse_ptr + offsets, lse)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N",
      "            tl.store(",
      "                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)",
      "            )",
      "",
      "            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "codes/173.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dA(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_V = tl.program_id(axis=0)\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nN_offsets = tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nA_grad_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(0 * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nfor idx_N in range(N // N_BLOCK_SIZE):\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n    A_block_ptr = tl.advance(A_block_ptr, [-H, 0])\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n        temp_AgradT = temp_Agrad.trans() / N\n        tl.store(A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr))\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])\n    A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])\n    N_offsets += N_BLOCK_SIZE"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0 * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "            x_block_ptr = tl.make_block_ptr(",
      "                base=x_ptr,",
      "                shape=(N, H),",
      "                strides=(stride_x_N, stride_x_H),",
      "                offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),",
      "                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_block_ptr = tl.make_block_ptr(",
      "                base=A_t_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_grad_block_ptr = tl.make_block_ptr(",
      "                base=x_grad_ptr,",
      "                shape=(N, H),",
      "                strides=(stride_x_N, stride_x_H),",
      "                offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_block_ptr = tl.make_block_ptr(",
      "                base=A_t_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_v = tl.load(A_block_ptr).trans()",
      "",
      "            temp_xgrad = tl.dot(softmax_z, A_v) / N",
      "            temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N",
      "            tl.store(",
      "                x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16)",
      "            )",
      "",
      "        V_offsets += V_BLOCK_SIZE"
    ],
    "file": "codes/173.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_N = tl.program_id(axis=0)\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = tl.arange(0, V_BLOCK_SIZE)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0 * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\ny = tl.load(y_ptr + N_offsets)\nlse = tl.load(lse_global_ptr + N_offsets)\nfor idx_V in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for idx_H_1 in range(H // H_BLOCK_SIZE):\n        x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n        A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n        A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n        A_v = tl.load(A_block_ptr).trans()\n        temp_xgrad = tl.dot(softmax_z, A_v) / N\n        temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N\n        tl.store(x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16))\n    V_offsets += V_BLOCK_SIZE"
  },
  {
    "name": "_state_passing_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "final_states_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_final_states_batch",
        "annotation": null
      },
      {
        "name": "stride_final_states_head",
        "annotation": null
      },
      {
        "name": "stride_final_states_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_fwd_kernel(",
      "    states_ptr,",
      "    out_ptr,",
      "    final_states_ptr,",
      "    dA_cs_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_final_states_batch,",
      "    stride_final_states_head,",
      "    stride_final_states_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    REVERSE: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    final_states_ptr += (",
      "        pid_b * stride_final_states_batch + pid_h * stride_final_states_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    states_ptrs = states_ptr + offs_m * stride_states_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim",
      "",
      "    if not REVERSE:",
      "",
      "        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "        tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        out_ptrs += stride_out_chunk",
      "        for c in range(nchunks):",
      "            new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(",
      "                tl.float32",
      "            )",
      "            dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "            scale = tl.exp(dA_cs)",
      "            states = scale * states + new_states",
      "            if c < nchunks - 1:",
      "                tl.store(out_ptrs, states, mask=offs_m < dim)",
      "            else:",
      "                tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "            states_ptrs += stride_states_chunk",
      "            dA_cs_ptr += stride_dA_cs_chunk",
      "            out_ptrs += stride_out_chunk",
      "    else:",
      "",
      "        states_ptrs += (nchunks - 1) * stride_states_chunk",
      "        dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk",
      "        out_ptrs += (nchunks - 1) * stride_out_chunk",
      "",
      "        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "        tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        out_ptrs -= stride_out_chunk",
      "        for c in range(nchunks - 1, -1, -1):",
      "            new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(",
      "                tl.float32",
      "            )",
      "            dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "            scale = tl.exp(dA_cs)",
      "            states = scale * states + new_states",
      "            if c > 0:",
      "                tl.store(out_ptrs, states, mask=offs_m < dim)",
      "            else:",
      "                tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "            states_ptrs -= stride_states_chunk",
      "            dA_cs_ptr -= stride_dA_cs_chunk",
      "            out_ptrs -= stride_out_chunk"
    ],
    "file": "codes/122.py",
    "header": "def _state_passing_fwd_kernel(states_ptr, out_ptr, final_states_ptr, dA_cs_ptr, dim, nchunks, seqlen, chunk_size, stride_states_batch, stride_states_chunk, stride_states_head, stride_states_dim, stride_out_batch, stride_out_chunk, stride_out_head, stride_out_dim, stride_final_states_batch, stride_final_states_head, stride_final_states_dim, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, REVERSE: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\nstates_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\ndA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\nout_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\nfinal_states_ptr += pid_b * stride_final_states_batch + pid_h * stride_final_states_head\noffs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nstates_ptrs = states_ptr + offs_m * stride_states_dim\nout_ptrs = out_ptr + offs_m * stride_out_dim\nfinal_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim\nif not REVERSE:\n    states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    tl.store(out_ptrs, states, mask=offs_m < dim)\n    out_ptrs += stride_out_chunk\n    for c in range(nchunks):\n        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n        scale = tl.exp(dA_cs)\n        states = scale * states + new_states\n        if c < nchunks - 1:\n            tl.store(out_ptrs, states, mask=offs_m < dim)\n        else:\n            tl.store(final_states_ptrs, states, mask=offs_m < dim)\n        states_ptrs += stride_states_chunk\n        dA_cs_ptr += stride_dA_cs_chunk\n        out_ptrs += stride_out_chunk\nelse:\n    states_ptrs += (nchunks - 1) * stride_states_chunk\n    dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk\n    out_ptrs += (nchunks - 1) * stride_out_chunk\n    states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    tl.store(out_ptrs, states, mask=offs_m < dim)\n    out_ptrs -= stride_out_chunk\n    for c in range(nchunks - 1, -1, -1):\n        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n        scale = tl.exp(dA_cs)\n        states = scale * states + new_states\n        if c > 0:\n            tl.store(out_ptrs, states, mask=offs_m < dim)\n        else:\n            tl.store(final_states_ptrs, states, mask=offs_m < dim)\n        states_ptrs -= stride_states_chunk\n        dA_cs_ptr -= stride_dA_cs_chunk\n        out_ptrs -= stride_out_chunk"
  },
  {
    "name": "_state_passing_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cs_ptr",
        "annotation": null
      },
      {
        "name": "states_converted_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_dim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "CONVERT_STATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_bwd_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dA_cs_ptr,",
      "    dstates_ptr,",
      "    ddA_cs_ptr,",
      "    states_converted_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_dstates_head,",
      "    stride_dstates_dim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    CONVERT_STATES: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    dstates_ptr += pid_b * stride_dstates_batch + pid_h * stride_dstates_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head",
      "    ddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    dout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head",
      "    if not REVERSE:",
      "        dstates_ptr += (nchunks - 1) * stride_dstates_chunk",
      "        dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk",
      "        ddA_cs_ptr += (nchunks - 1) * stride_ddA_cs_chunk",
      "        out_ptr += (nchunks - 1) * stride_out_chunk",
      "        dout_ptr += (nchunks - 1) * stride_dout_chunk",
      "",
      "    if CONVERT_STATES:",
      "        states_converted_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "        if not REVERSE:",
      "            states_converted_ptr += (nchunks - 1) * stride_out_chunk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    dout_ptrs = dout_ptr + offs_m * stride_dout_dim",
      "    if CONVERT_STATES:",
      "        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim",
      "",
      "    dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "    if not REVERSE:",
      "        dstates_ptrs -= stride_dstates_chunk",
      "    else:",
      "        dstates_ptrs += stride_dstates_chunk",
      "    for c in range(nchunks - 1):",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if CONVERT_STATES:",
      "            tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "        ddA = tl.sum(out * dstates) * scale",
      "        tl.store(ddA_cs_ptr, ddA)",
      "        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dstates = scale * dstates + dout",
      "        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "        if not REVERSE:",
      "            dout_ptrs -= stride_dout_chunk",
      "            dstates_ptrs -= stride_dstates_chunk",
      "            dA_cs_ptr -= stride_dA_cs_chunk",
      "            ddA_cs_ptr -= stride_ddA_cs_chunk",
      "            out_ptrs -= stride_out_chunk",
      "            if CONVERT_STATES:",
      "                states_converted_ptrs -= stride_out_chunk",
      "        else:",
      "            dout_ptrs += stride_dout_chunk",
      "            dstates_ptrs += stride_dstates_chunk",
      "            dA_cs_ptr += stride_dA_cs_chunk",
      "            ddA_cs_ptr += stride_ddA_cs_chunk",
      "            out_ptrs += stride_out_chunk",
      "            if CONVERT_STATES:",
      "                states_converted_ptrs += stride_out_chunk",
      "    if CONVERT_STATES:",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "    tl.store(ddA_cs_ptr, 0.0)"
    ],
    "file": "codes/122.py",
    "header": "def _state_passing_bwd_kernel(dout_ptr, out_ptr, dA_cs_ptr, dstates_ptr, ddA_cs_ptr, states_converted_ptr, dim, nchunks, seqlen, chunk_size, stride_dout_batch, stride_dout_chunk, stride_dout_head, stride_dout_dim, stride_out_batch, stride_out_chunk, stride_out_head, stride_out_dim, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dstates_batch, stride_dstates_chunk, stride_dstates_head, stride_dstates_dim, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, CONVERT_STATES: tl.constexpr, BLOCK_SIZE: tl.constexpr, REVERSE: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\npid_m = tl.program_id(axis=0)\ndstates_ptr += pid_b * stride_dstates_batch + pid_h * stride_dstates_head\ndA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head\nddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head\nout_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\ndout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head\nif not REVERSE:\n    dstates_ptr += (nchunks - 1) * stride_dstates_chunk\n    dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk\n    ddA_cs_ptr += (nchunks - 1) * stride_ddA_cs_chunk\n    out_ptr += (nchunks - 1) * stride_out_chunk\n    dout_ptr += (nchunks - 1) * stride_dout_chunk\nif CONVERT_STATES:\n    states_converted_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\n    if not REVERSE:\n        states_converted_ptr += (nchunks - 1) * stride_out_chunk\noffs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\ndstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim\nout_ptrs = out_ptr + offs_m * stride_out_dim\ndout_ptrs = dout_ptr + offs_m * stride_dout_dim\nif CONVERT_STATES:\n    states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim\ndstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\ntl.store(dstates_ptrs, dstates, mask=offs_m < dim)\nif not REVERSE:\n    dstates_ptrs -= stride_dstates_chunk\nelse:\n    dstates_ptrs += stride_dstates_chunk\nfor c in range(nchunks - 1):\n    dA_cs = tl.load(dA_cs_ptr).to(tl.float32)\n    scale = tl.exp(dA_cs)\n    out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if CONVERT_STATES:\n        tl.store(states_converted_ptrs, out, mask=offs_m < dim)\n    ddA = tl.sum(out * dstates) * scale\n    tl.store(ddA_cs_ptr, ddA)\n    dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    dstates = scale * dstates + dout\n    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)\n    if not REVERSE:\n        dout_ptrs -= stride_dout_chunk\n        dstates_ptrs -= stride_dstates_chunk\n        dA_cs_ptr -= stride_dA_cs_chunk\n        ddA_cs_ptr -= stride_ddA_cs_chunk\n        out_ptrs -= stride_out_chunk\n        if CONVERT_STATES:\n            states_converted_ptrs -= stride_out_chunk\n    else:\n        dout_ptrs += stride_dout_chunk\n        dstates_ptrs += stride_dstates_chunk\n        dA_cs_ptr += stride_dA_cs_chunk\n        ddA_cs_ptr += stride_ddA_cs_chunk\n        out_ptrs += stride_out_chunk\n        if CONVERT_STATES:\n            states_converted_ptrs += stride_out_chunk\nif CONVERT_STATES:\n    out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    tl.store(states_converted_ptrs, out, mask=offs_m < dim)\ntl.store(ddA_cs_ptr, 0.0)"
  },
  {
    "name": "parallel_attn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([8] if check_shared_mem('hopper') else []) for num_stages in [2, 3, 4, 5]], key=['B', 'H', 'HQ', 'G', 'K', 'V', 'BK', 'BV', 'USE_G', 'IS_VARLEN'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_attn_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    g_cumsum,",
      "    lse,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_lse = tl.make_block_ptr(",
      "        lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([BT], float(\"-inf\"), dtype=tl.float32)",
      "    b_acc = tl.zeros([BT], dtype=tl.float32)",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)",
      "    else:",
      "        b_gq = None",
      "",
      "    for i_s in range(0, i_t * BT, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = safe_exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "        b_mp = b_m",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        o_k = i_s + tl.arange(0, BS)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, float(\"-inf\"))",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = safe_exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "        b_mp = b_m",
      "",
      "    b_o = b_o / b_acc[:, None]",
      "    b_m += log(b_acc)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/365.py",
    "header": "def parallel_attn_fwd_kernel(q, k, v, o, g_cumsum, lse, scale, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\np_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_o = tl.make_block_ptr(o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_lse = tl.make_block_ptr(lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nb_m = tl.full([BT], float('-inf'), dtype=tl.float32)\nb_acc = tl.zeros([BT], dtype=tl.float32)\nif USE_G:\n    p_g = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)\nelse:\n    b_gq = None\nfor i_s in range(0, i_t * BT, BS):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    if USE_G:\n        p_gk = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_s += b_gq[:, None] - b_gk[None, :]\n    b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n    b_r = exp(b_mp - b_m)\n    b_p = safe_exp(b_s - b_m[:, None])\n    b_acc = b_acc * b_r + tl.sum(b_p, 1)\n    b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n    b_mp = b_m\no_q = i_t * BT + tl.arange(0, BT)\nfor i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    o_k = i_s + tl.arange(0, BS)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, float('-inf'))\n    if USE_G:\n        p_gk = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_s += b_gq[:, None] - b_gk[None, :]\n    b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n    b_r = exp(b_mp - b_m)\n    b_p = safe_exp(b_s - b_m[:, None])\n    b_acc = b_acc * b_r + tl.sum(b_p, 1)\n    b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n    b_mp = b_m\nb_o = b_o / b_acc[:, None]\nb_m += log(b_acc)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_lse, b_m.to(p_lse.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "parallel_attn_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([8] if check_shared_mem('hopper') else []) for num_stages in [2, 3, 4, 5]], key=['B', 'H', 'HQ', 'G', 'K', 'V', 'BK', 'BV', 'USE_G', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_attn_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    dg_cumsum,",
      "    g_cumsum,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_lse = tl.make_block_ptr(",
      "        lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_delta = tl.make_block_ptr(",
      "        delta + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse, boundary_check=(0,))",
      "    b_delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    if USE_G:",
      "        b_dg = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        p_gq = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "    else:",
      "        b_gq = None",
      "        b_dg = None",
      "",
      "    for i_s in range(0, i_t * BT, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_s),",
      "            (BV, BS),",
      "            (0, 1),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "",
      "        b_p = safe_exp(b_s - b_lse[:, None])",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "        if USE_G:",
      "            b_dg += tl.sum(b_ds, 1)",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_s),",
      "            (BV, BS),",
      "            (0, 1),",
      "        )",
      "",
      "        o_k = i_s + tl.arange(0, BS)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "            b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, -float(\"inf\"))",
      "",
      "        b_p = safe_exp(b_s - b_lse[:, None])",
      "        b_p = tl.where(o_q[:, None] >= o_k[None, :], b_p, 0)",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "        if USE_G:",
      "            b_dg += tl.sum(b_ds, 1)",
      "",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_dg = tl.make_block_ptr(",
      "            dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/365.py",
    "header": "def parallel_attn_bwd_kernel_dq(q, k, v, lse, delta, do, dq, dg_cumsum, g_cumsum, scale, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr, USE_G: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\np_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_dq = tl.make_block_ptr(dq + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_lse = tl.make_block_ptr(lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\np_delta = tl.make_block_ptr(delta + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_lse = tl.load(p_lse, boundary_check=(0,))\nb_delta = tl.load(p_delta, boundary_check=(0,))\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nif USE_G:\n    b_dg = tl.zeros([BT], dtype=tl.float32)\n    p_gq = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\nelse:\n    b_gq = None\n    b_dg = None\nfor i_s in range(0, i_t * BT, BS):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    if USE_G:\n        p_gk = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_s += b_gq[:, None] - b_gk[None, :]\n    b_p = safe_exp(b_s - b_lse[:, None])\n    b_dp = tl.dot(b_do, b_v)\n    b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n    b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n    if USE_G:\n        b_dg += tl.sum(b_ds, 1)\no_q = i_t * BT + tl.arange(0, BT)\nfor i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1))\n    o_k = i_s + tl.arange(0, BS)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    if USE_G:\n        p_gk = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n        b_s += b_gq[:, None] - b_gk[None, :]\n        b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, -float('inf'))\n    b_p = safe_exp(b_s - b_lse[:, None])\n    b_p = tl.where(o_q[:, None] >= o_k[None, :], b_p, 0)\n    b_dp = tl.dot(b_do, b_v)\n    b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n    b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\n    if USE_G:\n        b_dg += tl.sum(b_ds, 1)\nb_dq *= scale\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\nif USE_G:\n    p_dg = tl.make_block_ptr(dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "parallel_attn_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([8] if check_shared_mem('hopper') else []) for num_stages in [2, 3, 4, 5]], key=['B', 'H', 'HQ', 'G', 'K', 'V', 'BK', 'BV', 'USE_G', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_attn_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    dg_cumsum,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    o_k = i_t * BT + tl.arange(0, BT)",
      "",
      "    if USE_G:",
      "        p_gk = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "        b_dg = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_gk = None",
      "        b_dg = None",
      "",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = tl.make_block_ptr(",
      "            lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "        p_delta = tl.make_block_ptr(",
      "            delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "",
      "        o_q = i_s + tl.arange(0, BS)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse, boundary_check=(0,))",
      "        b_delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[None, :] - b_gk[:, None]",
      "            b_s = tl.where(o_k[:, None] <= o_q[None, :], b_s, -float(\"inf\"))",
      "        b_p = safe_exp(b_s - b_lse[None, :])",
      "        b_p = tl.where(o_k[:, None] <= o_q[None, :], b_p, 0)",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "        if USE_G:",
      "            b_dg -= tl.sum(b_ds, 1)",
      "",
      "    for i_s in range((i_t + 1) * BT, tl.cdiv(T, BS) * BS, BS):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = tl.make_block_ptr(",
      "            lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "        p_delta = tl.make_block_ptr(",
      "            delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "",
      "        o_q = i_s + tl.arange(0, BS)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse, boundary_check=(0,))",
      "        b_delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[None, :] - b_gk[:, None]",
      "        b_p = safe_exp(b_s - b_lse[None, :])",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "        if USE_G:",
      "            b_dg -= tl.sum(b_ds, 1)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_dg = tl.make_block_ptr(",
      "            dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/365.py",
    "header": "def parallel_attn_bwd_kernel_dkv(q, k, v, g_cumsum, lse, delta, do, dk, dv, dg_cumsum, cu_seqlens, chunk_indices, scale, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_dk = tl.make_block_ptr(dk + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_dv = tl.make_block_ptr(dv + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BT, BV], dtype=tl.float32)\no_k = i_t * BT + tl.arange(0, BT)\nif USE_G:\n    p_gk = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)\n    b_dg = tl.zeros([BT], dtype=tl.float32)\nelse:\n    b_gk = None\n    b_dg = None\nfor i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):\n    p_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    p_lse = tl.make_block_ptr(lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n    p_delta = tl.make_block_ptr(delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n    o_q = i_s + tl.arange(0, BS)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_lse = tl.load(p_lse, boundary_check=(0,))\n    b_delta = tl.load(p_delta, boundary_check=(0,))\n    b_s = tl.dot(b_k, tl.trans(b_q))\n    if USE_G:\n        p_gq = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\n        b_s += b_gq[None, :] - b_gk[:, None]\n        b_s = tl.where(o_k[:, None] <= o_q[None, :], b_s, -float('inf'))\n    b_p = safe_exp(b_s - b_lse[None, :])\n    b_p = tl.where(o_k[:, None] <= o_q[None, :], b_p, 0)\n    b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n    b_dp = tl.dot(b_v, tl.trans(b_do))\n    b_ds = b_p * (b_dp - b_delta[None, :])\n    b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n    if USE_G:\n        b_dg -= tl.sum(b_ds, 1)\nfor i_s in range((i_t + 1) * BT, tl.cdiv(T, BS) * BS, BS):\n    p_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n    p_lse = tl.make_block_ptr(lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n    p_delta = tl.make_block_ptr(delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n    o_q = i_s + tl.arange(0, BS)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_lse = tl.load(p_lse, boundary_check=(0,))\n    b_delta = tl.load(p_delta, boundary_check=(0,))\n    b_s = tl.dot(b_k, tl.trans(b_q))\n    if USE_G:\n        p_gq = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,))\n        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)\n        b_s += b_gq[None, :] - b_gk[:, None]\n    b_p = safe_exp(b_s - b_lse[None, :])\n    b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n    b_dp = tl.dot(b_v, tl.trans(b_do))\n    b_ds = b_p * (b_dp - b_delta[None, :])\n    b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\n    if USE_G:\n        b_dg -= tl.sum(b_ds, 1)\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\nif USE_G:\n    p_dg = tl.make_block_ptr(dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "_selective_scan_update_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not None})",
      "@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})",
      "@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.next_power_of_2(args['dstate'])})"
    ],
    "args": [
      {
        "name": "state_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_state_batch",
        "annotation": null
      },
      {
        "name": "stride_state_head",
        "annotation": null
      },
      {
        "name": "stride_state_dim",
        "annotation": null
      },
      {
        "name": "stride_state_dstate",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_dim",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_A_dim",
        "annotation": null
      },
      {
        "name": "stride_A_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_group",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_group",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_D_dim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TIE_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _selective_scan_update_kernel(",
      "    state_ptr,",
      "    x_ptr,",
      "    dt_ptr,",
      "    dt_bias_ptr,",
      "    A_ptr,",
      "    B_ptr,",
      "    C_ptr,",
      "    D_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    batch,",
      "    nheads,",
      "    dim,",
      "    dstate,",
      "    nheads_ngroups_ratio,",
      "    stride_state_batch,",
      "    stride_state_head,",
      "    stride_state_dim,",
      "    stride_state_dstate,",
      "    stride_x_batch,",
      "    stride_x_head,",
      "    stride_x_dim,",
      "    stride_dt_batch,",
      "    stride_dt_head,",
      "    stride_dt_dim,",
      "    stride_dt_bias_head,",
      "    stride_dt_bias_dim,",
      "    stride_A_head,",
      "    stride_A_dim,",
      "    stride_A_dstate,",
      "    stride_B_batch,",
      "    stride_B_group,",
      "    stride_B_dstate,",
      "    stride_C_batch,",
      "    stride_C_group,",
      "    stride_C_dstate,",
      "    stride_D_head,",
      "    stride_D_dim,",
      "    stride_z_batch,",
      "    stride_z_head,",
      "    stride_z_dim,",
      "    stride_out_batch,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    TIE_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head",
      "    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptr += pid_h * stride_dt_bias_head",
      "    A_ptr += pid_h * stride_A_head",
      "    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group",
      "    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group",
      "    if HAS_Z:",
      "        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "    state_ptrs = state_ptr + (",
      "        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate",
      "    )",
      "    x_ptrs = x_ptr + offs_m * stride_x_dim",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_dim",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim",
      "    if HAS_D:",
      "        D_ptr += pid_h * stride_D_head",
      "    A_ptrs = A_ptr + (",
      "        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate",
      "    )",
      "    B_ptrs = B_ptr + offs_n * stride_B_dstate",
      "    C_ptrs = C_ptr + offs_n * stride_C_dstate",
      "    if HAS_D:",
      "        D_ptrs = D_ptr + offs_m * stride_D_dim",
      "    if HAS_Z:",
      "        z_ptrs = z_ptr + offs_m * stride_z_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "",
      "    state = tl.load(",
      "        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "    )",
      "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if not TIE_HDIM:",
      "        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(",
      "            A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "        ).to(tl.float32)",
      "        dA = tl.exp(A * dt[:, None])",
      "    else:",
      "        dt = tl.load(dt_ptr).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptr).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(A_ptr).to(tl.float32)",
      "        dA = tl.exp(A * dt)",
      "",
      "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    if HAS_D:",
      "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if HAS_Z:",
      "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "",
      "    if not TIE_HDIM:",
      "        dB = B[None, :] * dt[:, None]",
      "    else:",
      "        dB = B * dt",
      "    state = state * dA + dB * x[:, None]",
      "    tl.store(",
      "        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)",
      "    )",
      "    out = tl.sum(state * C[None, :], axis=1)",
      "    if HAS_D:",
      "        out += x * D",
      "    if HAS_Z:",
      "        out *= z * tl.sigmoid(z)",
      "    tl.store(out_ptrs, out, mask=offs_m < dim)"
    ],
    "file": "codes/116.py",
    "header": "def _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr, A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, nheads, dim, dstate, nheads_ngroups_ratio, stride_state_batch, stride_state_head, stride_state_dim, stride_state_dstate, stride_x_batch, stride_x_head, stride_x_dim, stride_dt_batch, stride_dt_head, stride_dt_dim, stride_dt_bias_head, stride_dt_bias_dim, stride_A_head, stride_A_dim, stride_A_dstate, stride_B_batch, stride_B_group, stride_B_dstate, stride_C_batch, stride_C_group, stride_C_dstate, stride_D_head, stride_D_dim, stride_z_batch, stride_z_head, stride_z_dim, stride_out_batch, stride_out_head, stride_out_dim, DT_SOFTPLUS: tl.constexpr, TIE_HDIM: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, HAS_DT_BIAS: tl.constexpr, HAS_D: tl.constexpr, HAS_Z: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_m = tl.program_id(axis=0)\npid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\nstate_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\nx_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\nif HAS_DT_BIAS:\n    dt_bias_ptr += pid_h * stride_dt_bias_head\nA_ptr += pid_h * stride_A_head\nB_ptr += pid_b * stride_B_batch + pid_h // nheads_ngroups_ratio * stride_B_group\nC_ptr += pid_b * stride_C_batch + pid_h // nheads_ngroups_ratio * stride_C_group\nif HAS_Z:\n    z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\nout_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\nstate_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate)\nx_ptrs = x_ptr + offs_m * stride_x_dim\ndt_ptrs = dt_ptr + offs_m * stride_dt_dim\nif HAS_DT_BIAS:\n    dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\nif HAS_D:\n    D_ptr += pid_h * stride_D_head\nA_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate)\nB_ptrs = B_ptr + offs_n * stride_B_dstate\nC_ptrs = C_ptr + offs_n * stride_C_dstate\nif HAS_D:\n    D_ptrs = D_ptr + offs_m * stride_D_dim\nif HAS_Z:\n    z_ptrs = z_ptr + offs_m * stride_z_dim\nout_ptrs = out_ptr + offs_m * stride_out_dim\nstate = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0)\nx = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\nif not TIE_HDIM:\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    dA = tl.exp(A * dt[:, None])\nelse:\n    dt = tl.load(dt_ptr).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptr).to(tl.float32)\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n    A = tl.load(A_ptr).to(tl.float32)\n    dA = tl.exp(A * dt)\nB = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\nC = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\nif HAS_D:\n    D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\nif HAS_Z:\n    z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\nif not TIE_HDIM:\n    dB = B[None, :] * dt[:, None]\nelse:\n    dB = B * dt\nstate = state * dA + dB * x[:, None]\ntl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate))\nout = tl.sum(state * C[None, :], axis=1)\nif HAS_D:\n    out += x * D\nif HAS_Z:\n    out *= z * tl.sigmoid(z)\ntl.store(out_ptrs, out, mask=offs_m < dim)"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "need_silu",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    need_silu,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if need_silu:",
      "        sigmoid_x = 1.0 / (1.0 + tl.exp(-accumulator))",
      "        c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)",
      "    else:",
      "        c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/1264.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, need_silu, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nif need_silu:\n    sigmoid_x = 1.0 / (1.0 + tl.exp(-accumulator))\n    c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)\nelse:\n    c = accumulator.to(tl.float16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=AUTOTUNE_CONFIGS, key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    tl.device_assert(K % BLOCK_SIZE_K == 0)",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_ak = tl.arange(0, BLOCK_SIZE_K)",
      "    offs_bk = tl.arange(0, BLOCK_SIZE_K // 2)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs)",
      "        tl.static_assert(b.dtype == tl.int8)",
      "",
      "        _4_i8 = tl.full((1,), 4, dtype=tl.int8)",
      "        b_lo = (b << _4_i8) >> _4_i8",
      "        b_hi = b >> _4_i8",
      "",
      "        b_f16 = (",
      "            tl.join(b_lo.to(tl.bfloat16), b_hi.to(tl.bfloat16))",
      "            .permute(0, 2, 1)",
      "            .reshape(BLOCK_SIZE_K, BLOCK_SIZE_N)",
      "        )",
      "",
      "        accumulator += tl.dot(a, b_f16)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk // 2",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/658.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "tl.device_assert(K % BLOCK_SIZE_K == 0)\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_ak = tl.arange(0, BLOCK_SIZE_K)\noffs_bk = tl.arange(0, BLOCK_SIZE_K // 2)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs)\n    tl.static_assert(b.dtype == tl.int8)\n    _4_i8 = tl.full((1,), 4, dtype=tl.int8)\n    b_lo = b << _4_i8 >> _4_i8\n    b_hi = b >> _4_i8\n    b_f16 = tl.join(b_lo.to(tl.bfloat16), b_hi.to(tl.bfloat16)).permute(0, 2, 1).reshape(BLOCK_SIZE_K, BLOCK_SIZE_N)\n    accumulator += tl.dot(a, b_f16)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk // 2\nc = accumulator.to(tl.bfloat16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "triton_tem_fused_no_exp2",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_DMODEL': 64}, num_stages=3, num_warps=4)], key=['num_queries'])"
    ],
    "args": [
      {
        "name": "arg_Q",
        "annotation": null
      },
      {
        "name": "arg_K",
        "annotation": null
      },
      {
        "name": "arg_V",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "num_queries",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_tem_fused_no_exp2(",
      "    arg_Q,",
      "    arg_K,",
      "    arg_V,",
      "    out_ptr0,",
      "    num_queries: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "):",
      "    Q = arg_Q",
      "    K = arg_K",
      "    V = arg_V",
      "",
      "    stride_qz = 4194304",
      "    stride_qh = 262144",
      "    stride_qm = 64",
      "    stride_qk = 1",
      "",
      "    stride_kz = 4194304",
      "    stride_kh = 262144",
      "    stride_kn = 64",
      "    stride_kk = 1",
      "",
      "    stride_vz = 4194304",
      "    stride_vh = 262144",
      "    stride_vk = 64",
      "    stride_vn = 1",
      "",
      "    Z = 16",
      "    H = 16",
      "    N_CTX = 4096",
      "",
      "    qk_scale = 1.0",
      "    MATMUL_PRECISION = tl.float16",
      "",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    qkv_offset = off_hz * stride_qh",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qkv_offset,",
      "        shape=(BLOCK_DMODEL, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "    q = tl.load(Q_block_ptr)",
      "    q = (q * qk_scale).to(MATMUL_PRECISION)",
      "",
      "    lo = 0",
      "    hi = N_CTX",
      "    for start_n in range(lo, hi, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        k = tl.load(K_block_ptr)",
      "        v = tl.load(V_block_ptr)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k.to(MATMUL_PRECISION))",
      "",
      "        tmp0 = tl.full([1], 1024, tl.int64)",
      "        tmp1 = (offs_m[:, None]) <= tmp0",
      "        tmp2 = (start_n + offs_n[None, :]) <= tmp0",
      "        tmp3 = tmp1 & tmp2",
      "        tmp4 = (offs_m[:, None]) >= (start_n + offs_n[None, :])",
      "        tmp5 = tmp3 | tmp4",
      "        tmp6 = float(\"-inf\")",
      "        tmp7 = tmp6.to(tl.float32)",
      "        tmp8 = tl.where(tmp5, (qk), tmp7)",
      "        qk = tmp8",
      "",
      "        row_max = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, row_max)",
      "        masked_out_rows = m_i_new == float(\"-inf\")",
      "",
      "        alpha = tl.math.exp(m_i - m_i_new)",
      "        alpha = tl.where(masked_out_rows, 0, alpha)",
      "        p = tl.math.exp(qk - m_i_new[:, None])",
      "        p = tl.where(masked_out_rows[:, None], 0, p)",
      "",
      "        acc_scale = l_i * 0 + alpha",
      "        acc *= acc_scale[:, None]",
      "        acc += tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION))",
      "",
      "        l_i = l_i * alpha + tl.sum(p, 1)",
      "        m_i = m_i_new",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))",
      "",
      "    acc = acc / l_i[:, None]",
      "",
      "    idx_z = tl.program_id(1) // H",
      "    idx_h = tl.program_id(1) % H",
      "    idx_m = offs_m[:, None]",
      "    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]",
      "",
      "    mask = (idx_m != -1) & (idx_d != -1)",
      "    xindex = idx_d + (64 * idx_m) + (262144 * idx_h) + (4194304 * idx_z)",
      "    tl.store(out_ptr0 + (xindex), acc, None)"
    ],
    "file": "codes/678.py",
    "header": "def triton_tem_fused_no_exp2(arg_Q, arg_K, arg_V, out_ptr0, num_queries: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):",
    "body": "Q = arg_Q\nK = arg_K\nV = arg_V\nstride_qz = 4194304\nstride_qh = 262144\nstride_qm = 64\nstride_qk = 1\nstride_kz = 4194304\nstride_kh = 262144\nstride_kn = 64\nstride_kk = 1\nstride_vz = 4194304\nstride_vh = 262144\nstride_vk = 64\nstride_vn = 1\nZ = 16\nH = 16\nN_CTX = 4096\nqk_scale = 1.0\nMATMUL_PRECISION = tl.float16\nstart_m = tl.program_id(0)\noff_hz = tl.program_id(1)\nqkv_offset = off_hz * stride_qh\nQ_block_ptr = tl.make_block_ptr(base=Q + qkv_offset, shape=(N_CTX, BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\nK_block_ptr = tl.make_block_ptr(base=K + qkv_offset, shape=(BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\nV_block_ptr = tl.make_block_ptr(base=V + qkv_offset, shape=(N_CTX, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nl_i = tl.zeros([BLOCK_M], dtype=tl.float32)\nacc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\nq = tl.load(Q_block_ptr)\nq = (q * qk_scale).to(MATMUL_PRECISION)\nlo = 0\nhi = N_CTX\nfor start_n in range(lo, hi, BLOCK_N):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk += tl.dot(q, k.to(MATMUL_PRECISION))\n    tmp0 = tl.full([1], 1024, tl.int64)\n    tmp1 = offs_m[:, None] <= tmp0\n    tmp2 = start_n + offs_n[None, :] <= tmp0\n    tmp3 = tmp1 & tmp2\n    tmp4 = offs_m[:, None] >= start_n + offs_n[None, :]\n    tmp5 = tmp3 | tmp4\n    tmp6 = float('-inf')\n    tmp7 = tmp6.to(tl.float32)\n    tmp8 = tl.where(tmp5, qk, tmp7)\n    qk = tmp8\n    row_max = tl.max(qk, 1)\n    m_i_new = tl.maximum(m_i, row_max)\n    masked_out_rows = m_i_new == float('-inf')\n    alpha = tl.math.exp(m_i - m_i_new)\n    alpha = tl.where(masked_out_rows, 0, alpha)\n    p = tl.math.exp(qk - m_i_new[:, None])\n    p = tl.where(masked_out_rows[:, None], 0, p)\n    acc_scale = l_i * 0 + alpha\n    acc *= acc_scale[:, None]\n    acc += tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION))\n    l_i = l_i * alpha + tl.sum(p, 1)\n    m_i = m_i_new\n    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\nacc = acc / l_i[:, None]\nidx_z = tl.program_id(1) // H\nidx_h = tl.program_id(1) % H\nidx_m = offs_m[:, None]\nidx_d = tl.arange(0, BLOCK_DMODEL)[None, :]\nmask = (idx_m != -1) & (idx_d != -1)\nxindex = idx_d + 64 * idx_m + 262144 * idx_h + 4194304 * idx_z\ntl.store(out_ptr0 + xindex, acc, None)"
  },
  {
    "name": "triton_tem_fused_with_exp2",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_DMODEL': 64}, num_stages=3, num_warps=4)], key=['num_queries'])"
    ],
    "args": [
      {
        "name": "arg_Q",
        "annotation": null
      },
      {
        "name": "arg_K",
        "annotation": null
      },
      {
        "name": "arg_V",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "num_queries",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_tem_fused_with_exp2(",
      "    arg_Q,",
      "    arg_K,",
      "    arg_V,",
      "    out_ptr0,",
      "    num_queries: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "):",
      "",
      "    SCORE_MOD_IS_LINEAR: tl.constexpr = False",
      "    ROWS_GUARANTEED_SAFE: tl.constexpr = False",
      "    Q = arg_Q",
      "    K = arg_K",
      "    V = arg_V",
      "",
      "    stride_qz = 4194304",
      "    stride_qh = 262144",
      "    stride_qm = 64",
      "    stride_qk = 1",
      "",
      "    stride_kz = 4194304",
      "    stride_kh = 262144",
      "    stride_kn = 64",
      "    stride_kk = 1",
      "",
      "    stride_vz = 4194304",
      "    stride_vh = 262144",
      "    stride_vk = 64",
      "    stride_vn = 1",
      "",
      "    Z = 16",
      "    H = 16",
      "    N_CTX = 4096",
      "",
      "    qk_scale = 1.0",
      "    MATMUL_PRECISION = Q.dtype.element_ty",
      "",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    qkv_offset = off_hz * stride_qh",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qkv_offset,",
      "        shape=(BLOCK_DMODEL, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "    q = tl.load(Q_block_ptr)",
      "    if SCORE_MOD_IS_LINEAR:",
      "        qk_scale *= 1.44269504",
      "    q = (q * qk_scale).to(MATMUL_PRECISION)",
      "",
      "    lo = 0",
      "    hi = N_CTX",
      "    for start_n in range(lo, hi, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        k = tl.load(K_block_ptr)",
      "        v = tl.load(V_block_ptr)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk = tl.dot(q, k.to(MATMUL_PRECISION), acc=qk)",
      "",
      "        tmp0 = tl.full([1], 1024, tl.int64)",
      "        tmp1 = (offs_m[:, None]) <= tmp0",
      "        tmp2 = (start_n + offs_n[None, :]) <= tmp0",
      "        tmp3 = tmp1 & tmp2",
      "        tmp4 = (offs_m[:, None]) >= (start_n + offs_n[None, :])",
      "        tmp5 = tmp3 | tmp4",
      "        tmp6 = float(\"-inf\")",
      "        tmp7 = tmp6.to(tl.float32)",
      "        tmp8 = tl.where(tmp5, (qk), tmp7)",
      "        qk = tmp8",
      "",
      "        if not SCORE_MOD_IS_LINEAR:",
      "            qk *= 1.44269504",
      "",
      "        row_max = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, row_max)",
      "        masked_out_rows = m_i_new == float(\"-inf\")",
      "",
      "        alpha = tl.math.exp2(m_i - m_i_new)",
      "        p = tl.math.exp2(qk - m_i_new[:, None])",
      "        if not ROWS_GUARANTEED_SAFE:",
      "            alpha = tl.where(masked_out_rows, 0, alpha)",
      "            p = tl.where(masked_out_rows[:, None], 0, p)",
      "",
      "        acc_scale = l_i * 0 + alpha",
      "        acc *= acc_scale[:, None]",
      "        acc = tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION), acc)",
      "",
      "        l_i = l_i * alpha + tl.sum(p, 1)",
      "        m_i = m_i_new",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))",
      "",
      "    acc = acc / l_i[:, None]",
      "",
      "    idx_z = tl.program_id(1) // H",
      "    idx_h = tl.program_id(1) % H",
      "    idx_m = offs_m[:, None]",
      "    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]",
      "",
      "    mask = (idx_m != -1) & (idx_d != -1)",
      "    xindex = idx_d + (64 * idx_m) + (262144 * idx_h) + (4194304 * idx_z)",
      "    tl.store(out_ptr0 + (xindex), acc, None)"
    ],
    "file": "codes/678.py",
    "header": "def triton_tem_fused_with_exp2(arg_Q, arg_K, arg_V, out_ptr0, num_queries: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_DMODEL: tl.constexpr):",
    "body": "SCORE_MOD_IS_LINEAR: tl.constexpr = False\nROWS_GUARANTEED_SAFE: tl.constexpr = False\nQ = arg_Q\nK = arg_K\nV = arg_V\nstride_qz = 4194304\nstride_qh = 262144\nstride_qm = 64\nstride_qk = 1\nstride_kz = 4194304\nstride_kh = 262144\nstride_kn = 64\nstride_kk = 1\nstride_vz = 4194304\nstride_vh = 262144\nstride_vk = 64\nstride_vn = 1\nZ = 16\nH = 16\nN_CTX = 4096\nqk_scale = 1.0\nMATMUL_PRECISION = Q.dtype.element_ty\nstart_m = tl.program_id(0)\noff_hz = tl.program_id(1)\nqkv_offset = off_hz * stride_qh\nQ_block_ptr = tl.make_block_ptr(base=Q + qkv_offset, shape=(N_CTX, BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\nK_block_ptr = tl.make_block_ptr(base=K + qkv_offset, shape=(BLOCK_DMODEL, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\nV_block_ptr = tl.make_block_ptr(base=V + qkv_offset, shape=(N_CTX, BLOCK_DMODEL), strides=(stride_vk, stride_vn), offsets=(0, 0), block_shape=(BLOCK_N, BLOCK_DMODEL), order=(1, 0))\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_n = tl.arange(0, BLOCK_N)\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nl_i = tl.zeros([BLOCK_M], dtype=tl.float32)\nacc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)\nq = tl.load(Q_block_ptr)\nif SCORE_MOD_IS_LINEAR:\n    qk_scale *= 1.44269504\nq = (q * qk_scale).to(MATMUL_PRECISION)\nlo = 0\nhi = N_CTX\nfor start_n in range(lo, hi, BLOCK_N):\n    start_n = tl.multiple_of(start_n, BLOCK_N)\n    k = tl.load(K_block_ptr)\n    v = tl.load(V_block_ptr)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk = tl.dot(q, k.to(MATMUL_PRECISION), acc=qk)\n    tmp0 = tl.full([1], 1024, tl.int64)\n    tmp1 = offs_m[:, None] <= tmp0\n    tmp2 = start_n + offs_n[None, :] <= tmp0\n    tmp3 = tmp1 & tmp2\n    tmp4 = offs_m[:, None] >= start_n + offs_n[None, :]\n    tmp5 = tmp3 | tmp4\n    tmp6 = float('-inf')\n    tmp7 = tmp6.to(tl.float32)\n    tmp8 = tl.where(tmp5, qk, tmp7)\n    qk = tmp8\n    if not SCORE_MOD_IS_LINEAR:\n        qk *= 1.44269504\n    row_max = tl.max(qk, 1)\n    m_i_new = tl.maximum(m_i, row_max)\n    masked_out_rows = m_i_new == float('-inf')\n    alpha = tl.math.exp2(m_i - m_i_new)\n    p = tl.math.exp2(qk - m_i_new[:, None])\n    if not ROWS_GUARANTEED_SAFE:\n        alpha = tl.where(masked_out_rows, 0, alpha)\n        p = tl.where(masked_out_rows[:, None], 0, p)\n    acc_scale = l_i * 0 + alpha\n    acc *= acc_scale[:, None]\n    acc = tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION), acc)\n    l_i = l_i * alpha + tl.sum(p, 1)\n    m_i = m_i_new\n    K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))\n    V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))\nacc = acc / l_i[:, None]\nidx_z = tl.program_id(1) // H\nidx_h = tl.program_id(1) % H\nidx_m = offs_m[:, None]\nidx_d = tl.arange(0, BLOCK_DMODEL)[None, :]\nmask = (idx_m != -1) & (idx_d != -1)\nxindex = idx_d + 64 * idx_m + 262144 * idx_h + 4194304 * idx_z\ntl.store(out_ptr0 + xindex, acc, None)"
  },
  {
    "name": "_rope_embedding",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'BACKWARD_PASS': lambda args: bool(args['BACKWARD_PASS'])})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "Q_row_stride",
        "annotation": null
      },
      {
        "name": "cos",
        "annotation": null
      },
      {
        "name": "cos_row_stride",
        "annotation": null
      },
      {
        "name": "sin",
        "annotation": null
      },
      {
        "name": "sin_row_stride",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_heads",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BACKWARD_PASS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rope_embedding(",
      "    Q,",
      "    Q_row_stride,",
      "    cos,",
      "    cos_row_stride,",
      "    sin,",
      "    sin_row_stride,",
      "    seqlen,",
      "    head_dim: tl.constexpr,",
      "    n_heads: tl.constexpr,",
      "    BACKWARD_PASS: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    ROPE_GROUP_SIZE = 4",
      "    row_position = tl.program_id(0)",
      "    group_head_position = tl.program_id(1)",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    half_head_dim = head_dim // 2",
      "    mask = col_offsets < half_head_dim",
      "",
      "    sin1 = tl.load(",
      "        sin",
      "        + (row_position % seqlen) * sin_row_stride",
      "        + half_head_dim * 0",
      "        + col_offsets,",
      "        mask=mask,",
      "        other=0,",
      "    )",
      "    cos1 = tl.load(",
      "        cos",
      "        + (row_position % seqlen) * cos_row_stride",
      "        + half_head_dim * 0",
      "        + col_offsets,",
      "        mask=mask,",
      "        other=0,",
      "    )",
      "",
      "    if BACKWARD_PASS:",
      "",
      "        sin1 = -sin1",
      "    pass",
      "",
      "    head_start = group_head_position * ROPE_GROUP_SIZE",
      "    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)",
      "",
      "    for k in range(head_start, head_end):",
      "        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets",
      "        offs_q2 = (",
      "            row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim",
      "        )",
      "",
      "        Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)",
      "        Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)",
      "",
      "        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)",
      "        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)",
      "    pass"
    ],
    "file": "codes/314.py",
    "header": "def _rope_embedding(Q, Q_row_stride, cos, cos_row_stride, sin, sin_row_stride, seqlen, head_dim: tl.constexpr, n_heads: tl.constexpr, BACKWARD_PASS: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "ROPE_GROUP_SIZE = 4\nrow_position = tl.program_id(0)\ngroup_head_position = tl.program_id(1)\ncol_offsets = tl.arange(0, BLOCK_SIZE)\nhalf_head_dim = head_dim // 2\nmask = col_offsets < half_head_dim\nsin1 = tl.load(sin + row_position % seqlen * sin_row_stride + half_head_dim * 0 + col_offsets, mask=mask, other=0)\ncos1 = tl.load(cos + row_position % seqlen * cos_row_stride + half_head_dim * 0 + col_offsets, mask=mask, other=0)\nif BACKWARD_PASS:\n    sin1 = -sin1\npass\nhead_start = group_head_position * ROPE_GROUP_SIZE\nhead_end = min(head_start + ROPE_GROUP_SIZE, n_heads)\nfor k in range(head_start, head_end):\n    offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets\n    offs_q2 = row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim\n    Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)\n    Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)\n    tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)\n    tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)\npass"
  },
  {
    "name": "_paged_attn_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'num_warps': lambda args: get_num_warps(args['QUERY_GROUP_SIZE'], args['HEAD_SIZE'], args['KV_BLOCK_SIZE']), 'num_stages': lambda args: get_num_stages(args['QUERY_GROUP_SIZE'], args['KV_BLOCK_SIZE'])})"
    ],
    "args": [
      {
        "name": "m_i_ptr",
        "annotation": null
      },
      {
        "name": "l_i_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "q_ptr",
        "annotation": null
      },
      {
        "name": "k_cache_ptr",
        "annotation": null
      },
      {
        "name": "v_cache_ptr",
        "annotation": null
      },
      {
        "name": "context_lens_ptr",
        "annotation": null
      },
      {
        "name": "block_tables_ptr",
        "annotation": null
      },
      {
        "name": "attn_scale",
        "annotation": null
      },
      {
        "name": "stride_bt0",
        "annotation": null
      },
      {
        "name": "stride_bt1",
        "annotation": null
      },
      {
        "name": "stride_q0",
        "annotation": null
      },
      {
        "name": "stride_q1",
        "annotation": null
      },
      {
        "name": "stride_q2",
        "annotation": null
      },
      {
        "name": "stride_kv0",
        "annotation": null
      },
      {
        "name": "stride_kv1",
        "annotation": null
      },
      {
        "name": "stride_kv2",
        "annotation": null
      },
      {
        "name": "stride_kv3",
        "annotation": null
      },
      {
        "name": "stride_o0",
        "annotation": null
      },
      {
        "name": "stride_o1",
        "annotation": null
      },
      {
        "name": "stride_o2",
        "annotation": null
      },
      {
        "name": "stride_o3",
        "annotation": null
      },
      {
        "name": "stride_o4",
        "annotation": null
      },
      {
        "name": "HEAD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "QUERY_GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_QUERY_GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_KV_HEADS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "KV_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PARTITION_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _paged_attn_kernel(",
      "    m_i_ptr,",
      "    l_i_ptr,",
      "    out_ptr,",
      "    q_ptr,",
      "    k_cache_ptr,",
      "    v_cache_ptr,",
      "    context_lens_ptr,",
      "    block_tables_ptr,",
      "    attn_scale,",
      "    stride_bt0,",
      "    stride_bt1,",
      "    stride_q0,",
      "    stride_q1,",
      "    stride_q2,",
      "    stride_kv0,",
      "    stride_kv1,",
      "    stride_kv2,",
      "    stride_kv3,",
      "    stride_o0,",
      "    stride_o1,",
      "    stride_o2,",
      "    stride_o3,",
      "    stride_o4,",
      "    HEAD_SIZE: tl.constexpr,",
      "    QUERY_GROUP_SIZE: tl.constexpr,",
      "    PADDED_QUERY_GROUP_SIZE: tl.constexpr,",
      "    NUM_KV_HEADS: tl.constexpr,",
      "    KV_BLOCK_SIZE: tl.constexpr,",
      "    PARTITION_SIZE: tl.constexpr,",
      "):",
      "    seq_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "    part_idx = tl.program_id(2)",
      "    max_num_partitions = tl.num_programs(2)",
      "",
      "    log2e: tl.constexpr = 1.4426950408889634",
      "",
      "    USE_PARTITIONING = PARTITION_SIZE > 0",
      "    context_len = tl.load(context_lens_ptr + seq_idx)",
      "    if USE_PARTITIONING:",
      "        context_start_idx = part_idx * PARTITION_SIZE",
      "        if context_start_idx >= context_len:",
      "            return",
      "        context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE, context_len)",
      "        num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE)",
      "    else:",
      "        num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)",
      "",
      "    block_offset = tl.arange(0, KV_BLOCK_SIZE)",
      "    head_offset = tl.arange(0, HEAD_SIZE)",
      "    padding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)",
      "",
      "    kv_offset = (",
      "        kv_head_idx * stride_kv1",
      "        + block_offset[:, None] * stride_kv2",
      "        + head_offset[None, :] * stride_kv3",
      "    )",
      "",
      "    q_offset = (",
      "        seq_idx * stride_q0",
      "        + (kv_head_idx * QUERY_GROUP_SIZE + padding_group_offset[:, None]) * stride_q1",
      "        + head_offset[None, :] * stride_q2",
      "    )",
      "    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE",
      "",
      "    q = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)",
      "",
      "    m_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)",
      "    acc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)",
      "",
      "    num_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)",
      "    for i in range(num_blocks):",
      "        block_idx = num_prev_blocks + i",
      "        block_number = tl.load(",
      "            block_tables_ptr + seq_idx * stride_bt0 + block_idx * stride_bt1",
      "        )",
      "",
      "        kv_block_offset = block_number * stride_kv0 + kv_offset",
      "        mask_offset = block_idx * KV_BLOCK_SIZE + block_offset",
      "        kv_mask = mask_offset[:, None] < context_len",
      "",
      "        k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)",
      "",
      "        if PADDED_QUERY_GROUP_SIZE == 1:",
      "            qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)",
      "        else:",
      "            qk = tl.dot(q, k.T, out_dtype=tl.float32)",
      "",
      "        qk *= attn_scale",
      "        qk = tl.where(mask_offset < context_len, qk, float(\"-inf\"))",
      "",
      "        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))",
      "",
      "        p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)",
      "        alpha = tl.math.exp2((m_i - m_i_new) * log2e)",
      "        acc *= alpha[:, None]",
      "",
      "        v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)",
      "",
      "        if PADDED_QUERY_GROUP_SIZE == 1:",
      "            acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)",
      "        else:",
      "            p = p.to(v.dtype)",
      "            acc += tl.dot(p, v, out_dtype=tl.float32)",
      "",
      "        l_i = l_i * alpha + tl.sum(p, axis=1)",
      "        m_i = m_i_new",
      "    acc = acc / l_i[:, None]",
      "",
      "    if USE_PARTITIONING:",
      "        part_offset = (",
      "            (seq_idx * NUM_KV_HEADS + kv_head_idx)",
      "            * max_num_partitions",
      "            * QUERY_GROUP_SIZE",
      "            + part_idx * QUERY_GROUP_SIZE",
      "            + padding_group_offset",
      "        )",
      "        mask = padding_group_offset < QUERY_GROUP_SIZE",
      "        tl.store(m_i_ptr + part_offset, m_i, mask=mask)",
      "        tl.store(l_i_ptr + part_offset, l_i, mask=mask)",
      "",
      "    out_offset = seq_idx * stride_o0",
      "    if USE_PARTITIONING:",
      "        out_offset += kv_head_idx * stride_o1",
      "    else:",
      "        out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1",
      "    out_offset += (",
      "        part_idx * stride_o2",
      "        + padding_group_offset[:, None] * stride_o3",
      "        + head_offset[None, :] * stride_o4",
      "    )",
      "",
      "    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE",
      "    tl.store(out_ptr + out_offset, acc, mask=group_mask)"
    ],
    "file": "codes/102.py",
    "header": "def _paged_attn_kernel(m_i_ptr, l_i_ptr, out_ptr, q_ptr, k_cache_ptr, v_cache_ptr, context_lens_ptr, block_tables_ptr, attn_scale, stride_bt0, stride_bt1, stride_q0, stride_q1, stride_q2, stride_kv0, stride_kv1, stride_kv2, stride_kv3, stride_o0, stride_o1, stride_o2, stride_o3, stride_o4, HEAD_SIZE: tl.constexpr, QUERY_GROUP_SIZE: tl.constexpr, PADDED_QUERY_GROUP_SIZE: tl.constexpr, NUM_KV_HEADS: tl.constexpr, KV_BLOCK_SIZE: tl.constexpr, PARTITION_SIZE: tl.constexpr):",
    "body": "seq_idx = tl.program_id(0)\nkv_head_idx = tl.program_id(1)\npart_idx = tl.program_id(2)\nmax_num_partitions = tl.num_programs(2)\nlog2e: tl.constexpr = 1.4426950408889634\nUSE_PARTITIONING = PARTITION_SIZE > 0\ncontext_len = tl.load(context_lens_ptr + seq_idx)\nif USE_PARTITIONING:\n    context_start_idx = part_idx * PARTITION_SIZE\n    if context_start_idx >= context_len:\n        return\n    context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE, context_len)\n    num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE)\nelse:\n    num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)\nblock_offset = tl.arange(0, KV_BLOCK_SIZE)\nhead_offset = tl.arange(0, HEAD_SIZE)\npadding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)\nkv_offset = kv_head_idx * stride_kv1 + block_offset[:, None] * stride_kv2 + head_offset[None, :] * stride_kv3\nq_offset = seq_idx * stride_q0 + (kv_head_idx * QUERY_GROUP_SIZE + padding_group_offset[:, None]) * stride_q1 + head_offset[None, :] * stride_q2\ngroup_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\nq = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)\nm_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float('inf')\nl_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)\nacc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)\nnum_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)\nfor i in range(num_blocks):\n    block_idx = num_prev_blocks + i\n    block_number = tl.load(block_tables_ptr + seq_idx * stride_bt0 + block_idx * stride_bt1)\n    kv_block_offset = block_number * stride_kv0 + kv_offset\n    mask_offset = block_idx * KV_BLOCK_SIZE + block_offset\n    kv_mask = mask_offset[:, None] < context_len\n    k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n    if PADDED_QUERY_GROUP_SIZE == 1:\n        qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)\n    else:\n        qk = tl.dot(q, k.T, out_dtype=tl.float32)\n    qk *= attn_scale\n    qk = tl.where(mask_offset < context_len, qk, float('-inf'))\n    m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n    p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)\n    alpha = tl.math.exp2((m_i - m_i_new) * log2e)\n    acc *= alpha[:, None]\n    v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)\n    if PADDED_QUERY_GROUP_SIZE == 1:\n        acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)\n    else:\n        p = p.to(v.dtype)\n        acc += tl.dot(p, v, out_dtype=tl.float32)\n    l_i = l_i * alpha + tl.sum(p, axis=1)\n    m_i = m_i_new\nacc = acc / l_i[:, None]\nif USE_PARTITIONING:\n    part_offset = (seq_idx * NUM_KV_HEADS + kv_head_idx) * max_num_partitions * QUERY_GROUP_SIZE + part_idx * QUERY_GROUP_SIZE + padding_group_offset\n    mask = padding_group_offset < QUERY_GROUP_SIZE\n    tl.store(m_i_ptr + part_offset, m_i, mask=mask)\n    tl.store(l_i_ptr + part_offset, l_i, mask=mask)\nout_offset = seq_idx * stride_o0\nif USE_PARTITIONING:\n    out_offset += kv_head_idx * stride_o1\nelse:\n    out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1\nout_offset += part_idx * stride_o2 + padding_group_offset[:, None] * stride_o3 + head_offset[None, :] * stride_o4\ngroup_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE\ntl.store(out_ptr + out_offset, acc, mask=group_mask)"
  },
  {
    "name": "chunk_dplr_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BK', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "ag",
        "annotation": null
      },
      {
        "name": "bg",
        "annotation": null
      },
      {
        "name": "Aqk",
        "annotation": null
      },
      {
        "name": "Aqb",
        "annotation": null
      },
      {
        "name": "Aab",
        "annotation": null
      },
      {
        "name": "Aak",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GATHER_SUPPORTED",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    a,",
      "    b,",
      "    gi,",
      "    ge,",
      "    qg,",
      "    kg,",
      "    ag,",
      "    bg,",
      "    Aqk,",
      "    Aqb,",
      "    Aab,",
      "    Aak,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    GATHER_SUPPORTED: tl.constexpr,",
      "):",
      "    i_t, i_b, i_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + tl.arange(0, BC)) < T",
      "    last_idx = min((i_t + 1) * BT, T) - 1",
      "    o_A = (bos + i_t * BT + tl.arange(0, BC)) * H * BT + i_h * BT",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_a = tl.make_block_ptr(",
      "        a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_b = tl.make_block_ptr(",
      "        b + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_gi = tl.make_block_ptr(",
      "        gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_ge = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_g_last = gi + (bos * H + i_h) * K + last_idx * H * K + tl.arange(0, BK)",
      "    b_g_last = tl.load(p_g_last, mask=m_k, other=0)",
      "    p_qg = tl.make_block_ptr(",
      "        qg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_kg = tl.make_block_ptr(",
      "        kg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_ag = tl.make_block_ptr(",
      "        ag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_bg = tl.make_block_ptr(",
      "        bg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = b_q * scale",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_a = tl.load(p_a, boundary_check=(0, 1))",
      "    b_b = tl.load(p_b, boundary_check=(0, 1))",
      "    b_gi = tl.load(p_gi, boundary_check=(0, 1)).to(tl.float32)",
      "    b_ge = tl.load(p_ge, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    g_exp = exp(b_gi)",
      "    g_exp_inv = exp(-b_gi + b_g_last[None, :])",
      "    b_qg = b_q * g_exp",
      "    b_kg = b_k * g_exp_inv",
      "    b_bg = b_b * g_exp_inv",
      "    b_ag = b_a * exp(b_ge)",
      "    tl.store(",
      "        p_qg,",
      "        b_qg.to(p_qg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_bg,",
      "        b_bg.to(p_bg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_ag,",
      "        b_ag.to(p_ag.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_kg,",
      "        b_kg.to(p_kg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    b_q = b_q.to(b_k.dtype)",
      "",
      "    for j in range(0, min(BC, T - i_t * BT)):",
      "",
      "        if GATHER_SUPPORTED:",
      "            row_idx = tl.full([1, BK], j, dtype=tl.int16)",
      "",
      "            b_k_j = gather(b_k, row_idx, axis=0)",
      "            b_gk_j = gather(b_gi, row_idx, axis=0)",
      "            b_b_j = gather(b_b, row_idx, axis=0)",
      "        else:",
      "            mask = tl.arange(0, BC) == j",
      "            b_k_j = tl.sum(tl.where(mask[:, None], b_k, 0), 0)[None, :]",
      "            b_gk_j = tl.sum(tl.where(mask[:, None], b_gi, 0), 0)[None, :]",
      "            b_b_j = tl.sum(tl.where(mask[:, None], b_b, 0), 0)[None, :]",
      "        tmp = exp(b_gi - b_gk_j)",
      "        b_A_qk = tl.sum(b_q * b_k_j * tmp, 1)",
      "        m_i = (o_i >= j).to(tl.float32)",
      "        b_A_qk = b_A_qk * m_i",
      "        b_A_qb = tl.sum(b_q * b_b_j * tmp, 1)",
      "        b_A_qb = b_A_qb * m_i",
      "        tmp2 = exp(b_ge - b_gk_j)",
      "        b_A_ak = tl.sum(b_a * b_k_j * tmp2, 1)",
      "        m_i2 = (o_i > j).to(tl.float32)",
      "        b_A_ak = b_A_ak * m_i2",
      "        b_A_ab = tl.sum(b_a * b_b_j * tmp2, 1)",
      "        b_A_ab = b_A_ab * m_i2",
      "",
      "        tl.store(",
      "            Aqk + o_A + j,",
      "            b_A_qk.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )",
      "        tl.store(",
      "            Aqb + o_A + j,",
      "            b_A_qb.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )",
      "        tl.store(",
      "            Aab + o_A + j,",
      "            b_A_ab.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )",
      "        tl.store(",
      "            Aak + o_A + j,",
      "            b_A_ak.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )"
    ],
    "file": "codes/382.py",
    "header": "def chunk_dplr_fwd_A_kernel_intra_sub_intra(q, k, a, b, gi, ge, qg, kg, ag, bg, Aqk, Aqb, Aab, Aak, cu_seqlens, chunk_indices, scale: tl.constexpr, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr, GATHER_SUPPORTED: tl.constexpr):",
    "body": "i_t, i_b, i_h = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nif i_t * BT >= T:\n    return\no_i = tl.arange(0, BC)\no_k = tl.arange(0, BK)\nm_k = o_k < K\nm_A = i_t * BT + tl.arange(0, BC) < T\nlast_idx = min((i_t + 1) * BT, T) - 1\no_A = (bos + i_t * BT + tl.arange(0, BC)) * H * BT + i_h * BT\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_a = tl.make_block_ptr(a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_b = tl.make_block_ptr(b + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_gi = tl.make_block_ptr(gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_ge = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_g_last = gi + (bos * H + i_h) * K + last_idx * H * K + tl.arange(0, BK)\nb_g_last = tl.load(p_g_last, mask=m_k, other=0)\np_qg = tl.make_block_ptr(qg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_kg = tl.make_block_ptr(kg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_ag = tl.make_block_ptr(ag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\np_bg = tl.make_block_ptr(bg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = b_q * scale\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_a = tl.load(p_a, boundary_check=(0, 1))\nb_b = tl.load(p_b, boundary_check=(0, 1))\nb_gi = tl.load(p_gi, boundary_check=(0, 1)).to(tl.float32)\nb_ge = tl.load(p_ge, boundary_check=(0, 1)).to(tl.float32)\ng_exp = exp(b_gi)\ng_exp_inv = exp(-b_gi + b_g_last[None, :])\nb_qg = b_q * g_exp\nb_kg = b_k * g_exp_inv\nb_bg = b_b * g_exp_inv\nb_ag = b_a * exp(b_ge)\ntl.store(p_qg, b_qg.to(p_qg.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_bg, b_bg.to(p_bg.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_ag, b_ag.to(p_ag.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_kg, b_kg.to(p_kg.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\nb_q = b_q.to(b_k.dtype)\nfor j in range(0, min(BC, T - i_t * BT)):\n    if GATHER_SUPPORTED:\n        row_idx = tl.full([1, BK], j, dtype=tl.int16)\n        b_k_j = gather(b_k, row_idx, axis=0)\n        b_gk_j = gather(b_gi, row_idx, axis=0)\n        b_b_j = gather(b_b, row_idx, axis=0)\n    else:\n        mask = tl.arange(0, BC) == j\n        b_k_j = tl.sum(tl.where(mask[:, None], b_k, 0), 0)[None, :]\n        b_gk_j = tl.sum(tl.where(mask[:, None], b_gi, 0), 0)[None, :]\n        b_b_j = tl.sum(tl.where(mask[:, None], b_b, 0), 0)[None, :]\n    tmp = exp(b_gi - b_gk_j)\n    b_A_qk = tl.sum(b_q * b_k_j * tmp, 1)\n    m_i = (o_i >= j).to(tl.float32)\n    b_A_qk = b_A_qk * m_i\n    b_A_qb = tl.sum(b_q * b_b_j * tmp, 1)\n    b_A_qb = b_A_qb * m_i\n    tmp2 = exp(b_ge - b_gk_j)\n    b_A_ak = tl.sum(b_a * b_k_j * tmp2, 1)\n    m_i2 = (o_i > j).to(tl.float32)\n    b_A_ak = b_A_ak * m_i2\n    b_A_ab = tl.sum(b_a * b_b_j * tmp2, 1)\n    b_A_ab = b_A_ab * m_i2\n    tl.store(Aqk + o_A + j, b_A_qk.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding='rtne'), mask=m_A)\n    tl.store(Aqb + o_A + j, b_A_qb.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding='rtne'), mask=m_A)\n    tl.store(Aab + o_A + j, b_A_ab.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding='rtne'), mask=m_A)\n    tl.store(Aak + o_A + j, b_A_ak.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding='rtne'), mask=m_A)"
  },
  {
    "name": "chunk_fwd_kernel_h_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "hs",
        "annotation": null
      },
      {
        "name": "hr",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_split(",
      "    k,",
      "    v,",
      "    g,",
      "    gk,",
      "    gv,",
      "    hs,",
      "    hr,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    split_indices,",
      "    T,",
      "    S: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "",
      "    i_k, i_v, i_sh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_ss, i_h = i_sh // H, i_sh % H",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "    i_nh = i_n * H + i_h",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if i_s == 0:",
      "        if USE_INITIAL_STATE:",
      "            p_h0 = tl.make_block_ptr(",
      "                h0 + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_h += tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "        p_hr = tl.make_block_ptr(",
      "            hr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))",
      "    for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        if USE_G:",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_h *= exp(b_g_last)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_h *= exp(b_gk_last)[:, None]",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_h *= exp(b_gv_last)[None, :]",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "        b_h += tl.dot(b_k, b_v)",
      "",
      "    if NS > 1:",
      "        p_hs = tl.make_block_ptr(",
      "            hs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_hs, b_h.to(p_hs.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/371.py",
    "header": "def chunk_fwd_kernel_h_split(k, v, g, gk, gv, hs, hr, h0, ht, cu_seqlens, split_indices, T, S: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_sh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_ss, i_h = (i_sh // H, i_sh % H)\nif IS_VARLEN:\n    i_n, i_s = (tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(split_indices + i_ss * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NS = tl.cdiv(T, S)\nelse:\n    NS = tl.cdiv(T, S)\n    i_n, i_s = (i_ss // NS, i_ss % NS)\n    bos, eos = (i_n * T, i_n * T + T)\ni_nh = i_n * H + i_h\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif i_s == 0:\n    if USE_INITIAL_STATE:\n        p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\n    p_hr = tl.make_block_ptr(hr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))\nfor i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    last_idx = min(i_t * BT + BT, T) - 1\n    if USE_G:\n        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_h *= exp(b_g_last)\n        b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n        b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\n    if USE_GK:\n        p_gk = tl.make_block_ptr(gk + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n        b_h *= exp(b_gk_last)[:, None]\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\n    if USE_GV:\n        p_gv = tl.make_block_ptr(gv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n        b_h *= exp(b_gv_last)[None, :]\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\n    b_h += tl.dot(b_k, b_v)\nif NS > 1:\n    p_hs = tl.make_block_ptr(hs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_hs, b_h.to(p_hs.dtype.element_ty), boundary_check=(0, 1))\nelif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_fwd_kernel_h_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "hs",
        "annotation": null
      },
      {
        "name": "hr",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_reduction(",
      "    g,",
      "    gk,",
      "    gv,",
      "    hs,",
      "    hr,",
      "    ht,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    S: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NS = tl.cdiv(T, S)",
      "        boh = i_n * NS",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for i_s in range(1, NS):",
      "        p_hs = tl.make_block_ptr(",
      "            hs + ((boh + i_s - 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_hr = tl.make_block_ptr(",
      "            hr + ((boh + i_s) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):",
      "            last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "            if USE_G:",
      "                b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "                b_h *= exp(b_g_last)",
      "",
      "            if USE_GK:",
      "                p_gk_last = (",
      "                    gk",
      "                    + (bos + last_idx) * H * K",
      "                    + i_h * K",
      "                    + i_k * BK",
      "                    + tl.arange(0, BK)",
      "                )",
      "                b_gk_last = tl.load(",
      "                    p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "                )",
      "                b_h *= exp(b_gk_last)[:, None]",
      "",
      "            if USE_GV:",
      "                p_gv_last = (",
      "                    gv",
      "                    + (bos + last_idx) * H * V",
      "                    + i_h * V",
      "                    + i_v * BV",
      "                    + tl.arange(0, BV)",
      "                )",
      "                b_gv_last = tl.load(",
      "                    p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "                )",
      "                b_h *= exp(b_gv_last)[None, :]",
      "",
      "    if NS > 1:",
      "        if STORE_FINAL_STATE:",
      "            p_hs = tl.make_block_ptr(",
      "                hs + ((boh + NS - 1) * H + i_h) * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            p_ht = tl.make_block_ptr(",
      "                ht + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)",
      "            tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/371.py",
    "header": "def chunk_fwd_kernel_h_reduction(g, gk, gv, hs, hr, ht, cu_seqlens, split_offsets, T, S: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NS = tl.cdiv(T, S)\n    boh = tl.load(split_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NS = tl.cdiv(T, S)\n    boh = i_n * NS\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nfor i_s in range(1, NS):\n    p_hs = tl.make_block_ptr(hs + ((boh + i_s - 1) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_hr = tl.make_block_ptr(hr + ((boh + i_s) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)\n    tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))\n    for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):\n        last_idx = min(i_t * BT + BT, T) - 1\n        if USE_G:\n            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n            b_h *= exp(b_g_last)\n        if USE_GK:\n            p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n            b_h *= exp(b_gk_last)[:, None]\n        if USE_GV:\n            p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n            b_h *= exp(b_gv_last)[None, :]\nif NS > 1:\n    if STORE_FINAL_STATE:\n        p_hs = tl.make_block_ptr(hs + ((boh + NS - 1) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)\n        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dh_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dhs",
        "annotation": null
      },
      {
        "name": "dhr",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_split(",
      "    q,",
      "    g,",
      "    gk,",
      "    gv,",
      "    do,",
      "    dht,",
      "    dhs,",
      "    dhr,",
      "    dh0,",
      "    cu_seqlens,",
      "    split_indices,",
      "    scale,",
      "    T,",
      "    S: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "",
      "    i_k, i_v, i_sh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_ss, i_hq = i_sh // HQ, i_sh % HQ",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "    i_nh = i_n * HQ + i_hq",
      "    i_h = i_hq // NG",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if i_s == NS - 1:",
      "        if USE_FINAL_STATE_GRADIENT:",
      "            p_dht = tl.make_block_ptr(",
      "                dht + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "        p_dhr = tl.make_block_ptr(",
      "            dhr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_t in range(",
      "        tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1",
      "    ):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (K, T),",
      "            (1, HQ * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "        if USE_G:",
      "            p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)",
      "            b_dh *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_q = (b_q * exp(b_gk)).to(b_q.dtype)",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_dh *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_do = (b_do * exp(b_gv)).to(b_do.dtype)",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_dh *= exp(b_gv_last)[None, :]",
      "",
      "        b_dh += tl.dot(b_q, b_do)",
      "",
      "    if NS > 1:",
      "        p_dhs = tl.make_block_ptr(",
      "            dhs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dhs, b_dh.to(p_dhs.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/371.py",
    "header": "def chunk_bwd_kernel_dh_split(q, g, gk, gv, do, dht, dhs, dhr, dh0, cu_seqlens, split_indices, scale, T, S: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_sh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_ss, i_hq = (i_sh // HQ, i_sh % HQ)\nif IS_VARLEN:\n    i_n, i_s = (tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(split_indices + i_ss * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NS = tl.cdiv(T, S)\nelse:\n    NS = tl.cdiv(T, S)\n    i_n, i_s = (i_ss // NS, i_ss % NS)\n    bos, eos = (i_n * T, i_n * T + T)\ni_nh = i_n * HQ + i_hq\ni_h = i_hq // NG\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif i_s == NS - 1:\n    if USE_FINAL_STATE_GRADIENT:\n        p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\n    p_dhr = tl.make_block_ptr(dhr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))\nfor i_t in range(tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1):\n    p_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (K, T), (1, HQ * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    last_idx = min(i_t * BT + BT, T) - 1\n    if USE_G:\n        p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n        b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n        b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)\n        b_dh *= exp(b_g_last)\n    if USE_GK:\n        p_gk = tl.make_block_ptr(gk + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = (b_q * exp(b_gk)).to(b_q.dtype)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n        b_dh *= exp(b_gk_last)[:, None]\n    if USE_GV:\n        p_gv = tl.make_block_ptr(gv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_gv)).to(b_do.dtype)\n        b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n        b_dh *= exp(b_gv_last)[None, :]\n    b_dh += tl.dot(b_q, b_do)\nif NS > 1:\n    p_dhs = tl.make_block_ptr(dhs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dhs, b_dh.to(p_dhs.dtype.element_ty), boundary_check=(0, 1))\nelif STORE_INITIAL_STATE_GRADIENT:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dh_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "dhs",
        "annotation": null
      },
      {
        "name": "dhr",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_reduction(",
      "    g,",
      "    gk,",
      "    gv,",
      "    dhs,",
      "    dhr,",
      "    dh0,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    S: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NS = tl.cdiv(T, S)",
      "        boh = i_n * NS",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_s in range(NS - 2, -1, -1):",
      "        p_dhs = tl.make_block_ptr(",
      "            dhs + ((boh + i_s + 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dhr = tl.make_block_ptr(",
      "            dhr + ((boh + i_s) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        for i_t in range(",
      "            tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1",
      "        ):",
      "            last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "            if USE_G:",
      "                b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "                b_dh *= exp(b_g_last)",
      "",
      "            if USE_GK:",
      "                p_gk_last = (",
      "                    gk",
      "                    + (bos + last_idx) * H * K",
      "                    + i_h * K",
      "                    + i_k * BK",
      "                    + tl.arange(0, BK)",
      "                )",
      "                b_gk_last = tl.load(",
      "                    p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "                )",
      "                b_dh *= exp(b_gk_last)[:, None]",
      "",
      "            if USE_GV:",
      "                p_gv_last = (",
      "                    gv",
      "                    + (bos + last_idx) * H * V",
      "                    + i_h * V",
      "                    + i_v * BV",
      "                    + tl.arange(0, BV)",
      "                )",
      "                b_gv_last = tl.load(",
      "                    p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "                )",
      "                b_dh *= exp(b_gv_last)[None, :]",
      "",
      "    if NS > 1:",
      "        if STORE_INITIAL_STATE_GRADIENT:",
      "            p_dhs = tl.make_block_ptr(",
      "                dhs + (boh * H + i_h) * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            p_dh0 = tl.make_block_ptr(",
      "                dh0 + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)",
      "            tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/371.py",
    "header": "def chunk_bwd_kernel_dh_reduction(g, gk, gv, dhs, dhr, dh0, cu_seqlens, split_offsets, T, S: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NS = tl.cdiv(T, S)\n    boh = tl.load(split_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NS = tl.cdiv(T, S)\n    boh = i_n * NS\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nfor i_s in range(NS - 2, -1, -1):\n    p_dhs = tl.make_block_ptr(dhs + ((boh + i_s + 1) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_dhr = tl.make_block_ptr(dhr + ((boh + i_s) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)\n    tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))\n    for i_t in range(tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1):\n        last_idx = min(i_t * BT + BT, T) - 1\n        if USE_G:\n            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n            b_dh *= exp(b_g_last)\n        if USE_GK:\n            p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n            b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n            b_dh *= exp(b_gk_last)[:, None]\n        if USE_GV:\n            p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n            b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n            b_dh *= exp(b_gv_last)[None, :]\nif NS > 1:\n    if STORE_INITIAL_STATE_GRADIENT:\n        p_dhs = tl.make_block_ptr(dhs + (boh * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n        b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)\n        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "tuned_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 0, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 1, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 2, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 3, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 4, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 0, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 1, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 2, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 3, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 4, 'pre_load_v': False}, num_stages=1, num_warps=4)], key=['seqlen_q', 'seqlen_k', 'STAGE'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "encoded_softmax",
        "annotation": null
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "pre_load_v",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RETURN_ENCODED_SOFTMAX",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    encoded_softmax,",
      "    STAGE: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    pre_load_v: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "):",
      "    bare_attn_fwd(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        seqlen_q,",
      "        seqlen_k,",
      "        dropout_p,",
      "        philox_seed,",
      "        philox_offset_base,",
      "        encoded_softmax,",
      "        STAGE,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        pre_load_v,",
      "        ENABLE_DROPOUT,",
      "        RETURN_ENCODED_SOFTMAX,",
      "    )"
    ],
    "file": "codes/231.py",
    "header": "def tuned_attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, seqlen_q, seqlen_k, dropout_p, philox_seed, philox_offset_base, encoded_softmax, STAGE: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, pre_load_v: tl.constexpr, ENABLE_DROPOUT: tl.constexpr, RETURN_ENCODED_SOFTMAX: tl.constexpr):",
    "body": "bare_attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, seqlen_q, seqlen_k, dropout_p, philox_seed, philox_offset_base, encoded_softmax, STAGE, BLOCK_M, BLOCK_DMODEL, BLOCK_N, pre_load_v, ENABLE_DROPOUT, RETURN_ENCODED_SOFTMAX)"
  },
  {
    "name": "_attn_bwd",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M1': lambda args: args['QSeq'] % args['BLOCK_M1'] == 0, 'EVEN_N1': lambda args: args['KSeq'] % args['BLOCK_N1'] == 0, 'EVEN_M2': lambda args: args['QSeq'] % args['BLOCK_M2'] == 0, 'EVEN_N2': lambda args: args['KSeq'] % args['BLOCK_N2'] == 0, 'HEADS_PADDED': lambda args: args['headdim'] != args['BLOCK_HEADDIM'], 'NUM_BLOCKS_KV': lambda args: math.ceil(args['KSeq'] / args['BLOCK_N1'])})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Do",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "dropout_prob",
        "annotation": null
      },
      {
        "name": "dropout_seed",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_doz",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dqz",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dkz",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dvz",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "nheads_q",
        "annotation": null
      },
      {
        "name": "num_repeats",
        "annotation": null
      },
      {
        "name": "QSeq",
        "annotation": null
      },
      {
        "name": "cum_seqlens_q",
        "annotation": null
      },
      {
        "name": "KSeq",
        "annotation": null
      },
      {
        "name": "cum_seqlens_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CQSeq",
        "annotation": null
      },
      {
        "name": "CKSeq",
        "annotation": null
      },
      {
        "name": "DRuntime",
        "annotation": null
      },
      {
        "name": "Dq",
        "annotation": null
      },
      {
        "name": "Dk",
        "annotation": null
      },
      {
        "name": "Dv",
        "annotation": null
      },
      {
        "name": "VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_ON",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_BLOCKS_KV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEADS_PADDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N2",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_bwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    Do,",
      "    M,",
      "    D,",
      "    softmax_scale,",
      "    dropout_prob,",
      "    dropout_seed,",
      "    stride_qz,",
      "    stride_qm,",
      "    stride_qh,",
      "    stride_kz,",
      "    stride_kn,",
      "    stride_kh,",
      "    stride_vz,",
      "    stride_vn,",
      "    stride_vh,",
      "    stride_bz,",
      "    stride_bm,",
      "    stride_bh,",
      "    stride_doz,",
      "    stride_dom,",
      "    stride_doh,",
      "    stride_dqz,",
      "    stride_dqm,",
      "    stride_dqh,",
      "    stride_dkz,",
      "    stride_dkn,",
      "    stride_dkh,",
      "    stride_dvz,",
      "    stride_dvn,",
      "    stride_dvh,",
      "    nheads_q,",
      "    num_repeats,",
      "    QSeq,",
      "    cum_seqlens_q,",
      "    KSeq,",
      "    cum_seqlens_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CQSeq,",
      "    CKSeq,",
      "    DRuntime,",
      "    Dq,",
      "    Dk,",
      "    Dv,",
      "    VARLEN: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BIAS_ON: tl.constexpr,",
      "    USE_DROPOUT: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M1: tl.constexpr,",
      "    EVEN_N1: tl.constexpr,",
      "    EVEN_M2: tl.constexpr,",
      "    EVEN_N2: tl.constexpr,",
      "    NUM_BLOCKS_KV: tl.constexpr,",
      "    HEADS_PADDED: tl.constexpr,",
      "    BLOCK_M1: tl.constexpr,",
      "    BLOCK_N1: tl.constexpr,",
      "    BLOCK_M2: tl.constexpr,",
      "    BLOCK_N2: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    off_zh = tl.program_id(1)",
      "    off_z = off_zh // nheads_q",
      "    off_head_q = off_zh % nheads_q",
      "    off_head_kv = off_head_q // num_repeats",
      "",
      "    if VARLEN:",
      "        cu_seq_start_q = tl.load(cum_seqlens_q + off_z)",
      "        cu_seq_start_k = tl.load(cum_seqlens_k + off_z)",
      "        actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q",
      "        actual_seqlen_k = tl.load(cum_seqlens_k + off_z + 1) - cu_seq_start_k",
      "        off_z = 0",
      "    else:",
      "        cu_seq_start_q = 0",
      "        cu_seq_start_k = 0",
      "        actual_seqlen_q = QSeq",
      "        actual_seqlen_k = KSeq",
      "",
      "    Q += off_z * stride_qz + off_head_q * stride_qh + cu_seq_start_q * stride_qm",
      "    K += off_z * stride_kz + off_head_kv * stride_kh + cu_seq_start_k * stride_kn",
      "    V += off_z * stride_vz + off_head_kv * stride_vh + cu_seq_start_k * stride_vn",
      "",
      "    Do += off_z * stride_doz + off_head_q * stride_doh + cu_seq_start_q * stride_dom",
      "    Dq += off_z * stride_dqz + off_head_q * stride_dqh + cu_seq_start_q * stride_dqm",
      "    Dk += off_z * stride_dkz + off_head_q * stride_dkh + cu_seq_start_k * stride_dkn",
      "    Dv += off_z * stride_dvz + off_head_q * stride_dvh + cu_seq_start_k * stride_dvn",
      "",
      "    if BIAS_ON:",
      "        B += off_z * stride_bz + off_head_q * stride_bh + cu_seq_start_q * stride_bm",
      "    if USE_DROPOUT:",
      "        Dropout = actual_seqlen_k * (",
      "            cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z)",
      "        )",
      "    else:",
      "        Dropout = None",
      "",
      "    D += off_zh * seqlen_q_rounded",
      "    M += off_zh * seqlen_q_rounded",
      "",
      "    if pid < NUM_BLOCKS_KV:",
      "        i_start_n = pid",
      "        pad_cols = (not EVEN_N1) or (",
      "            VARLEN and ((i_start_n + 1) * BLOCK_N1 > actual_seqlen_k)",
      "        )",
      "        _attn_bwd_block_dkdv(",
      "            i_start_n * BLOCK_N1,",
      "            Q,",
      "            K,",
      "            V,",
      "            B,",
      "            Dropout,",
      "            Do,",
      "            Dk,",
      "            Dv,",
      "            M,",
      "            D,",
      "            softmax_scale,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dkn,",
      "            stride_dvn,",
      "            actual_seqlen_q,",
      "            actual_seqlen_k,",
      "            headdim,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BIAS_ON=BIAS_ON,",
      "            USE_DROPOUT=USE_DROPOUT,",
      "            PAD_COLS=pad_cols,",
      "            HEADS_PADDED=HEADS_PADDED,",
      "            BLOCK_M=BLOCK_M1,",
      "            BLOCK_N=BLOCK_N1,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "        )",
      "",
      "    else:",
      "        i_start_m = pid - NUM_BLOCKS_KV",
      "        pad_rows = (not EVEN_M2) or (",
      "            VARLEN and ((i_start_m + 1) * BLOCK_M2 > actual_seqlen_q)",
      "        )",
      "        _attn_bwd_block_dq(",
      "            i_start_m * BLOCK_M2,",
      "            Q,",
      "            K,",
      "            V,",
      "            B,",
      "            Dropout,",
      "            Do,",
      "            Dq,",
      "            M,",
      "            D,",
      "            softmax_scale,",
      "            dropout_prob,",
      "            dropout_seed,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dqm,",
      "            actual_seqlen_q,",
      "            actual_seqlen_k,",
      "            headdim,",
      "            VARLEN=VARLEN,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BIAS_ON=BIAS_ON,",
      "            USE_DROPOUT=USE_DROPOUT,",
      "            PAD_ROWS=pad_rows,",
      "            HEADS_PADDED=HEADS_PADDED,",
      "            BLOCK_M=BLOCK_M2,",
      "            BLOCK_N=BLOCK_N2,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "            EVEN_N=EVEN_N2,",
      "        )"
    ],
    "file": "codes/320.py",
    "header": "def _attn_bwd(Q, K, V, B, Do, M, D, softmax_scale, dropout_prob, dropout_seed, stride_qz, stride_qm, stride_qh, stride_kz, stride_kn, stride_kh, stride_vz, stride_vn, stride_vh, stride_bz, stride_bm, stride_bh, stride_doz, stride_dom, stride_doh, stride_dqz, stride_dqm, stride_dqh, stride_dkz, stride_dkn, stride_dkh, stride_dvz, stride_dvn, stride_dvh, nheads_q, num_repeats, QSeq, cum_seqlens_q, KSeq, cum_seqlens_k, seqlen_q_rounded, headdim, CQSeq, CKSeq, DRuntime, Dq, Dk, Dv, VARLEN: tl.constexpr, IS_CAUSAL: tl.constexpr, BIAS_ON: tl.constexpr, USE_DROPOUT: tl.constexpr, BLOCK_HEADDIM: tl.constexpr, EVEN_M1: tl.constexpr, EVEN_N1: tl.constexpr, EVEN_M2: tl.constexpr, EVEN_N2: tl.constexpr, NUM_BLOCKS_KV: tl.constexpr, HEADS_PADDED: tl.constexpr, BLOCK_M1: tl.constexpr, BLOCK_N1: tl.constexpr, BLOCK_M2: tl.constexpr, BLOCK_N2: tl.constexpr):",
    "body": "pid = tl.program_id(0)\noff_zh = tl.program_id(1)\noff_z = off_zh // nheads_q\noff_head_q = off_zh % nheads_q\noff_head_kv = off_head_q // num_repeats\nif VARLEN:\n    cu_seq_start_q = tl.load(cum_seqlens_q + off_z)\n    cu_seq_start_k = tl.load(cum_seqlens_k + off_z)\n    actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q\n    actual_seqlen_k = tl.load(cum_seqlens_k + off_z + 1) - cu_seq_start_k\n    off_z = 0\nelse:\n    cu_seq_start_q = 0\n    cu_seq_start_k = 0\n    actual_seqlen_q = QSeq\n    actual_seqlen_k = KSeq\nQ += off_z * stride_qz + off_head_q * stride_qh + cu_seq_start_q * stride_qm\nK += off_z * stride_kz + off_head_kv * stride_kh + cu_seq_start_k * stride_kn\nV += off_z * stride_vz + off_head_kv * stride_vh + cu_seq_start_k * stride_vn\nDo += off_z * stride_doz + off_head_q * stride_doh + cu_seq_start_q * stride_dom\nDq += off_z * stride_dqz + off_head_q * stride_dqh + cu_seq_start_q * stride_dqm\nDk += off_z * stride_dkz + off_head_q * stride_dkh + cu_seq_start_k * stride_dkn\nDv += off_z * stride_dvz + off_head_q * stride_dvh + cu_seq_start_k * stride_dvn\nif BIAS_ON:\n    B += off_z * stride_bz + off_head_q * stride_bh + cu_seq_start_q * stride_bm\nif USE_DROPOUT:\n    Dropout = actual_seqlen_k * (cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z))\nelse:\n    Dropout = None\nD += off_zh * seqlen_q_rounded\nM += off_zh * seqlen_q_rounded\nif pid < NUM_BLOCKS_KV:\n    i_start_n = pid\n    pad_cols = not EVEN_N1 or (VARLEN and (i_start_n + 1) * BLOCK_N1 > actual_seqlen_k)\n    _attn_bwd_block_dkdv(i_start_n * BLOCK_N1, Q, K, V, B, Dropout, Do, Dk, Dv, M, D, softmax_scale, stride_qm, stride_kn, stride_vn, stride_bm, stride_dom, stride_dkn, stride_dvn, actual_seqlen_q, actual_seqlen_k, headdim, IS_CAUSAL=IS_CAUSAL, BIAS_ON=BIAS_ON, USE_DROPOUT=USE_DROPOUT, PAD_COLS=pad_cols, HEADS_PADDED=HEADS_PADDED, BLOCK_M=BLOCK_M1, BLOCK_N=BLOCK_N1, BLOCK_HEADDIM=BLOCK_HEADDIM)\nelse:\n    i_start_m = pid - NUM_BLOCKS_KV\n    pad_rows = not EVEN_M2 or (VARLEN and (i_start_m + 1) * BLOCK_M2 > actual_seqlen_q)\n    _attn_bwd_block_dq(i_start_m * BLOCK_M2, Q, K, V, B, Dropout, Do, Dq, M, D, softmax_scale, dropout_prob, dropout_seed, stride_qm, stride_kn, stride_vn, stride_bm, stride_dom, stride_dqm, actual_seqlen_q, actual_seqlen_k, headdim, VARLEN=VARLEN, IS_CAUSAL=IS_CAUSAL, BIAS_ON=BIAS_ON, USE_DROPOUT=USE_DROPOUT, PAD_ROWS=pad_rows, HEADS_PADDED=HEADS_PADDED, BLOCK_M=BLOCK_M2, BLOCK_N=BLOCK_N2, BLOCK_HEADDIM=BLOCK_HEADDIM, EVEN_N=EVEN_N2)"
  },
  {
    "name": "rotary_embedding_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['B', 'H', 'D', 'INTERLEAVED'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "cos",
        "annotation": null
      },
      {
        "name": "sin",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "seq_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "R",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TR",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_SEQLEN_OFFSETS_TENSOR",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "INTERLEAVED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CONJUGATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rotary_embedding_kernel(",
      "    x,",
      "    cos,",
      "    sin,",
      "    y,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    seq_offsets,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    D: tl.constexpr,",
      "    R: tl.constexpr,",
      "    TR: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    INTERLEAVED: tl.constexpr,",
      "    CONJUGATE: tl.constexpr,",
      "):",
      "    i_t, i_b, i_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)",
      "        T = eos - bos",
      "        x = x + bos * H * D + i_h * D",
      "        y = y + bos * H * D + i_h * D",
      "    else:",
      "        i_n = i_b",
      "        x = x + i_n * T * H * D + i_h * D",
      "        y = y + i_n * T * H * D + i_h * D",
      "",
      "    if i_t * BT >= T:",
      "        return",
      "",
      "    o_t = i_t * BT + tl.arange(0, BT)",
      "    if not IS_SEQLEN_OFFSETS_TENSOR:",
      "        o_cs = o_t + seq_offsets",
      "    else:",
      "        o_cs = o_t + tl.load(seq_offsets + i_n)",
      "    m_t = (o_t >= 0) & (o_t < T) & (o_cs >= 0) & (o_cs < TR)",
      "",
      "    if not INTERLEAVED:",
      "",
      "        o_r = tl.arange(0, BD // 2)",
      "        p_x = x + o_t[:, None] * H * D + o_r[None, :]",
      "        p_cos = cos + (o_cs[:, None] * R + o_r[None, :])",
      "        p_sin = sin + (o_cs[:, None] * R + o_r[None, :])",
      "        mask = m_t[:, None] & (o_r < R)[None, :]",
      "",
      "        b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)",
      "        b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)",
      "        b_x0 = tl.load(p_x, mask=mask, other=0.0).to(tl.float32)",
      "        b_x1 = tl.load(p_x + R, mask=mask, other=0.0).to(tl.float32)",
      "        if CONJUGATE:",
      "            b_sin = -b_sin",
      "        b_o0 = b_x0 * b_cos - b_x1 * b_sin",
      "        b_o1 = b_x0 * b_sin + b_x1 * b_cos",
      "",
      "        p_y = y + (o_t[:, None] * H * D + o_r[None, :])",
      "        tl.store(p_y, b_o0, mask=mask)",
      "        tl.store(p_y + R, b_o1, mask=mask)",
      "    else:",
      "",
      "        o_d = tl.arange(0, BD)",
      "        o_d_swap = o_d + ((o_d + 1) % 2) * 2 - 1",
      "        o_d_repeat = tl.arange(0, BD) // 2",
      "        p_x0 = x + o_t[:, None] * H * D + o_d[None, :]",
      "        p_x1 = x + o_t[:, None] * H * D + o_d_swap[None, :]",
      "        p_cos = cos + (o_cs[:, None] * R + o_d_repeat[None, :])",
      "        p_sin = sin + (o_cs[:, None] * R + o_d_repeat[None, :])",
      "        mask = m_t[:, None] & (o_d_repeat < R)[None, :]",
      "",
      "        b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)",
      "        b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)",
      "        b_x0 = tl.load(p_x0, mask=mask, other=0.0).to(tl.float32)",
      "        b_x1 = tl.load(p_x1, mask=mask, other=0.0).to(tl.float32)",
      "        if CONJUGATE:",
      "            b_sin = -b_sin",
      "        b_o0 = b_x0 * b_cos",
      "        b_o1 = b_x1 * b_sin",
      "        b_y = tl.where(o_d[None, :] % 2 == 0, b_o0 - b_o1, b_o0 + b_o1)",
      "        p_y = y + (o_t[:, None] * H * D + o_d[None, :])",
      "        tl.store(p_y, b_y, mask=mask)"
    ],
    "file": "codes/361.py",
    "header": "def rotary_embedding_kernel(x, cos, sin, y, cu_seqlens, chunk_indices, seq_offsets, T, B: tl.constexpr, H: tl.constexpr, D: tl.constexpr, R: tl.constexpr, TR: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr, IS_VARLEN: tl.constexpr, INTERLEAVED: tl.constexpr, CONJUGATE: tl.constexpr):",
    "body": "i_t, i_b, i_h = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1))\n    T = eos - bos\n    x = x + bos * H * D + i_h * D\n    y = y + bos * H * D + i_h * D\nelse:\n    i_n = i_b\n    x = x + i_n * T * H * D + i_h * D\n    y = y + i_n * T * H * D + i_h * D\nif i_t * BT >= T:\n    return\no_t = i_t * BT + tl.arange(0, BT)\nif not IS_SEQLEN_OFFSETS_TENSOR:\n    o_cs = o_t + seq_offsets\nelse:\n    o_cs = o_t + tl.load(seq_offsets + i_n)\nm_t = (o_t >= 0) & (o_t < T) & (o_cs >= 0) & (o_cs < TR)\nif not INTERLEAVED:\n    o_r = tl.arange(0, BD // 2)\n    p_x = x + o_t[:, None] * H * D + o_r[None, :]\n    p_cos = cos + (o_cs[:, None] * R + o_r[None, :])\n    p_sin = sin + (o_cs[:, None] * R + o_r[None, :])\n    mask = m_t[:, None] & (o_r < R)[None, :]\n    b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)\n    b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)\n    b_x0 = tl.load(p_x, mask=mask, other=0.0).to(tl.float32)\n    b_x1 = tl.load(p_x + R, mask=mask, other=0.0).to(tl.float32)\n    if CONJUGATE:\n        b_sin = -b_sin\n    b_o0 = b_x0 * b_cos - b_x1 * b_sin\n    b_o1 = b_x0 * b_sin + b_x1 * b_cos\n    p_y = y + (o_t[:, None] * H * D + o_r[None, :])\n    tl.store(p_y, b_o0, mask=mask)\n    tl.store(p_y + R, b_o1, mask=mask)\nelse:\n    o_d = tl.arange(0, BD)\n    o_d_swap = o_d + (o_d + 1) % 2 * 2 - 1\n    o_d_repeat = tl.arange(0, BD) // 2\n    p_x0 = x + o_t[:, None] * H * D + o_d[None, :]\n    p_x1 = x + o_t[:, None] * H * D + o_d_swap[None, :]\n    p_cos = cos + (o_cs[:, None] * R + o_d_repeat[None, :])\n    p_sin = sin + (o_cs[:, None] * R + o_d_repeat[None, :])\n    mask = m_t[:, None] & (o_d_repeat < R)[None, :]\n    b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)\n    b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)\n    b_x0 = tl.load(p_x0, mask=mask, other=0.0).to(tl.float32)\n    b_x1 = tl.load(p_x1, mask=mask, other=0.0).to(tl.float32)\n    if CONJUGATE:\n        b_sin = -b_sin\n    b_o0 = b_x0 * b_cos\n    b_o1 = b_x1 * b_sin\n    b_y = tl.where(o_d[None, :] % 2 == 0, b_o0 - b_o1, b_o0 + b_o1)\n    p_y = y + (o_t[:, None] * H * D + o_d[None, :])\n    tl.store(p_y, b_y, mask=mask)"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk32",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BK'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk32(",
      "    a,",
      "    b,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_a = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_b = tl.make_block_ptr(",
      "            b + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, K * H),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        b_a = tl.load(p_a, boundary_check=(0, 1))",
      "        b_b = tl.load(p_b, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_a, b_b)",
      "",
      "    b_A = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A, 0)",
      "    for i in range(1, BT):",
      "        mask = tl.arange(0, BT) == i",
      "        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "    b_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/392.py",
    "header": "def prepare_wy_repr_fwd_kernel_chunk32(a, b, A, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_a = tl.make_block_ptr(a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_b = tl.make_block_ptr(b + (bos * H + i_h) * K, (K, T), (1, K * H), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    b_a = tl.load(p_a, boundary_check=(0, 1))\n    b_b = tl.load(p_b, boundary_check=(0, 1))\n    b_A += tl.dot(b_a, b_b)\nb_A = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A, 0)\nfor i in range(1, BT):\n    mask = tl.arange(0, BT) == i\n    b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n    b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)\n    b_A = tl.where(mask[:, None], b_a, b_A)\nb_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\ntl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BK'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk64(",
      "    a,",
      "    b,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    b_A2 = tl.zeros([BC, BC], dtype=tl.float32)",
      "    b_A3 = tl.zeros([BC, BC], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_a1 = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_a2 = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_b1 = tl.make_block_ptr(",
      "            b + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, K * H),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_b2 = tl.make_block_ptr(",
      "            b + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, K * H),",
      "            (i_k * BK, i_t * BT + BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        b_a1 = tl.load(p_a1, boundary_check=(0, 1))",
      "        b_a2 = tl.load(p_a2, boundary_check=(0, 1))",
      "        b_b1 = tl.load(p_b1, boundary_check=(0, 1))",
      "        b_b2 = tl.load(p_b2, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_a1, b_b1, allow_tf32=False)",
      "        b_A2 += tl.dot(b_a2, b_b2, allow_tf32=False)",
      "        b_A3 += tl.dot(b_a2, b_b1, allow_tf32=False)",
      "",
      "    b_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)",
      "    b_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)",
      "",
      "    for i in range(1, BC):",
      "        mask = tl.arange(0, BC) == i",
      "        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)",
      "        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)",
      "        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "        b_A2 = tl.where(mask[:, None], b_a2, b_A2)",
      "",
      "    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A3 = tl.dot(tl.dot(b_A2, b_A3, allow_tf32=False), b_A, allow_tf32=False)",
      "",
      "    p_A1 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_A2 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A3 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A4 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0)",
      "    )",
      "    tl.store(p_A1, b_A.to(p_A1.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_A2, b_A2.to(p_A2.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_A3, b_A3.to(p_A3.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.store(",
      "        p_A4,",
      "        tl.zeros([BC, BC], dtype=tl.float32).to(p_A4.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/392.py",
    "header": "def prepare_wy_repr_fwd_kernel_chunk64(a, b, A, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nb_A = tl.zeros([BC, BC], dtype=tl.float32)\nb_A2 = tl.zeros([BC, BC], dtype=tl.float32)\nb_A3 = tl.zeros([BC, BC], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_a1 = tl.make_block_ptr(a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\n    p_a2 = tl.make_block_ptr(a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + BC, i_k * BK), (BC, BK), (1, 0))\n    p_b1 = tl.make_block_ptr(b + (bos * H + i_h) * K, (K, T), (1, K * H), (i_k * BK, i_t * BT), (BK, BC), (0, 1))\n    p_b2 = tl.make_block_ptr(b + (bos * H + i_h) * K, (K, T), (1, K * H), (i_k * BK, i_t * BT + BC), (BK, BC), (0, 1))\n    b_a1 = tl.load(p_a1, boundary_check=(0, 1))\n    b_a2 = tl.load(p_a2, boundary_check=(0, 1))\n    b_b1 = tl.load(p_b1, boundary_check=(0, 1))\n    b_b2 = tl.load(p_b2, boundary_check=(0, 1))\n    b_A += tl.dot(b_a1, b_b1, allow_tf32=False)\n    b_A2 += tl.dot(b_a2, b_b2, allow_tf32=False)\n    b_A3 += tl.dot(b_a2, b_b1, allow_tf32=False)\nb_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)\nb_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)\nfor i in range(1, BC):\n    mask = tl.arange(0, BC) == i\n    b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)\n    b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)\n    b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)\n    b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)\n    b_A = tl.where(mask[:, None], b_a, b_A)\n    b_A2 = tl.where(mask[:, None], b_a2, b_A2)\nb_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\nb_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]\nb_A3 = tl.dot(tl.dot(b_A2, b_A3, allow_tf32=False), b_A, allow_tf32=False)\np_A1 = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0))\np_A2 = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + BC, BC), (BC, BC), (1, 0))\np_A3 = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + BC, 0), (BC, BC), (1, 0))\np_A4 = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0))\ntl.store(p_A1, b_A.to(p_A1.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_A2, b_A2.to(p_A2.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_A3, b_A3.to(p_A3.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_A4, tl.zeros([BC, BC], dtype=tl.float32).to(p_A4.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "wu_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in NUM_WARPS], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def wu_fwd_kernel(",
      "    w,",
      "    u,",
      "    a,",
      "    k,",
      "    v,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_Aak = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_a = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_a = tl.load(p_a, boundary_check=(0, 1))",
      "        b_w = tl.dot(b_A, b_a)",
      "        b_Aak += tl.dot(b_a, tl.trans(b_k))",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_Aak = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_Aak, 0)",
      "    b_Aak = b_Aak.to(k.dtype.element_ty)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v = tl.dot(b_Aak, b_v).to(v.dtype.element_ty)",
      "        b_u = tl.dot(b_A, b_v)",
      "        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/392.py",
    "header": "def wu_fwd_kernel(w, u, a, k, v, A, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_Aak = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_a = tl.make_block_ptr(a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_a = tl.load(p_a, boundary_check=(0, 1))\n    b_w = tl.dot(b_A, b_a)\n    b_Aak += tl.dot(b_a, tl.trans(b_k))\n    tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))\nb_Aak = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_Aak, 0)\nb_Aak = b_Aak.to(k.dtype.element_ty)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v = tl.dot(b_Aak, b_v).to(v.dtype.element_ty)\n    b_u = tl.dot(b_A, b_v)\n    tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "apply_clip_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size, num_warps in itertools.product([32, 64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=['n_audios', 'audio_len'])"
    ],
    "args": [
      {
        "name": "samples_ptr",
        "annotation": null
      },
      {
        "name": "min",
        "annotation": null
      },
      {
        "name": "max",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "n_audios",
        "annotation": null
      },
      {
        "name": "audio_len",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def apply_clip_kernel(",
      "    samples_ptr, min, max, output_ptr, n_audios, audio_len, BLOCK_SIZE: tl.constexpr",
      "):",
      "    audio_idx = tl.program_id(0)",
      "",
      "    if audio_idx >= n_audios:",
      "        return",
      "",
      "    for i in range(0, audio_len, BLOCK_SIZE):",
      "        sample_idx = i + tl.arange(0, BLOCK_SIZE)",
      "        mask = sample_idx < audio_len",
      "",
      "        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)",
      "        result = tl.where(samples > max, max, samples)",
      "        result = tl.where(result < min, min, result)",
      "        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)"
    ],
    "file": "codes/184.py",
    "header": "def apply_clip_kernel(samples_ptr, min, max, output_ptr, n_audios, audio_len, BLOCK_SIZE: tl.constexpr):",
    "body": "audio_idx = tl.program_id(0)\nif audio_idx >= n_audios:\n    return\nfor i in range(0, audio_len, BLOCK_SIZE):\n    sample_idx = i + tl.arange(0, BLOCK_SIZE)\n    mask = sample_idx < audio_len\n    samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)\n    result = tl.where(samples > max, max, samples)\n    result = tl.where(result < min, min, result)\n    tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)"
  },
  {
    "name": "_selective_scan_update_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not None})",
      "@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})",
      "@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.next_power_of_2(args['dstate'])})"
    ],
    "args": [
      {
        "name": "state_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_state_batch",
        "annotation": null
      },
      {
        "name": "stride_state_head",
        "annotation": null
      },
      {
        "name": "stride_state_dim",
        "annotation": null
      },
      {
        "name": "stride_state_dstate",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_dim",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_A_dim",
        "annotation": null
      },
      {
        "name": "stride_A_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_group",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_group",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_D_dim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TIE_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _selective_scan_update_kernel(",
      "    state_ptr,",
      "    x_ptr,",
      "    dt_ptr,",
      "    dt_bias_ptr,",
      "    A_ptr,",
      "    B_ptr,",
      "    C_ptr,",
      "    D_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    batch,",
      "    nheads,",
      "    dim,",
      "    dstate,",
      "    nheads_ngroups_ratio,",
      "    stride_state_batch,",
      "    stride_state_head,",
      "    stride_state_dim,",
      "    stride_state_dstate,",
      "    stride_x_batch,",
      "    stride_x_head,",
      "    stride_x_dim,",
      "    stride_dt_batch,",
      "    stride_dt_head,",
      "    stride_dt_dim,",
      "    stride_dt_bias_head,",
      "    stride_dt_bias_dim,",
      "    stride_A_head,",
      "    stride_A_dim,",
      "    stride_A_dstate,",
      "    stride_B_batch,",
      "    stride_B_group,",
      "    stride_B_dstate,",
      "    stride_C_batch,",
      "    stride_C_group,",
      "    stride_C_dstate,",
      "    stride_D_head,",
      "    stride_D_dim,",
      "    stride_z_batch,",
      "    stride_z_head,",
      "    stride_z_dim,",
      "    stride_out_batch,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    TIE_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head",
      "    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptr += pid_h * stride_dt_bias_head",
      "    A_ptr += pid_h * stride_A_head",
      "    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group",
      "    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group",
      "    if HAS_Z:",
      "        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "    state_ptrs = state_ptr + (",
      "        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate",
      "    )",
      "    x_ptrs = x_ptr + offs_m * stride_x_dim",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_dim",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim",
      "    if HAS_D:",
      "        D_ptr += pid_h * stride_D_head",
      "    A_ptrs = A_ptr + (",
      "        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate",
      "    )",
      "    B_ptrs = B_ptr + offs_n * stride_B_dstate",
      "    C_ptrs = C_ptr + offs_n * stride_C_dstate",
      "    if HAS_D:",
      "        D_ptrs = D_ptr + offs_m * stride_D_dim",
      "    if HAS_Z:",
      "        z_ptrs = z_ptr + offs_m * stride_z_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "",
      "    state = tl.load(",
      "        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "    )",
      "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if not TIE_HDIM:",
      "        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(",
      "            A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "        ).to(tl.float32)",
      "        dA = tl.exp(A * dt[:, None])",
      "    else:",
      "        dt = tl.load(dt_ptr).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptr).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(A_ptr).to(tl.float32)",
      "        dA = tl.exp(A * dt)",
      "",
      "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    if HAS_D:",
      "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if HAS_Z:",
      "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "",
      "    if not TIE_HDIM:",
      "        dB = B[None, :] * dt[:, None]",
      "    else:",
      "        dB = B * dt",
      "    state = state * dA + dB * x[:, None]",
      "    tl.store(",
      "        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)",
      "    )",
      "    out = tl.sum(state * C[None, :], axis=1)",
      "    if HAS_D:",
      "        out += x * D",
      "    if HAS_Z:",
      "        out *= z * tl.sigmoid(z)",
      "    tl.store(out_ptrs, out, mask=offs_m < dim)"
    ],
    "file": "codes/126.py",
    "header": "def _selective_scan_update_kernel(state_ptr, x_ptr, dt_ptr, dt_bias_ptr, A_ptr, B_ptr, C_ptr, D_ptr, z_ptr, out_ptr, batch, nheads, dim, dstate, nheads_ngroups_ratio, stride_state_batch, stride_state_head, stride_state_dim, stride_state_dstate, stride_x_batch, stride_x_head, stride_x_dim, stride_dt_batch, stride_dt_head, stride_dt_dim, stride_dt_bias_head, stride_dt_bias_dim, stride_A_head, stride_A_dim, stride_A_dstate, stride_B_batch, stride_B_group, stride_B_dstate, stride_C_batch, stride_C_group, stride_C_dstate, stride_D_head, stride_D_dim, stride_z_batch, stride_z_head, stride_z_dim, stride_out_batch, stride_out_head, stride_out_dim, DT_SOFTPLUS: tl.constexpr, TIE_HDIM: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, HAS_DT_BIAS: tl.constexpr, HAS_D: tl.constexpr, HAS_Z: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_m = tl.program_id(axis=0)\npid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\nstate_ptr += pid_b * stride_state_batch + pid_h * stride_state_head\nx_ptr += pid_b * stride_x_batch + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head\nif HAS_DT_BIAS:\n    dt_bias_ptr += pid_h * stride_dt_bias_head\nA_ptr += pid_h * stride_A_head\nB_ptr += pid_b * stride_B_batch + pid_h // nheads_ngroups_ratio * stride_B_group\nC_ptr += pid_b * stride_C_batch + pid_h // nheads_ngroups_ratio * stride_C_group\nif HAS_Z:\n    z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head\nout_ptr += pid_b * stride_out_batch + pid_h * stride_out_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_DSTATE)\nstate_ptrs = state_ptr + (offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate)\nx_ptrs = x_ptr + offs_m * stride_x_dim\ndt_ptrs = dt_ptr + offs_m * stride_dt_dim\nif HAS_DT_BIAS:\n    dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim\nif HAS_D:\n    D_ptr += pid_h * stride_D_head\nA_ptrs = A_ptr + (offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate)\nB_ptrs = B_ptr + offs_n * stride_B_dstate\nC_ptrs = C_ptr + offs_n * stride_C_dstate\nif HAS_D:\n    D_ptrs = D_ptr + offs_m * stride_D_dim\nif HAS_Z:\n    z_ptrs = z_ptr + offs_m * stride_z_dim\nout_ptrs = out_ptr + offs_m * stride_out_dim\nstate = tl.load(state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0)\nx = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\nif not TIE_HDIM:\n    dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n    A = tl.load(A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    dA = tl.exp(A * dt[:, None])\nelse:\n    dt = tl.load(dt_ptr).to(tl.float32)\n    if HAS_DT_BIAS:\n        dt += tl.load(dt_bias_ptr).to(tl.float32)\n    if DT_SOFTPLUS:\n        dt = softplus(dt)\n    A = tl.load(A_ptr).to(tl.float32)\n    dA = tl.exp(A * dt)\nB = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\nC = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)\nif HAS_D:\n    D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\nif HAS_Z:\n    z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)\nif not TIE_HDIM:\n    dB = B[None, :] * dt[:, None]\nelse:\n    dB = B * dt\nstate = state * dA + dB * x[:, None]\ntl.store(state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate))\nout = tl.sum(state * C[None, :], axis=1)\nif HAS_D:\n    out += x * D\nif HAS_Z:\n    out *= z * tl.sigmoid(z)\ntl.store(out_ptrs, out, mask=offs_m < dim)"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 2}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 4}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=2)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 32,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "codes/177.py",
    "header": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16, GROUP_SIZE: tl.constexpr = 32):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nnum_idx_N, num_idx_V_group = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_V_group = tl.swizzle2d(idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE)\nV_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nm = tl.max(z_j_to_k, 1)\ns = tl.sum(tl.exp(z_j_to_k - m[:, None]), axis=1)\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nmask = y[:, None] == V_range[None, :]\nloss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\ntl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\nlse = m + tl.log(s)\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\ntl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\ntl.store(lse_row_ptr, lse[:, None])"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4)], key=['V', 'N', 'H'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
    ],
    "file": "codes/177.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(z_nv_ptr, y_ptr, A_t_ptr, x_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16, GROUP_SIZE: tl.constexpr = 1):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_H = tl.program_id(axis=1)\nidx_V = 0\nnum_idx_N, num_idx_H = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = 0 + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nlse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\nfor _ in range(V // V_BLOCK_SIZE):\n    mask = y[:, None] == v_range[None, :]\n    A_v = tl.load(A_t_block_ptr)\n    z_j_to_k = tl.load(z_block_ptr)\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n    x_grad_acc = tl.dot(z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    v_range += V_BLOCK_SIZE\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\ntl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3)], key=['V', 'N', 'H'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(N_group // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + idx_N_group * N_group + N_range)",
      "        lse = tl.load(lse_ptr + N_range)",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    A_grad_T_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "    if idx_N_group > 0:",
      "        tl.store(",
      "            A_grad_T_block_ptr,",
      "            tl.load(A_grad_T_block_ptr)",
      "            + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "        )",
      "    else:",
      "        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
    ],
    "file": "codes/177.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(z_nv_ptr, y_ptr, x_ptr, A_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16, GROUP_SIZE: tl.constexpr = 16):",
    "body": "idx_V = tl.program_id(axis=0)\nidx_H = tl.program_id(axis=1)\nnum_idx_V, num_idx_H = (tl.num_programs(0), tl.num_programs(1))\nidx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\nfor _ in range(N_group // N_BLOCK_SIZE):\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range)\n    lse = tl.load(lse_ptr + N_range)\n    mask = y[:, None] == V_range[None, :]\n    x_chunk = tl.load(x_block_ptr)\n    z_j_to_k = tl.load(z_block_ptr)\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n    A_grad_acc = tl.dot(x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n    N_range += N_BLOCK_SIZE\nA_grad_T_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nif idx_N_group > 0:\n    tl.store(A_grad_T_block_ptr, tl.load(A_grad_T_block_ptr) + (A_grad_acc / N).to(A_grad_ptr.type.element_ty))\nelse:\n    tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
  },
  {
    "name": "kernel_consumer_gemm_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "ready_ptr",
        "annotation": null
      },
      {
        "name": "num_barriers_wait_per_block",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EPILOGUE_SUBTILE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "needs_wait",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_consumer_gemm_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    rank,",
      "    num_ranks,",
      "    ready_ptr,",
      "    num_barriers_wait_per_block,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    EPILOGUE_SUBTILE: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    needs_wait: tl.constexpr,",
      "):",
      "",
      "    dtype = c_ptr.dtype.element_ty",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    a_desc = tl.make_tensor_descriptor(",
      "        a_ptr,",
      "        shape=[M, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],",
      "    )",
      "    b_desc = tl.make_tensor_descriptor(",
      "        b_ptr,",
      "        shape=[N, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],",
      "    )",
      "    c_desc = tl.make_tensor_descriptor(",
      "        c_ptr,",
      "        shape=[M, N],",
      "        strides=[N, 1],",
      "        block_shape=[",
      "            BLOCK_SIZE_M,",
      "            BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2,",
      "        ],",
      "    )",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "            if needs_wait:",
      "                num_barriers_to_wait = num_barriers_wait_per_block",
      "                token = dl.wait(ready_ptr, num_barriers_to_wait, \"gpu\", \"acquire\")",
      "                a_desc = dl.consume_token(a_desc, token)",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = a_desc.load([offs_am, offs_k])",
      "        b = b_desc.load([offs_bn, offs_k])",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "",
      "            if EPILOGUE_SUBTILE:",
      "                acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))",
      "                acc = tl.permute(acc, (0, 2, 1))",
      "                acc0, acc1 = tl.split(acc)",
      "                c0 = acc0.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c0)",
      "                c1 = acc1.to(dtype)",
      "                c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)",
      "            else:",
      "                c = accumulator.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c)",
      "",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/58.py",
    "header": "def kernel_consumer_gemm_persistent(a_ptr, b_ptr, c_ptr, M, N, K, rank, num_ranks, ready_ptr, num_barriers_wait_per_block, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, EPILOGUE_SUBTILE: tl.constexpr, NUM_SMS: tl.constexpr, needs_wait: tl.constexpr):",
    "body": "dtype = c_ptr.dtype.element_ty\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\na_desc = tl.make_tensor_descriptor(a_ptr, shape=[M, K], strides=[K, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K])\nb_desc = tl.make_tensor_descriptor(b_ptr, shape=[N, K], strides=[K, 1], block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K])\nc_desc = tl.make_tensor_descriptor(c_ptr, shape=[M, N], strides=[N, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2])\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\npid_m = 0\npid_n = 0\noffs_am = 0\noffs_bn = 0\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n        if needs_wait:\n            num_barriers_to_wait = num_barriers_wait_per_block\n            token = dl.wait(ready_ptr, num_barriers_to_wait, 'gpu', 'acquire')\n            a_desc = dl.consume_token(a_desc, token)\n    offs_k = ki * BLOCK_SIZE_K\n    a = a_desc.load([offs_am, offs_k])\n    b = b_desc.load([offs_bn, offs_k])\n    accumulator = tl.dot(a, b.T, accumulator)\n    if ki == k_tiles - 1:\n        if EPILOGUE_SUBTILE:\n            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n            acc = tl.permute(acc, (0, 2, 1))\n            acc0, acc1 = tl.split(acc)\n            c0 = acc0.to(dtype)\n            c_desc.store([offs_am, offs_bn], c0)\n            c1 = acc1.to(dtype)\n            c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)\n        else:\n            c = accumulator.to(dtype)\n            c_desc.store([offs_am, offs_bn], c)\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "chunk_transform_qk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BT', 'K', 'V'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "A_local",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "OUTPUT_ATTENTIONS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_transform_qk_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    beta,",
      "    o,",
      "    A,",
      "    q_new,",
      "    k_new,",
      "    A_local,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    OUTPUT_ATTENTIONS: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(p_q.dtype.element_ty)",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    p_T = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_T = tl.load(p_T, boundary_check=(0, 1))",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_t = o_i[:, None] >= o_i[None, :]",
      "    b_qk = tl.where(m_t, tl.dot(b_q, tl.trans(b_k), allow_tf32=False), 0).to(b_q.dtype)",
      "    m_t = o_i[:, None] > o_i[None, :]",
      "    b_kk = tl.where(m_t, tl.dot(b_k, tl.trans(b_k), allow_tf32=False), 0).to(b_k.dtype)",
      "",
      "    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "",
      "    b_qkT = tl.dot(b_qk, b_T, allow_tf32=False).to(b_k.dtype)",
      "",
      "    if OUTPUT_ATTENTIONS:",
      "        p_a = tl.make_block_ptr(",
      "            A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "        )",
      "        tl.store(p_a, b_qkT.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_kkT = tl.dot(b_kk, b_T, allow_tf32=False).to(b_k.dtype)",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o, tl.dot(b_qkT, b_v).to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_q_new,",
      "        (b_q - tl.dot(b_qkT, b_k_beta, allow_tf32=False)).to(p_q_new.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    p_k_new = tl.make_block_ptr(",
      "        k_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_k_new = b_k - tl.dot(tl.trans(b_kkT), b_k_beta, allow_tf32=False)",
      "    tl.store(p_k_new, b_k_new.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/376.py",
    "header": "def chunk_transform_qk_fwd_kernel(q, k, v, beta, o, A, q_new, k_new, A_local, scale, T, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, BT: tl.constexpr, OUTPUT_ATTENTIONS: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(p_q.dtype.element_ty)\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_v = tl.load(p_v, boundary_check=(0, 1))\np_T = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_T = tl.load(p_T, boundary_check=(0, 1))\no_i = tl.arange(0, BT)\nm_t = o_i[:, None] >= o_i[None, :]\nb_qk = tl.where(m_t, tl.dot(b_q, tl.trans(b_k), allow_tf32=False), 0).to(b_q.dtype)\nm_t = o_i[:, None] > o_i[None, :]\nb_kk = tl.where(m_t, tl.dot(b_k, tl.trans(b_k), allow_tf32=False), 0).to(b_k.dtype)\np_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\nb_qkT = tl.dot(b_qk, b_T, allow_tf32=False).to(b_k.dtype)\nif OUTPUT_ATTENTIONS:\n    p_a = tl.make_block_ptr(A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    tl.store(p_a, b_qkT.to(p_a.dtype.element_ty), boundary_check=(0, 1))\nb_kkT = tl.dot(b_kk, b_T, allow_tf32=False).to(b_k.dtype)\np_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\ntl.store(p_o, tl.dot(b_qkT, b_v).to(p_o.dtype.element_ty), boundary_check=(0, 1))\np_q_new = tl.make_block_ptr(q_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_q_new, (b_q - tl.dot(b_qkT, b_k_beta, allow_tf32=False)).to(p_q_new.dtype.element_ty), boundary_check=(0, 1))\np_k_new = tl.make_block_ptr(k_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_k_new = b_k - tl.dot(tl.trans(b_kkT), b_k_beta, allow_tf32=False)\ntl.store(p_k_new, b_k_new.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "save_intra_chunk_attn",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2)], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A_local",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def save_intra_chunk_attn(",
      "    A,",
      "    A_local,",
      "    T,",
      "    BT: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * T, (T, T), (T, 1), (i_t * BT, i_t * BT), (BT, BT), (1, 0)",
      "    )",
      "    p_A_local = tl.make_block_ptr(",
      "        A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_A_local = tl.load(p_A_local, boundary_check=(0, 1))",
      "    tl.store(p_A, b_A_local.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/376.py",
    "header": "def save_intra_chunk_attn(A, A_local, T, BT: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\np_A = tl.make_block_ptr(A + i_bh * T * T, (T, T), (T, 1), (i_t * BT, i_t * BT), (BT, BT), (1, 0))\np_A_local = tl.make_block_ptr(A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_A_local = tl.load(p_A_local, boundary_check=(0, 1))\ntl.store(p_A, b_A_local.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "k2",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "o_new",
        "annotation": null
      },
      {
        "name": "attn",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "OUTPUT_ATTENTIONS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    k2,",
      "    v,",
      "    beta,",
      "    o,",
      "    o_new,",
      "    attn,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    OUTPUT_ATTENTIONS: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_q += tl.load(p_q, boundary_check=(0, 1))",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_o += tl.load(p_o, boundary_check=(0, 1))",
      "",
      "    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_k2 = tl.make_block_ptr(",
      "            k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "        m_s = tl.arange(0, BT) >= (offset - i_t * BT + BS)",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)",
      "        b_s = tl.where(m_s[:, None], b_s, 0)",
      "",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)",
      "        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False)",
      "",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn + i_bh * T * T,",
      "                (T, T),",
      "                (T, 1),",
      "                (i_t * BT, offset),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for offset in range(i_t * BT - BS, -BS, -BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))",
      "        p_k2 = tl.make_block_ptr(",
      "            k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)",
      "",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)",
      "        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False).to(b_q.dtype)",
      "",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn + i_bh * T * T,",
      "                (T, T),",
      "                (T, 1),",
      "                (i_t * BT, offset),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_o_new = tl.make_block_ptr(",
      "        o_new + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o_new, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/376.py",
    "header": "def parallel_delta_rule_fwd_kernel(q, k, k2, v, beta, o, o_new, attn, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, OUTPUT_ATTENTIONS: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_q = tl.zeros([BT, BK], dtype=tl.float32)\nb_q += tl.load(p_q, boundary_check=(0, 1))\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\np_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_o += tl.load(p_o, boundary_check=(0, 1))\nfor offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1))\n    p_k2 = tl.make_block_ptr(k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0))\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    m_s = tl.arange(0, BT) >= offset - i_t * BT + BS\n    b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)\n    b_s = tl.where(m_s[:, None], b_s, 0)\n    b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n    b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)\n    b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False)\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + i_bh * T * T, (T, T), (T, 1), (i_t * BT, offset), (BT, BS), (1, 0))\n        tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\nfor offset in range(i_t * BT - BS, -BS, -BS):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0))\n    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))\n    p_k2 = tl.make_block_ptr(k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_beta = tl.load(p_beta, boundary_check=(0,))\n    b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)\n    b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n    b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)\n    b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False).to(b_q.dtype)\n    if OUTPUT_ATTENTIONS:\n        p_a = tl.make_block_ptr(attn + i_bh * T * T, (T, T), (T, 1), (i_t * BT, offset), (BT, BS), (1, 0))\n        tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))\np_o_new = tl.make_block_ptr(o_new + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\ntl.store(p_o_new, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_dplr_bwd_kernel_dAu",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BV', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "A_qb",
        "annotation": null
      },
      {
        "name": "dA_qk",
        "annotation": null
      },
      {
        "name": "dA_qb",
        "annotation": null
      },
      {
        "name": "dv_new",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_dAu(",
      "    v,",
      "    do,",
      "    v_new,",
      "    A_qb,",
      "    dA_qk,",
      "    dA_qb,",
      "    dv_new,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "",
      "    b_dA_qk = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dA_qb = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    p_A_qb = tl.make_block_ptr(",
      "        A_qb + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A_qb = tl.load(p_A_qb, boundary_check=(0, 1))",
      "",
      "    b_A_qb = tl.where(",
      "        tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_A_qb, 0.0",
      "    ).to(b_A_qb.dtype)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        p_dv_new = tl.make_block_ptr(",
      "            dv_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_v_new = tl.load(p_v_new, boundary_check=(0, 1))",
      "        b_dA_qk += tl.dot(b_do, b_v)",
      "        b_dA_qb += tl.dot(b_do, b_v_new)",
      "        b_dv_new = tl.dot(tl.trans(b_A_qb), b_do)",
      "",
      "        tl.store(",
      "            p_dv_new, b_dv_new.to(p_dv_new.dtype.element_ty), boundary_check=(0, 1)",
      "        )",
      "",
      "    p_dA_qk = tl.make_block_ptr(",
      "        dA_qk + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_dA_qb = tl.make_block_ptr(",
      "        dA_qb + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "    b_dA_qk = tl.where(m_s, b_dA_qk * scale, 0.0)",
      "    tl.store(p_dA_qk, b_dA_qk.to(p_dA_qk.dtype.element_ty), boundary_check=(0, 1))",
      "    b_dA_qb = tl.where(m_s, b_dA_qb * scale, 0.0)",
      "    tl.store(p_dA_qb, b_dA_qb.to(p_dA_qb.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/385.py",
    "header": "def chunk_dplr_bwd_kernel_dAu(v, do, v_new, A_qb, dA_qk, dA_qb, dv_new, cu_seqlens, chunk_indices, scale: tl.constexpr, T, H: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nT = eos - bos\nb_dA_qk = tl.zeros([BT, BT], dtype=tl.float32)\nb_dA_qb = tl.zeros([BT, BT], dtype=tl.float32)\np_A_qb = tl.make_block_ptr(A_qb + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_A_qb = tl.load(p_A_qb, boundary_check=(0, 1))\nb_A_qb = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_A_qb, 0.0).to(b_A_qb.dtype)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_t * BT), (BV, BT), (0, 1))\n    p_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_t * BT), (BV, BT), (0, 1))\n    p_dv_new = tl.make_block_ptr(dv_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_v_new = tl.load(p_v_new, boundary_check=(0, 1))\n    b_dA_qk += tl.dot(b_do, b_v)\n    b_dA_qb += tl.dot(b_do, b_v_new)\n    b_dv_new = tl.dot(tl.trans(b_A_qb), b_do)\n    tl.store(p_dv_new, b_dv_new.to(p_dv_new.dtype.element_ty), boundary_check=(0, 1))\np_dA_qk = tl.make_block_ptr(dA_qk + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_dA_qb = tl.make_block_ptr(dA_qb + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nm_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]\nb_dA_qk = tl.where(m_s, b_dA_qk * scale, 0.0)\ntl.store(p_dA_qk, b_dA_qk.to(p_dA_qk.dtype.element_ty), boundary_check=(0, 1))\nb_dA_qb = tl.where(m_s, b_dA_qb * scale, 0.0)\ntl.store(p_dA_qb, b_dA_qb.to(p_dA_qb.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_dplr_bwd_o_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "dgk_last",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_o_kernel(",
      "    v,",
      "    v_new,",
      "    h,",
      "    do,",
      "    dh,",
      "    dk,",
      "    db,",
      "    w,",
      "    dq,",
      "    dv,",
      "    dw,",
      "    gk,",
      "    dgk_last,",
      "    k,",
      "    b,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    v_new += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    dh += (i_tg * H + i_h) * K * V",
      "    dk += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    db += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    dw += (bos * H + i_h) * K",
      "    dv += (bos * H + i_h) * V",
      "    dq += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "",
      "    dgk_last += (i_tg * H + i_h) * K",
      "    gk += (bos * H + i_h) * K",
      "",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dw = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_db = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dgk_last = tl.zeros([BK], dtype=tl.float32)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v_new = tl.load(p_v_new, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_dgk_last += tl.sum((b_h * b_dh).to(tl.float32), axis=0)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "        b_db += tl.dot(b_v_new, b_dh.to(b_v_new.dtype))",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        b_dv = tl.load(p_dv, boundary_check=(0, 1))",
      "        b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))",
      "",
      "    m_k = (i_k * BK + tl.arange(0, BK)) < K",
      "    last_idx = min(i_t * BT + BT, T) - 1",
      "    b_gk_last = tl.load(",
      "        gk + last_idx * stride_qk + i_k * BK + tl.arange(0, BK),",
      "        mask=m_k,",
      "        other=float(\"-inf\"),",
      "    )",
      "    b_dgk_last *= exp(b_gk_last)",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_b = tl.make_block_ptr(",
      "        b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_b = tl.load(p_b, boundary_check=(0, 1))",
      "    b_dgk_last += tl.sum(b_k * b_dk, axis=0)",
      "    b_dgk_last += tl.sum(b_b * b_db, axis=0)",
      "    tl.store(dgk_last + tl.arange(0, BK) + i_k * BK, b_dgk_last, mask=m_k)",
      "",
      "    p_dw = tl.make_block_ptr(",
      "        dw, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_db = tl.make_block_ptr(",
      "        db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/385.py",
    "header": "def chunk_dplr_bwd_o_kernel(v, v_new, h, do, dh, dk, db, w, dq, dv, dw, gk, dgk_last, k, b, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nv += (bos * H + i_h) * V\nv_new += (bos * H + i_h) * V\ndo += (bos * H + i_h) * V\nh += (i_tg * H + i_h) * K * V\ndh += (i_tg * H + i_h) * K * V\ndk += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\ndb += (bos * H + i_h) * K\nb += (bos * H + i_h) * K\ndw += (bos * H + i_h) * K\ndv += (bos * H + i_h) * V\ndq += (bos * H + i_h) * K\nw += (bos * H + i_h) * K\ndgk_last += (i_tg * H + i_h) * K\ngk += (bos * H + i_h) * K\nstride_qk = H * K\nstride_vo = H * V\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_dw = tl.zeros([BT, BK], dtype=tl.float32)\nb_db = tl.zeros([BT, BK], dtype=tl.float32)\nb_dgk_last = tl.zeros([BK], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v_new = tl.load(p_v_new, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dgk_last += tl.sum((b_h * b_dh).to(tl.float32), axis=0)\n    b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    b_db += tl.dot(b_v_new, b_dh.to(b_v_new.dtype))\n    p_dv = tl.make_block_ptr(dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_dv = tl.load(p_dv, boundary_check=(0, 1))\n    b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))\nm_k = i_k * BK + tl.arange(0, BK) < K\nlast_idx = min(i_t * BT + BT, T) - 1\nb_gk_last = tl.load(gk + last_idx * stride_qk + i_k * BK + tl.arange(0, BK), mask=m_k, other=float('-inf'))\nb_dgk_last *= exp(b_gk_last)\np_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_b = tl.make_block_ptr(b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_b = tl.load(p_b, boundary_check=(0, 1))\nb_dgk_last += tl.sum(b_k * b_dk, axis=0)\nb_dgk_last += tl.sum(b_b * b_db, axis=0)\ntl.store(dgk_last + tl.arange(0, BK) + i_k * BK, b_dgk_last, mask=m_k)\np_dw = tl.make_block_ptr(dw, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_db = tl.make_block_ptr(db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dq = tl.make_block_ptr(dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\ntl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_dplr_bwd_kernel_dv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4] for BK in BK_LIST for BV in BK_LIST], key=['BT'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "A_qk",
        "annotation": null
      },
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_dv(",
      "    A_qk,",
      "    kg,",
      "    do,",
      "    dv,",
      "    dh,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    A_qk += (bos * H + i_h) * BT",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "    kg += (bos * H + i_h) * K",
      "    dh += (i_tg * H + i_h) * K * V",
      "",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_A = H * BT",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_kg = tl.make_block_ptr(",
      "            kg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_kg = tl.load(p_kg, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_kg, b_dh.to(b_kg.dtype))",
      "",
      "    p_Aqk = tl.make_block_ptr(",
      "        A_qk, (BT, T), (1, stride_A), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    b_A = tl.where(",
      "        tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],",
      "        tl.load(p_Aqk, boundary_check=(0, 1)),",
      "        0,",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_dv += tl.dot(b_A.to(b_do.dtype), b_do)",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/385.py",
    "header": "def chunk_dplr_bwd_kernel_dv(A_qk, kg, do, dv, dh, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nb_dv = tl.zeros([BT, BV], dtype=tl.float32)\nA_qk += (bos * H + i_h) * BT\ndo += (bos * H + i_h) * V\ndv += (bos * H + i_h) * V\nkg += (bos * H + i_h) * K\ndh += (i_tg * H + i_h) * K * V\nstride_qk = H * K\nstride_vo = H * V\nstride_A = H * BT\nfor i_k in range(tl.cdiv(K, BK)):\n    p_dh = tl.make_block_ptr(dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_kg = tl.make_block_ptr(kg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_kg = tl.load(p_kg, boundary_check=(0, 1))\n    b_dv += tl.dot(b_kg, b_dh.to(b_kg.dtype))\np_Aqk = tl.make_block_ptr(A_qk, (BT, T), (1, stride_A), (0, i_t * BT), (BT, BT), (0, 1))\nb_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], tl.load(p_Aqk, boundary_check=(0, 1)), 0)\np_do = tl.make_block_ptr(do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_dv = tl.make_block_ptr(dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_dv += tl.dot(b_A.to(b_do.dtype), b_do)\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_rms_layernorm_backward",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128, 'NUM_WARPS': 4}), triton.Config({'BLOCK_SIZE': 256, 'NUM_WARPS': 8}), triton.Config({'BLOCK_SIZE': 512, 'NUM_WARPS': 16}), triton.Config({'BLOCK_SIZE': 1024, 'NUM_WARPS': 16}), triton.Config({'BLOCK_SIZE': 2048, 'NUM_WARPS': 32}), triton.Config({'BLOCK_SIZE': 4096, 'NUM_WARPS': 32}), triton.Config({'BLOCK_SIZE': 8192, 'NUM_WARPS': 48})], key=['n_cols'])"
    ],
    "args": [
      {
        "name": "dY",
        "annotation": null
      },
      {
        "name": "dY_row_stride",
        "annotation": null
      },
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "X_row_stride",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "W_row_stride",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "r_row_stride",
        "annotation": null
      },
      {
        "name": "dX",
        "annotation": null
      },
      {
        "name": "dX_row_stride",
        "annotation": null
      },
      {
        "name": "dW",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_WARPS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_layernorm_backward(",
      "    dY,",
      "    dY_row_stride,",
      "    X,",
      "    X_row_stride,",
      "    W,",
      "    W_row_stride,",
      "    r,",
      "    r_row_stride,",
      "    dX,",
      "    dX_row_stride,",
      "    dW,",
      "    n_cols,",
      "    eps,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    NUM_WARPS: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_pids = tl.num_programs(0)",
      "",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    mask = col_offsets < n_cols",
      "",
      "    dY_ptr = dY + pid * dY_row_stride + col_offsets",
      "    X_ptr = X + pid * X_row_stride + col_offsets",
      "    dX_ptr = dX + pid * dX_row_stride + col_offsets",
      "",
      "    dY_row = tl.load(dY_ptr, mask=mask, other=0).to(tl.float32)",
      "    X_row = tl.load(X_ptr, mask=mask, other=0).to(tl.float32)",
      "    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)",
      "    rms = tl.load(r + pid).to(tl.float32)",
      "",
      "    X_norm = X_row * rms",
      "    dY_W = dY_row * W_row",
      "    sum_dY_X = tl.sum(dY_W * X_norm, axis=0)",
      "    dX = rms * (dY_W - X_norm * (sum_dY_X / n_cols))",
      "    dW_row = dY_row * X_norm",
      "    tl.atomic_add(dW + col_offsets, dW_row, mask=mask)",
      "    tl.store(dX_ptr, dX, mask=mask)"
    ],
    "file": "codes/313.py",
    "header": "def _rms_layernorm_backward(dY, dY_row_stride, X, X_row_stride, W, W_row_stride, r, r_row_stride, dX, dX_row_stride, dW, n_cols, eps, BLOCK_SIZE: tl.constexpr, NUM_WARPS: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_pids = tl.num_programs(0)\ncol_offsets = tl.arange(0, BLOCK_SIZE)\nmask = col_offsets < n_cols\ndY_ptr = dY + pid * dY_row_stride + col_offsets\nX_ptr = X + pid * X_row_stride + col_offsets\ndX_ptr = dX + pid * dX_row_stride + col_offsets\ndY_row = tl.load(dY_ptr, mask=mask, other=0).to(tl.float32)\nX_row = tl.load(X_ptr, mask=mask, other=0).to(tl.float32)\nW_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)\nrms = tl.load(r + pid).to(tl.float32)\nX_norm = X_row * rms\ndY_W = dY_row * W_row\nsum_dY_X = tl.sum(dY_W * X_norm, axis=0)\ndX = rms * (dY_W - X_norm * (sum_dY_X / n_cols))\ndW_row = dY_row * X_norm\ntl.atomic_add(dW + col_offsets, dW_row, mask=mask)\ntl.store(dX_ptr, dX, mask=mask)"
  },
  {
    "name": "_chunk_scan_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_dstate",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    D_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_D_head,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_dstates_head,",
      "    stride_dstates_hdim,",
      "    stride_dstates_dstate,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_dstates_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_last - dA_cs_m)",
      "    else:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)",
      "",
      "    offs_dstate = tl.arange(",
      "        0,",
      "        (",
      "            BLOCK_SIZE_DSTATE",
      "            if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128",
      "            else BLOCK_SIZE_K",
      "        ),",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_dstates_hdim",
      "        + offs_dstate[:, None] * stride_dstates_dstate",
      "    )",
      "    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates) * scale[:, None]",
      "    else:",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_dstate[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_dstates_dstate",
      "        acc *= scale[:, None]",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    K_MAX = chunk_size_limit",
      "    K_MIN = pid_m * BLOCK_SIZE_M",
      "    cb_ptrs += K_MIN * stride_cb_csize_k",
      "    dout_ptrs += K_MIN * stride_dout_seqlen",
      "    dA_cumsum_ptrs += K_MIN * stride_dA_cs_csize",
      "    for k in range(K_MIN, K_MAX, BLOCK_SIZE_K):",
      "        k = tl.multiple_of(k, BLOCK_SIZE_K)",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])",
      "",
      "        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)",
      "        cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "            dD = tl.sum(dout_res * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "        else:",
      "            dD = tl.sum(dout_res * x)",
      "            tl.store(dD_ptr, dD)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "codes/131.py",
    "header": "def _chunk_scan_chunk_state_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, D_ptr, b_ptr, dstates_ptr, dx_ptr, ddt_ptr, dD_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_D_head, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch, stride_dstates_chunk, stride_dstates_head, stride_dstates_hdim, stride_dstates_dstate, stride_dx_batch, stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, stride_dD_batch, stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr, IS_TRITON_22: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\ndstates_ptr += pid_b * stride_dstates_batch + pid_c * stride_dstates_chunk + pid_h * stride_dstates_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\nif not HAS_SEQ_IDX:\n    scale = tl.exp(dA_cs_last - dA_cs_m)\nelse:\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen)\n    scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\noffs_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\nb_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate)\ndstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_dstates_hdim + offs_dstate[:, None] * stride_dstates_dstate)\nif IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate), other=0.0)\n    dstates = tl.load(dstates_ptrs, mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates = dstates.to(b_ptr.dtype.element_ty)\n    acc = tl.dot(b, dstates) * scale[:, None]\nelse:\n    for k in range(0, dstate, BLOCK_SIZE_K):\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate - k), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates)\n        b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n        dstates_ptrs += BLOCK_SIZE_K * stride_dstates_dstate\n    acc *= scale[:, None]\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k)\ndout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nK_MAX = chunk_size_limit\nK_MIN = pid_m * BLOCK_SIZE_M\ncb_ptrs += K_MIN * stride_cb_csize_k\ndout_ptrs += K_MIN * stride_dout_seqlen\ndA_cumsum_ptrs += K_MIN * stride_dA_cs_csize\nfor k in range(K_MIN, K_MAX, BLOCK_SIZE_K):\n    k = tl.multiple_of(k, BLOCK_SIZE_K)\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k), other=0.0)\n    dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim), other=0.0)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(tl.float32)\n    cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])\n    mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n    cb = tl.where(mask, cb, 0.0)\n    cb = cb.to(dout_ptr.dtype.element_ty)\n    acc += tl.dot(cb, dout)\n    cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n    dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndx = acc * dt_m[:, None]\ndx_ptr += pid_b * stride_dx_batch + pid_c * chunk_size * stride_dx_seqlen + pid_h * stride_dx_head\ndx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim)\nif HAS_D:\n    dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\n    dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    dx += dout_res * D\ntl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nif HAS_D:\n    dD_ptr += pid_b * stride_dD_batch + pid_c * stride_dD_chunk + pid_h * stride_dD_head + pid_m * stride_dD_csize\n    if D_HAS_HDIM:\n        dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n        dD = tl.sum(dout_res * x, axis=0)\n        tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n    else:\n        dD = tl.sum(dout_res * x)\n        tl.store(dD_ptr, dD)\nddt = tl.sum(acc * x, axis=1)\nddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\ntl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
  },
  {
    "name": "_chunk_scan_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_f_ptr",
        "annotation": null
      },
      {
        "name": "dstates_b_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    D_ptr,",
      "    b_ptr,",
      "    dstates_f_ptr,",
      "    dstates_b_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_D_head,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_f_batch,",
      "    stride_dstates_f_chunk,",
      "    stride_dstates_f_head,",
      "    stride_dstates_f_hdim,",
      "    stride_dstates_f_dstate,",
      "    stride_dstates_b_batch,",
      "    stride_dstates_b_chunk,",
      "    stride_dstates_b_head,",
      "    stride_dstates_b_hdim,",
      "    stride_dstates_b_dstate,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_f_ptr += (",
      "        pid_b * stride_dstates_f_batch",
      "        + pid_c * stride_dstates_f_chunk",
      "        + pid_h * stride_dstates_f_head",
      "    )",
      "    dstates_b_ptr += (",
      "        pid_b * stride_dstates_b_batch",
      "        + pid_c * stride_dstates_b_chunk",
      "        + pid_h * stride_dstates_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    dA_cs_f_m = tl.load(",
      "        dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dA_cs_b_m = tl.load(",
      "        dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    dA_cs_f_last = tl.load(",
      "        dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize",
      "    ).to(tl.float32)",
      "    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)",
      "    scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)",
      "    scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)",
      "",
      "    offs_dstate = tl.arange(",
      "        0,",
      "        (",
      "            BLOCK_SIZE_DSTATE",
      "            if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128",
      "            else BLOCK_SIZE_K",
      "        ),",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate",
      "    )",
      "    dstates_f_ptrs = dstates_f_ptr + (",
      "        offs_n[None, :] * stride_dstates_f_hdim",
      "        + offs_dstate[:, None] * stride_dstates_f_dstate",
      "    )",
      "    dstates_b_ptrs = dstates_b_ptr + (",
      "        offs_n[None, :] * stride_dstates_b_hdim",
      "        + offs_dstate[:, None] * stride_dstates_b_dstate",
      "    )",
      "    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates_f = tl.load(",
      "            dstates_f_ptrs,",
      "            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates_f = dstates_f.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates_f) * scale_f[:, None]",
      "        dstates_b = tl.load(",
      "            dstates_b_ptrs,",
      "            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates_b = dstates_b.to(b_ptr.dtype.element_ty)",
      "        acc += tl.dot(b, dstates_b) * scale_b[:, None]",
      "    else:",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_dstate[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates_f = tl.load(",
      "                dstates_f_ptrs,",
      "                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates_f = dstates_f.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates_f) * scale_f[:, None]",
      "            dstates_b = tl.load(",
      "                dstates_b_ptrs,",
      "                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates_b = dstates_b.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates_b) * scale_b[:, None]",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_f_ptrs += BLOCK_SIZE_K * stride_dstates_f_dstate",
      "            dstates_b_ptrs += BLOCK_SIZE_K * stride_dstates_b_dstate",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "    K_MAX = chunk_size_limit",
      "",
      "    K_F_MAX = min((pid_m) * BLOCK_SIZE_M, chunk_size_limit)",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "        k = tl.multiple_of(k, BLOCK_SIZE_K)",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        if (k <= K_F_MAX + BLOCK_SIZE_M) or (k + BLOCK_SIZE_K >= K_F_MAX):",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (",
      "                k + offs_k[None, :] < K_MAX",
      "            )",
      "            a_f = tl.where(",
      "                mask_f, tl.exp((dA_cs_f_k[None, :] - dA_cs_f_m[:, None])), 0.0",
      "            )",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_b = k + offs_k[None, :] <= offs_m[:, None]",
      "            cb *= a_f + tl.where(",
      "                mask_b, tl.exp((dA_cs_b_k[None, :] - dA_cs_b_m[:, None])), 0.0",
      "            )",
      "",
      "        elif k < K_F_MAX:",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_b = k + offs_k[None, :] <= offs_m[:, None]",
      "            cb *= tl.where(",
      "                mask_b, tl.exp((dA_cs_b_k[None, :] - dA_cs_b_m[:, None])), 0.0",
      "            )",
      "        else:",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (",
      "                k + offs_k[None, :] < K_MAX",
      "            )",
      "            cb *= tl.where(",
      "                mask_f, tl.exp((dA_cs_f_k[None, :] - dA_cs_f_m[:, None])), 0.0",
      "            )",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "            dD = tl.sum(dout_res * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "        else:",
      "            dD = tl.sum(dout_res * x)",
      "            tl.store(dD_ptr, dD)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "codes/121.py",
    "header": "def _chunk_scan_chunk_state_bwd_dx_kernel(x_ptr, cb_ptr, dout_ptr, dt_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, D_ptr, b_ptr, dstates_f_ptr, dstates_b_ptr, dx_ptr, ddt_ptr, dD_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_cb_batch, stride_cb_chunk, stride_cb_head, stride_cb_csize_m, stride_cb_csize_k, stride_dout_batch, stride_dout_seqlen, stride_dout_head, stride_dout_hdim, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, stride_D_head, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_f_batch, stride_dstates_f_chunk, stride_dstates_f_head, stride_dstates_f_hdim, stride_dstates_f_dstate, stride_dstates_b_batch, stride_dstates_b_chunk, stride_dstates_b_head, stride_dstates_b_hdim, stride_dstates_b_dstate, stride_dx_batch, stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, stride_dD_batch, stride_dD_chunk, stride_dD_head, stride_dD_csize, stride_dD_hdim, HAS_D: tl.constexpr, D_HAS_HDIM: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr, IS_TRITON_22: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ncb_ptr += pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_h // nheads_ngroups_ratio * stride_cb_head\ndout_ptr += pid_b * stride_dout_batch + pid_c * chunk_size * stride_dout_seqlen + pid_h * stride_dout_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + pid_h * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + pid_h * stride_dA_cs_b_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\ndstates_f_ptr += pid_b * stride_dstates_f_batch + pid_c * stride_dstates_f_chunk + pid_h * stride_dstates_f_head\ndstates_b_ptr += pid_b * stride_dstates_b_batch + pid_c * stride_dstates_b_chunk + pid_h * stride_dstates_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\ndA_cs_f_m = tl.load(dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndA_cs_b_m = tl.load(dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndA_cs_f_last = tl.load(dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize).to(tl.float32)\ndA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)\nscale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)\nscale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)\noffs_dstate = tl.arange(0, BLOCK_SIZE_DSTATE if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\nb_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate)\ndstates_f_ptrs = dstates_f_ptr + (offs_n[None, :] * stride_dstates_f_hdim + offs_dstate[:, None] * stride_dstates_f_dstate)\ndstates_b_ptrs = dstates_b_ptr + (offs_n[None, :] * stride_dstates_b_hdim + offs_dstate[:, None] * stride_dstates_b_dstate)\nif IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate), other=0.0)\n    dstates_f = tl.load(dstates_f_ptrs, mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates_f = dstates_f.to(b_ptr.dtype.element_ty)\n    acc = tl.dot(b, dstates_f) * scale_f[:, None]\n    dstates_b = tl.load(dstates_b_ptrs, mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates_b = dstates_b.to(b_ptr.dtype.element_ty)\n    acc += tl.dot(b, dstates_b) * scale_b[:, None]\nelse:\n    for k in range(0, dstate, BLOCK_SIZE_K):\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate - k), other=0.0)\n        dstates_f = tl.load(dstates_f_ptrs, mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates_f = dstates_f.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates_f) * scale_f[:, None]\n        dstates_b = tl.load(dstates_b_ptrs, mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates_b = dstates_b.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates_b) * scale_b[:, None]\n        b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n        dstates_f_ptrs += BLOCK_SIZE_K * stride_dstates_f_dstate\n        dstates_b_ptrs += BLOCK_SIZE_K * stride_dstates_b_dstate\noffs_k = tl.arange(0, BLOCK_SIZE_K)\ncb_ptrs = cb_ptr + (offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k)\ndout_ptrs = dout_ptr + (offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\ndA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\ndA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\nK_MAX = chunk_size_limit\nK_F_MAX = min(pid_m * BLOCK_SIZE_M, chunk_size_limit)\nfor k in range(0, K_MAX, BLOCK_SIZE_K):\n    k = tl.multiple_of(k, BLOCK_SIZE_K)\n    cb = tl.load(cb_ptrs, mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k), other=0.0)\n    dout = tl.load(dout_ptrs, mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim), other=0.0)\n    if k <= K_F_MAX + BLOCK_SIZE_M or k + BLOCK_SIZE_K >= K_F_MAX:\n        dA_cs_f_k = tl.load(dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n        a_f = tl.where(mask_f, tl.exp(dA_cs_f_k[None, :] - dA_cs_f_m[:, None]), 0.0)\n        dA_cs_b_k = tl.load(dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask_b = k + offs_k[None, :] <= offs_m[:, None]\n        cb *= a_f + tl.where(mask_b, tl.exp(dA_cs_b_k[None, :] - dA_cs_b_m[:, None]), 0.0)\n    elif k < K_F_MAX:\n        dA_cs_b_k = tl.load(dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask_b = k + offs_k[None, :] <= offs_m[:, None]\n        cb *= tl.where(mask_b, tl.exp(dA_cs_b_k[None, :] - dA_cs_b_m[:, None]), 0.0)\n    else:\n        dA_cs_f_k = tl.load(dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)\n        mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)\n        cb *= tl.where(mask_f, tl.exp(dA_cs_f_k[None, :] - dA_cs_f_m[:, None]), 0.0)\n    cb = cb.to(dout_ptr.dtype.element_ty)\n    acc += tl.dot(cb, dout)\n    cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k\n    dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen\n    dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)\ndx = acc * dt_m[:, None]\ndx_ptr += pid_b * stride_dx_batch + pid_c * chunk_size * stride_dx_seqlen + pid_h * stride_dx_head\ndx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim)\nif HAS_D:\n    dout_res_ptrs = dout_ptr + (offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim)\n    dout_res = tl.load(dout_res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\n    if D_HAS_HDIM:\n        D = tl.load(D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0).to(tl.float32)\n    else:\n        D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)\n    dx += dout_res * D\ntl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nif HAS_D:\n    dD_ptr += pid_b * stride_dD_batch + pid_c * stride_dD_chunk + pid_h * stride_dD_head + pid_m * stride_dD_csize\n    if D_HAS_HDIM:\n        dD_ptrs = dD_ptr + offs_n * stride_dD_hdim\n        dD = tl.sum(dout_res * x, axis=0)\n        tl.store(dD_ptrs, dD, mask=offs_n < hdim)\n    else:\n        dD = tl.sum(dout_res * x)\n        tl.store(dD_ptr, dD)\nddt = tl.sum(acc * x, axis=1)\nddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\ntl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['loss_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    loss_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.atomic_add(loss_ptr, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "codes/170.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, loss_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr):",
    "body": "idx = tl.program_id(axis=0)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\noffsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + offsets)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    local_x_block_ptr = x_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == v_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    m = m_new\n    A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n    v_range = v_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\nloss += tl.sum(lse) / N\ntl.atomic_add(loss_ptr, loss)\ntl.store(lse_ptr + offsets, lse)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_prologue",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512})], key=['V', 'N', 'H'], reset_to_zero=['sz_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_prologue(",
      "    sz_ptr,",
      "    x_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    lse = tl.load(lse_global_ptr + offsets)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    tl.store(sz_block_ptr, softmax_z.to(tl.float16))"
    ],
    "file": "codes/170.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_prologue(sz_ptr, x_ptr, A_t_ptr, lse_global_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_sz_N, stride_sz_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V = tl.program_id(axis=1)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\noffsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nlse = tl.load(lse_global_ptr + offsets)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nsz_block_ptr = tl.make_block_ptr(base=sz_ptr, shape=(N, V), strides=(stride_sz_N, stride_sz_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nsoftmax_z = (z_j_to_k - lse[:, None]).exp()\ntl.store(sz_block_ptr, softmax_z.to(tl.float16))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr', 'x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_x_grad_N",
        "annotation": null
      },
      {
        "name": "stride_x_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue(",
      "    sz_ptr,",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_x_grad_N,",
      "    stride_x_grad_H,",
      "    stride_A_grad_H,",
      "    stride_A_grad_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    idx_NV = tl.program_id(axis=1)",
      "    if idx_NV < (N // N_BLOCK_SIZE):",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "            sz_ptr,",
      "            y_ptr,",
      "            A_t_ptr,",
      "            x_grad_ptr,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_x_grad_N,",
      "            stride_x_grad_H,",
      "            stride_A_grad_H,",
      "            stride_A_grad_V,",
      "            stride_sz_N,",
      "            stride_sz_V,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "        )",
      "    else:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "            sz_ptr,",
      "            x_ptr,",
      "            y_ptr,",
      "            A_grad_ptr,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_x_grad_N,",
      "            stride_x_grad_H,",
      "            stride_A_grad_H,",
      "            stride_A_grad_V,",
      "            stride_sz_N,",
      "            stride_sz_V,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "        )"
    ],
    "file": "codes/170.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue(sz_ptr, x_ptr, y_ptr, A_t_ptr, x_grad_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_x_grad_N, stride_x_grad_H, stride_A_grad_H, stride_A_grad_V, stride_sz_N, stride_sz_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nidx_NV = tl.program_id(axis=1)\nif idx_NV < N // N_BLOCK_SIZE:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dx(sz_ptr, y_ptr, A_t_ptr, x_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_x_grad_N, stride_x_grad_H, stride_A_grad_H, stride_A_grad_V, stride_sz_N, stride_sz_V, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nelse:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dA(sz_ptr, x_ptr, y_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_x_grad_N, stride_x_grad_H, stride_A_grad_H, stride_A_grad_V, stride_sz_N, stride_sz_V, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_x_grad_N",
        "annotation": null
      },
      {
        "name": "stride_x_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    sz_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_x_grad_N,",
      "    stride_x_grad_H,",
      "    stride_A_grad_H,",
      "    stride_A_grad_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_H = tl.program_id(axis=0)",
      "    idx_N = tl.program_id(axis=1)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_grad_N, stride_x_grad_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_offsets)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "        A_v = tl.load(A_t_block_ptr).trans()",
      "        sz = tl.load(sz_block_ptr)",
      "",
      "        x_grad_acc = tl.dot(sz, A_v, x_grad_acc)",
      "        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        sz_block_ptr = tl.advance(sz_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    tl.store(x_grad_block_ptr, x_grad_acc / N)"
    ],
    "file": "codes/170.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(sz_ptr, y_ptr, A_t_ptr, x_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_x_grad_N, stride_x_grad_H, stride_A_grad_H, stride_A_grad_V, stride_sz_N, stride_sz_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_H = tl.program_id(axis=0)\nidx_N = tl.program_id(axis=1)\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_grad_N, stride_x_grad_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nsz_block_ptr = tl.make_block_ptr(base=sz_ptr, shape=(N, V), strides=(stride_sz_N, stride_sz_V), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_offsets)\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\nfor idx_V in range(V // V_BLOCK_SIZE):\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    A_v = tl.load(A_t_block_ptr).trans()\n    sz = tl.load(sz_block_ptr)\n    x_grad_acc = tl.dot(sz, A_v, x_grad_acc)\n    x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    sz_block_ptr = tl.advance(sz_block_ptr, [0, V_BLOCK_SIZE])\n    V_offsets += V_BLOCK_SIZE\ntl.store(x_grad_block_ptr, x_grad_acc / N)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_x_grad_N",
        "annotation": null
      },
      {
        "name": "stride_x_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    sz_ptr,",
      "    x_ptr,",
      "    y_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_x_grad_N,",
      "    stride_x_grad_H,",
      "    stride_A_grad_H,",
      "    stride_A_grad_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_H = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1) - (N // N_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_grad_H, stride_A_grad_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + N_offsets)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "        x_chunk = tl.load(x_block_ptr)",
      "        sz = tl.load(sz_block_ptr).trans()",
      "",
      "        A_grad_acc = tl.dot(sz, x_chunk, A_grad_acc)",
      "        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        sz_block_ptr = tl.advance(sz_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_offsets += N_BLOCK_SIZE",
      "",
      "    tl.store(A_t_grad_block_ptr, A_grad_acc.trans() / N)"
    ],
    "file": "codes/170.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(sz_ptr, x_ptr, y_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_x_grad_N, stride_x_grad_H, stride_A_grad_H, stride_A_grad_V, stride_sz_N, stride_sz_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_H = tl.program_id(axis=0)\nidx_V = tl.program_id(axis=1) - N // N_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(0, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_t_grad_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_grad_H, stride_A_grad_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nsz_block_ptr = tl.make_block_ptr(base=sz_ptr, shape=(N, V), strides=(stride_sz_N, stride_sz_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_offsets = tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)\nfor idx_N in range(N // N_BLOCK_SIZE):\n    y = tl.load(y_ptr + N_offsets)\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    x_chunk = tl.load(x_block_ptr)\n    sz = tl.load(sz_block_ptr).trans()\n    A_grad_acc = tl.dot(sz, x_chunk, A_grad_acc)\n    A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    sz_block_ptr = tl.advance(sz_block_ptr, [N_BLOCK_SIZE, 0])\n    N_offsets += N_BLOCK_SIZE\ntl.store(A_t_grad_block_ptr, A_grad_acc.trans() / N)"
  },
  {
    "name": "layer_norm_fwd_kernel_quant",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel_quant(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    RESIDUAL_OUT,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    N,",
      "    eps,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    if HAS_WEIGHT:",
      "        w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "",
      "    y = x_hat * w if HAS_WEIGHT else x_hat",
      "    if HAS_BIAS:",
      "        y = y + b",
      "",
      "    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-5)",
      "",
      "    y = tl.extra.cuda.libdevice.round(y * scale)",
      "    y = tl.maximum(tl.minimum(y, 127), -128) / scale",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "codes/352.py",
    "header": "def layer_norm_fwd_kernel_quant(X, Y, W, B, RESIDUAL, RESIDUAL_OUT, Mean, Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, N, eps, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr):",
    "body": "row = tl.program_id(0)\nX += row * stride_x_row\nY += row * stride_y_row\nif HAS_RESIDUAL:\n    RESIDUAL += row * stride_res_row\nif STORE_RESIDUAL_OUT:\n    RESIDUAL_OUT += row * stride_res_out_row\ncols = tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\nif HAS_RESIDUAL:\n    residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x += residual\nif STORE_RESIDUAL_OUT:\n    tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\nif not IS_RMS_NORM:\n    mean = tl.sum(x, axis=0) / N\n    tl.store(Mean + row, mean)\n    xbar = tl.where(cols < N, x - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nelse:\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nmask = cols < N\nif HAS_WEIGHT:\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\nif HAS_BIAS:\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\nx_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\ny = x_hat * w if HAS_WEIGHT else x_hat\nif HAS_BIAS:\n    y = y + b\nscale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-05)\ny = tl.extra.cuda.libdevice.round(y * scale)\ny = tl.maximum(tl.minimum(y, 127), -128) / scale\ntl.store(Y + cols, y, mask=mask)"
  },
  {
    "name": "layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    DRESIDUAL_IN,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    if HAS_WEIGHT:",
      "        w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w if HAS_WEIGHT else xhat",
      "            if HAS_BIAS:",
      "                y = y + b",
      "",
      "            scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-5)",
      "",
      "            y = tl.extra.cuda.libdevice.round(y * scale)",
      "            y = tl.maximum(tl.minimum(y, 127), -128) / scale",
      "",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = dy",
      "        if HAS_WEIGHT:",
      "            wdy = dy * w",
      "            dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    if HAS_WEIGHT:",
      "        tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)"
    ],
    "file": "codes/352.py",
    "header": "def layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, DRESIDUAL_IN, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row, stride_dres_in_row, M, N, eps, rows_per_program, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, BLOCK_N)\nmask = cols < N\nX += row_start * stride_x_row\nif HAS_DRESIDUAL:\n    DRESIDUAL += row_start * stride_dres_row\nif STORE_DRESIDUAL:\n    DRESIDUAL_IN += row_start * stride_dres_in_row\nDY += row_start * stride_dy_row\nDX += row_start * stride_dx_row\nif RECOMPUTE_OUTPUT:\n    Y += row_start * stride_y_row\nif HAS_WEIGHT:\n    w = tl.load(W + cols, mask=mask).to(tl.float32)\n    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif RECOMPUTE_OUTPUT and HAS_BIAS:\n    b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\nif HAS_BIAS:\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\nrow_end = min((row_block_id + 1) * rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    if RECOMPUTE_OUTPUT:\n        y = xhat * w if HAS_WEIGHT else xhat\n        if HAS_BIAS:\n            y = y + b\n        scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-05)\n        y = tl.extra.cuda.libdevice.round(y * scale)\n        y = tl.maximum(tl.minimum(y, 127), -128) / scale\n        tl.store(Y + cols, y, mask=mask)\n    wdy = dy\n    if HAS_WEIGHT:\n        wdy = dy * w\n        dw += dy * xhat\n    if HAS_BIAS:\n        db += dy\n    if not IS_RMS_NORM:\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        c2 = tl.sum(wdy, axis=0) / N\n        dx = (wdy - (xhat * c1 + c2)) * rstd\n    else:\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        dx = (wdy - xhat * c1) * rstd\n    if HAS_DRESIDUAL:\n        dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n        dx += dres\n    if STORE_DRESIDUAL:\n        tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n    tl.store(DX + cols, dx, mask=mask)\n    X += stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += stride_dres_in_row\n    if RECOMPUTE_OUTPUT:\n        Y += stride_y_row\n    DY += stride_dy_row\n    DX += stride_dx_row\nif HAS_WEIGHT:\n    tl.store(DW + row_block_id * N + cols, dw, mask=mask)\nif HAS_BIAS:\n    tl.store(DB + row_block_id * N + cols, db, mask=mask)"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "codes/183.py",
    "header": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nnum_idx_N, num_idx_V_group = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_V_group = tl.swizzle2d(idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)\nV_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nm = tl.max(z_j_to_k, 1)\ns = tl.sum(tl.exp(z_j_to_k - m[:, None]), axis=1)\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nmask = y[:, None] == V_range[None, :]\nloss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\ntl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\nlse = m + tl.log(s)\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\ntl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\ntl.store(lse_row_ptr, lse[:, None])"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(",
      "            x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "codes/183.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(z_nv_ptr, y_ptr, A_t_ptr, x_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0) // SPLIT_V\nidx_H = tl.program_id(axis=1)\nidx_V_tile = tl.program_id(axis=0) % SPLIT_V\nnum_idx_N, num_idx_H = (tl.num_programs(0) - triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N, tl.num_programs(1))\nidx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE)\nV_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, V_split_offset), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, V_split_offset), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range, eviction_policy='evict_last')\nlse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), eviction_policy='evict_last')\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\nfor _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n    mask = y[:, None] == v_range[None, :]\n    A_v = tl.load(A_t_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n    x_grad_acc = tl.dot(z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    v_range += V_BLOCK_SIZE\nif SPLIT_V == 1:\n    x_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))\nelse:\n    row_n = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    x_grad_simple_ptr = x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n    tl.atomic_add(x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr)",
      "                + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(",
      "                A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "            )",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(",
      "            A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "codes/183.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(z_nv_ptr, y_ptr, x_ptr, A_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N\nidx_H = tl.program_id(axis=1)\nidx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N\nnum_idx_V, num_idx_H = (tl.num_programs(0) - N_group // N_BLOCK_SIZE * SPLIT_V, tl.num_programs(1))\nidx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE)\nN_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(N_split_offset, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\nfor _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range, eviction_policy='evict_last')\n    lse = tl.load(lse_ptr + N_range, eviction_policy='evict_last')\n    mask = y[:, None] == V_range[None, :]\n    x_chunk = tl.load(x_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n    A_grad_acc = tl.dot(x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n    N_range += N_BLOCK_SIZE\nif SPLIT_N == 1:\n    A_grad_T_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\n    if idx_N_group > 0:\n        tl.store(A_grad_T_block_ptr, tl.load(A_grad_T_block_ptr) + (A_grad_acc / N).to(A_grad_ptr.type.element_ty))\n    else:\n        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))\nelse:\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    A_grad_T_simple_ptr = A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n    tl.atomic_add(A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_dispatcher",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=bwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad",
        "annotation": null
      },
      {
        "name": "At_grad",
        "annotation": null
      },
      {
        "name": "lse_global",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_dispatcher(",
      "    logits_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_t_ptr,",
      "    x_grad,",
      "    At_grad,",
      "    lse_global,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "",
      "    idx_NV = tl.program_id(axis=0)",
      "    tl.static_print(",
      "        V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V",
      "    )",
      "    if idx_NV < (N_group // N_BLOCK_SIZE * SPLIT_V):",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "            logits_ptr,",
      "            y_ptr,",
      "            A_t_ptr,",
      "            x_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N,",
      "            SPLIT_V,",
      "        )",
      "    else:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "            logits_ptr,",
      "            y_ptr,",
      "            x_ptr,",
      "            At_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N,",
      "            SPLIT_V,",
      "        )"
    ],
    "file": "codes/183.py",
    "header": "def linear_xent_bwd_dispatcher(logits_ptr, y_ptr, x_ptr, A_t_ptr, x_grad, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_NV = tl.program_id(axis=0)\ntl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)\nif idx_NV < N_group // N_BLOCK_SIZE * SPLIT_V:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dx(logits_ptr, y_ptr, A_t_ptr, x_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)\nelse:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dA(logits_ptr, y_ptr, x_ptr, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N, SPLIT_V)"
  },
  {
    "name": "chunk_gsa_fwd_k_kernel_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_fwd_k_kernel_inter(",
      "    q,",
      "    k,",
      "    h,",
      "    g,",
      "    o,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (T, K),",
      "            (HQ * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h)",
      "",
      "        b_A += tl.dot(b_q, b_k)",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    b_o = b_o * exp(b_g)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    if i_v == 0:",
      "        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/395.py",
    "header": "def chunk_gsa_fwd_k_kernel_inter(q, k, h, g, o, A, cu_seqlens, chunk_indices, scale, T, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_o += tl.dot(b_q, b_h)\n    b_A += tl.dot(b_q, b_k)\np_g = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_A = tl.make_block_ptr(A + (bos * HQ + i_hq) * BT, (T, BT), (HQ * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_g = tl.load(p_g, boundary_check=(0, 1))\nb_o = b_o * exp(b_g)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\nb_A = tl.where(m_s, b_A, 0.0)\nif i_v == 0:\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gsa_fwd_k_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_fwd_k_kernel_intra(",
      "    v,",
      "    g,",
      "    o,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    m_v = o_v < V",
      "",
      "    if i_t * BT + i_i * BC > T:",
      "        return",
      "",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_gn = g + (bos + min(i_t * BT + i_i * BC, T)) * H * V + i_h * V + o_v",
      "",
      "    b_gn = tl.load(p_gn, mask=m_v, other=0)",
      "",
      "    b_o = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(0, i_i):",
      "        p_A = tl.make_block_ptr(",
      "            A + (bos * HQ + i_hq) * BT,",
      "            (T, BT),",
      "            (HQ * BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_gv = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_vg = (b_v * exp(b_gn[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_A, b_vg)",
      "",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    b_o *= exp(b_g - b_gn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_A = (",
      "        (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * HQ * BT + i_hq * BT + i_i * BC",
      "    )",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        p_v = v + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v",
      "        p_gv = g + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v",
      "",
      "        b_A = tl.load(A + o_A + j, mask=m_A, other=0)",
      "",
      "        b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)",
      "        b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)",
      "",
      "        b_vg = b_v[None, :] * exp(b_g - b_gv[None, :])",
      "",
      "        b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    b_o += tl.load(p_o, boundary_check=(0, 1))",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/395.py",
    "header": "def chunk_gsa_fwd_k_kernel_intra(v, g, o, A, cu_seqlens, chunk_indices, T, HQ: tl.constexpr, H: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr, NG: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // NG\ni_t, i_i = (i_c // NC, i_c % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\no_v = i_v * BV + tl.arange(0, BV)\nm_v = o_v < V\nif i_t * BT + i_i * BC > T:\n    return\np_g = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_gn = g + (bos + min(i_t * BT + i_i * BC, T)) * H * V + i_h * V + o_v\nb_gn = tl.load(p_gn, mask=m_v, other=0)\nb_o = tl.zeros([BC, BV], dtype=tl.float32)\nfor i_j in range(0, i_i):\n    p_A = tl.make_block_ptr(A + (bos * HQ + i_hq) * BT, (T, BT), (HQ * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    p_gv = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_vg = (b_v * exp(b_gn[None, :] - b_gv)).to(b_v.dtype)\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, b_vg)\nb_g = tl.load(p_g, boundary_check=(0, 1))\nb_o *= exp(b_g - b_gn[None, :])\no_i = tl.arange(0, BC)\no_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * HQ * BT + i_hq * BT + i_i * BC\nm_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    p_v = v + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v\n    p_gv = g + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v\n    b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n    b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)\n    b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)\n    b_vg = b_v[None, :] * exp(b_g - b_gv[None, :])\n    b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)\np_o = tl.make_block_ptr(o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\nb_o += tl.load(p_o, boundary_check=(0, 1))\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gsa_bwd_k_kernel_dA",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_bwd_k_kernel_dA(",
      "    v,",
      "    g,",
      "    do,",
      "    dA,",
      "    chunk_indices,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    m_v = o_v < V",
      "",
      "    if i_t * BT + i_i * BC > T:",
      "        return",
      "",
      "    p_dA = tl.make_block_ptr(",
      "        dA + ((i_v * all + bos) * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "",
      "    b_dA = tl.zeros([BC, BC], dtype=tl.float32)",
      "    if i_i > i_j:",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT + i_j * BC),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "        p_gv = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT + i_j * BC),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_gn = tl.load(p_gn, mask=m_v, other=0.0)",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_g - b_gn[None, :]) * scale).to(b_do.dtype)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_vg = (b_v * exp(b_gn[:, None] - b_gv)).to(b_v.dtype)",
      "",
      "        b_dA = tl.dot(b_do, b_vg)",
      "    elif i_i == i_j:",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = v + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v",
      "        p_gv = g + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale",
      "        m_v = o_v < V",
      "",
      "        o_i = tl.arange(0, BC)",
      "",
      "        m_dA = o_i[:, None] >= o_i[None, :]",
      "        for j in range(0, min(BC, T - i_t * BT - i_j * BC)):",
      "",
      "            b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)",
      "            b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)",
      "",
      "            b_dAj = tl.sum(b_do * b_v[None, :] * exp(b_g - b_gv[None, :]), 1)",
      "            b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)",
      "",
      "            p_v += H * V",
      "            p_gv += H * V",
      "        b_dA = tl.where(m_dA, b_dA, 0.0)",
      "    tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/395.py",
    "header": "def chunk_gsa_bwd_k_kernel_dA(v, g, do, dA, chunk_indices, cu_seqlens, scale, T, B: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr, NG: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // NG\ni_t, i_i, i_j = (i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC) % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    all = B * T\no_v = i_v * BV + tl.arange(0, BV)\nm_v = o_v < V\nif i_t * BT + i_i * BC > T:\n    return\np_dA = tl.make_block_ptr(dA + ((i_v * all + bos) * HQ + i_hq) * BT, (T, BT), (HQ * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\nb_dA = tl.zeros([BC, BC], dtype=tl.float32)\nif i_i > i_j:\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n    p_gv = tl.make_block_ptr(g + (bos * H + i_h) * V, (V, T), (1, H * V), (i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n    p_gn = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v\n    p_g = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_gn = tl.load(p_gn, mask=m_v, other=0.0)\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_do = (b_do * exp(b_g - b_gn[None, :]) * scale).to(b_do.dtype)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_gv = tl.load(p_gv, boundary_check=(0, 1))\n    b_vg = (b_v * exp(b_gn[:, None] - b_gv)).to(b_v.dtype)\n    b_dA = tl.dot(b_do, b_vg)\nelif i_i == i_j:\n    p_g = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_v = v + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v\n    p_gv = g + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n    m_v = o_v < V\n    o_i = tl.arange(0, BC)\n    m_dA = o_i[:, None] >= o_i[None, :]\n    for j in range(0, min(BC, T - i_t * BT - i_j * BC)):\n        b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)\n        b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)\n        b_dAj = tl.sum(b_do * b_v[None, :] * exp(b_g - b_gv[None, :]), 1)\n        b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)\n        p_v += H * V\n        p_gv += H * V\n    b_dA = tl.where(m_dA, b_dA, 0.0)\ntl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gsa_bwd_k_kernel_dqkvg",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [2, 3, 4]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dgv",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_bwd_k_kernel_dqkvg(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    A,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dg,",
      "    dgv,",
      "    dA,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    o_t = min(i_t * BT + BT, T)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (HQ * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + ((i_k * all + bos) * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k))",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        o_v = i_v * BV + tl.arange(0, BV)",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_gn = g + (bos + o_t - 1) * H * V + i_h * V + o_v",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + ((i_k * all + bos) * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dg = tl.make_block_ptr(",
      "            dg + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dgv = tl.make_block_ptr(",
      "            dgv + ((i_k * all + bos) * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * HQ + i_hq) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        m_v = o_v < V",
      "",
      "        b_gn = tl.load(p_gn, mask=m_v, other=0)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_gv = exp(b_gn[None, :] - b_g)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_g) * scale).to(b_do.dtype)",
      "",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dg = tl.sum(tl.trans(b_h) * b_dh, 0) * exp(b_gn)",
      "",
      "        b_dh = b_dh.to(b_k.dtype)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_k.dtype))",
      "        b_dk += tl.dot((b_v * b_gv).to(b_v.dtype), tl.trans(b_dh))",
      "",
      "        b_dv = tl.dot(b_k, b_dh) * b_gv",
      "",
      "        b_dg += tl.sum(b_dv * b_v, 0)",
      "",
      "        if i_k == 0:",
      "            b_dgv = tl.load(p_dg, boundary_check=(0, 1)) + b_dg[None, :]",
      "        else:",
      "            b_dgv = tl.zeros([BT, BV], dtype=tl.float32) + b_dg[None, :]",
      "",
      "        tl.store(p_dgv, b_dgv.to(p_dgv.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dA = tl.make_block_ptr(",
      "        dA + (bos * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (HQ * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (HQ * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "    b_dq += tl.dot(b_dA, b_k)",
      "    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q)",
      "",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/395.py",
    "header": "def chunk_gsa_bwd_k_kernel_dqkvg(q, k, v, h, g, A, do, dh, dq, dk, dv, dg, dgv, dA, cu_seqlens, chunk_indices, scale, T, B: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    all = T\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\n    all = B * T\no_i = tl.arange(0, BT)\no_t = min(i_t * BT + BT, T)\nm_s = o_i[:, None] >= o_i[None, :]\np_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_A = tl.make_block_ptr(A + ((i_k * all + bos) * HQ + i_hq) * BT, (T, BT), (HQ * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k))\nb_A = tl.where(m_s, b_A, 0.0)\ntl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    o_v = i_v * BV + tl.arange(0, BV)\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_g = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_gn = g + (bos + o_t - 1) * H * V + i_h * V + o_v\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + ((i_k * all + bos) * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dg = tl.make_block_ptr(dg + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dgv = tl.make_block_ptr(dgv + ((i_k * all + bos) * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh + (i_tg * HQ + i_hq) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    m_v = o_v < V\n    b_gn = tl.load(p_gn, mask=m_v, other=0)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_gv = exp(b_gn[None, :] - b_g)\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_do = (b_do * exp(b_g) * scale).to(b_do.dtype)\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dg = tl.sum(tl.trans(b_h) * b_dh, 0) * exp(b_gn)\n    b_dh = b_dh.to(b_k.dtype)\n    b_dq += tl.dot(b_do, b_h.to(b_k.dtype))\n    b_dk += tl.dot((b_v * b_gv).to(b_v.dtype), tl.trans(b_dh))\n    b_dv = tl.dot(b_k, b_dh) * b_gv\n    b_dg += tl.sum(b_dv * b_v, 0)\n    if i_k == 0:\n        b_dgv = tl.load(p_dg, boundary_check=(0, 1)) + b_dg[None, :]\n    else:\n        b_dgv = tl.zeros([BT, BV], dtype=tl.float32) + b_dg[None, :]\n    tl.store(p_dgv, b_dgv.to(p_dgv.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\np_dA = tl.make_block_ptr(dA + (bos * HQ + i_hq) * BT, (T, BT), (HQ * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\np_dq = tl.make_block_ptr(dq + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_dA = tl.load(p_dA, boundary_check=(0, 1))\nb_dq += tl.dot(b_dA, b_k)\nb_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q)\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gsa_bwd_k_kernel_intra_dvg",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_bwd_k_kernel_intra_dvg(",
      "    v,",
      "    g,",
      "    o,",
      "    A,",
      "    do,",
      "    dv,",
      "    dg,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    m_v = o_v < V",
      "",
      "    if i_t * BT + i_i * BC > T:",
      "        return",
      "",
      "    p_gv = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * V + i_h * V + o_v",
      "",
      "    b_gn = tl.load(p_gn, mask=m_v, other=0)",
      "",
      "    b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + (bos * HQ + i_hq) * BT,",
      "            (BT, T),",
      "            (1, HQ * BT),",
      "            (i_i * BC, i_t * BT + i_j * BC),",
      "            (BC, BC),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)) * safe_exp(b_g - b_gn[None, :])",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "        b_dv += tl.dot(b_A, b_do.to(b_A.dtype))",
      "    b_dv *= exp(b_gn[None, :] - b_gv)",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_c = i_i * BC + tl.arange(0, BC)",
      "",
      "    p_g = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v",
      "    p_A = A + (bos + i_t * BT + i_i * BC) * HQ * BT + i_hq * BT + o_c",
      "    p_do = do + (bos + i_t * BT + i_i * BC) * HQ * V + i_hq * V + o_v",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_A = tl.load(p_A)",
      "",
      "        b_g = tl.load(p_g, mask=m_v, other=0)",
      "        b_do = tl.load(p_do, mask=m_v, other=0)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dv += tl.where(",
      "            m_i, exp(b_g[None, :] - b_gv) * b_A[:, None] * b_do[None, :], 0.0",
      "        )",
      "",
      "        p_g += H * V",
      "        p_A += HQ * BT",
      "        p_do += HQ * V",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)",
      "    b_v = tl.load(p_v, boundary_check=(0, 1)).to(tl.float32)",
      "    b_do = tl.load(p_do, boundary_check=(0, 1)).to(tl.float32)",
      "    b_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1)).to(tl.float32)",
      "    b_dg = b_o * b_do - b_v * b_dv",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/395.py",
    "header": "def chunk_gsa_bwd_k_kernel_intra_dvg(v, g, o, A, do, dv, dg, cu_seqlens, chunk_indices, T, HQ: tl.constexpr, H: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr, NG: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // NG\ni_t, i_i = (i_c // NC, i_c % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\no_v = i_v * BV + tl.arange(0, BV)\nm_v = o_v < V\nif i_t * BT + i_i * BC > T:\n    return\np_gv = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * V + i_h * V + o_v\nb_gn = tl.load(p_gn, mask=m_v, other=0)\nb_gv = tl.load(p_gv, boundary_check=(0, 1))\nb_dv = tl.zeros([BC, BV], dtype=tl.float32)\nfor i_j in range(i_i + 1, NC):\n    p_g = tl.make_block_ptr(g + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + (bos * HQ + i_hq) * BT, (BT, T), (1, HQ * BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    b_g = tl.load(p_g, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)) * safe_exp(b_g - b_gn[None, :])\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A, b_do.to(b_A.dtype))\nb_dv *= exp(b_gn[None, :] - b_gv)\no_i = tl.arange(0, BC)\no_c = i_i * BC + tl.arange(0, BC)\np_g = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v\np_A = A + (bos + i_t * BT + i_i * BC) * HQ * BT + i_hq * BT + o_c\np_do = do + (bos + i_t * BT + i_i * BC) * HQ * V + i_hq * V + o_v\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_A = tl.load(p_A)\n    b_g = tl.load(p_g, mask=m_v, other=0)\n    b_do = tl.load(p_do, mask=m_v, other=0)\n    m_i = o_i[:, None] <= j\n    b_dv += tl.where(m_i, exp(b_g[None, :] - b_gv) * b_A[:, None] * b_do[None, :], 0.0)\n    p_g += H * V\n    p_A += HQ * BT\n    p_do += HQ * V\np_o = tl.make_block_ptr(o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_dv = tl.make_block_ptr(dv + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_dg = tl.make_block_ptr(dg + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\nb_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)\nb_v = tl.load(p_v, boundary_check=(0, 1)).to(tl.float32)\nb_do = tl.load(p_do, boundary_check=(0, 1)).to(tl.float32)\nb_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1)).to(tl.float32)\nb_dg = b_o * b_do - b_v * b_dv\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    Z,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "codes/115.py",
    "header": "def _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row, stride_y_row, stride_z_row, M, N, eps, BLOCK_N: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.constexpr):",
    "body": "row = tl.program_id(0)\ngroup = tl.program_id(1)\nX += row * stride_x_row + group * N\nY += row * stride_y_row + group * N\nif HAS_Z:\n    Z += row * stride_z_row + group * N\nif not IS_RMS_NORM:\n    Mean += group * M\nRstd += group * M\nW += group * N\nif HAS_BIAS:\n    B += group * N\ncols = tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\nif HAS_Z and (not NORM_BEFORE_GATE):\n    z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n    x *= z * tl.sigmoid(z)\nif not IS_RMS_NORM:\n    mean = tl.sum(x, axis=0) / N\n    tl.store(Mean + row, mean)\n    xbar = tl.where(cols < N, x - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nelse:\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nmask = cols < N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif HAS_BIAS:\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\nx_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\ny = x_hat * w + b if HAS_BIAS else x_hat * w\nif HAS_Z and NORM_BEFORE_GATE:\n    z = tl.load(Z + cols, mask=mask).to(tl.float32)\n    y *= z * tl.sigmoid(z)\ntl.store(Y + cols, y, mask=mask)"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DZ",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dz_row",
        "annotation": null
      },
      {
        "name": "stride_dw_row",
        "annotation": null
      },
      {
        "name": "stride_db_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Z,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DZ,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_z_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dz_row,",
      "    stride_dw_row,",
      "    stride_db_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row + group * N",
      "    if HAS_Z:",
      "        Z += row_start * stride_z_row + group * N",
      "        DZ += row_start * stride_dz_row + group * N",
      "    DY += row_start * stride_dy_row + group * N",
      "    DX += row_start * stride_dx_row + group * N",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:",
      "        B += group * N",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            x_og = x",
      "            x = x_og * z * tl.sigmoid(z)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if HAS_Z and NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            z_sigmoid = tl.sigmoid(z)",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            if RECOMPUTE_OUTPUT:",
      "                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)",
      "            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dy *= z * z_sigmoid",
      "        else:",
      "            if RECOMPUTE_OUTPUT:",
      "                y = xhat * w + b if HAS_BIAS else xhat * w",
      "                tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        c1 = tl.sum(xhat * wdy, axis=0) / N",
      "        if not IS_RMS_NORM:",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            dx = (wdy - xhat * c1) * rstd",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z_sigmoid = tl.sigmoid(z)",
      "            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dx *= z * z_sigmoid",
      "",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_Z:",
      "            Z += stride_z_row",
      "            DZ += stride_dz_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
    ],
    "file": "codes/115.py",
    "header": "def _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd, stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dz_row, stride_dw_row, stride_db_row, M, N, eps, rows_per_program, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\ngroup = tl.program_id(1)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, BLOCK_N)\nmask = cols < N\nX += row_start * stride_x_row + group * N\nif HAS_Z:\n    Z += row_start * stride_z_row + group * N\n    DZ += row_start * stride_dz_row + group * N\nDY += row_start * stride_dy_row + group * N\nDX += row_start * stride_dx_row + group * N\nif RECOMPUTE_OUTPUT:\n    Y += row_start * stride_y_row + group * N\nif not IS_RMS_NORM:\n    Mean += group * M\nRstd += group * M\nW += group * N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n    B += group * N\n    b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\ndw = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_BIAS:\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\nrow_end = min((row_block_id + 1) * rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        mean = tl.load(Mean + row)\n    if HAS_Z and (not NORM_BEFORE_GATE):\n        z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n        x_og = x\n        x = x_og * z * tl.sigmoid(z)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n        z_sigmoid = tl.sigmoid(z)\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        if RECOMPUTE_OUTPUT:\n            tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n        dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n        tl.store(DZ + cols, dz, mask=mask)\n        dy *= z * z_sigmoid\n    elif RECOMPUTE_OUTPUT:\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        tl.store(Y + cols, y, mask=mask)\n    wdy = w * dy\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    if not IS_RMS_NORM:\n        c2 = tl.sum(wdy, axis=0) / N\n        dx = (wdy - (xhat * c1 + c2)) * rstd\n    else:\n        dx = (wdy - xhat * c1) * rstd\n    dw += dy * xhat\n    if HAS_BIAS:\n        db += dy\n    if HAS_Z and (not NORM_BEFORE_GATE):\n        z_sigmoid = tl.sigmoid(z)\n        dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n        tl.store(DZ + cols, dz, mask=mask)\n        dx *= z * z_sigmoid\n    tl.store(DX + cols, dx, mask=mask)\n    X += stride_x_row\n    if HAS_Z:\n        Z += stride_z_row\n        DZ += stride_dz_row\n    if RECOMPUTE_OUTPUT:\n        Y += stride_y_row\n    DY += stride_dy_row\n    DX += stride_dx_row\ntl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)\nif HAS_BIAS:\n    tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
  },
  {
    "name": "flash_attention2_nopad_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=attention_configs, key=['BLOCK_DHEAD_SIZE', 'heads', 'num_kv_groups'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "B_Start_Loc",
        "annotation": null
      },
      {
        "name": "B_Seqlen",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "heads",
        "annotation": null
      },
      {
        "name": "num_kv_groups",
        "annotation": null
      },
      {
        "name": "stride_q_bs",
        "annotation": null
      },
      {
        "name": "stride_q_heads",
        "annotation": null
      },
      {
        "name": "stride_q_dim",
        "annotation": null
      },
      {
        "name": "stride_k_bs",
        "annotation": null
      },
      {
        "name": "stride_k_heads",
        "annotation": null
      },
      {
        "name": "stride_k_dim",
        "annotation": null
      },
      {
        "name": "stride_v_bs",
        "annotation": null
      },
      {
        "name": "stride_v_heads",
        "annotation": null
      },
      {
        "name": "stride_v_dim",
        "annotation": null
      },
      {
        "name": "stride_o_bs",
        "annotation": null
      },
      {
        "name": "stride_o_heads",
        "annotation": null
      },
      {
        "name": "stride_o_dim",
        "annotation": null
      },
      {
        "name": "BLOCK_DHEAD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def flash_attention2_nopad_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    O,",
      "    B_Start_Loc,",
      "    B_Seqlen,",
      "    sm_scale,",
      "    heads,",
      "    num_kv_groups,",
      "    stride_q_bs,",
      "    stride_q_heads,",
      "    stride_q_dim,",
      "    stride_k_bs,",
      "    stride_k_heads,",
      "    stride_k_dim,",
      "    stride_v_bs,",
      "    stride_v_heads,",
      "    stride_v_dim,",
      "    stride_o_bs,",
      "    stride_o_heads,",
      "    stride_o_dim,",
      "    BLOCK_DHEAD_SIZE: tl.constexpr,",
      "    BLOCK_M_SIZE: tl.constexpr,",
      "    BLOCK_N_SIZE: tl.constexpr,",
      "):",
      "",
      "    block_m_idx = tl.program_id(0)",
      "    cur_bh = tl.program_id(1)",
      "    cur_batch_idx = cur_bh // heads",
      "    cur_head_idx = cur_bh % heads",
      "    cur_kv_head_idx = cur_head_idx // num_kv_groups",
      "",
      "    cur_seq_len = tl.load(B_Seqlen + cur_batch_idx)",
      "",
      "    cur_seq_start_loc = tl.load(B_Start_Loc + cur_batch_idx)",
      "",
      "    block_start_loc = block_m_idx * BLOCK_M_SIZE",
      "",
      "    offs_n = tl.arange(0, BLOCK_N_SIZE)",
      "    offs_d = tl.arange(0, BLOCK_DHEAD_SIZE)",
      "    offs_m = block_start_loc + tl.arange(0, BLOCK_M_SIZE)",
      "",
      "    q_offs = (",
      "        (cur_seq_start_loc + offs_m[:, None]) * stride_q_bs",
      "        + cur_head_idx * stride_q_heads",
      "        + offs_d[None, :] * stride_q_dim",
      "    )",
      "    q = tl.load(Q + q_offs, mask=offs_m[:, None] < cur_seq_len, other=0.0)",
      "",
      "    k_offs = (",
      "        offs_n[None, :] * stride_k_bs",
      "        + cur_kv_head_idx * stride_k_heads",
      "        + offs_d[:, None] * stride_k_dim",
      "    )",
      "    v_offs = (",
      "        offs_n[:, None] * stride_v_bs",
      "        + cur_kv_head_idx * stride_v_heads",
      "        + offs_d[None, :] * stride_v_dim",
      "    )",
      "",
      "    k_ptrs = K + k_offs",
      "    v_ptrs = V + v_offs",
      "",
      "    m_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32) - float(\"inf\")",
      "    d_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32)",
      "    acc = tl.zeros((BLOCK_M_SIZE, BLOCK_DHEAD_SIZE), dtype=tl.float32)",
      "",
      "    block_mask = tl.where(block_start_loc < cur_seq_len, 1, 0)",
      "    block_end_loc = tl.minimum(block_start_loc + BLOCK_M_SIZE, cur_seq_len)",
      "",
      "    for start_n in range(0, block_mask * block_end_loc, BLOCK_N_SIZE):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N_SIZE)",
      "",
      "        k = tl.load(",
      "            k_ptrs + (cur_seq_start_loc + start_n) * stride_k_bs,",
      "            mask=(start_n + offs_n[None, :]) < block_end_loc,",
      "            other=0.0,",
      "        )",
      "",
      "        qk = tl.dot(q, k)",
      "",
      "        casual_mask = offs_m[:, None] >= (start_n + offs_n[None, :])",
      "        qk = tl.where(casual_mask, qk * sm_scale, -1.0e8)",
      "",
      "        m_ij = tl.maximum(m_i, tl.max(qk, 1))",
      "        qk -= m_ij[:, None]",
      "        p = tl.math.exp2(qk)",
      "        d_ij = tl.sum(p, 1)",
      "",
      "        alpha = tl.math.exp2(m_i - m_ij)",
      "        d_i = d_i * alpha + d_ij",
      "",
      "        acc = acc * alpha[:, None]",
      "",
      "        v = tl.load(",
      "            v_ptrs + (cur_seq_start_loc + start_n) * stride_v_bs,",
      "            mask=(start_n + offs_n[:, None]) < block_end_loc,",
      "            other=0.0,",
      "        )",
      "        p = p.to(v.dtype)",
      "        acc = tl.dot(p, v, acc)",
      "",
      "        m_i = m_ij",
      "",
      "    acc = acc / d_i[:, None]",
      "    off_o = (",
      "        (cur_seq_start_loc + offs_m[:, None]) * stride_o_bs",
      "        + cur_head_idx * stride_o_heads",
      "        + offs_d[None, :] * stride_o_dim",
      "    )",
      "    out_ptrs = O + off_o",
      "    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_seq_len)"
    ],
    "file": "codes/1267.py",
    "header": "def flash_attention2_nopad_kernel(Q, K, V, O, B_Start_Loc, B_Seqlen, sm_scale, heads, num_kv_groups, stride_q_bs, stride_q_heads, stride_q_dim, stride_k_bs, stride_k_heads, stride_k_dim, stride_v_bs, stride_v_heads, stride_v_dim, stride_o_bs, stride_o_heads, stride_o_dim, BLOCK_DHEAD_SIZE: tl.constexpr, BLOCK_M_SIZE: tl.constexpr, BLOCK_N_SIZE: tl.constexpr):",
    "body": "block_m_idx = tl.program_id(0)\ncur_bh = tl.program_id(1)\ncur_batch_idx = cur_bh // heads\ncur_head_idx = cur_bh % heads\ncur_kv_head_idx = cur_head_idx // num_kv_groups\ncur_seq_len = tl.load(B_Seqlen + cur_batch_idx)\ncur_seq_start_loc = tl.load(B_Start_Loc + cur_batch_idx)\nblock_start_loc = block_m_idx * BLOCK_M_SIZE\noffs_n = tl.arange(0, BLOCK_N_SIZE)\noffs_d = tl.arange(0, BLOCK_DHEAD_SIZE)\noffs_m = block_start_loc + tl.arange(0, BLOCK_M_SIZE)\nq_offs = (cur_seq_start_loc + offs_m[:, None]) * stride_q_bs + cur_head_idx * stride_q_heads + offs_d[None, :] * stride_q_dim\nq = tl.load(Q + q_offs, mask=offs_m[:, None] < cur_seq_len, other=0.0)\nk_offs = offs_n[None, :] * stride_k_bs + cur_kv_head_idx * stride_k_heads + offs_d[:, None] * stride_k_dim\nv_offs = offs_n[:, None] * stride_v_bs + cur_kv_head_idx * stride_v_heads + offs_d[None, :] * stride_v_dim\nk_ptrs = K + k_offs\nv_ptrs = V + v_offs\nm_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32) - float('inf')\nd_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32)\nacc = tl.zeros((BLOCK_M_SIZE, BLOCK_DHEAD_SIZE), dtype=tl.float32)\nblock_mask = tl.where(block_start_loc < cur_seq_len, 1, 0)\nblock_end_loc = tl.minimum(block_start_loc + BLOCK_M_SIZE, cur_seq_len)\nfor start_n in range(0, block_mask * block_end_loc, BLOCK_N_SIZE):\n    start_n = tl.multiple_of(start_n, BLOCK_N_SIZE)\n    k = tl.load(k_ptrs + (cur_seq_start_loc + start_n) * stride_k_bs, mask=start_n + offs_n[None, :] < block_end_loc, other=0.0)\n    qk = tl.dot(q, k)\n    casual_mask = offs_m[:, None] >= start_n + offs_n[None, :]\n    qk = tl.where(casual_mask, qk * sm_scale, -100000000.0)\n    m_ij = tl.maximum(m_i, tl.max(qk, 1))\n    qk -= m_ij[:, None]\n    p = tl.math.exp2(qk)\n    d_ij = tl.sum(p, 1)\n    alpha = tl.math.exp2(m_i - m_ij)\n    d_i = d_i * alpha + d_ij\n    acc = acc * alpha[:, None]\n    v = tl.load(v_ptrs + (cur_seq_start_loc + start_n) * stride_v_bs, mask=start_n + offs_n[:, None] < block_end_loc, other=0.0)\n    p = p.to(v.dtype)\n    acc = tl.dot(p, v, acc)\n    m_i = m_ij\nacc = acc / d_i[:, None]\noff_o = (cur_seq_start_loc + offs_m[:, None]) * stride_o_bs + cur_head_idx * stride_o_heads + offs_d[None, :] * stride_o_dim\nout_ptrs = O + off_o\ntl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_seq_len)"
  },
  {
    "name": "fused_chunk_based_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_based_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    z,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    o_i = tl.arange(0, BT)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_h_0o = tl.zeros([BV], dtype=tl.float32)",
      "",
      "    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + i_k * B * H) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (0, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)",
      "    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)",
      "    k_1o = tl.zeros([1, BK], dtype=tl.float32)",
      "    k_0o = 0",
      "",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_k_2o = b_k[:, None, :] * b_k[None, :, :]",
      "        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT]).to(b_k.dtype)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(b_k.dtype)",
      "        b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "        b_z = tl.zeros([BT], dtype=tl.float32)",
      "",
      "        b_o += b_h_0o",
      "        b_z += k_0o",
      "",
      "        b_o += tl.dot(b_q, b_h_1o.to(b_q.dtype), allow_tf32=False)",
      "        b_z += tl.sum(b_q * k_1o, axis=1)",
      "",
      "        b_q_2o = b_q[:, :, None] * b_q[:, None, :]",
      "        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK]).to(b_k.dtype)",
      "        b_o += tl.dot(b_q_2o, b_h_2o.to(b_q_2o.dtype), allow_tf32=False) * 0.5",
      "        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5",
      "",
      "        k_1o += tl.sum(b_k, axis=1)[None, :]",
      "        k_2o += tl.sum(b_k_2o, axis=1)[None, :]",
      "        k_0o += BT",
      "",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = 1 + b_s + 0.5 * b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_z += tl.sum(b_s, axis=1)",
      "        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(",
      "            p_z, b_z.to(p_z.dtype.element_ty), mask=(i * BT + tl.arange(0, BT)) < T",
      "        )",
      "",
      "        b_h_2o = b_h_2o + tl.dot(b_k_2o.to(b_v.dtype), b_v, allow_tf32=False)",
      "        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)",
      "        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)",
      "",
      "        p_q = tl.advance(p_q, (BT, 0))",
      "        p_k = tl.advance(p_k, (0, BT))",
      "        p_v = tl.advance(p_v, (BT, 0))",
      "        p_o = tl.advance(p_o, (BT, 0))",
      "        p_z += BT"
    ],
    "file": "codes/366.py",
    "header": "def fused_chunk_based_fwd_kernel(q, k, v, o, z, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\nb_h_0o = tl.zeros([BV], dtype=tl.float32)\nb_h_1o = tl.zeros([BK, BV], dtype=tl.float32)\nb_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0))\np_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)\nk_2o = tl.zeros([1, BK * BK], dtype=tl.float32)\nk_1o = tl.zeros([1, BK], dtype=tl.float32)\nk_0o = 0\nfor i in range(0, tl.cdiv(T, BT)):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k_2o = b_k[:, None, :] * b_k[None, :, :]\n    b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT]).to(b_k.dtype)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(b_k.dtype)\n    b_o = tl.zeros([BT, BV], dtype=tl.float32)\n    b_z = tl.zeros([BT], dtype=tl.float32)\n    b_o += b_h_0o\n    b_z += k_0o\n    b_o += tl.dot(b_q, b_h_1o.to(b_q.dtype), allow_tf32=False)\n    b_z += tl.sum(b_q * k_1o, axis=1)\n    b_q_2o = b_q[:, :, None] * b_q[:, None, :]\n    b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK]).to(b_k.dtype)\n    b_o += tl.dot(b_q_2o, b_h_2o.to(b_q_2o.dtype), allow_tf32=False) * 0.5\n    b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5\n    k_1o += tl.sum(b_k, axis=1)[None, :]\n    k_2o += tl.sum(b_k_2o, axis=1)[None, :]\n    k_0o += BT\n    b_s = tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = 1 + b_s + 0.5 * b_s * b_s\n    b_s = tl.where(m_s, b_s, 0)\n    b_z += tl.sum(b_s, axis=1)\n    b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_z, b_z.to(p_z.dtype.element_ty), mask=i * BT + tl.arange(0, BT) < T)\n    b_h_2o = b_h_2o + tl.dot(b_k_2o.to(b_v.dtype), b_v, allow_tf32=False)\n    b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)\n    b_h_0o = b_h_0o + tl.sum(b_v, axis=0)\n    p_q = tl.advance(p_q, (BT, 0))\n    p_k = tl.advance(p_k, (0, BT))\n    p_v = tl.advance(p_v, (BT, 0))\n    p_o = tl.advance(p_o, (BT, 0))\n    p_z += BT"
  },
  {
    "name": "reduce_scatter_ring_push_1d_intra_node_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'num_ranks'])"
    ],
    "args": [
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "symm_input_flag_ptr",
        "annotation": null
      },
      {
        "name": "symm_reduce_ptr",
        "annotation": null
      },
      {
        "name": "symm_reduce_flag_ptr",
        "annotation": null
      },
      {
        "name": "grid_barrier_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "elems_per_rank",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def reduce_scatter_ring_push_1d_intra_node_kernel(",
      "    rank,",
      "    num_ranks,",
      "    input_ptr,",
      "    symm_input_flag_ptr,",
      "    symm_reduce_ptr,",
      "    symm_reduce_flag_ptr,",
      "    grid_barrier_ptr,",
      "    output_ptr,",
      "    elems_per_rank,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    to_rank = (rank - 1 + num_ranks) % num_ranks",
      "    peer_reduce_ptr = dl.symm_at(symm_reduce_ptr, to_rank)",
      "    peer_symm_reduce_flag_ptr = dl.symm_at(symm_reduce_flag_ptr, to_rank)",
      "    thread_idx = tid(0)",
      "    pid = tl.program_id(0)",
      "",
      "    for stage in range(num_ranks):",
      "        segment = (rank + stage + 1) % num_ranks",
      "        src_ptr = input_ptr + segment * elems_per_rank",
      "        dst_ptr = peer_reduce_ptr + segment * elems_per_rank",
      "",
      "        if thread_idx == 0:",
      "            while (",
      "                ld(symm_input_flag_ptr + segment, semantic=\"acquire\", scope=\"gpu\") != 1",
      "            ):",
      "                pass",
      "        __syncthreads()",
      "",
      "        if stage == 0:",
      "            copy_continuous_kernel(src_ptr, dst_ptr, elems_per_rank, BLOCK_SIZE)",
      "        else:",
      "",
      "            if thread_idx == 0:",
      "                while (",
      "                    ld(symm_reduce_flag_ptr + segment, semantic=\"acquire\", scope=\"sys\")",
      "                    != 1",
      "                ):",
      "                    pass",
      "            __syncthreads()",
      "",
      "            reduce_buffer_ptr = symm_reduce_ptr + elems_per_rank * segment",
      "            add_continuous_kernel(",
      "                src_ptr,",
      "                reduce_buffer_ptr,",
      "                output_ptr if stage == num_ranks - 1 else dst_ptr,",
      "                elems_per_rank,",
      "                BLOCK_SIZE,",
      "            )",
      "",
      "        barrier_on_this_grid(grid_barrier_ptr)",
      "",
      "        if pid == 0 and thread_idx == 0:",
      "            st(peer_symm_reduce_flag_ptr + segment, 1, semantic=\"release\", scope=\"sys\")",
      "        __syncthreads()",
      "    if pid == 0:",
      "        libshmem_device.barrier_all_block()"
    ],
    "file": "codes/48.py",
    "header": "def reduce_scatter_ring_push_1d_intra_node_kernel(rank, num_ranks, input_ptr, symm_input_flag_ptr, symm_reduce_ptr, symm_reduce_flag_ptr, grid_barrier_ptr, output_ptr, elems_per_rank, BLOCK_SIZE: tl.constexpr):",
    "body": "to_rank = (rank - 1 + num_ranks) % num_ranks\npeer_reduce_ptr = dl.symm_at(symm_reduce_ptr, to_rank)\npeer_symm_reduce_flag_ptr = dl.symm_at(symm_reduce_flag_ptr, to_rank)\nthread_idx = tid(0)\npid = tl.program_id(0)\nfor stage in range(num_ranks):\n    segment = (rank + stage + 1) % num_ranks\n    src_ptr = input_ptr + segment * elems_per_rank\n    dst_ptr = peer_reduce_ptr + segment * elems_per_rank\n    if thread_idx == 0:\n        while ld(symm_input_flag_ptr + segment, semantic='acquire', scope='gpu') != 1:\n            pass\n    __syncthreads()\n    if stage == 0:\n        copy_continuous_kernel(src_ptr, dst_ptr, elems_per_rank, BLOCK_SIZE)\n    else:\n        if thread_idx == 0:\n            while ld(symm_reduce_flag_ptr + segment, semantic='acquire', scope='sys') != 1:\n                pass\n        __syncthreads()\n        reduce_buffer_ptr = symm_reduce_ptr + elems_per_rank * segment\n        add_continuous_kernel(src_ptr, reduce_buffer_ptr, output_ptr if stage == num_ranks - 1 else dst_ptr, elems_per_rank, BLOCK_SIZE)\n    barrier_on_this_grid(grid_barrier_ptr)\n    if pid == 0 and thread_idx == 0:\n        st(peer_symm_reduce_flag_ptr + segment, 1, semantic='release', scope='sys')\n    __syncthreads()\nif pid == 0:\n    libshmem_device.barrier_all_block()"
  },
  {
    "name": "reduce_scatter_ring_push_1d_intra_node_rma_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'num_ranks'])"
    ],
    "args": [
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "symm_input_ptr",
        "annotation": null
      },
      {
        "name": "symm_input_flag_ptr",
        "annotation": null
      },
      {
        "name": "symm_reduce_ptr",
        "annotation": null
      },
      {
        "name": "symm_reduce_flag_ptr",
        "annotation": null
      },
      {
        "name": "grid_barrier_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "elems_per_rank",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def reduce_scatter_ring_push_1d_intra_node_rma_kernel(",
      "    rank,",
      "    num_ranks,",
      "    symm_input_ptr,",
      "    symm_input_flag_ptr,",
      "    symm_reduce_ptr,",
      "    symm_reduce_flag_ptr,",
      "    grid_barrier_ptr,",
      "    output_ptr,",
      "    elems_per_rank,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    to_rank = (rank - 1 + num_ranks) % num_ranks",
      "    thread_idx = tid(0)",
      "    pid = tl.program_id(0)",
      "",
      "    ITEM_SIZE = tl.constexpr(symm_input_ptr.dtype.primitive_bitwidth) // 8",
      "    NUMA_WORLD_SIZE = 4",
      "    use_rma = rank % NUMA_WORLD_SIZE == 0",
      "",
      "    if not use_rma:",
      "        return reduce_scatter_ring_push_1d_intra_node_kernel(",
      "            rank,",
      "            num_ranks,",
      "            symm_input_ptr,",
      "            symm_input_flag_ptr,",
      "            symm_reduce_ptr,",
      "            symm_reduce_flag_ptr,",
      "            grid_barrier_ptr,",
      "            output_ptr,",
      "            elems_per_rank,",
      "            BLOCK_SIZE=BLOCK_SIZE,",
      "        )",
      "",
      "    for stage in range(num_ranks):",
      "        segment = (rank + stage + 1) % num_ranks",
      "        src_ptr = symm_input_ptr + segment * elems_per_rank",
      "        dst_ptr = symm_reduce_ptr + segment * elems_per_rank",
      "",
      "        if thread_idx == 0:",
      "            while (",
      "                ld(symm_input_flag_ptr + segment, semantic=\"acquire\", scope=\"gpu\") != 1",
      "            ):",
      "                pass",
      "        __syncthreads()",
      "",
      "        if stage != 0:",
      "",
      "            if thread_idx == 0:",
      "                while (",
      "                    ld(symm_reduce_flag_ptr + segment, semantic=\"acquire\", scope=\"sys\")",
      "                    != 1",
      "                ):",
      "                    pass",
      "            __syncthreads()",
      "",
      "            add_continuous_kernel(",
      "                src_ptr,",
      "                dst_ptr,",
      "                output_ptr if stage == num_ranks - 1 else dst_ptr,",
      "                elems_per_rank,",
      "                BLOCK_SIZE,",
      "            )",
      "            barrier_on_this_grid(grid_barrier_ptr)",
      "",
      "        if stage != num_ranks - 1 and pid == 0:",
      "",
      "            libshmem_device.putmem_signal_nbi_block(",
      "                dst_ptr,",
      "                dst_ptr if stage != 0 else src_ptr,",
      "                elems_per_rank * ITEM_SIZE,",
      "                symm_reduce_flag_ptr,",
      "                1,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                to_rank,",
      "            )",
      "    if pid == 0:",
      "        libshmem_device.barrier_all_block()"
    ],
    "file": "codes/48.py",
    "header": "def reduce_scatter_ring_push_1d_intra_node_rma_kernel(rank, num_ranks, symm_input_ptr, symm_input_flag_ptr, symm_reduce_ptr, symm_reduce_flag_ptr, grid_barrier_ptr, output_ptr, elems_per_rank, BLOCK_SIZE: tl.constexpr):",
    "body": "to_rank = (rank - 1 + num_ranks) % num_ranks\nthread_idx = tid(0)\npid = tl.program_id(0)\nITEM_SIZE = tl.constexpr(symm_input_ptr.dtype.primitive_bitwidth) // 8\nNUMA_WORLD_SIZE = 4\nuse_rma = rank % NUMA_WORLD_SIZE == 0\nif not use_rma:\n    return reduce_scatter_ring_push_1d_intra_node_kernel(rank, num_ranks, symm_input_ptr, symm_input_flag_ptr, symm_reduce_ptr, symm_reduce_flag_ptr, grid_barrier_ptr, output_ptr, elems_per_rank, BLOCK_SIZE=BLOCK_SIZE)\nfor stage in range(num_ranks):\n    segment = (rank + stage + 1) % num_ranks\n    src_ptr = symm_input_ptr + segment * elems_per_rank\n    dst_ptr = symm_reduce_ptr + segment * elems_per_rank\n    if thread_idx == 0:\n        while ld(symm_input_flag_ptr + segment, semantic='acquire', scope='gpu') != 1:\n            pass\n    __syncthreads()\n    if stage != 0:\n        if thread_idx == 0:\n            while ld(symm_reduce_flag_ptr + segment, semantic='acquire', scope='sys') != 1:\n                pass\n        __syncthreads()\n        add_continuous_kernel(src_ptr, dst_ptr, output_ptr if stage == num_ranks - 1 else dst_ptr, elems_per_rank, BLOCK_SIZE)\n        barrier_on_this_grid(grid_barrier_ptr)\n    if stage != num_ranks - 1 and pid == 0:\n        libshmem_device.putmem_signal_nbi_block(dst_ptr, dst_ptr if stage != 0 else src_ptr, elems_per_rank * ITEM_SIZE, symm_reduce_flag_ptr, 1, libshmem_device.NVSHMEM_SIGNAL_SET, to_rank)\nif pid == 0:\n    libshmem_device.barrier_all_block()"
  },
  {
    "name": "kernel_ring_reduce_non_tma",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['begin_idx'])"
    ],
    "args": [
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "elems_per_rank",
        "annotation": null
      },
      {
        "name": "begin_idx",
        "annotation": null
      },
      {
        "name": "NUM_SPLITS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_ring_reduce_non_tma(",
      "    c_ptr,",
      "    out_ptr,",
      "    elems_per_rank,",
      "    begin_idx,",
      "    NUM_SPLITS: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    num_blocks = tl.cdiv(elems_per_rank, BLOCK_SIZE)",
      "    pid = tl.program_id(0)",
      "    npid = tl.num_programs(0)",
      "    for n in range(pid, num_blocks, npid):",
      "        segment = (begin_idx + 1) % NUM_SPLITS",
      "        c_offs = elems_per_rank * segment + BLOCK_SIZE * n + tl.arange(0, BLOCK_SIZE)",
      "        mask = c_offs < elems_per_rank * NUM_SPLITS",
      "        accum = tl.load(c_ptr + c_offs, mask=mask)",
      "        for i in range(1, NUM_SPLITS):",
      "            segment = (i + begin_idx + 1) % NUM_SPLITS",
      "            c_offs = (",
      "                elems_per_rank * segment + BLOCK_SIZE * n + tl.arange(0, BLOCK_SIZE)",
      "            )",
      "            data = tl.load(c_ptr + c_offs)",
      "            accum += data",
      "",
      "        out_offs = BLOCK_SIZE * n + tl.arange(0, BLOCK_SIZE)",
      "        tl.store(out_ptr + out_offs, accum, mask=mask)"
    ],
    "file": "codes/48.py",
    "header": "def kernel_ring_reduce_non_tma(c_ptr, out_ptr, elems_per_rank, begin_idx, NUM_SPLITS: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "num_blocks = tl.cdiv(elems_per_rank, BLOCK_SIZE)\npid = tl.program_id(0)\nnpid = tl.num_programs(0)\nfor n in range(pid, num_blocks, npid):\n    segment = (begin_idx + 1) % NUM_SPLITS\n    c_offs = elems_per_rank * segment + BLOCK_SIZE * n + tl.arange(0, BLOCK_SIZE)\n    mask = c_offs < elems_per_rank * NUM_SPLITS\n    accum = tl.load(c_ptr + c_offs, mask=mask)\n    for i in range(1, NUM_SPLITS):\n        segment = (i + begin_idx + 1) % NUM_SPLITS\n        c_offs = elems_per_rank * segment + BLOCK_SIZE * n + tl.arange(0, BLOCK_SIZE)\n        data = tl.load(c_ptr + c_offs)\n        accum += data\n    out_offs = BLOCK_SIZE * n + tl.arange(0, BLOCK_SIZE)\n    tl.store(out_ptr + out_offs, accum, mask=mask)"
  },
  {
    "name": "prepare_position_ids_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [4, 8, 16, 32]], key=['B'])"
    ],
    "args": [
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_position_ids_kernel(y, cu_seqlens, B: tl.constexpr):",
      "    i_n = tl.program_id(0)",
      "    bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(",
      "        tl.int32",
      "    )",
      "    T = eos - bos",
      "",
      "    o = tl.arange(0, B)",
      "    for i in range(0, tl.cdiv(T, B) * B, B):",
      "        o_i = o + i",
      "        tl.store(y + bos + o_i, o_i, o_i < T)"
    ],
    "file": "codes/428.py",
    "header": "def prepare_position_ids_kernel(y, cu_seqlens, B: tl.constexpr):",
    "body": "i_n = tl.program_id(0)\nbos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nT = eos - bos\no = tl.arange(0, B)\nfor i in range(0, tl.cdiv(T, B) * B, B):\n    o_i = o + i\n    tl.store(y + bos + o_i, o_i, o_i < T)"
  },
  {
    "name": "barrier_all_intra_node_atomic_cas_block",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['local_rank', 'rank', 'local_world_size'])"
    ],
    "args": [
      {
        "name": "local_rank",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "local_world_size",
        "annotation": null
      },
      {
        "name": "symm_flag_ptr",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def barrier_all_intra_node_atomic_cas_block(",
      "    local_rank, rank, local_world_size, symm_flag_ptr",
      "):",
      "",
      "    thread_idx = tid(axis=0)",
      "    local_rank_offset = rank - local_rank",
      "    if thread_idx < local_world_size:",
      "        remote_ptr = dl.symm_at(",
      "            symm_flag_ptr + local_rank, thread_idx + local_rank_offset",
      "        )",
      "        while atomic_cas(remote_ptr, 0, 1, \"sys\", \"release\") != 0:",
      "            pass",
      "",
      "    if thread_idx < local_world_size:",
      "        while atomic_cas(symm_flag_ptr + thread_idx, 1, 0, \"sys\", \"acquire\") != 1:",
      "            pass",
      "    __syncthreads()"
    ],
    "file": "codes/39.py",
    "header": "def barrier_all_intra_node_atomic_cas_block(local_rank, rank, local_world_size, symm_flag_ptr):",
    "body": "thread_idx = tid(axis=0)\nlocal_rank_offset = rank - local_rank\nif thread_idx < local_world_size:\n    remote_ptr = dl.symm_at(symm_flag_ptr + local_rank, thread_idx + local_rank_offset)\n    while atomic_cas(remote_ptr, 0, 1, 'sys', 'release') != 0:\n        pass\nif thread_idx < local_world_size:\n    while atomic_cas(symm_flag_ptr + thread_idx, 1, 0, 'sys', 'acquire') != 1:\n        pass\n__syncthreads()"
  },
  {
    "name": "barrier_all_intra_node_non_atomic_block",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['local_rank', 'rank', 'num_ranks', 'target_value'])"
    ],
    "args": [
      {
        "name": "local_rank",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "symm_flags",
        "annotation": null
      },
      {
        "name": "target_value",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def barrier_all_intra_node_non_atomic_block(",
      "    local_rank, rank, num_ranks, symm_flags, target_value",
      "):",
      "",
      "    tl.static_assert(symm_flags.dtype.element_ty == tl.int32)",
      "    _barrier_all_intra_node_non_atomic_once_block(",
      "        local_rank, rank, num_ranks, symm_flags, target_value",
      "    )",
      "",
      "    barrier_on_this_grid(symm_flags + 2 * num_ranks)",
      "",
      "    _barrier_all_intra_node_non_atomic_once_block(",
      "        local_rank, rank, num_ranks, symm_flags + num_ranks, target_value",
      "    )",
      "",
      "    barrier_on_this_grid(symm_flags + 2 * num_ranks)"
    ],
    "file": "codes/39.py",
    "header": "def barrier_all_intra_node_non_atomic_block(local_rank, rank, num_ranks, symm_flags, target_value):",
    "body": "tl.static_assert(symm_flags.dtype.element_ty == tl.int32)\n_barrier_all_intra_node_non_atomic_once_block(local_rank, rank, num_ranks, symm_flags, target_value)\nbarrier_on_this_grid(symm_flags + 2 * num_ranks)\n_barrier_all_intra_node_non_atomic_once_block(local_rank, rank, num_ranks, symm_flags + num_ranks, target_value)\nbarrier_on_this_grid(symm_flags + 2 * num_ranks)"
  },
  {
    "name": "barrier_all_intra_node_non_atomic",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['local_rank', 'rank', 'num_ranks', 'target_value'])"
    ],
    "args": [
      {
        "name": "local_rank",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "symm_flags",
        "annotation": null
      },
      {
        "name": "target_value",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def barrier_all_intra_node_non_atomic(",
      "    local_rank, rank, num_ranks, symm_flags, target_value",
      "):",
      "",
      "    tl.static_assert(symm_flags.dtype.element_ty == tl.int32)",
      "    pid = tl.program_id(axis=0)",
      "    if pid == 0:",
      "        _barrier_all_intra_node_non_atomic_once_block(",
      "            local_rank, rank, num_ranks, symm_flags, target_value",
      "        )",
      "",
      "    barrier_on_this_grid(symm_flags + 2 * num_ranks)",
      "",
      "    if pid == 0:",
      "        _barrier_all_intra_node_non_atomic_once_block(",
      "            local_rank, rank, num_ranks, symm_flags + num_ranks, target_value",
      "        )",
      "",
      "    barrier_on_this_grid(symm_flags + 2 * num_ranks)"
    ],
    "file": "codes/39.py",
    "header": "def barrier_all_intra_node_non_atomic(local_rank, rank, num_ranks, symm_flags, target_value):",
    "body": "tl.static_assert(symm_flags.dtype.element_ty == tl.int32)\npid = tl.program_id(axis=0)\nif pid == 0:\n    _barrier_all_intra_node_non_atomic_once_block(local_rank, rank, num_ranks, symm_flags, target_value)\nbarrier_on_this_grid(symm_flags + 2 * num_ranks)\nif pid == 0:\n    _barrier_all_intra_node_non_atomic_once_block(local_rank, rank, num_ranks, symm_flags + num_ranks, target_value)\nbarrier_on_this_grid(symm_flags + 2 * num_ranks)"
  },
  {
    "name": "intra_chunk_preprocess_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def intra_chunk_preprocess_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    beta,",
      "    g_cumsum,",
      "    o,",
      "    A,",
      "    L,",
      "    M,",
      "    h,",
      "    q_new,",
      "    k_new,",
      "    scale,",
      "    indices,",
      "    offsets,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    sm_scale = scale * 1.44269504",
      "",
      "    A += (bos * H + i_h) * BT",
      "    q += (bos * HQ + i_hq) * K",
      "    q_new += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * HQ + i_hq) * V",
      "    beta += bos * H + i_h",
      "    h += ((boh + i_t) * H + i_h) * K * K",
      "    if USE_G:",
      "        g_cumsum += bos * HQ + i_hq",
      "    L += bos * HQ + i_hq",
      "    M += bos * HQ + i_hq",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))",
      "    p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_kt = tl.load(p_k, boundary_check=(0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_w = tl.load(p_w, boundary_check=(0, 1))",
      "    p_T = tl.make_block_ptr(A, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))",
      "    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_t = o_i[:, None] >= o_i[None, :]",
      "    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)",
      "",
      "    b_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)",
      "    b_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)",
      "    b_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(",
      "        b_w.dtype",
      "    )",
      "    b_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)",
      "",
      "    b_q = b_q - tl.dot(b_qwT, b_w_beta)",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0)",
      "    )",
      "    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if i_hq % G == 0:",
      "        b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)",
      "        b_h = tl.dot(tl.trans(b_w), b_Twb)",
      "        p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)",
      "        p_k_new = tl.make_block_ptr(",
      "            k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        tl.store(",
      "            p_k_new,",
      "            (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "    if USE_G:",
      "        p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))",
      "        b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])",
      "        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\"))",
      "",
      "    b_qkT_softmax = tl.where(",
      "        o_i[:, None] >= o_i[None, :], b_A * sm_scale, float(\"-inf\")",
      "    )",
      "    m_i = tl.max(b_qkT_softmax, 1)",
      "    b_qkT_softmax = tl.math.exp2(b_qkT_softmax - m_i[:, None])",
      "    l_i = tl.sum(b_qkT_softmax, 1)",
      "    b_o = tl.dot(b_qkT_softmax.to(b_v.dtype), b_v)",
      "    p_o = tl.make_block_ptr(o, (T, V), (V * HQ, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    p_m = tl.make_block_ptr(M, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_m, m_i.to(p_m.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_l, l_i.to(p_l.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/408.py",
    "header": "def intra_chunk_preprocess_fwd_kernel(q, k, v, w, beta, g_cumsum, o, A, L, M, h, q_new, k_new, scale, indices, offsets, chunk_offsets, T, H: tl.constexpr, G: tl.constexpr, HQ: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, BT: tl.constexpr, IS_VARLEN: tl.constexpr, USE_G: tl.constexpr):",
    "body": "i_t, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nsm_scale = scale * 1.44269504\nA += (bos * H + i_h) * BT\nq += (bos * HQ + i_hq) * K\nq_new += (bos * HQ + i_hq) * K\nk += (bos * H + i_h) * K\nk_new += (bos * H + i_h) * K\nw += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\no += (bos * HQ + i_hq) * V\nbeta += bos * H + i_h\nh += ((boh + i_t) * H + i_h) * K * K\nif USE_G:\n    g_cumsum += bos * HQ + i_hq\nL += bos * HQ + i_hq\nM += bos * HQ + i_hq\np_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\np_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_kt = tl.load(p_k, boundary_check=(0, 1))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_w = tl.load(p_w, boundary_check=(0, 1))\np_T = tl.make_block_ptr(A, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)\no_i = tl.arange(0, BT)\nm_t = o_i[:, None] >= o_i[None, :]\np_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)\nb_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)\nb_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)\nb_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(b_w.dtype)\nb_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)\nb_q = b_q - tl.dot(b_qwT, b_w_beta)\np_q_new = tl.make_block_ptr(q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0))\ntl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))\nif i_hq % G == 0:\n    b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)\n    b_h = tl.dot(tl.trans(b_w), b_Twb)\n    p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)\n    p_k_new = tl.make_block_ptr(k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1))\n    tl.store(p_k_new, (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty), boundary_check=(0, 1))\nif USE_G:\n    p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))\n    b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])\n    b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float('-inf'))\nb_qkT_softmax = tl.where(o_i[:, None] >= o_i[None, :], b_A * sm_scale, float('-inf'))\nm_i = tl.max(b_qkT_softmax, 1)\nb_qkT_softmax = tl.math.exp2(b_qkT_softmax - m_i[:, None])\nl_i = tl.sum(b_qkT_softmax, 1)\nb_o = tl.dot(b_qkT_softmax.to(b_v.dtype), b_v)\np_o = tl.make_block_ptr(o, (T, V), (V * HQ, 1), (i_t * BT, 0), (BT, BV), (1, 0))\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\np_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\np_m = tl.make_block_ptr(M, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\ntl.store(p_m, m_i.to(p_m.dtype.element_ty), boundary_check=(0,))\ntl.store(p_l, l_i.to(p_l.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h).to(tl.int64) * K * V",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h)",
      "",
      "        b_A += tl.dot(b_q, b_k)",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_o = b_o * exp(b_g)[:, None]",
      "        b_A = b_A * safe_exp(b_g[:, None] - b_g[None, :])",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_A = tl.where(m_A, b_A, 0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    b_o = b_o * scale + tl.dot(b_A.to(b_v.dtype), b_v) * scale",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/372.py",
    "header": "def chunk_fwd_kernel_o(q, k, v, h, g, o, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\no += (bos * H + i_h) * V\nh += (i_tg * H + i_h).to(tl.int64) * K * V\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_q = tl.make_block_ptr(q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_h = tl.make_block_ptr(h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_o += tl.dot(b_q, b_h)\n    b_A += tl.dot(b_q, b_k)\nif USE_G:\n    g += bos * H + i_h\n    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_o = b_o * exp(b_g)[:, None]\n    b_A = b_A * safe_exp(b_g[:, None] - b_g[None, :])\no_i = tl.arange(0, BT)\nm_A = o_i[:, None] >= o_i[None, :]\nb_A = tl.where(m_A, b_A, 0)\np_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_o = b_o * scale + tl.dot(b_A.to(b_v.dtype), b_v) * scale\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dqkwg",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_G': lambda args: args['g'] is not None, 'USE_DW': lambda args: args['dw'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'USE_G', 'USE_DW'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dqkwg(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dg,",
      "    w,",
      "    dv,",
      "    dw,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_DW: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if USE_G:",
      "        dg += i_k * B * H * T",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    dh += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "",
      "    if USE_DW:",
      "        dw += (bos * H + i_h) * K",
      "        dv += (bos * H + i_h) * V",
      "        w += (bos * H + i_h) * K",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dg_last = (",
      "        tl.zeros(",
      "            [",
      "                1,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        if USE_G",
      "        else None",
      "    )",
      "    b_dw = tl.zeros([BT, BK], dtype=tl.float32) if USE_DW else None",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        if USE_G:",
      "            b_dg_last += tl.sum(b_h * b_dh)",
      "",
      "        b_ds += tl.dot(b_do, tl.trans(b_v))",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "        if USE_DW:",
      "            p_dv = tl.make_block_ptr(",
      "                dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            b_dv = tl.load(p_dv, boundary_check=(0, 1))",
      "            b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))",
      "",
      "    if USE_DW and not USE_G:",
      "        p_dw = tl.make_block_ptr(",
      "            dw, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    o_i = tl.arange(0, BT)",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "",
      "    if USE_G:",
      "        b_dg = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        g += bos * H + i_h",
      "        dg += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)",
      "        b_dg_last *= exp(b_g_last)",
      "",
      "        if USE_DW:",
      "            p_w = tl.make_block_ptr(",
      "                w, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "            )",
      "            p_dw = tl.make_block_ptr(",
      "                dw, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "            )",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_dw = b_dw * exp(b_g)[:, None]",
      "            tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "            b_dg -= tl.sum(b_w * b_dw, axis=1)",
      "",
      "        b_dq = b_dq * exp(b_g)[:, None] * scale",
      "        b_dg += tl.sum(b_dq * b_q, axis=1)",
      "",
      "        b_dk = b_dk * safe_exp(-b_g + b_g_last)[:, None]",
      "        b_dg -= tl.sum(b_k * b_dk, axis=1)",
      "        b_dg_last += tl.sum(b_dk * b_k)",
      "",
      "        b_ds = (",
      "            tl.where(",
      "                o_i[:, None] >= o_i[None, :],",
      "                b_ds * safe_exp(b_g[:, None] - b_g[None, :]),",
      "                0,",
      "            )",
      "            * scale",
      "        )",
      "        b_ds2 = b_ds * tl.dot(b_q, tl.trans(b_k))",
      "        b_dg += tl.sum(b_ds2, axis=1)",
      "        b_dg -= tl.sum(b_ds2, axis=0)",
      "",
      "        b_ds = b_ds.to(b_k.dtype)",
      "",
      "        b_dq += tl.dot(b_ds, b_k)",
      "        b_dk += tl.dot(tl.trans(b_ds), b_q)",
      "        p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "",
      "        b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "    else:",
      "        b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)",
      "        b_ds = b_ds.to(b_k.dtype)",
      "        b_dq += tl.dot(b_ds, b_k)",
      "        b_dk += tl.dot(tl.trans(b_ds), b_q) * scale",
      "        b_dq *= scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/372.py",
    "header": "def chunk_bwd_kernel_dqkwg(q, k, v, h, g, do, dh, dq, dk, dg, w, dv, dw, cu_seqlens, chunk_indices, scale, B: tl.constexpr, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_DW: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif USE_G:\n    dg += i_k * B * H * T\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nv += (bos * H + i_h) * V\ndo += (bos * H + i_h) * V\nh += (i_tg * H + i_h).to(tl.int64) * K * V\ndh += (i_tg * H + i_h).to(tl.int64) * K * V\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\ndq += (bos * H + i_h) * K\ndk += (bos * H + i_h) * K\nif USE_DW:\n    dw += (bos * H + i_h) * K\n    dv += (bos * H + i_h) * V\n    w += (bos * H + i_h) * K\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_ds = tl.zeros([BT, BT], dtype=tl.float32)\nb_dg_last = tl.zeros([1], dtype=tl.float32) if USE_G else None\nb_dw = tl.zeros([BT, BK], dtype=tl.float32) if USE_DW else None\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    if USE_G:\n        b_dg_last += tl.sum(b_h * b_dh)\n    b_ds += tl.dot(b_do, tl.trans(b_v))\n    b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\n    if USE_DW:\n        p_dv = tl.make_block_ptr(dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        b_dv = tl.load(p_dv, boundary_check=(0, 1))\n        b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))\nif USE_DW and (not USE_G):\n    p_dw = tl.make_block_ptr(dw, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\ntl.debug_barrier()\no_i = tl.arange(0, BT)\np_q = tl.make_block_ptr(q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\np_dq = tl.make_block_ptr(dq, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nif USE_G:\n    b_dg = tl.zeros([BT], dtype=tl.float32)\n    g += bos * H + i_h\n    dg += bos * H + i_h\n    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)\n    b_dg_last *= exp(b_g_last)\n    if USE_DW:\n        p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        p_dw = tl.make_block_ptr(dw, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n        b_w = tl.load(p_w, boundary_check=(0, 1))\n        b_dw = b_dw * exp(b_g)[:, None]\n        tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\n        b_dg -= tl.sum(b_w * b_dw, axis=1)\n    b_dq = b_dq * exp(b_g)[:, None] * scale\n    b_dg += tl.sum(b_dq * b_q, axis=1)\n    b_dk = b_dk * safe_exp(-b_g + b_g_last)[:, None]\n    b_dg -= tl.sum(b_k * b_dk, axis=1)\n    b_dg_last += tl.sum(b_dk * b_k)\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds * safe_exp(b_g[:, None] - b_g[None, :]), 0) * scale\n    b_ds2 = b_ds * tl.dot(b_q, tl.trans(b_k))\n    b_dg += tl.sum(b_ds2, axis=1)\n    b_dg -= tl.sum(b_ds2, axis=0)\n    b_ds = b_ds.to(b_k.dtype)\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q)\n    p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\nelse:\n    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)\n    b_ds = b_ds.to(b_k.dtype)\n    b_dq += tl.dot(b_ds, b_k)\n    b_dk += tl.dot(tl.trans(b_ds), b_q) * scale\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_G': lambda args: args['g'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dv(",
      "    q,",
      "    k,",
      "    g,",
      "    do,",
      "    dv,",
      "    dh,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "    dh += (i_tg * H + i_h).to(tl.int64) * K * V",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_k, b_q)",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)",
      "        b_dv *= safe_exp(-b_g + b_g_last)[:, None]",
      "",
      "    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]",
      "    if USE_G:",
      "        b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(",
      "            do.dtype.element_ty",
      "        )",
      "    else:",
      "        b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)",
      "    p_do = tl.make_block_ptr(",
      "        do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_dv += tl.dot(b_A.to(b_do.dtype), b_do)",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/372.py",
    "header": "def chunk_bwd_kernel_dv(q, k, g, do, dv, dh, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nb_dv = tl.zeros([BT, BV], dtype=tl.float32)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\ndo += (bos * H + i_h) * V\ndv += (bos * H + i_h) * V\ndh += (i_tg * H + i_h).to(tl.int64) * K * V\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A += tl.dot(b_k, b_q)\n    p_dh = tl.make_block_ptr(dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))\nif USE_G:\n    g += bos * H + i_h\n    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)\n    b_dv *= safe_exp(-b_g + b_g_last)[:, None]\nmask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\nif USE_G:\n    b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(do.dtype.element_ty)\nelse:\n    b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)\np_do = tl.make_block_ptr(do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_dv = tl.make_block_ptr(dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_dv += tl.dot(b_A.to(b_do.dtype), b_do)\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dv_local",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dv_local(",
      "    q,",
      "    k,",
      "    g,",
      "    do,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_k, b_q)",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "",
      "    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]",
      "    if USE_G:",
      "        b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(",
      "            do.dtype.element_ty",
      "        )",
      "    else:",
      "        b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dv = tl.dot(b_A.to(b_do.dtype), b_do)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/372.py",
    "header": "def chunk_bwd_kernel_dv_local(q, k, g, do, dv, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\ndo += (bos * H + i_h) * V\ndv += (bos * H + i_h) * V\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q = tl.make_block_ptr(q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_A += tl.dot(b_k, b_q)\nif USE_G:\n    g += bos * H + i_h\n    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\nmask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]\nif USE_G:\n    b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(do.dtype.element_ty)\nelse:\n    b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_do = tl.make_block_ptr(do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.dot(b_A.to(b_do.dtype), b_do)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_dplr_bwd_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BK', 'BT', 'K'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "dAqk",
        "annotation": null
      },
      {
        "name": "dAqb",
        "annotation": null
      },
      {
        "name": "dAak",
        "annotation": null
      },
      {
        "name": "dAab",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "da",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dqg",
        "annotation": null
      },
      {
        "name": "dkg",
        "annotation": null
      },
      {
        "name": "dag",
        "annotation": null
      },
      {
        "name": "dbg",
        "annotation": null
      },
      {
        "name": "dgk",
        "annotation": null
      },
      {
        "name": "dgk_offset",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GATHER_SUPPORTED",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_intra(",
      "    q,",
      "    k,",
      "    a,",
      "    b,",
      "    gi,",
      "    ge,",
      "    dAqk,",
      "    dAqb,",
      "    dAak,",
      "    dAab,",
      "    dq,",
      "    dk,",
      "    da,",
      "    db,",
      "    dqg,",
      "    dkg,",
      "    dag,",
      "    dbg,",
      "    dgk,",
      "    dgk_offset,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    GATHER_SUPPORTED: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = (i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32)",
      "",
      "    if i_t * BT >= T:",
      "        return",
      "",
      "    ge += (bos * H + i_h) * K",
      "    gi += (bos * H + i_h) * K",
      "    q += (bos * H + i_h) * K",
      "    a += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    da += (bos * H + i_h) * K",
      "    db += (bos * H + i_h) * K",
      "    dqg += (bos * H + i_h) * K",
      "    dag += (bos * H + i_h) * K",
      "    dkg += (bos * H + i_h) * K",
      "    dbg += (bos * H + i_h) * K",
      "    dgk += (bos * H + i_h) * K",
      "    dgk_offset += (bos * H + i_h) * K",
      "    dAqk += (bos * H + i_h) * BT",
      "    dAqb += (bos * H + i_h) * BT",
      "    dAak += (bos * H + i_h) * BT",
      "    dAab += (bos * H + i_h) * BT",
      "",
      "    stride_qk = H * K",
      "    stride_A = H * BT",
      "",
      "    p_ge = tl.make_block_ptr(",
      "        ge, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_gi = tl.make_block_ptr(",
      "        gi, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "",
      "    b_ge = tl.load(p_ge, boundary_check=(0, 1))",
      "    b_gi = tl.load(p_gi, boundary_check=(0, 1))",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    b_da = tl.zeros([BC, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "    b_db = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    p_dAqk = tl.make_block_ptr(",
      "        dAqk, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_dAab = tl.make_block_ptr(",
      "        dAab, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_dAqb = tl.make_block_ptr(",
      "        dAqb, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_dAak = tl.make_block_ptr(",
      "        dAak, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    o_i = tl.arange(0, BC)",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_b = tl.make_block_ptr(",
      "        b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_a = tl.make_block_ptr(",
      "        a, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_b = tl.load(p_b, boundary_check=(0, 1))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_a = tl.load(p_a, boundary_check=(0, 1))",
      "    b_dAqk = tl.load(p_dAqk, boundary_check=(0, 1))",
      "    b_dAab = tl.load(p_dAab, boundary_check=(0, 1))",
      "    b_dAqb = tl.load(p_dAqb, boundary_check=(0, 1))",
      "    b_dAak = tl.load(p_dAak, boundary_check=(0, 1))",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    for j in range(0, min(BC, T - i_t * BT)):",
      "",
      "        if GATHER_SUPPORTED:",
      "            row_idx = tl.full([1, BK], j, dtype=tl.int16)",
      "            col_idx = tl.full([BC, 1], j, dtype=tl.int16)",
      "            row_idx_bc = tl.full([1, BC], j, dtype=tl.int16)",
      "",
      "            b_kj = gather(b_k, row_idx, axis=0)",
      "            b_bj = gather(b_b, row_idx, axis=0)",
      "            b_gij = gather(b_gi, row_idx, axis=0)",
      "            b_gej = gather(b_ge, row_idx, axis=0)",
      "            b_qj = gather(b_q, row_idx, axis=0)",
      "            b_aj = gather(b_a, row_idx, axis=0)",
      "",
      "            b_dAqk_j = gather(b_dAqk, col_idx, axis=1)",
      "            b_dAab_j = gather(b_dAab, col_idx, axis=1)",
      "            b_dAqb_j = gather(b_dAqb, col_idx, axis=1)",
      "            b_dAak_j = gather(b_dAak, col_idx, axis=1)",
      "",
      "            b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_ab_j = tl.sum(gather(b_dAab, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_qb_j = tl.sum(gather(b_dAqb, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_ak_j = tl.sum(gather(b_dAak, row_idx_bc, axis=0), 0)[:, None]",
      "        else:",
      "            mask_idx = tl.arange(0, BC) == j",
      "            b_kj = tl.sum(tl.where(mask_idx[:, None], b_k, 0), 0)[None, :]",
      "            b_bj = tl.sum(tl.where(mask_idx[:, None], b_b, 0), 0)[None, :]",
      "            b_gij = tl.sum(tl.where(mask_idx[:, None], b_gi, 0), 0)[None, :]",
      "            b_gej = tl.sum(tl.where(mask_idx[:, None], b_ge, 0), 0)[None, :]",
      "            b_dAqk_j = tl.sum(tl.where(mask_idx[None, :], b_dAqk, 0), 1)[:, None]",
      "            b_dAab_j = tl.sum(tl.where(mask_idx[None, :], b_dAab, 0), 1)[:, None]",
      "            b_dAqb_j = tl.sum(tl.where(mask_idx[None, :], b_dAqb, 0), 1)[:, None]",
      "            b_dAak_j = tl.sum(tl.where(mask_idx[None, :], b_dAak, 0), 1)[:, None]",
      "            b_dA_qk_j = tl.sum(tl.where(mask_idx[:, None], b_dAqk, 0), 0)[:, None]",
      "            b_dA_ab_j = tl.sum(tl.where(mask_idx[:, None], b_dAab, 0), 0)[:, None]",
      "            b_dA_qb_j = tl.sum(tl.where(mask_idx[:, None], b_dAqb, 0), 0)[:, None]",
      "            b_dA_ak_j = tl.sum(tl.where(mask_idx[:, None], b_dAak, 0), 0)[:, None]",
      "",
      "            b_qj = tl.sum(tl.where(mask_idx[:, None], b_q, 0), 0)[None, :]",
      "            b_aj = tl.sum(tl.where(mask_idx[:, None], b_a, 0), 0)[None, :]",
      "",
      "        m_e = o_i[:, None] > j",
      "        m_i = o_i[:, None] >= j",
      "        tmp1 = exp(b_gi - b_gij)",
      "        tmp2 = exp(b_ge - b_gij)",
      "        b_dq += tl.where(m_i, b_dAqk_j * b_kj * tmp1, 0.0)",
      "        b_dq += tl.where(m_i, b_dAqb_j * b_bj * tmp1, 0.0)",
      "        b_da += tl.where(m_e, b_dAab_j * b_bj * tmp2, 0.0)",
      "        b_da += tl.where(m_e, b_dAak_j * b_kj * tmp2, 0.0)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        m_e = o_i[:, None] < j",
      "        tmp1 = exp(b_gij - b_gi)",
      "        tmp2 = exp(b_gej - b_gi)",
      "        b_dk += tl.where(m_i, b_dA_qk_j * b_qj * tmp1, 0.0)",
      "        b_dk += tl.where(m_e, b_dA_ak_j * b_aj * tmp2, 0.0)",
      "        b_db += tl.where(m_i, b_dA_qb_j * b_qj * tmp1, 0.0)",
      "        b_db += tl.where(m_e, b_dA_ab_j * b_aj * tmp2, 0.0)",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_da = tl.make_block_ptr(",
      "        da, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_db = tl.make_block_ptr(",
      "        db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dgk = tl.make_block_ptr(",
      "        dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dgk_offset = tl.make_block_ptr(",
      "        dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dqg = tl.make_block_ptr(",
      "        dqg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dkg = tl.make_block_ptr(",
      "        dkg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dag = tl.make_block_ptr(",
      "        dag, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dbg = tl.make_block_ptr(",
      "        dbg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_gn = gi + (min(i_t * BT + BT, T) - 1) * stride_qk + o_k",
      "    p_gn = tl.max_contiguous(tl.multiple_of(p_gn, BK), BK)",
      "    b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "    b_da += tl.load(p_dag, boundary_check=(0, 1)) * exp(b_ge)",
      "    b_dq += tl.load(p_dqg, boundary_check=(0, 1)) * exp(b_gi) * scale",
      "    tmp = exp(b_gn[None, :] - b_gi)",
      "    b_dk += tl.load(p_dkg, boundary_check=(0, 1)).to(tl.float32) * tmp",
      "    b_db += tl.load(p_dbg, boundary_check=(0, 1)).to(tl.float32) * tmp",
      "    tl.store(p_dq, (b_dq).to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_da, b_da.to(p_da.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))",
      "    b_dgk = (b_dq * b_q + b_da * b_a - b_dk * b_k - b_db * b_b).to(tl.float32)",
      "    b_dgk_offset = b_da * b_a",
      "    tl.store(p_dgk, b_dgk.to(p_dgk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(",
      "        p_dgk_offset,",
      "        b_dgk_offset.to(p_dgk_offset.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/381.py",
    "header": "def chunk_dplr_bwd_kernel_intra(q, k, a, b, gi, ge, dAqk, dAqb, dAak, dAab, dq, dk, da, db, dqg, dkg, dag, dbg, dgk, dgk_offset, cu_seqlens, chunk_indices, scale: tl.constexpr, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr, GATHER_SUPPORTED: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = ((i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32))\nif i_t * BT >= T:\n    return\nge += (bos * H + i_h) * K\ngi += (bos * H + i_h) * K\nq += (bos * H + i_h) * K\na += (bos * H + i_h) * K\nb += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\ndq += (bos * H + i_h) * K\ndk += (bos * H + i_h) * K\nda += (bos * H + i_h) * K\ndb += (bos * H + i_h) * K\ndqg += (bos * H + i_h) * K\ndag += (bos * H + i_h) * K\ndkg += (bos * H + i_h) * K\ndbg += (bos * H + i_h) * K\ndgk += (bos * H + i_h) * K\ndgk_offset += (bos * H + i_h) * K\ndAqk += (bos * H + i_h) * BT\ndAqb += (bos * H + i_h) * BT\ndAak += (bos * H + i_h) * BT\ndAab += (bos * H + i_h) * BT\nstride_qk = H * K\nstride_A = H * BT\np_ge = tl.make_block_ptr(ge, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_gi = tl.make_block_ptr(gi, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\nb_ge = tl.load(p_ge, boundary_check=(0, 1))\nb_gi = tl.load(p_gi, boundary_check=(0, 1))\nb_dq = tl.zeros([BC, BK], dtype=tl.float32)\nb_da = tl.zeros([BC, BK], dtype=tl.float32)\nb_dk = tl.zeros([BC, BK], dtype=tl.float32)\nb_db = tl.zeros([BC, BK], dtype=tl.float32)\np_dAqk = tl.make_block_ptr(dAqk, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0))\np_dAab = tl.make_block_ptr(dAab, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0))\np_dAqb = tl.make_block_ptr(dAqb, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0))\np_dAak = tl.make_block_ptr(dAak, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0))\no_i = tl.arange(0, BC)\np_k = tl.make_block_ptr(k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_b = tl.make_block_ptr(b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_a = tl.make_block_ptr(a, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_b = tl.load(p_b, boundary_check=(0, 1))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_a = tl.load(p_a, boundary_check=(0, 1))\nb_dAqk = tl.load(p_dAqk, boundary_check=(0, 1))\nb_dAab = tl.load(p_dAab, boundary_check=(0, 1))\nb_dAqb = tl.load(p_dAqb, boundary_check=(0, 1))\nb_dAak = tl.load(p_dAak, boundary_check=(0, 1))\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\nfor j in range(0, min(BC, T - i_t * BT)):\n    if GATHER_SUPPORTED:\n        row_idx = tl.full([1, BK], j, dtype=tl.int16)\n        col_idx = tl.full([BC, 1], j, dtype=tl.int16)\n        row_idx_bc = tl.full([1, BC], j, dtype=tl.int16)\n        b_kj = gather(b_k, row_idx, axis=0)\n        b_bj = gather(b_b, row_idx, axis=0)\n        b_gij = gather(b_gi, row_idx, axis=0)\n        b_gej = gather(b_ge, row_idx, axis=0)\n        b_qj = gather(b_q, row_idx, axis=0)\n        b_aj = gather(b_a, row_idx, axis=0)\n        b_dAqk_j = gather(b_dAqk, col_idx, axis=1)\n        b_dAab_j = gather(b_dAab, col_idx, axis=1)\n        b_dAqb_j = gather(b_dAqb, col_idx, axis=1)\n        b_dAak_j = gather(b_dAak, col_idx, axis=1)\n        b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]\n        b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]\n        b_dA_ab_j = tl.sum(gather(b_dAab, row_idx_bc, axis=0), 0)[:, None]\n        b_dA_qb_j = tl.sum(gather(b_dAqb, row_idx_bc, axis=0), 0)[:, None]\n        b_dA_ak_j = tl.sum(gather(b_dAak, row_idx_bc, axis=0), 0)[:, None]\n    else:\n        mask_idx = tl.arange(0, BC) == j\n        b_kj = tl.sum(tl.where(mask_idx[:, None], b_k, 0), 0)[None, :]\n        b_bj = tl.sum(tl.where(mask_idx[:, None], b_b, 0), 0)[None, :]\n        b_gij = tl.sum(tl.where(mask_idx[:, None], b_gi, 0), 0)[None, :]\n        b_gej = tl.sum(tl.where(mask_idx[:, None], b_ge, 0), 0)[None, :]\n        b_dAqk_j = tl.sum(tl.where(mask_idx[None, :], b_dAqk, 0), 1)[:, None]\n        b_dAab_j = tl.sum(tl.where(mask_idx[None, :], b_dAab, 0), 1)[:, None]\n        b_dAqb_j = tl.sum(tl.where(mask_idx[None, :], b_dAqb, 0), 1)[:, None]\n        b_dAak_j = tl.sum(tl.where(mask_idx[None, :], b_dAak, 0), 1)[:, None]\n        b_dA_qk_j = tl.sum(tl.where(mask_idx[:, None], b_dAqk, 0), 0)[:, None]\n        b_dA_ab_j = tl.sum(tl.where(mask_idx[:, None], b_dAab, 0), 0)[:, None]\n        b_dA_qb_j = tl.sum(tl.where(mask_idx[:, None], b_dAqb, 0), 0)[:, None]\n        b_dA_ak_j = tl.sum(tl.where(mask_idx[:, None], b_dAak, 0), 0)[:, None]\n        b_qj = tl.sum(tl.where(mask_idx[:, None], b_q, 0), 0)[None, :]\n        b_aj = tl.sum(tl.where(mask_idx[:, None], b_a, 0), 0)[None, :]\n    m_e = o_i[:, None] > j\n    m_i = o_i[:, None] >= j\n    tmp1 = exp(b_gi - b_gij)\n    tmp2 = exp(b_ge - b_gij)\n    b_dq += tl.where(m_i, b_dAqk_j * b_kj * tmp1, 0.0)\n    b_dq += tl.where(m_i, b_dAqb_j * b_bj * tmp1, 0.0)\n    b_da += tl.where(m_e, b_dAab_j * b_bj * tmp2, 0.0)\n    b_da += tl.where(m_e, b_dAak_j * b_kj * tmp2, 0.0)\n    m_i = o_i[:, None] <= j\n    m_e = o_i[:, None] < j\n    tmp1 = exp(b_gij - b_gi)\n    tmp2 = exp(b_gej - b_gi)\n    b_dk += tl.where(m_i, b_dA_qk_j * b_qj * tmp1, 0.0)\n    b_dk += tl.where(m_e, b_dA_ak_j * b_aj * tmp2, 0.0)\n    b_db += tl.where(m_i, b_dA_qb_j * b_qj * tmp1, 0.0)\n    b_db += tl.where(m_e, b_dA_ab_j * b_aj * tmp2, 0.0)\np_dq = tl.make_block_ptr(dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dk = tl.make_block_ptr(dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_da = tl.make_block_ptr(da, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_db = tl.make_block_ptr(db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dgk = tl.make_block_ptr(dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dgk_offset = tl.make_block_ptr(dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dqg = tl.make_block_ptr(dqg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dkg = tl.make_block_ptr(dkg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dag = tl.make_block_ptr(dag, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_dbg = tl.make_block_ptr(dbg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0))\np_gn = gi + (min(i_t * BT + BT, T) - 1) * stride_qk + o_k\np_gn = tl.max_contiguous(tl.multiple_of(p_gn, BK), BK)\nb_gn = tl.load(p_gn, mask=m_k, other=0)\nb_da += tl.load(p_dag, boundary_check=(0, 1)) * exp(b_ge)\nb_dq += tl.load(p_dqg, boundary_check=(0, 1)) * exp(b_gi) * scale\ntmp = exp(b_gn[None, :] - b_gi)\nb_dk += tl.load(p_dkg, boundary_check=(0, 1)).to(tl.float32) * tmp\nb_db += tl.load(p_dbg, boundary_check=(0, 1)).to(tl.float32) * tmp\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_da, b_da.to(p_da.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))\nb_dgk = (b_dq * b_q + b_da * b_a - b_dk * b_k - b_db * b_b).to(tl.float32)\nb_dgk_offset = b_da * b_a\ntl.store(p_dgk, b_dgk.to(p_dgk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dgk_offset, b_dgk_offset.to(p_dgk_offset.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_dplr_bwd_dgk_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4] for BK in [32, 64]], key=['BK', 'BT', 'K'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "dgk",
        "annotation": null
      },
      {
        "name": "dgk_offset",
        "annotation": null
      },
      {
        "name": "dgk_last",
        "annotation": null
      },
      {
        "name": "dgk_output",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_dgk_kernel(",
      "    dgk,",
      "    dgk_offset,",
      "    dgk_last,",
      "    dgk_output,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = (i_b * NT + i_t).to(tl.int32)",
      "        bos, eos = (i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32)",
      "",
      "    stride_qk = H * K",
      "    dgk += (bos * H + i_h) * K",
      "    dgk_offset += (bos * H + i_h) * K",
      "    dgk_last += (i_tg * H + i_h) * K",
      "    dgk_output += (bos * H + i_h) * K",
      "    p_dgk_last = dgk_last + tl.arange(0, BK) + i_k * BK",
      "    m_k = tl.arange(0, BK) + i_k * BK < K",
      "    b_dgk_last = tl.load(p_dgk_last, mask=m_k, other=0)",
      "    p_dgk_offset = tl.make_block_ptr(",
      "        dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dgk = tl.make_block_ptr(",
      "        dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_dgk = tl.load(p_dgk, boundary_check=(0, 1))",
      "    b_dgk_offset = tl.load(p_dgk_offset, boundary_check=(0, 1))",
      "",
      "    b_dgk_cumsum = tl.cumsum(b_dgk, 0, reverse=True)",
      "    b_dgk_cumsum += b_dgk_last[None, :]",
      "    b_dgk_cumsum -= b_dgk_offset",
      "    p_dgk_output = tl.make_block_ptr(",
      "        dgk_output, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_dgk_output,",
      "        b_dgk_cumsum.to(p_dgk_output.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/381.py",
    "header": "def chunk_dplr_bwd_dgk_kernel(dgk, dgk_offset, dgk_last, dgk_output, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = (i_b * NT + i_t).to(tl.int32)\n    bos, eos = ((i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32))\nstride_qk = H * K\ndgk += (bos * H + i_h) * K\ndgk_offset += (bos * H + i_h) * K\ndgk_last += (i_tg * H + i_h) * K\ndgk_output += (bos * H + i_h) * K\np_dgk_last = dgk_last + tl.arange(0, BK) + i_k * BK\nm_k = tl.arange(0, BK) + i_k * BK < K\nb_dgk_last = tl.load(p_dgk_last, mask=m_k, other=0)\np_dgk_offset = tl.make_block_ptr(dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dgk = tl.make_block_ptr(dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_dgk = tl.load(p_dgk, boundary_check=(0, 1))\nb_dgk_offset = tl.load(p_dgk_offset, boundary_check=(0, 1))\nb_dgk_cumsum = tl.cumsum(b_dgk, 0, reverse=True)\nb_dgk_cumsum += b_dgk_last[None, :]\nb_dgk_cumsum -= b_dgk_offset\np_dgk_output = tl.make_block_ptr(dgk_output, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\ntl.store(p_dgk_output, b_dgk_cumsum.to(p_dgk_output.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_recurrent_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BV in [32, 64] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "ha",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    a,",
      "    b,",
      "    o,",
      "    ha,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    H,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_q = q + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_k = k + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_a = a + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_b = b + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_ha = ha + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_o = o + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    mask_k = tl.arange(0, BK) < K",
      "    mask_v = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[None, :]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "",
      "        tmp = tl.sum(b_h * b_a[None, :], axis=1)",
      "        b_h += tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        tl.store(p_ha, tmp.to(p_ha.dtype.element_ty), mask=mask_v)",
      "        p_q += K * H",
      "        p_k += K * H",
      "        p_o += V * H",
      "        p_v += V * H",
      "        p_ha += V * H",
      "        p_a += K * H",
      "        p_b += K * H",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[None, :]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/391.py",
    "header": "def fused_recurrent_fwd_kernel(q, k, v, a, b, o, ha, h0, ht, cu_seqlens, scale, H, T, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\np_q = q + (bos * H + i_h) * K + tl.arange(0, BK)\np_k = k + (bos * H + i_h) * K + tl.arange(0, BK)\np_a = a + (bos * H + i_h) * K + tl.arange(0, BK)\np_b = b + (bos * H + i_h) * K + tl.arange(0, BK)\np_ha = ha + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\np_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\np_o = o + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)\nmask_k = tl.arange(0, BK) < K\nmask_v = i_v * BV + tl.arange(0, BV) < V\nmask_h = mask_k[None, :] & mask_v[:, None]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + tl.arange(0, BK)[None, :] * V + (i_v * BV + tl.arange(0, BV))[:, None]\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n    b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n    tmp = tl.sum(b_h * b_a[None, :], axis=1)\n    b_h += tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]\n    b_o = b_h * b_q[None, :]\n    b_o = tl.sum(b_o, axis=1)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    tl.store(p_ha, tmp.to(p_ha.dtype.element_ty), mask=mask_v)\n    p_q += K * H\n    p_k += K * H\n    p_o += V * H\n    p_v += V * H\n    p_ha += V * H\n    p_a += K * H\n    p_b += K * H\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + tl.arange(0, BK)[None, :] * V + (i_v * BV + tl.arange(0, BV))[:, None]\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "fused_recurrent_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_DHT': lambda args: args['dht'] is not None, 'USE_DH0': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3]], key=['BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "ha",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "da",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dha",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DH0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    a,",
      "    b,",
      "    ha,",
      "    dht,",
      "    dh0,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    da,",
      "    db,",
      "    dha,",
      "    h0,",
      "    scale,",
      "    cu_seqlens,",
      "    B,",
      "    H,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_DH0: tl.constexpr,",
      "    USE_DHT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    dk += i_v * B * H * K * T",
      "    db += i_v * B * H * K * T",
      "    dq += i_v * B * H * K * T",
      "    da += i_v * B * H * K * T",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    mask_k = tl.arange(0, BK) < K",
      "    mask_v = (tl.arange(0, BV) + i_v * BV) < V",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V + i_v * BV",
      "    ha += (bos * H + i_h) * V + i_v * BV",
      "    a += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V + i_v * BV",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    dv += (bos * H + i_h) * V + i_v * BV",
      "    da += (bos * H + i_h) * K",
      "    db += (bos * H + i_h) * K",
      "    dha += (bos * H + i_h) * V + i_v * BV",
      "",
      "    p_q = q + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_k = k + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_v = v + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_ha = ha + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_a = a + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_b = b + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_do = do + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_dk = dk + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_dv = dv + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_dha = dha + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_db = db + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_da = da + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_dq = dq + tl.arange(0, BK) + (T - 1) * H * K",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_DHT:",
      "        p_ht = (",
      "            dht",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[:, None]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[None, :])",
      "        )",
      "        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "    for _ in range(T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)",
      "",
      "        b_dh += b_q[:, None] * b_do[None, :]",
      "        d_k = tl.sum(b_dh * b_v[None, :], axis=1)",
      "        d_v = tl.sum(b_dh * b_k[:, None], axis=0)",
      "        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_v)",
      "",
      "        b_dha = tl.sum(b_dh * b_b[:, None], axis=0)",
      "        tl.store(p_dha, b_dha.to(p_dha.dtype.element_ty), mask=mask_v)",
      "        b_db = tl.sum(b_dh * b_ha[None, :], axis=1)",
      "        tl.store(p_db, b_db.to(p_db.dtype.element_ty), mask=mask_k)",
      "",
      "        b_dh += b_dha[None, :] * b_a[:, None]",
      "        p_do -= H * V",
      "        p_q -= H * K",
      "        p_k -= H * K",
      "        p_v -= H * V",
      "        p_dk -= H * K",
      "        p_dv -= H * V",
      "        p_b -= H * K",
      "        p_db -= H * K",
      "        p_a -= H * K",
      "        p_dha -= H * V",
      "        p_ha -= H * V",
      "",
      "    if USE_DH0:",
      "        p_dh0 = (",
      "            dh0",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        tl.store(",
      "            p_dh0,",
      "            b_dh.to(p_dh0.dtype.element_ty),",
      "            mask=mask_k[:, None] & mask_v[None, :],",
      "        )",
      "",
      "    tl.debug_barrier()",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        mask_kv = mask_k[:, None] & mask_v[None, :]",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[:, None]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)",
      "",
      "    p_k = k + tl.arange(0, BK)",
      "    p_v = v + tl.arange(0, BV)",
      "    p_ha = ha + tl.arange(0, BV)",
      "    p_do = do + tl.arange(0, BV)",
      "    p_dha = dha + tl.arange(0, BV)",
      "    p_da = da + tl.arange(0, BK)",
      "    p_dq = dq + tl.arange(0, BK)",
      "    p_b = b + tl.arange(0, BK)",
      "",
      "    for i in range(0, T):",
      "        b_dha = tl.load(p_dha, mask=mask_v, other=0).to(tl.float32)",
      "        d_a = tl.sum(b_dha[None, :] * b_h, axis=1)",
      "        tl.store(p_da, d_a.to(p_da.dtype.element_ty), mask=mask_k)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "        b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)",
      "        b_h += b_k[:, None] * b_v[None, :] + b_b[:, None] * b_ha[None, :]",
      "        _d_q = b_h * b_do[None, :]",
      "        d_q = tl.sum(_d_q, axis=1) * scale",
      "        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += H * K",
      "        p_do += H * V",
      "        p_v += H * V",
      "        p_da += H * K",
      "        p_dha += H * V",
      "        p_ha += H * V",
      "        p_dq += H * K",
      "        p_b += H * K"
    ],
    "file": "codes/391.py",
    "header": "def fused_recurrent_bwd_kernel(q, k, v, a, b, ha, dht, dh0, do, dq, dk, dv, da, db, dha, h0, scale, cu_seqlens, B, H, T, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_DH0: tl.constexpr, USE_DHT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_h = (i_nh // H, i_nh % H)\ndk += i_v * B * H * K * T\ndb += i_v * B * H * K * T\ndq += i_v * B * H * K * T\nda += i_v * B * H * K * T\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\nmask_k = tl.arange(0, BK) < K\nmask_v = tl.arange(0, BV) + i_v * BV < V\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V + i_v * BV\nha += (bos * H + i_h) * V + i_v * BV\na += (bos * H + i_h) * K\nb += (bos * H + i_h) * K\ndo += (bos * H + i_h) * V + i_v * BV\ndq += (bos * H + i_h) * K\ndk += (bos * H + i_h) * K\ndv += (bos * H + i_h) * V + i_v * BV\nda += (bos * H + i_h) * K\ndb += (bos * H + i_h) * K\ndha += (bos * H + i_h) * V + i_v * BV\np_q = q + tl.arange(0, BK) + (T - 1) * H * K\np_k = k + tl.arange(0, BK) + (T - 1) * H * K\np_v = v + tl.arange(0, BV) + (T - 1) * H * V\np_ha = ha + tl.arange(0, BV) + (T - 1) * H * V\np_a = a + tl.arange(0, BK) + (T - 1) * H * K\np_b = b + tl.arange(0, BK) + (T - 1) * H * K\np_do = do + tl.arange(0, BV) + (T - 1) * H * V\np_dk = dk + tl.arange(0, BK) + (T - 1) * H * K\np_dv = dv + tl.arange(0, BV) + (T - 1) * H * V\np_dha = dha + tl.arange(0, BV) + (T - 1) * H * V\np_db = db + tl.arange(0, BK) + (T - 1) * H * K\np_da = da + tl.arange(0, BK) + (T - 1) * H * K\np_dq = dq + tl.arange(0, BK) + (T - 1) * H * K\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_DHT:\n    p_ht = dht + i_nh * K * V + tl.arange(0, BK)[:, None] * V + (i_v * BV + tl.arange(0, BV))[None, :]\n    b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(tl.float32)\nfor _ in range(T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n    b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n    b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)\n    b_dh += b_q[:, None] * b_do[None, :]\n    d_k = tl.sum(b_dh * b_v[None, :], axis=1)\n    d_v = tl.sum(b_dh * b_k[:, None], axis=0)\n    tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_k)\n    tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_v)\n    b_dha = tl.sum(b_dh * b_b[:, None], axis=0)\n    tl.store(p_dha, b_dha.to(p_dha.dtype.element_ty), mask=mask_v)\n    b_db = tl.sum(b_dh * b_ha[None, :], axis=1)\n    tl.store(p_db, b_db.to(p_db.dtype.element_ty), mask=mask_k)\n    b_dh += b_dha[None, :] * b_a[:, None]\n    p_do -= H * V\n    p_q -= H * K\n    p_k -= H * K\n    p_v -= H * V\n    p_dk -= H * K\n    p_dv -= H * V\n    p_b -= H * K\n    p_db -= H * K\n    p_a -= H * K\n    p_dha -= H * V\n    p_ha -= H * V\nif USE_DH0:\n    p_dh0 = dh0 + i_nh * K * V + tl.arange(0, BK)[:, None] * V + (i_v * BV + tl.arange(0, BV)[None, :])\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_k[:, None] & mask_v[None, :])\ntl.debug_barrier()\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    mask_kv = mask_k[:, None] & mask_v[None, :]\n    p_h0 = h0 + i_nh * K * V + tl.arange(0, BK)[:, None] * V + (i_v * BV + tl.arange(0, BV))[None, :]\n    b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)\np_k = k + tl.arange(0, BK)\np_v = v + tl.arange(0, BV)\np_ha = ha + tl.arange(0, BV)\np_do = do + tl.arange(0, BV)\np_dha = dha + tl.arange(0, BV)\np_da = da + tl.arange(0, BK)\np_dq = dq + tl.arange(0, BK)\np_b = b + tl.arange(0, BK)\nfor i in range(0, T):\n    b_dha = tl.load(p_dha, mask=mask_v, other=0).to(tl.float32)\n    d_a = tl.sum(b_dha[None, :] * b_h, axis=1)\n    tl.store(p_da, d_a.to(p_da.dtype.element_ty), mask=mask_k)\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)\n    b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)\n    b_h += b_k[:, None] * b_v[None, :] + b_b[:, None] * b_ha[None, :]\n    _d_q = b_h * b_do[None, :]\n    d_q = tl.sum(_d_q, axis=1) * scale\n    tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)\n    p_k += H * K\n    p_do += H * V\n    p_v += H * V\n    p_da += H * K\n    p_dha += H * V\n    p_ha += H * V\n    p_dq += H * K\n    p_b += H * K"
  },
  {
    "name": "token_shift_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [1, 2, 3, 4]], key=['BD'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def token_shift_fwd_kernel(",
      "    x,",
      "    y,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "",
      "    if IS_VARLEN:",
      "        i_n = i_b",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        g_t = i_t + bos",
      "",
      "        if g_t >= eos:",
      "            return",
      "",
      "        is_first_pos = i_t == 0",
      "    else:",
      "        g_t = i_t",
      "        is_first_pos = g_t == 0",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    if IS_VARLEN:",
      "        base_offset = g_t * D + o_d",
      "    else:",
      "        base_offset = i_b * T * D + g_t * D + o_d",
      "",
      "    b_x = tl.load(x + base_offset, mask=m_d)",
      "",
      "    if is_first_pos:",
      "",
      "        tl.store(y + base_offset, -b_x, mask=m_d)",
      "    else:",
      "",
      "        if IS_VARLEN:",
      "            prev_offset = (g_t - 1) * D + o_d",
      "        else:",
      "            prev_offset = i_b * T * D + (g_t - 1) * D + o_d",
      "",
      "        prev_values = tl.load(x + prev_offset, mask=m_d)",
      "        delta = prev_values - b_x",
      "        tl.store(y + base_offset, delta, mask=m_d)"
    ],
    "file": "codes/362.py",
    "header": "def token_shift_fwd_kernel(x, y, cu_seqlens, T, D: tl.constexpr, BD: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_b, i_t = (tl.program_id(0), tl.program_id(1))\nif IS_VARLEN:\n    i_n = i_b\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    g_t = i_t + bos\n    if g_t >= eos:\n        return\n    is_first_pos = i_t == 0\nelse:\n    g_t = i_t\n    is_first_pos = g_t == 0\no_d = tl.arange(0, BD)\nm_d = o_d < D\nif IS_VARLEN:\n    base_offset = g_t * D + o_d\nelse:\n    base_offset = i_b * T * D + g_t * D + o_d\nb_x = tl.load(x + base_offset, mask=m_d)\nif is_first_pos:\n    tl.store(y + base_offset, -b_x, mask=m_d)\nelse:\n    if IS_VARLEN:\n        prev_offset = (g_t - 1) * D + o_d\n    else:\n        prev_offset = i_b * T * D + (g_t - 1) * D + o_d\n    prev_values = tl.load(x + prev_offset, mask=m_d)\n    delta = prev_values - b_x\n    tl.store(y + base_offset, delta, mask=m_d)"
  },
  {
    "name": "token_shift_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [1, 2, 3, 4]], key=['D'])"
    ],
    "args": [
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def token_shift_bwd_kernel(",
      "    dx,",
      "    dy,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "    if IS_VARLEN:",
      "        i_n = i_b",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        g_t = i_t + bos",
      "        if g_t >= eos:",
      "            return",
      "",
      "        is_last_pos = g_t == eos - 1",
      "    else:",
      "        g_t = i_t",
      "        is_last_pos = g_t == T - 1",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    if IS_VARLEN:",
      "        base_offset = g_t * D + o_d",
      "    else:",
      "        base_offset = i_b * T * D + g_t * D + o_d",
      "",
      "    b_dy = tl.load(dy + base_offset, mask=m_d)",
      "",
      "    if is_last_pos:",
      "",
      "        b_dx = -b_dy",
      "    else:",
      "",
      "        if IS_VARLEN:",
      "            next_offset = (g_t + 1) * D + o_d",
      "        else:",
      "            next_offset = i_b * T * D + (g_t + 1) * D + o_d",
      "",
      "        b_dx = -b_dy + tl.load(dy + next_offset, mask=m_d)",
      "",
      "    tl.store(dx + base_offset, b_dx, mask=m_d)"
    ],
    "file": "codes/362.py",
    "header": "def token_shift_bwd_kernel(dx, dy, cu_seqlens, T, D: tl.constexpr, BD: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_b, i_t = (tl.program_id(0), tl.program_id(1))\nif IS_VARLEN:\n    i_n = i_b\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    g_t = i_t + bos\n    if g_t >= eos:\n        return\n    is_last_pos = g_t == eos - 1\nelse:\n    g_t = i_t\n    is_last_pos = g_t == T - 1\no_d = tl.arange(0, BD)\nm_d = o_d < D\nif IS_VARLEN:\n    base_offset = g_t * D + o_d\nelse:\n    base_offset = i_b * T * D + g_t * D + o_d\nb_dy = tl.load(dy + base_offset, mask=m_d)\nif is_last_pos:\n    b_dx = -b_dy\nelse:\n    if IS_VARLEN:\n        next_offset = (g_t + 1) * D + o_d\n    else:\n        next_offset = i_b * T * D + (g_t + 1) * D + o_d\n    b_dx = -b_dy + tl.load(dy + next_offset, mask=m_d)\ntl.store(dx + base_offset, b_dx, mask=m_d)"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "codes/174.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr):",
    "body": "idx = tl.program_id(axis=0)\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\noffsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + offsets)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    local_x_block_ptr = x_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == v_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    m = m_new\n    A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n    v_range = v_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\nloss += tl.sum(lse) / N\ntl.store(losses_ptr + idx, loss)\ntl.store(lse_ptr + offsets, lse)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N",
      "            tl.store(",
      "                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)",
      "            )",
      "",
      "            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "codes/174.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dA(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, A_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_V = tl.program_id(axis=0)\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nN_offsets = tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nA_grad_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(0 * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nfor idx_N in range(N // N_BLOCK_SIZE):\n    y = tl.load(y_ptr + N_offsets)\n    lse = tl.load(lse_global_ptr + N_offsets)\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n    A_block_ptr = tl.advance(A_block_ptr, [-H, 0])\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n    for idx_H in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n        temp_AgradT = temp_Agrad.trans() / N\n        tl.store(A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr))\n        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])\n    A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])\n    N_offsets += N_BLOCK_SIZE"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_GROUP_SIZE: tl.constexpr = 4,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H_group = tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    H_GROUPS: tl.constexpr = H // (H_GROUP_SIZE * H_BLOCK_SIZE)",
      "    tl.static_print(H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE, H_GROUPS)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "    H_group_offsets = tl.arange(0, H_GROUP_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE), dtype=tl.float16)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "            x_block_ptr = tl.make_block_ptr(",
      "                base=x_ptr,",
      "                shape=(N, H),",
      "                strides=(stride_x_N, stride_x_H),",
      "                offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),",
      "                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_block_ptr = tl.make_block_ptr(",
      "                base=A_t_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=((H_GROUP_SIZE * H_BLOCK_SIZE) * idx_H_group, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        for idx_H_in_group in range(H_GROUP_SIZE):",
      "",
      "            A_v = tl.load(A_block_ptr).trans()",
      "",
      "            x_grad_block = tl.dot(softmax_z, A_v) / N",
      "            x_grad_block -= tl.sum(tl.where(mask, A_v[None, :, :], 0), axis=1) / N",
      "            x_grad_slice = x_grad_block[:, :, None].to(tl.float16)",
      "",
      "            accum_mask = (idx_H_in_group == H_group_offsets)[None, None, :]",
      "            x_grad_acc += tl.where(accum_mask, x_grad_slice, 0)",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_H_group * H_GROUP_SIZE * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(",
      "        x_grad_block_ptr, x_grad_acc.reshape(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE)",
      "    )"
    ],
    "file": "codes/174.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16, H_GROUP_SIZE: tl.constexpr = 4):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_H_group = tl.program_id(axis=1)\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nH_GROUPS: tl.constexpr = H // (H_GROUP_SIZE * H_BLOCK_SIZE)\ntl.static_print(H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE, H_GROUPS)\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = tl.arange(0, V_BLOCK_SIZE)\nH_group_offsets = tl.arange(0, H_GROUP_SIZE)\ny = tl.load(y_ptr + N_offsets)\nlse = tl.load(lse_global_ptr + N_offsets)\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE), dtype=tl.float16)\nfor idx_V in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for idx_H_1 in range(H // H_BLOCK_SIZE):\n        x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n        A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    mask = (y[:, None] == V_offsets[None, :])[:, :, None]\n    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\n    A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(H_GROUP_SIZE * H_BLOCK_SIZE * idx_H_group, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n    for idx_H_in_group in range(H_GROUP_SIZE):\n        A_v = tl.load(A_block_ptr).trans()\n        x_grad_block = tl.dot(softmax_z, A_v) / N\n        x_grad_block -= tl.sum(tl.where(mask, A_v[None, :, :], 0), axis=1) / N\n        x_grad_slice = x_grad_block[:, :, None].to(tl.float16)\n        accum_mask = (idx_H_in_group == H_group_offsets)[None, None, :]\n        x_grad_acc += tl.where(accum_mask, x_grad_slice, 0)\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    V_offsets += V_BLOCK_SIZE\nx_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H_group * H_GROUP_SIZE * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE), order=(1, 0))\ntl.store(x_grad_block_ptr, x_grad_acc.reshape(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE))"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    Z,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "codes/125.py",
    "header": "def _layer_norm_fwd_1pass_kernel(X, Y, W, B, Z, Mean, Rstd, stride_x_row, stride_y_row, stride_z_row, M, N, eps, BLOCK_N: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.constexpr):",
    "body": "row = tl.program_id(0)\ngroup = tl.program_id(1)\nX += row * stride_x_row + group * N\nY += row * stride_y_row + group * N\nif HAS_Z:\n    Z += row * stride_z_row + group * N\nif not IS_RMS_NORM:\n    Mean += group * M\nRstd += group * M\nW += group * N\nif HAS_BIAS:\n    B += group * N\ncols = tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\nif HAS_Z and (not NORM_BEFORE_GATE):\n    z = tl.load(Z + cols, mask=cols < N).to(tl.float32)\n    x *= z * tl.sigmoid(z)\nif not IS_RMS_NORM:\n    mean = tl.sum(x, axis=0) / N\n    tl.store(Mean + row, mean)\n    xbar = tl.where(cols < N, x - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nelse:\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nmask = cols < N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif HAS_BIAS:\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\nx_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\ny = x_hat * w + b if HAS_BIAS else x_hat * w\nif HAS_Z and NORM_BEFORE_GATE:\n    z = tl.load(Z + cols, mask=mask).to(tl.float32)\n    y *= z * tl.sigmoid(z)\ntl.store(Y + cols, y, mask=mask)"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DZ",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dz_row",
        "annotation": null
      },
      {
        "name": "stride_dw_row",
        "annotation": null
      },
      {
        "name": "stride_db_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Z,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DZ,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_z_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dz_row,",
      "    stride_dw_row,",
      "    stride_db_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row + group * N",
      "    if HAS_Z:",
      "        Z += row_start * stride_z_row + group * N",
      "        DZ += row_start * stride_dz_row + group * N",
      "    DY += row_start * stride_dy_row + group * N",
      "    DX += row_start * stride_dx_row + group * N",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:",
      "        B += group * N",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            x_og = x",
      "            x = x_og * z * tl.sigmoid(z)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if HAS_Z and NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            z_sigmoid = tl.sigmoid(z)",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            if RECOMPUTE_OUTPUT:",
      "                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)",
      "            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dy *= z * z_sigmoid",
      "        else:",
      "            if RECOMPUTE_OUTPUT:",
      "                y = xhat * w + b if HAS_BIAS else xhat * w",
      "                tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        c1 = tl.sum(xhat * wdy, axis=0) / N",
      "        if not IS_RMS_NORM:",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            dx = (wdy - xhat * c1) * rstd",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z_sigmoid = tl.sigmoid(z)",
      "            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dx *= z * z_sigmoid",
      "",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_Z:",
      "            Z += stride_z_row",
      "            DZ += stride_dz_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
    ],
    "file": "codes/125.py",
    "header": "def _layer_norm_bwd_kernel(X, W, B, Z, Y, DY, DX, DW, DB, DZ, Mean, Rstd, stride_x_row, stride_z_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dz_row, stride_dw_row, stride_db_row, M, N, eps, rows_per_program, NORM_BEFORE_GATE: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_Z: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\ngroup = tl.program_id(1)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, BLOCK_N)\nmask = cols < N\nX += row_start * stride_x_row + group * N\nif HAS_Z:\n    Z += row_start * stride_z_row + group * N\n    DZ += row_start * stride_dz_row + group * N\nDY += row_start * stride_dy_row + group * N\nDX += row_start * stride_dx_row + group * N\nif RECOMPUTE_OUTPUT:\n    Y += row_start * stride_y_row + group * N\nif not IS_RMS_NORM:\n    Mean += group * M\nRstd += group * M\nW += group * N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:\n    B += group * N\n    b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\ndw = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_BIAS:\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\nrow_end = min((row_block_id + 1) * rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        mean = tl.load(Mean + row)\n    if HAS_Z and (not NORM_BEFORE_GATE):\n        z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n        x_og = x\n        x = x_og * z * tl.sigmoid(z)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    if HAS_Z and NORM_BEFORE_GATE:\n        z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)\n        z_sigmoid = tl.sigmoid(z)\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        if RECOMPUTE_OUTPUT:\n            tl.store(Y + cols, y * z * z_sigmoid, mask=mask)\n        dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))\n        tl.store(DZ + cols, dz, mask=mask)\n        dy *= z * z_sigmoid\n    elif RECOMPUTE_OUTPUT:\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        tl.store(Y + cols, y, mask=mask)\n    wdy = w * dy\n    c1 = tl.sum(xhat * wdy, axis=0) / N\n    if not IS_RMS_NORM:\n        c2 = tl.sum(wdy, axis=0) / N\n        dx = (wdy - (xhat * c1 + c2)) * rstd\n    else:\n        dx = (wdy - xhat * c1) * rstd\n    dw += dy * xhat\n    if HAS_BIAS:\n        db += dy\n    if HAS_Z and (not NORM_BEFORE_GATE):\n        z_sigmoid = tl.sigmoid(z)\n        dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))\n        tl.store(DZ + cols, dz, mask=mask)\n        dx *= z * z_sigmoid\n    tl.store(DX + cols, dx, mask=mask)\n    X += stride_x_row\n    if HAS_Z:\n        Z += stride_z_row\n        DZ += stride_dz_row\n    if RECOMPUTE_OUTPUT:\n        Y += stride_y_row\n    DY += stride_dy_row\n    DX += stride_dx_row\ntl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)\nif HAS_BIAS:\n    tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
  },
  {
    "name": "layer_norm_gated_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_RESIDUAL_OUT': lambda args: args['residual_out'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [8, 16, 32, 64] for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'NB', 'IS_RMS_NORM', 'STORE_RESIDUAL_OUT', 'HAS_RESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "residual_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_fwd_kernel(",
      "    x,",
      "    g,",
      "    y,",
      "    w,",
      "    b,",
      "    residual,",
      "    residual_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        p_res = tl.make_block_ptr(",
      "            residual, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)",
      "        )",
      "        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        p_res_out = tl.make_block_ptr(",
      "            residual_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)",
      "        )",
      "        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=1) / D",
      "        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))",
      "        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    else:",
      "        b_xbar = tl.where(m_d[None, :], b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "",
      "    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)",
      "    b_x_hat = (",
      "        (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "        if not IS_RMS_NORM",
      "        else b_x * b_rstd[:, None]",
      "    )",
      "    b_y = b_x_hat * b_w[None, :] if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b[None, :]",
      "",
      "    p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)",
      "    if ACTIVATION == \"swish\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"silu\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"sigmoid\":",
      "        b_y = b_y * tl.sigmoid(b_g)",
      "",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/356.py",
    "header": "def layer_norm_gated_fwd_kernel(x, g, y, w, b, residual, residual_out, mean, rstd, eps, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, NB: tl.constexpr, ACTIVATION: tl.constexpr, IS_RMS_NORM: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_RESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\no_d = tl.arange(0, BD)\nm_d = o_d < D\np_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\nb_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\nif HAS_RESIDUAL:\n    p_res = tl.make_block_ptr(residual, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)\nif STORE_RESIDUAL_OUT:\n    p_res_out = tl.make_block_ptr(residual_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))\nif not IS_RMS_NORM:\n    b_mean = tl.sum(b_x, axis=1) / D\n    p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))\n    b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\nelse:\n    b_xbar = tl.where(m_d[None, :], b_x, 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\nb_rstd = 1 / tl.sqrt(b_var + eps)\np_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\ntl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))\nif HAS_WEIGHT:\n    b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)\nb_x_hat = (b_x - b_mean[:, None]) * b_rstd[:, None] if not IS_RMS_NORM else b_x * b_rstd[:, None]\nb_y = b_x_hat * b_w[None, :] if HAS_WEIGHT else b_x_hat\nif HAS_BIAS:\n    b_y = b_y + b_b[None, :]\np_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\nb_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\nif ACTIVATION == 'swish':\n    b_y = b_y * b_g * tl.sigmoid(b_g)\nelif ACTIVATION == 'silu':\n    b_y = b_y * b_g * tl.sigmoid(b_g)\nelif ACTIVATION == 'sigmoid':\n    b_y = b_y * tl.sigmoid(b_g)\np_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\ntl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "layer_norm_gated_fwd_kernel1",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_RESIDUAL_OUT': lambda args: args['residual_out'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'IS_RMS_NORM', 'STORE_RESIDUAL_OUT', 'HAS_RESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "residual_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_fwd_kernel1(",
      "    x,",
      "    g,",
      "    y,",
      "    w,",
      "    b,",
      "    residual,",
      "    residual_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    y += i_t * D",
      "    g += i_t * D",
      "    if HAS_RESIDUAL:",
      "        residual += i_t * D",
      "    if STORE_RESIDUAL_OUT:",
      "        residual_out += i_t * D",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    b_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        b_x += tl.load(residual + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(residual_out + o_d, b_x, mask=m_d)",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=0) / D",
      "        tl.store(mean + i_t, b_mean)",
      "        b_xbar = tl.where(m_d, b_x - b_mean, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    else:",
      "        b_xbar = tl.where(m_d, b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    tl.store(rstd + i_t, b_rstd)",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)",
      "    b_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b",
      "",
      "    b_g = tl.load(g + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if ACTIVATION == \"swish\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"silu\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"sigmoid\":",
      "        b_y = b_y * tl.sigmoid(b_g)",
      "",
      "    tl.store(y + o_d, b_y, mask=m_d)"
    ],
    "file": "codes/356.py",
    "header": "def layer_norm_gated_fwd_kernel1(x, g, y, w, b, residual, residual_out, mean, rstd, eps, D: tl.constexpr, BD: tl.constexpr, ACTIVATION: tl.constexpr, IS_RMS_NORM: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_RESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\nx += i_t * D\ny += i_t * D\ng += i_t * D\nif HAS_RESIDUAL:\n    residual += i_t * D\nif STORE_RESIDUAL_OUT:\n    residual_out += i_t * D\no_d = tl.arange(0, BD)\nm_d = o_d < D\nb_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)\nif HAS_RESIDUAL:\n    b_x += tl.load(residual + o_d, mask=m_d, other=0.0).to(tl.float32)\nif STORE_RESIDUAL_OUT:\n    tl.store(residual_out + o_d, b_x, mask=m_d)\nif not IS_RMS_NORM:\n    b_mean = tl.sum(b_x, axis=0) / D\n    tl.store(mean + i_t, b_mean)\n    b_xbar = tl.where(m_d, b_x - b_mean, 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\nelse:\n    b_xbar = tl.where(m_d, b_x, 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\nb_rstd = 1 / tl.sqrt(b_var + eps)\ntl.store(rstd + i_t, b_rstd)\nif HAS_WEIGHT:\n    b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)\nb_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\nb_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat\nif HAS_BIAS:\n    b_y = b_y + b_b\nb_g = tl.load(g + o_d, mask=m_d, other=0.0).to(tl.float32)\nif ACTIVATION == 'swish':\n    b_y = b_y * b_g * tl.sigmoid(b_g)\nelif ACTIVATION == 'silu':\n    b_y = b_y * b_g * tl.sigmoid(b_g)\nelif ACTIVATION == 'sigmoid':\n    b_y = b_y * tl.sigmoid(b_g)\ntl.store(y + o_d, b_y, mask=m_d)"
  },
  {
    "name": "layer_norm_gated_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DRESIDUAL': lambda args: args['dresidual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None, 'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [8, 16, 32, 64] for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'NB', 'IS_RMS_NORM', 'HAS_DRESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dresidual",
        "annotation": null
      },
      {
        "name": "dresidual_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_bwd_kernel(",
      "    x,",
      "    g,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dg,",
      "    dw,",
      "    db,",
      "    dresidual,",
      "    dresidual_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)",
      "        b_dw = tl.zeros((BT, BD), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "        b_db = tl.zeros((BT, BD), dtype=tl.float32)",
      "",
      "    T = min(i_s * BS + BS, T)",
      "    for i_t in range(i_s * BS, T, BT):",
      "        p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_dg = tl.make_block_ptr(dg, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "",
      "        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t,), (BT,), (0,))",
      "            b_mean = tl.load(p_mean, boundary_check=(0,))",
      "        p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t,), (BT,), (0,))",
      "        b_rstd = tl.load(p_rstd, boundary_check=(0,))",
      "",
      "        b_xhat = (",
      "            (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "            if not IS_RMS_NORM",
      "            else b_x * b_rstd[:, None]",
      "        )",
      "        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)",
      "",
      "        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat",
      "        if HAS_BIAS:",
      "            b_y = b_y + b_b[None, :]",
      "        if RECOMPUTE_OUTPUT:",
      "            p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_sigmoid_g = tl.sigmoid(b_g)",
      "        if ACTIVATION == \"swish\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"silu\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"sigmoid\":",
      "            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)",
      "            b_dy = b_dy * b_sigmoid_g",
      "        b_wdy = b_dy",
      "",
      "        if HAS_WEIGHT or HAS_BIAS:",
      "            m_t = (i_t + tl.arange(0, BT)) < T",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)",
      "        if HAS_BIAS:",
      "            b_db += tl.where(m_t[:, None], b_dy, 0.0)",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_c2 = tl.sum(b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]",
      "        if HAS_DRESIDUAL:",
      "            p_dres = tl.make_block_ptr(",
      "                dresidual, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            p_dres_in = tl.make_block_ptr(",
      "                dresidual_in, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            tl.store(",
      "                p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)"
    ],
    "file": "codes/356.py",
    "header": "def layer_norm_gated_bwd_kernel(x, g, w, b, y, dy, dx, dg, dw, db, dresidual, dresidual_in, mean, rstd, T, D: tl.constexpr, BS: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, NB: tl.constexpr, ACTIVATION: tl.constexpr, IS_RMS_NORM: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "i_s = tl.program_id(0)\no_d = tl.arange(0, BD)\nm_d = o_d < D\nif HAS_WEIGHT:\n    b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)\n    b_dw = tl.zeros((BT, BD), dtype=tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + o_d, mask=m_d, other=0.0).to(tl.float32)\n    b_db = tl.zeros((BT, BD), dtype=tl.float32)\nT = min(i_s * BS + BS, T)\nfor i_t in range(i_s * BS, T, BT):\n    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n    p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n    p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n    p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n    p_dg = tl.make_block_ptr(dg, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)\n    b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n    if not IS_RMS_NORM:\n        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t,), (BT,), (0,))\n        b_mean = tl.load(p_mean, boundary_check=(0,))\n    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t,), (BT,), (0,))\n    b_rstd = tl.load(p_rstd, boundary_check=(0,))\n    b_xhat = (b_x - b_mean[:, None]) * b_rstd[:, None] if not IS_RMS_NORM else b_x * b_rstd[:, None]\n    b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)\n    b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat\n    if HAS_BIAS:\n        b_y = b_y + b_b[None, :]\n    if RECOMPUTE_OUTPUT:\n        p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n    b_sigmoid_g = tl.sigmoid(b_g)\n    if ACTIVATION == 'swish':\n        b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n        b_dy = b_dy * b_g * b_sigmoid_g\n    elif ACTIVATION == 'silu':\n        b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n        b_dy = b_dy * b_g * b_sigmoid_g\n    elif ACTIVATION == 'sigmoid':\n        b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)\n        b_dy = b_dy * b_sigmoid_g\n    b_wdy = b_dy\n    if HAS_WEIGHT or HAS_BIAS:\n        m_t = i_t + tl.arange(0, BT) < T\n    if HAS_WEIGHT:\n        b_wdy = b_dy * b_w\n        b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)\n    if HAS_BIAS:\n        b_db += tl.where(m_t[:, None], b_dy, 0.0)\n    if not IS_RMS_NORM:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n        b_c2 = tl.sum(b_wdy, axis=1) / D\n        b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]\n    else:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n        b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]\n    if HAS_DRESIDUAL:\n        p_dres = tl.make_block_ptr(dresidual, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)\n        b_dx += b_dres\n    if STORE_DRESIDUAL:\n        p_dres_in = tl.make_block_ptr(dresidual_in, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))\n        tl.store(p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))\nif HAS_WEIGHT:\n    tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)\nif HAS_BIAS:\n    tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)"
  },
  {
    "name": "layer_norm_gated_bwd_kernel1",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DRESIDUAL': lambda args: args['dresidual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None, 'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'IS_RMS_NORM', 'STORE_DRESIDUAL', 'HAS_DRESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dresidual",
        "annotation": null
      },
      {
        "name": "dresidual_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_bwd_kernel1(",
      "    x,",
      "    g,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dg,",
      "    dw,",
      "    db,",
      "    dresidual,",
      "    dresidual_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    o_d = tl.arange(0, BD)",
      "    mask = o_d < D",
      "    x += i_s * BS * D",
      "    g += i_s * BS * D",
      "    if HAS_DRESIDUAL:",
      "        dresidual += i_s * BS * D",
      "    if STORE_DRESIDUAL:",
      "        dresidual_in += i_s * BS * D",
      "    dy += i_s * BS * D",
      "    dx += i_s * BS * D",
      "    dg += i_s * BS * D",
      "    if RECOMPUTE_OUTPUT:",
      "        y += i_s * BS * D",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=mask).to(tl.float32)",
      "        b_dw = tl.zeros((BD,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=mask, other=0.0).to(tl.float32)",
      "        b_db = tl.zeros((BD,), dtype=tl.float32)",
      "",
      "    for i_t in range(i_s * BS, min(i_s * BS + BS, T)):",
      "",
      "        b_x = tl.load(x + o_d, mask=mask, other=0).to(tl.float32)",
      "        b_g = tl.load(g + o_d, mask=mask, other=0).to(tl.float32)",
      "        b_dy = tl.load(dy + o_d, mask=mask, other=0).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            b_mean = tl.load(mean + i_t)",
      "        b_rstd = tl.load(rstd + i_t)",
      "",
      "        b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "        b_xhat = tl.where(mask, b_xhat, 0.0)",
      "",
      "        b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat",
      "        if HAS_BIAS:",
      "            b_y = b_y + b_b",
      "        if RECOMPUTE_OUTPUT:",
      "            tl.store(y + o_d, b_y, mask=mask)",
      "",
      "        b_sigmoid_g = tl.sigmoid(b_g)",
      "        if ACTIVATION == \"swish\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"silu\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"sigmoid\":",
      "            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)",
      "            b_dy = b_dy * b_sigmoid_g",
      "        b_wdy = b_dy",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += b_dy * b_xhat",
      "        if HAS_BIAS:",
      "            b_db += b_dy",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_c2 = tl.sum(b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1) * b_rstd",
      "        if HAS_DRESIDUAL:",
      "            b_dres = tl.load(dresidual + o_d, mask=mask, other=0).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(dresidual_in + o_d, b_dx, mask=mask)",
      "        tl.store(dx + o_d, b_dx, mask=mask)",
      "        tl.store(dg + o_d, b_dg, mask=mask)",
      "",
      "        x += D",
      "        g += D",
      "        if HAS_DRESIDUAL:",
      "            dresidual += D",
      "        if STORE_DRESIDUAL:",
      "            dresidual_in += D",
      "        if RECOMPUTE_OUTPUT:",
      "            y += D",
      "        dy += D",
      "        dx += D",
      "        dg += D",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, b_dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, b_db, mask=mask)"
    ],
    "file": "codes/356.py",
    "header": "def layer_norm_gated_bwd_kernel1(x, g, w, b, y, dy, dx, dg, dw, db, dresidual, dresidual_in, mean, rstd, T, D: tl.constexpr, BS: tl.constexpr, BD: tl.constexpr, ACTIVATION: tl.constexpr, IS_RMS_NORM: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "i_s = tl.program_id(0)\no_d = tl.arange(0, BD)\nmask = o_d < D\nx += i_s * BS * D\ng += i_s * BS * D\nif HAS_DRESIDUAL:\n    dresidual += i_s * BS * D\nif STORE_DRESIDUAL:\n    dresidual_in += i_s * BS * D\ndy += i_s * BS * D\ndx += i_s * BS * D\ndg += i_s * BS * D\nif RECOMPUTE_OUTPUT:\n    y += i_s * BS * D\nif HAS_WEIGHT:\n    b_w = tl.load(w + o_d, mask=mask).to(tl.float32)\n    b_dw = tl.zeros((BD,), dtype=tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + o_d, mask=mask, other=0.0).to(tl.float32)\n    b_db = tl.zeros((BD,), dtype=tl.float32)\nfor i_t in range(i_s * BS, min(i_s * BS + BS, T)):\n    b_x = tl.load(x + o_d, mask=mask, other=0).to(tl.float32)\n    b_g = tl.load(g + o_d, mask=mask, other=0).to(tl.float32)\n    b_dy = tl.load(dy + o_d, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        b_mean = tl.load(mean + i_t)\n    b_rstd = tl.load(rstd + i_t)\n    b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\n    b_xhat = tl.where(mask, b_xhat, 0.0)\n    b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat\n    if HAS_BIAS:\n        b_y = b_y + b_b\n    if RECOMPUTE_OUTPUT:\n        tl.store(y + o_d, b_y, mask=mask)\n    b_sigmoid_g = tl.sigmoid(b_g)\n    if ACTIVATION == 'swish':\n        b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n        b_dy = b_dy * b_g * b_sigmoid_g\n    elif ACTIVATION == 'silu':\n        b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))\n        b_dy = b_dy * b_g * b_sigmoid_g\n    elif ACTIVATION == 'sigmoid':\n        b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)\n        b_dy = b_dy * b_sigmoid_g\n    b_wdy = b_dy\n    if HAS_WEIGHT:\n        b_wdy = b_dy * b_w\n        b_dw += b_dy * b_xhat\n    if HAS_BIAS:\n        b_db += b_dy\n    if not IS_RMS_NORM:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n        b_c2 = tl.sum(b_wdy, axis=0) / D\n        b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd\n    else:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n        b_dx = (b_wdy - b_xhat * b_c1) * b_rstd\n    if HAS_DRESIDUAL:\n        b_dres = tl.load(dresidual + o_d, mask=mask, other=0).to(tl.float32)\n        b_dx += b_dres\n    if STORE_DRESIDUAL:\n        tl.store(dresidual_in + o_d, b_dx, mask=mask)\n    tl.store(dx + o_d, b_dx, mask=mask)\n    tl.store(dg + o_d, b_dg, mask=mask)\n    x += D\n    g += D\n    if HAS_DRESIDUAL:\n        dresidual += D\n    if STORE_DRESIDUAL:\n        dresidual_in += D\n    if RECOMPUTE_OUTPUT:\n        y += D\n    dy += D\n    dx += D\n    dg += D\nif HAS_WEIGHT:\n    tl.store(dw + i_s * D + o_d, b_dw, mask=mask)\nif HAS_BIAS:\n    tl.store(db + i_s * D + o_d, b_db, mask=mask)"
  },
  {
    "name": "sum_with_snr_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_SUM': block_size_sum}, num_warps=num_warps) for block_size_sum, num_warps in itertools.product([512, 1024], [2, 4, 8, 16])], key=['clean_audio_max_len', 'noisy_audio_max_len'])"
    ],
    "args": [
      {
        "name": "clean_audio",
        "annotation": null
      },
      {
        "name": "clean_audio_real_lens",
        "annotation": null
      },
      {
        "name": "clean_audio_max_len",
        "annotation": null
      },
      {
        "name": "desired_rms",
        "annotation": null
      },
      {
        "name": "noisy_audio_ptr",
        "annotation": null
      },
      {
        "name": "noisy_audio_real_lens",
        "annotation": null
      },
      {
        "name": "noisy_audio_max_len",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_SUM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_RMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def sum_with_snr_kernel(",
      "    clean_audio,",
      "    clean_audio_real_lens,",
      "    clean_audio_max_len,",
      "    desired_rms,",
      "    noisy_audio_ptr,",
      "    noisy_audio_real_lens,",
      "    noisy_audio_max_len,",
      "    output_ptr,",
      "    BLOCK_SIZE_SUM: tl.constexpr,",
      "    BLOCK_SIZE_RMS: tl.constexpr,",
      "):",
      "    batch_idx = tl.program_id(0)",
      "",
      "    clean_audio_real_lens_val = tl.load(clean_audio_real_lens + batch_idx)",
      "    clean_audio_rms = rms_kernel(",
      "        clean_audio,",
      "        clean_audio_real_lens,",
      "        clean_audio_max_len,",
      "        batch_idx,",
      "        BLOCK_SIZE_RMS,",
      "    )",
      "",
      "    noisy_audio_real_lens_val = tl.load(noisy_audio_real_lens + batch_idx)",
      "",
      "    noisy_audio_rms = rms_kernel(",
      "        noisy_audio_ptr,",
      "        noisy_audio_real_lens,",
      "        noisy_audio_max_len,",
      "        batch_idx,",
      "        BLOCK_SIZE_RMS,",
      "    )",
      "",
      "    desired_rms_val = tl.load(desired_rms + batch_idx)",
      "    relative_rms = clean_audio_rms / tl.math.pow(10.0, desired_rms_val / 20.0)",
      "",
      "    for offset in range(0, clean_audio_max_len, BLOCK_SIZE_SUM):",
      "        clean_audio_block_ptr = offset + tl.arange(0, BLOCK_SIZE_SUM)",
      "        clean_audio_mask = clean_audio_block_ptr < clean_audio_real_lens_val",
      "        clean_audio_vals = tl.load(",
      "            clean_audio + batch_idx * clean_audio_max_len + clean_audio_block_ptr,",
      "            mask=clean_audio_mask,",
      "        )",
      "",
      "        offset_over_max = offset % noisy_audio_real_lens_val",
      "",
      "        offset_adjusted = offset_over_max - tl.math.min(",
      "            offset_over_max,",
      "            tl.math.max(",
      "                0, (offset_over_max + BLOCK_SIZE_SUM) - noisy_audio_real_lens_val",
      "            ),",
      "        )",
      "",
      "        noisy_audio_block_ptr = offset_adjusted + tl.arange(0, BLOCK_SIZE_SUM)",
      "",
      "        noisy_audio_val = tl.load(",
      "            noisy_audio_ptr + batch_idx * noisy_audio_max_len + noisy_audio_block_ptr,",
      "            mask=noisy_audio_block_ptr < noisy_audio_real_lens_val,",
      "        )",
      "",
      "        tl.store(",
      "            output_ptr + batch_idx * clean_audio_max_len + clean_audio_block_ptr,",
      "            clean_audio_vals + noisy_audio_val * (relative_rms / noisy_audio_rms),",
      "            mask=clean_audio_mask,",
      "        )"
    ],
    "file": "codes/187.py",
    "header": "def sum_with_snr_kernel(clean_audio, clean_audio_real_lens, clean_audio_max_len, desired_rms, noisy_audio_ptr, noisy_audio_real_lens, noisy_audio_max_len, output_ptr, BLOCK_SIZE_SUM: tl.constexpr, BLOCK_SIZE_RMS: tl.constexpr):",
    "body": "batch_idx = tl.program_id(0)\nclean_audio_real_lens_val = tl.load(clean_audio_real_lens + batch_idx)\nclean_audio_rms = rms_kernel(clean_audio, clean_audio_real_lens, clean_audio_max_len, batch_idx, BLOCK_SIZE_RMS)\nnoisy_audio_real_lens_val = tl.load(noisy_audio_real_lens + batch_idx)\nnoisy_audio_rms = rms_kernel(noisy_audio_ptr, noisy_audio_real_lens, noisy_audio_max_len, batch_idx, BLOCK_SIZE_RMS)\ndesired_rms_val = tl.load(desired_rms + batch_idx)\nrelative_rms = clean_audio_rms / tl.math.pow(10.0, desired_rms_val / 20.0)\nfor offset in range(0, clean_audio_max_len, BLOCK_SIZE_SUM):\n    clean_audio_block_ptr = offset + tl.arange(0, BLOCK_SIZE_SUM)\n    clean_audio_mask = clean_audio_block_ptr < clean_audio_real_lens_val\n    clean_audio_vals = tl.load(clean_audio + batch_idx * clean_audio_max_len + clean_audio_block_ptr, mask=clean_audio_mask)\n    offset_over_max = offset % noisy_audio_real_lens_val\n    offset_adjusted = offset_over_max - tl.math.min(offset_over_max, tl.math.max(0, offset_over_max + BLOCK_SIZE_SUM - noisy_audio_real_lens_val))\n    noisy_audio_block_ptr = offset_adjusted + tl.arange(0, BLOCK_SIZE_SUM)\n    noisy_audio_val = tl.load(noisy_audio_ptr + batch_idx * noisy_audio_max_len + noisy_audio_block_ptr, mask=noisy_audio_block_ptr < noisy_audio_real_lens_val)\n    tl.store(output_ptr + batch_idx * clean_audio_max_len + clean_audio_block_ptr, clean_audio_vals + noisy_audio_val * (relative_rms / noisy_audio_rms), mask=clean_audio_mask)"
  },
  {
    "name": "fused_recurrent_rwkv6_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    u,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_q = q + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_o = o + ((i_k * all + bos) + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_u = u + i_h * K + o_k",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_kv = b_k[:, None] * b_v[None, :]",
      "        b_o = tl.sum((b_h + b_kv * b_u[:, None]) * b_q[:, None], 0)",
      "        b_h = b_h * exp(b_w)[:, None] + b_kv",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        p_q += (-1 if REVERSE else 1) * H * K",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_w += (-1 if REVERSE else 1) * H * K",
      "        p_o += (-1 if REVERSE else 1) * H * V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/418.py",
    "header": "def fused_recurrent_rwkv6_fwd_kernel(q, k, v, w, u, o, h0, ht, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64), tl.program_id(2).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\no_k = i_k * BK + tl.arange(0, BK)\no_v = i_v * BV + tl.arange(0, BV)\np_q = q + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_k = k + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_v = v + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\np_w = w + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_o = o + (i_k * all + bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\np_u = u + i_h * K + o_k\nmask_k = o_k < K\nmask_v = o_v < V\nmask_h = mask_k[:, None] & mask_v[None, :]\nb_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n    b_kv = b_k[:, None] * b_v[None, :]\n    b_o = tl.sum((b_h + b_kv * b_u[:, None]) * b_q[:, None], 0)\n    b_h = b_h * exp(b_w)[:, None] + b_kv\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    p_q += (-1 if REVERSE else 1) * H * K\n    p_k += (-1 if REVERSE else 1) * H * K\n    p_v += (-1 if REVERSE else 1) * H * V\n    p_w += (-1 if REVERSE else 1) * H * K\n    p_o += (-1 if REVERSE else 1) * H * V\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "fused_recurrent_rwkv6_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dq1",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_bwd_kernel_dq(",
      "    k,",
      "    v,",
      "    w,",
      "    u,",
      "    do,",
      "    dq,",
      "    dq1,",
      "    h0,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_do = do + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_dq = (",
      "        dq + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    )",
      "    p_dq1 = (",
      "        dq1 + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    )",
      "    p_u = u + i_h * K + o_k",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_kv = b_k[:, None] * b_v[None, :]",
      "",
      "        b_hq = b_h * b_do[None, :]",
      "        b_dq = tl.sum(b_hq + b_kv * b_u[:, None] * b_do[None, :], 1) * scale",
      "        b_dq1 = tl.sum(b_hq, 1)",
      "        b_h = b_h * exp(b_w)[:, None]",
      "        b_h += b_kv",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dq1, b_dq1.to(p_dq1.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_w += (-1 if REVERSE else 1) * H * K",
      "        p_do += (-1 if REVERSE else 1) * H * V",
      "        p_dq += (-1 if REVERSE else 1) * H * K",
      "        p_dq1 += (-1 if REVERSE else 1) * H * K"
    ],
    "file": "codes/418.py",
    "header": "def fused_recurrent_rwkv6_bwd_kernel_dq(k, v, w, u, do, dq, dq1, h0, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64), tl.program_id(2).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\no_k = i_k * BK + tl.arange(0, BK)\no_v = i_v * BV + tl.arange(0, BV)\np_k = k + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_v = v + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\np_w = w + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_do = do + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\np_dq = dq + (i_v * all + bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_dq1 = dq1 + (i_v * all + bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_u = u + i_h * K + o_k\nmask_k = o_k < K\nmask_v = o_v < V\nmask_h = mask_k[:, None] & mask_v[None, :]\nb_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    b_kv = b_k[:, None] * b_v[None, :]\n    b_hq = b_h * b_do[None, :]\n    b_dq = tl.sum(b_hq + b_kv * b_u[:, None] * b_do[None, :], 1) * scale\n    b_dq1 = tl.sum(b_hq, 1)\n    b_h = b_h * exp(b_w)[:, None]\n    b_h += b_kv\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)\n    tl.store(p_dq1, b_dq1.to(p_dq1.dtype.element_ty), mask=mask_k)\n    p_k += (-1 if REVERSE else 1) * H * K\n    p_v += (-1 if REVERSE else 1) * H * V\n    p_w += (-1 if REVERSE else 1) * H * K\n    p_do += (-1 if REVERSE else 1) * H * V\n    p_dq += (-1 if REVERSE else 1) * H * K\n    p_dq1 += (-1 if REVERSE else 1) * H * K"
  },
  {
    "name": "fused_recurrent_rwkv6_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dk1",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    u,",
      "    do,",
      "    dk,",
      "    dk1,",
      "    dv,",
      "    dh0,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_q = q + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if not REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_w = w + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_do = do + (bos + ((T - 1) if not REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_dk = (",
      "        dk",
      "        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + o_k",
      "    )",
      "    p_dk1 = (",
      "        dk1",
      "        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + o_k",
      "    )",
      "    p_dv = (",
      "        dv",
      "        + ((i_k * all + bos) + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + o_v",
      "    )",
      "    p_u = u + i_h * K + o_k",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for _ in range(T - 1, -1, -1):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_dkv = b_q[:, None] * b_do[None, :]",
      "        b_dk = tl.sum(b_dh * b_v[None, :], 1)",
      "        tl.store(p_dk1, b_dk.to(p_dk1.dtype.element_ty), mask=mask_k)",
      "        b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], 1)",
      "        b_dv = tl.sum((b_dh + (b_dkv * b_u[:, None])) * b_k[:, None], 0)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)",
      "        b_dh *= exp(b_w)[:, None]",
      "        b_dh += b_dkv",
      "",
      "        p_q += (-1 if not REVERSE else 1) * H * K",
      "        p_k += (-1 if not REVERSE else 1) * H * K",
      "        p_v += (-1 if not REVERSE else 1) * H * V",
      "        p_w += (-1 if not REVERSE else 1) * H * K",
      "        p_do += (-1 if not REVERSE else 1) * H * V",
      "        p_dk += (-1 if not REVERSE else 1) * H * K",
      "        p_dk1 += (-1 if not REVERSE else 1) * H * K",
      "        p_dv += (-1 if not REVERSE else 1) * H * V",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = dh0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/418.py",
    "header": "def fused_recurrent_rwkv6_bwd_kernel_dkv(q, k, v, w, u, do, dk, dk1, dv, dh0, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_k, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64), tl.program_id(2).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\no_k = i_k * BK + tl.arange(0, BK)\no_v = i_v * BV + tl.arange(0, BV)\np_q = q + (bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + o_k\np_k = k + (bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + o_k\np_v = v + (bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + o_v\np_w = w + (bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + o_k\np_do = do + (bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + o_v\np_dk = dk + (i_v * all + bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + o_k\np_dk1 = dk1 + (i_v * all + bos + (T - 1 if not REVERSE else 0)) * H * K + i_h * K + o_k\np_dv = dv + (i_k * all + bos + (T - 1 if not REVERSE else 0)) * H * V + i_h * V + o_v\np_u = u + i_h * K + o_k\nmask_k = o_k < K\nmask_v = o_v < V\nmask_h = mask_k[:, None] & mask_v[None, :]\nb_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nfor _ in range(T - 1, -1, -1):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n    b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)\n    b_dkv = b_q[:, None] * b_do[None, :]\n    b_dk = tl.sum(b_dh * b_v[None, :], 1)\n    tl.store(p_dk1, b_dk.to(p_dk1.dtype.element_ty), mask=mask_k)\n    b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], 1)\n    b_dv = tl.sum((b_dh + b_dkv * b_u[:, None]) * b_k[:, None], 0)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)\n    b_dh *= exp(b_w)[:, None]\n    b_dh += b_dkv\n    p_q += (-1 if not REVERSE else 1) * H * K\n    p_k += (-1 if not REVERSE else 1) * H * K\n    p_v += (-1 if not REVERSE else 1) * H * V\n    p_w += (-1 if not REVERSE else 1) * H * K\n    p_do += (-1 if not REVERSE else 1) * H * V\n    p_dk += (-1 if not REVERSE else 1) * H * K\n    p_dk1 += (-1 if not REVERSE else 1) * H * K\n    p_dv += (-1 if not REVERSE else 1) * H * V\nif USE_INITIAL_STATE:\n    p_dh0 = dh0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "fused_recurrent_rwkv6_bwd_kernel_dw",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT, 'BK': BK}, num_warps=num_warps) for BT in [16, 32, 64] for BK in [32, 64] for num_warps in [1, 2, 4, 8]], key=['K'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_bwd_kernel_dw(",
      "    q,",
      "    k,",
      "    dq,",
      "    dk,",
      "    dw,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    T = eos - bos",
      "    NT = tl.cdiv(T, BT)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_i = (",
      "        tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "        if not REVERSE",
      "        else tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "    )",
      "",
      "    b_z = tl.zeros([BK], dtype=tl.float32)",
      "",
      "    i_t = 0 if not REVERSE else NT - 1",
      "    for _ in range(NT):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + 1, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T - 1, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + 1, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T - 1, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(tl.float32)",
      "        b_k = tl.load(p_k, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dw = (b_q * b_dq * scale) - b_k * b_dk",
      "        b_c = b_z[None, :] + tl.dot(m_i, b_dw, allow_tf32=False)",
      "        tl.store(p_dw, b_c.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_t >= 0:",
      "            b_z += tl.sum(b_dw, 0)",
      "",
      "        i_t += 1 if not REVERSE else -1"
    ],
    "file": "codes/418.py",
    "header": "def fused_recurrent_rwkv6_bwd_kernel_dw(q, k, dq, dk, dw, cu_seqlens, scale, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, REVERSE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\nT = eos - bos\nNT = tl.cdiv(T, BT)\no_i = tl.arange(0, BT)\nm_i = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0) if not REVERSE else tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\nb_z = tl.zeros([BK], dtype=tl.float32)\ni_t = 0 if not REVERSE else NT - 1\nfor _ in range(NT):\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + 1, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T - 1, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dq = tl.make_block_ptr(dq + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + 1, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T - 1, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dw = tl.make_block_ptr(dw + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1)).to(tl.float32)\n    b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(tl.float32)\n    b_k = tl.load(p_k, boundary_check=(0, 1)).to(tl.float32)\n    b_dk = tl.load(p_dk, boundary_check=(0, 1)).to(tl.float32)\n    b_dw = b_q * b_dq * scale - b_k * b_dk\n    b_c = b_z[None, :] + tl.dot(m_i, b_dw, allow_tf32=False)\n    tl.store(p_dw, b_c.to(p_dw.dtype.element_ty), boundary_check=(0, 1))\n    if i_t >= 0:\n        b_z += tl.sum(b_dw, 0)\n    i_t += 1 if not REVERSE else -1"
  },
  {
    "name": "kernel_gemm_rs_producer_fuse_scatter",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "use_cuda_graph": true
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 16, 'GROUP_SIZE_M': 1, 'M_PER_COPY_CHUNK': 128, 'waves_per_eu': 2}, num_warps=4, num_stages=2), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 1, 'M_PER_COPY_CHUNK': 128, 'waves_per_eu': 2}, num_warps=8, num_stages=2)], key=['M', 'N', 'K'], use_cuda_graph=True)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "scatter_bufs_ptr",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M_PER_COPY_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_gemm_rs_producer_fuse_scatter(",
      "    a_ptr,",
      "    b_ptr,",
      "    scatter_bufs_ptr,",
      "    rank,",
      "    num_ranks,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    M_PER_COPY_CHUNK: tl.constexpr,",
      "):",
      "",
      "    tl.assume(stride_am > 0)",
      "    tl.assume(stride_ak > 0)",
      "    tl.assume(stride_bk > 0)",
      "    tl.assume(stride_bn > 0)",
      "    tl.assume(stride_cm > 0)",
      "    tl.assume(stride_cn > 0)",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    M_per_rank = M // num_ranks",
      "    num_pid_m_per_copy_chunk = M_PER_COPY_CHUNK // BLOCK_SIZE_M",
      "    chunk_offset = pid_m // (num_pid_m_per_copy_chunk * num_ranks)",
      "    rank_offset = (",
      "        pid_m % (num_pid_m_per_copy_chunk * num_ranks) // num_pid_m_per_copy_chunk",
      "    )",
      "    block_offset = pid_m % num_pid_m_per_copy_chunk",
      "",
      "    rank_offset = (rank_offset + rank + 1) % num_ranks",
      "    pid_m = (",
      "        rank_offset * M_per_rank",
      "        + chunk_offset * M_PER_COPY_CHUNK",
      "        + block_offset * BLOCK_SIZE_M",
      "    ) // BLOCK_SIZE_M",
      "",
      "    tl.assume(pid_m >= 0)",
      "    tl.assume(pid_n >= 0)",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    dtype = a_ptr.dtype.element_ty",
      "    c = accumulator.to(dtype)",
      "",
      "    target_m = (pid_m * BLOCK_SIZE_M % M_per_rank) + M_per_rank * rank",
      "    offs_cm = target_m + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptr = tl.load(scatter_bufs_ptr + rank_offset).to(tl.pointer_type(dtype))",
      "    c_ptr = tl.multiple_of(c_ptr, 16)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/78.py",
    "header": "def kernel_gemm_rs_producer_fuse_scatter(a_ptr, b_ptr, scatter_bufs_ptr, rank, num_ranks, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, M_PER_COPY_CHUNK: tl.constexpr):",
    "body": "tl.assume(stride_am > 0)\ntl.assume(stride_ak > 0)\ntl.assume(stride_bk > 0)\ntl.assume(stride_bn > 0)\ntl.assume(stride_cm > 0)\ntl.assume(stride_cn > 0)\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\nM_per_rank = M // num_ranks\nnum_pid_m_per_copy_chunk = M_PER_COPY_CHUNK // BLOCK_SIZE_M\nchunk_offset = pid_m // (num_pid_m_per_copy_chunk * num_ranks)\nrank_offset = pid_m % (num_pid_m_per_copy_chunk * num_ranks) // num_pid_m_per_copy_chunk\nblock_offset = pid_m % num_pid_m_per_copy_chunk\nrank_offset = (rank_offset + rank + 1) % num_ranks\npid_m = (rank_offset * M_per_rank + chunk_offset * M_PER_COPY_CHUNK + block_offset * BLOCK_SIZE_M) // BLOCK_SIZE_M\ntl.assume(pid_m >= 0)\ntl.assume(pid_n >= 0)\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs)\n    b = tl.load(b_ptrs)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\ndtype = a_ptr.dtype.element_ty\nc = accumulator.to(dtype)\ntarget_m = pid_m * BLOCK_SIZE_M % M_per_rank + M_per_rank * rank\noffs_cm = target_m + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptr = tl.load(scatter_bufs_ptr + rank_offset).to(tl.pointer_type(dtype))\nc_ptr = tl.multiple_of(c_ptr, 16)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_group_boundary_check': lambda args: args['group_size'] % args['group_block_size'], 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "group_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "num_groups",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "group_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_group_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    group_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    num_groups: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    eps: tl.float32,",
      "    dtype: tl.constexpr,",
      "    group_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_group_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    batch = pid // num_groups",
      "    group = pid % num_groups",
      "    num_elements = group_size * x_size",
      "    batch_offset = batch * num_groups * num_elements",
      "    group_offset = batch_offset + group * num_elements",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        input = tl.load(",
      "            input_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"",
      "        )",
      "        mean = tl.sum(",
      "            tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1",
      "        )",
      "        group_condition = tl.arange(0, group_block_size) < group_size",
      "        x_condition = tl.arange(0, x_block_size) < x_size",
      "        condition = group_condition[:, None] & x_condition[None, :]",
      "        centered_mean = tl.where(condition, input - mean, 0)",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        mean = tl.sum(",
      "            tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1",
      "        )",
      "        centered_mean = input - mean",
      "",
      "    var = tl.sum(",
      "        tl.view(",
      "            centered_mean * centered_mean / num_elements,",
      "            (1, group_block_size * x_block_size),",
      "        ),",
      "        1,",
      "    )",
      "    rstd = tl.math.rsqrt(var + eps)",
      "    output = centered_mean * rstd",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(y_size, 1),",
      "            strides=(1, y_size),",
      "            offsets=(group * group_size, 0),",
      "            block_shape=(group_block_size, 1),",
      "            order=(0, 1),",
      "        )",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "        output *= weight",
      "",
      "    if bias_ptr is not None:",
      "        bias_block_ptr = tl.make_block_ptr(",
      "            bias_ptr,",
      "            shape=(y_size, 1),",
      "            strides=(1, y_size),",
      "            offsets=(group * group_size, 0),",
      "            block_shape=(group_block_size, 1),",
      "            order=(0, 1),",
      "        )",
      "        bias = tl.load(bias_block_ptr, boundary_check=(0,))",
      "        output += bias",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))",
      "",
      "    tl.store(rstd_block_ptr, rstd.to(dtype))",
      "    tl.store(mean_block_ptr, mean.to(dtype))"
    ],
    "file": "codes/518.py",
    "header": "def forward(output_ptr: tl.tensor, input_ptr: tl.tensor, rstd_ptr: tl.tensor, mean_ptr: tl.tensor, group_size: tl.int32, y_size: tl.int32, x_size: tl.int32, num_groups: tl.int32, weight_ptr: tl.tensor, bias_ptr: tl.tensor, eps: tl.float32, dtype: tl.constexpr, group_block_size: tl.constexpr, x_block_size: tl.constexpr, require_group_boundary_check: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nbatch = pid // num_groups\ngroup = pid % num_groups\nnum_elements = group_size * x_size\nbatch_offset = batch * num_groups * num_elements\ngroup_offset = batch_offset + group * num_elements\noutput_block_ptr = tl.make_block_ptr(output_ptr + group_offset, shape=(group_size, x_size), strides=(x_size, 1), offsets=(0, 0), block_shape=(group_block_size, x_block_size), order=(1, 0))\ninput_block_ptr = tl.make_block_ptr(input_ptr + group_offset, shape=(group_size, x_size), strides=(x_size, 1), offsets=(0, 0), block_shape=(group_block_size, x_block_size), order=(1, 0))\nrstd_block_ptr = tl.make_block_ptr(rstd_ptr + batch * num_groups, shape=(group_size,), strides=(1,), offsets=(group,), block_shape=(1,), order=(0,))\nmean_block_ptr = tl.make_block_ptr(mean_ptr + batch * num_groups, shape=(group_size,), strides=(1,), offsets=(group,), block_shape=(1,), order=(0,))\nif require_group_boundary_check | require_x_boundary_check:\n    input = tl.load(input_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    mean = tl.sum(tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1)\n    group_condition = tl.arange(0, group_block_size) < group_size\n    x_condition = tl.arange(0, x_block_size) < x_size\n    condition = group_condition[:, None] & x_condition[None, :]\n    centered_mean = tl.where(condition, input - mean, 0)\nelse:\n    input = tl.load(input_block_ptr)\n    mean = tl.sum(tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1)\n    centered_mean = input - mean\nvar = tl.sum(tl.view(centered_mean * centered_mean / num_elements, (1, group_block_size * x_block_size)), 1)\nrstd = tl.math.rsqrt(var + eps)\noutput = centered_mean * rstd\nif weight_ptr is not None:\n    weight_block_ptr = tl.make_block_ptr(weight_ptr, shape=(y_size, 1), strides=(1, y_size), offsets=(group * group_size, 0), block_shape=(group_block_size, 1), order=(0, 1))\n    weight = tl.load(weight_block_ptr, boundary_check=(0,))\n    output *= weight\nif bias_ptr is not None:\n    bias_block_ptr = tl.make_block_ptr(bias_ptr, shape=(y_size, 1), strides=(1, y_size), offsets=(group * group_size, 0), block_shape=(group_block_size, 1), order=(0, 1))\n    bias = tl.load(bias_block_ptr, boundary_check=(0,))\n    output += bias\nif require_group_boundary_check | require_x_boundary_check:\n    tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))\nelse:\n    tl.store(output_block_ptr, output.to(dtype))\ntl.store(rstd_block_ptr, rstd.to(dtype))\ntl.store(mean_block_ptr, mean.to(dtype))"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_group_boundary_check': lambda args: args['group_size'] % args['group_block_size'], 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_bias_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "group_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "num_groups",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "group_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_group_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_bias_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    group_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    num_groups: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    dtype: tl.constexpr,",
      "    group_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_group_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    batch = pid // num_groups",
      "    group = pid % num_groups",
      "    num_elements = group_size * x_size",
      "    batch_offset = batch * num_groups * num_elements",
      "    group_offset = batch_offset + group * num_elements",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    rstd = tl.load(rstd_block_ptr)",
      "    mean = tl.load(mean_block_ptr)",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0, 1))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(y_size, 1),",
      "            strides=(1, y_size),",
      "            offsets=(group * group_size, 0),",
      "            block_shape=(group_block_size, 1),",
      "            order=(0, 1),",
      "        )",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "        grad_norm = weight * grad_output",
      "    else:",
      "        grad_norm = grad_output",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(0, 1))",
      "        group_condition = tl.arange(0, group_block_size) < group_size",
      "        x_condition = tl.arange(0, x_block_size) < x_size",
      "        condition = group_condition[:, None] & x_condition[None, :]",
      "        centered_mean = tl.where(condition, input - mean, 0)",
      "        grad_std = tl.sum(",
      "            tl.view(",
      "                grad_norm * centered_mean, (1, group_block_size * x_block_size)",
      "            ),",
      "            1,",
      "        )",
      "        grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)",
      "        grad_distance = 2 * centered_mean * grad_var",
      "        grad_centered_mean = tl.where(",
      "            condition, grad_norm * rstd + grad_distance, 0",
      "        )",
      "        grad_mean = (",
      "            -tl.sum(",
      "                tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1",
      "            )",
      "            / num_elements",
      "        )",
      "        grad_input = grad_centered_mean + grad_mean",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0, 1))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        centered_mean = input - mean",
      "        grad_std = tl.sum(",
      "            tl.view(",
      "                grad_norm * centered_mean, (1, group_block_size * x_block_size)",
      "            ),",
      "            1,",
      "        )",
      "        grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)",
      "        grad_distance = 2 * centered_mean * grad_var",
      "        grad_centered_mean = grad_norm * rstd + grad_distance",
      "        grad_mean = (",
      "            -tl.sum(",
      "                tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1",
      "            )",
      "            / num_elements",
      "        )",
      "        grad_input = grad_centered_mean + grad_mean",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))",
      "",
      "    if grad_weight_staging_ptr is not None:",
      "        norm = centered_mean * rstd",
      "        grad_weight = tl.sum(norm * grad_output, 1)",
      "        offset = batch * y_size + group * group_size",
      "        grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "            grad_weight_staging_ptr + offset,",
      "            shape=(group_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(group_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_group_boundary_check:",
      "            tl.store(",
      "                grad_weight_staging_block_ptr,",
      "                grad_weight.to(dtype),",
      "                boundary_check=(0,),",
      "            )",
      "        else:",
      "            tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))",
      "",
      "    if grad_bias_staging_ptr is not None:",
      "        grad_bias = tl.sum(grad_output, 1)",
      "        offset = batch * y_size + group * group_size",
      "        grad_bias_staging_block_ptr = tl.make_block_ptr(",
      "            grad_bias_staging_ptr + offset,",
      "            shape=(group_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(group_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_group_boundary_check:",
      "            tl.store(",
      "                grad_bias_staging_block_ptr,",
      "                grad_bias.to(dtype),",
      "                boundary_check=(0,),",
      "            )",
      "        else:",
      "            tl.store(grad_bias_staging_block_ptr, grad_bias.to(dtype))"
    ],
    "file": "codes/518.py",
    "header": "def backward(grad_input_ptr: tl.tensor, grad_weight_staging_ptr: tl.tensor, grad_bias_staging_ptr: tl.tensor, grad_output_ptr: tl.tensor, input_ptr: tl.tensor, group_size: tl.int32, y_size: tl.int32, x_size: tl.int32, num_groups: tl.int32, weight_ptr: tl.tensor, rstd_ptr: tl.tensor, mean_ptr: tl.tensor, dtype: tl.constexpr, group_block_size: tl.constexpr, x_block_size: tl.constexpr, require_group_boundary_check: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nbatch = pid // num_groups\ngroup = pid % num_groups\nnum_elements = group_size * x_size\nbatch_offset = batch * num_groups * num_elements\ngroup_offset = batch_offset + group * num_elements\ngrad_input_block_ptr = tl.make_block_ptr(grad_input_ptr + group_offset, shape=(group_size, x_size), strides=(x_size, 1), offsets=(0, 0), block_shape=(group_block_size, x_block_size), order=(1, 0))\ngrad_output_block_ptr = tl.make_block_ptr(grad_output_ptr + group_offset, shape=(group_size, x_size), strides=(x_size, 1), offsets=(0, 0), block_shape=(group_block_size, x_block_size), order=(1, 0))\ninput_block_ptr = tl.make_block_ptr(input_ptr + group_offset, shape=(group_size, x_size), strides=(x_size, 1), offsets=(0, 0), block_shape=(group_block_size, x_block_size), order=(1, 0))\nrstd_block_ptr = tl.make_block_ptr(rstd_ptr + batch * num_groups, shape=(group_size,), strides=(1,), offsets=(group,), block_shape=(1,), order=(0,))\nmean_block_ptr = tl.make_block_ptr(mean_ptr + batch * num_groups, shape=(group_size,), strides=(1,), offsets=(group,), block_shape=(1,), order=(0,))\nrstd = tl.load(rstd_block_ptr)\nmean = tl.load(mean_block_ptr)\nif require_group_boundary_check | require_x_boundary_check:\n    grad_output = tl.load(grad_output_block_ptr, boundary_check=(0, 1))\nelse:\n    grad_output = tl.load(grad_output_block_ptr)\nif weight_ptr is not None:\n    weight_block_ptr = tl.make_block_ptr(weight_ptr, shape=(y_size, 1), strides=(1, y_size), offsets=(group * group_size, 0), block_shape=(group_block_size, 1), order=(0, 1))\n    weight = tl.load(weight_block_ptr, boundary_check=(0,))\n    grad_norm = weight * grad_output\nelse:\n    grad_norm = grad_output\nif require_group_boundary_check | require_x_boundary_check:\n    input = tl.load(input_block_ptr, boundary_check=(0, 1))\n    group_condition = tl.arange(0, group_block_size) < group_size\n    x_condition = tl.arange(0, x_block_size) < x_size\n    condition = group_condition[:, None] & x_condition[None, :]\n    centered_mean = tl.where(condition, input - mean, 0)\n    grad_std = tl.sum(tl.view(grad_norm * centered_mean, (1, group_block_size * x_block_size)), 1)\n    grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)\n    grad_distance = 2 * centered_mean * grad_var\n    grad_centered_mean = tl.where(condition, grad_norm * rstd + grad_distance, 0)\n    grad_mean = -tl.sum(tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1) / num_elements\n    grad_input = grad_centered_mean + grad_mean\n    tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0, 1))\nelse:\n    input = tl.load(input_block_ptr)\n    centered_mean = input - mean\n    grad_std = tl.sum(tl.view(grad_norm * centered_mean, (1, group_block_size * x_block_size)), 1)\n    grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)\n    grad_distance = 2 * centered_mean * grad_var\n    grad_centered_mean = grad_norm * rstd + grad_distance\n    grad_mean = -tl.sum(tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1) / num_elements\n    grad_input = grad_centered_mean + grad_mean\n    tl.store(grad_input_block_ptr, grad_input.to(dtype))\nif grad_weight_staging_ptr is not None:\n    norm = centered_mean * rstd\n    grad_weight = tl.sum(norm * grad_output, 1)\n    offset = batch * y_size + group * group_size\n    grad_weight_staging_block_ptr = tl.make_block_ptr(grad_weight_staging_ptr + offset, shape=(group_size,), strides=(1,), offsets=(0,), block_shape=(group_block_size,), order=(0,))\n    if require_group_boundary_check:\n        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))\nif grad_bias_staging_ptr is not None:\n    grad_bias = tl.sum(grad_output, 1)\n    offset = batch * y_size + group * group_size\n    grad_bias_staging_block_ptr = tl.make_block_ptr(grad_bias_staging_ptr + offset, shape=(group_size,), strides=(1,), offsets=(0,), block_shape=(group_block_size,), order=(0,))\n    if require_group_boundary_check:\n        tl.store(grad_bias_staging_block_ptr, grad_bias.to(dtype), boundary_check=(0,))\n    else:\n        tl.store(grad_bias_staging_block_ptr, grad_bias.to(dtype))"
  },
  {
    "name": "kernel_consumer_m_parallel_scatter_group_gemm",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank'], repr=_kernel_consumer_gemm_non_persistent_repr)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "block_barrier_ptr",
        "annotation": null
      },
      {
        "name": "sorted_token_ids_ptr",
        "annotation": null
      },
      {
        "name": "token_expert_ids_ptr",
        "annotation": null
      },
      {
        "name": "num_tokens_post_padded",
        "annotation": null
      },
      {
        "name": "block_barrier_id_ptr",
        "annotation": null
      },
      {
        "name": "num_valid_tokens",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_be",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TOP_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SWIZZLE_OFFSET",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_consumer_m_parallel_scatter_group_gemm(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    block_barrier_ptr,",
      "    sorted_token_ids_ptr,",
      "    token_expert_ids_ptr,",
      "    num_tokens_post_padded,",
      "    block_barrier_id_ptr,",
      "    num_valid_tokens,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_be,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    TOP_K: tl.constexpr,",
      "    rank,",
      "    WORLD_SIZE: tl.constexpr,",
      "    SWIZZLE_OFFSET: tl.constexpr = 3,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_block_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_block_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    num_blocks_per_group = GROUP_SIZE_M * num_block_n",
      "    group_id = pid // num_blocks_per_group",
      "    group_size = min(num_block_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M)",
      "    pid_m = group_id * GROUP_SIZE_M + pid % group_size",
      "    pid_n = pid % num_blocks_per_group // group_size",
      "",
      "    m_per_rank = num_block_m // WORLD_SIZE",
      "    m_offset = m_per_rank * ((rank + SWIZZLE_OFFSET) % WORLD_SIZE)",
      "    pid_m = (pid_m + m_offset) % num_block_m",
      "",
      "    num_tokens_post_padded_value = tl.load(num_tokens_post_padded)",
      "",
      "    if pid_m * BLOCK_SIZE_M >= num_tokens_post_padded_value:",
      "        return",
      "",
      "    offs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_token = tl.load(sorted_token_ids_ptr + offs_token_id)",
      "    token_mask = offs_token < num_valid_tokens",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = (",
      "        a_ptr + offs_token[:, None] // TOP_K * stride_am + offs_k[None, :] * stride_ak",
      "    )",
      "",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_be = tl.load(token_expert_ids_ptr + pid_m)",
      "",
      "    b_ptrs = (",
      "        b_ptr",
      "        + offs_be * stride_be",
      "        + offs_k[:, None] * stride_bk",
      "        + offs_bn[None, :] * stride_bn",
      "    )",
      "",
      "    offs_barrier = tl.load(block_barrier_id_ptr + pid_m)",
      "    token = dl.wait(block_barrier_ptr + offs_barrier, 1, \"gpu\", \"acquire\")",
      "    a_ptrs = dl.consume_token(a_ptrs, token)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(",
      "            a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K - k * BLOCK_SIZE_K)",
      "        )",
      "        b = tl.load(b_ptrs, mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K))",
      "",
      "        accumulator += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    accumulator = accumulator.to(c_ptr.dtype.element_ty)",
      "",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + offs_token[:, None] * stride_cm + offs_cn[None, :] * stride_cn",
      "    c_mask = token_mask[:, None] & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)"
    ],
    "file": "codes/38.py",
    "header": "def kernel_consumer_m_parallel_scatter_group_gemm(a_ptr, b_ptr, c_ptr, block_barrier_ptr, sorted_token_ids_ptr, token_expert_ids_ptr, num_tokens_post_padded, block_barrier_id_ptr, num_valid_tokens, M, N, K, stride_am, stride_ak, stride_be, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, TOP_K: tl.constexpr, rank, WORLD_SIZE: tl.constexpr, SWIZZLE_OFFSET: tl.constexpr = 3):",
    "body": "pid = tl.program_id(axis=0)\nnum_block_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_block_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_blocks_per_group = GROUP_SIZE_M * num_block_n\ngroup_id = pid // num_blocks_per_group\ngroup_size = min(num_block_m - group_id * GROUP_SIZE_M, GROUP_SIZE_M)\npid_m = group_id * GROUP_SIZE_M + pid % group_size\npid_n = pid % num_blocks_per_group // group_size\nm_per_rank = num_block_m // WORLD_SIZE\nm_offset = m_per_rank * ((rank + SWIZZLE_OFFSET) % WORLD_SIZE)\npid_m = (pid_m + m_offset) % num_block_m\nnum_tokens_post_padded_value = tl.load(num_tokens_post_padded)\nif pid_m * BLOCK_SIZE_M >= num_tokens_post_padded_value:\n    return\noffs_token_id = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_token = tl.load(sorted_token_ids_ptr + offs_token_id)\ntoken_mask = offs_token < num_valid_tokens\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + offs_token[:, None] // TOP_K * stride_am + offs_k[None, :] * stride_ak\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_be = tl.load(token_expert_ids_ptr + pid_m)\nb_ptrs = b_ptr + offs_be * stride_be + offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn\noffs_barrier = tl.load(block_barrier_id_ptr + pid_m)\ntoken = dl.wait(block_barrier_ptr + offs_barrier, 1, 'gpu', 'acquire')\na_ptrs = dl.consume_token(a_ptrs, token)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=token_mask[:, None] & (offs_k[None, :] < K - k * BLOCK_SIZE_K))\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K)\n    accumulator += tl.dot(a, b)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\naccumulator = accumulator.to(c_ptr.dtype.element_ty)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + offs_token[:, None] * stride_cm + offs_cn[None, :] * stride_cn\nc_mask = token_mask[:, None] & (offs_cn[None, :] < N)\ntl.store(c_ptrs, accumulator, mask=c_mask)"
  },
  {
    "name": "parallel_path_bwd_dkv_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "hc_whole",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_bwd_dkv_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    hc_whole,",
      "    scale,",
      "    L,",
      "    D,",
      "    dk,",
      "    dv,",
      "    do,",
      "    dg_cumsum,",
      "    cu_seqlens,",
      "    indices,",
      "    split_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    do += (bos * HQ + i_hq) * V",
      "    dk += (bos * HQ + i_hq) * K",
      "    dv += (bos * HQ + i_hq) * K",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    hc_whole += (boh_large * H + i_h) * K * K",
      "",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "",
      "    stride_h = H * K * K",
      "    sm_scale = scale * 1.44269504",
      "",
      "    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_k_origin = tl.load(p_k, boundary_check=(0, 1))",
      "    p_v = tl.make_block_ptr(v, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    if USE_GATE:",
      "        b_g_cumsum_k = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        p_g_cumsum_k = tl.make_block_ptr(",
      "            g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g_cumsum_k += tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "        b_dg_cumsum_k = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_g_cumsum_k = None",
      "        b_dg_cumsum_k = None",
      "",
      "    b_dk = tl.zeros([BT, K], dtype=tl.float32)",
      "    b_dv = tl.zeros([BT, K], dtype=tl.float32)",
      "    idx_i = (i_t * BT // S).to(tl.int32)",
      "",
      "    last_chunk_start = tl.floor(T / S).to(tl.int32) * S",
      "",
      "    if i_t * BT < last_chunk_start:",
      "",
      "        if T % S != 0:",
      "            idx_j = last_chunk_start // S",
      "            b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)",
      "            b_k_accum += b_k_origin",
      "            for i in range(idx_i + 1, idx_j):",
      "                p_h = tl.make_block_ptr(",
      "                    hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "                )",
      "                b_h = tl.load(p_h, boundary_check=(0, 1))",
      "                b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))",
      "            b_k = b_k_accum.to(b_k_origin.dtype)",
      "",
      "            for offset in range(",
      "                tl.ceil(T / BS).to(tl.int32) * BS - BS, last_chunk_start - BS, -BS",
      "            ):",
      "                p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                b_delta = tl.load(p_delta, boundary_check=(0,))",
      "                b_l = tl.load(p_l, boundary_check=(0,))",
      "                p_q = tl.make_block_ptr(",
      "                    q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0)",
      "                )",
      "                b_q = tl.load(p_q, boundary_check=(0, 1))",
      "                b_A = tl.dot(b_q, tl.trans(b_k))",
      "                if USE_GATE:",
      "                    p_g_cumsum_q = tl.make_block_ptr(",
      "                        g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)",
      "                    )",
      "                    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "                    b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "                    b_A = tl.where(",
      "                        (offset + tl.arange(0, BS) < T)[:, None], b_A, float(\"-inf\")",
      "                    )",
      "                b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "                p_do = tl.make_block_ptr(",
      "                    do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0)",
      "                )",
      "                b_do = tl.load(p_do, boundary_check=(0, 1))",
      "                b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "                b_dp = tl.dot(b_do, tl.trans(b_v))",
      "                b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "                if USE_GATE:",
      "                    b_dg_cumsum_k -= tl.sum(b_dA, axis=0)",
      "                b_dA = b_dA.to(b_v.dtype)",
      "                b_dk += tl.dot(tl.trans(b_dA), b_q)",
      "",
      "        for offset_outer in range(last_chunk_start, i_t * BT + S, -S):",
      "            idx_j = (offset_outer // S) - 1",
      "            b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)",
      "            b_k_accum += b_k_origin",
      "            for i in range(idx_i + 1, idx_j):",
      "                p_h = tl.make_block_ptr(",
      "                    hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "                )",
      "                b_h = tl.load(p_h, boundary_check=(0, 1))",
      "                b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))",
      "            b_k = b_k_accum.to(b_k_origin.dtype)",
      "",
      "            p_h = tl.make_block_ptr(",
      "                hc_whole + (idx_j) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "            )",
      "            b_h = tl.load(p_h, boundary_check=(0, 1))",
      "            b_dk = b_dk - tl.dot(b_dk.to(b_h.dtype), b_h)",
      "",
      "            for offset in range(offset_outer - BS, offset_outer - S - BS, -BS):",
      "                p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                b_delta = tl.load(p_delta, boundary_check=(0,))",
      "                b_l = tl.load(p_l, boundary_check=(0,))",
      "                p_q = tl.make_block_ptr(",
      "                    q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0)",
      "                )",
      "                b_q = tl.load(p_q, boundary_check=(0, 1))",
      "                b_A = tl.dot(b_q, tl.trans(b_k))",
      "                if USE_GATE:",
      "                    p_g_cumsum_q = tl.make_block_ptr(",
      "                        g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)",
      "                    )",
      "                    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "                    b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "                    b_A = tl.where(",
      "                        (offset + tl.arange(0, BS) < T)[:, None], b_A, float(\"-inf\")",
      "                    )",
      "                b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "                p_do = tl.make_block_ptr(",
      "                    do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0)",
      "                )",
      "                b_do = tl.load(p_do, boundary_check=(0, 1))",
      "                b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "                b_dp = tl.dot(b_do, tl.trans(b_v))",
      "",
      "                b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "                if USE_GATE:",
      "                    b_dg_cumsum_k -= tl.sum(b_dA, axis=0)",
      "                b_dA = b_dA.to(b_v.dtype)",
      "                b_dk += tl.dot(tl.trans(b_dA), b_q)",
      "",
      "    p_dk = tl.make_block_ptr(dk, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    tl.store(p_dk, b_dk.to(dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.atomic_add(",
      "        dv + (i_t * BT + tl.arange(0, BT))[:, None] * HQ * K + tl.arange(0, K)[None, :],",
      "        b_dv,",
      "        sem=\"relaxed\",",
      "    )",
      "    if USE_GATE:",
      "        tl.atomic_add(",
      "            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_k, sem=\"relaxed\"",
      "        )"
    ],
    "file": "codes/409.py",
    "header": "def parallel_path_bwd_dkv_kernel(q, k, v, g_cumsum, hc_whole, scale, L, D, dk, dv, do, dg_cumsum, cu_seqlens, indices, split_offsets, T, G: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, S: tl.constexpr, IS_VARLEN: tl.constexpr, USE_GATE: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    boh_large = tl.load(split_offsets + i_n).to(tl.int32)\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\n    boh_large = i_n * tl.cdiv(T, S)\nq += (bos * HQ + i_hq) * K\ndo += (bos * HQ + i_hq) * V\ndk += (bos * HQ + i_hq) * K\ndv += (bos * HQ + i_hq) * K\nL += bos * HQ + i_hq\nD += bos * HQ + i_hq\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\nhc_whole += (boh_large * H + i_h) * K * K\nif USE_GATE:\n    g_cumsum += bos * HQ + i_hq\n    dg_cumsum += bos * HQ + i_hq\nstride_h = H * K * K\nsm_scale = scale * 1.44269504\np_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_k_origin = tl.load(p_k, boundary_check=(0, 1))\np_v = tl.make_block_ptr(v, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nif USE_GATE:\n    b_g_cumsum_k = tl.zeros([BT], dtype=tl.float32)\n    p_g_cumsum_k = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_g_cumsum_k += tl.load(p_g_cumsum_k, boundary_check=(0,))\n    b_dg_cumsum_k = tl.zeros([BT], dtype=tl.float32)\nelse:\n    b_g_cumsum_k = None\n    b_dg_cumsum_k = None\nb_dk = tl.zeros([BT, K], dtype=tl.float32)\nb_dv = tl.zeros([BT, K], dtype=tl.float32)\nidx_i = (i_t * BT // S).to(tl.int32)\nlast_chunk_start = tl.floor(T / S).to(tl.int32) * S\nif i_t * BT < last_chunk_start:\n    if T % S != 0:\n        idx_j = last_chunk_start // S\n        b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k_accum += b_k_origin\n        for i in range(idx_i + 1, idx_j):\n            p_h = tl.make_block_ptr(hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))\n        b_k = b_k_accum.to(b_k_origin.dtype)\n        for offset in range(tl.ceil(T / BS).to(tl.int32) * BS - BS, last_chunk_start - BS, -BS):\n            p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))\n            p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))\n            b_delta = tl.load(p_delta, boundary_check=(0,))\n            b_l = tl.load(p_l, boundary_check=(0,))\n            p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_A = tl.dot(b_q, tl.trans(b_k))\n            if USE_GATE:\n                p_g_cumsum_q = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,))\n                b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))\n                b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n                b_A = tl.where((offset + tl.arange(0, BS) < T)[:, None], b_A, float('-inf'))\n            b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n            p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n            b_dp = tl.dot(b_do, tl.trans(b_v))\n            b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n            if USE_GATE:\n                b_dg_cumsum_k -= tl.sum(b_dA, axis=0)\n            b_dA = b_dA.to(b_v.dtype)\n            b_dk += tl.dot(tl.trans(b_dA), b_q)\n    for offset_outer in range(last_chunk_start, i_t * BT + S, -S):\n        idx_j = offset_outer // S - 1\n        b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)\n        b_k_accum += b_k_origin\n        for i in range(idx_i + 1, idx_j):\n            p_h = tl.make_block_ptr(hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n            b_h = tl.load(p_h, boundary_check=(0, 1))\n            b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))\n        b_k = b_k_accum.to(b_k_origin.dtype)\n        p_h = tl.make_block_ptr(hc_whole + idx_j * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n        b_h = tl.load(p_h, boundary_check=(0, 1))\n        b_dk = b_dk - tl.dot(b_dk.to(b_h.dtype), b_h)\n        for offset in range(offset_outer - BS, offset_outer - S - BS, -BS):\n            p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))\n            p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))\n            b_delta = tl.load(p_delta, boundary_check=(0,))\n            b_l = tl.load(p_l, boundary_check=(0,))\n            p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0))\n            b_q = tl.load(p_q, boundary_check=(0, 1))\n            b_A = tl.dot(b_q, tl.trans(b_k))\n            if USE_GATE:\n                p_g_cumsum_q = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,))\n                b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))\n                b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n                b_A = tl.where((offset + tl.arange(0, BS) < T)[:, None], b_A, float('-inf'))\n            b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])\n            p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0))\n            b_do = tl.load(p_do, boundary_check=(0, 1))\n            b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\n            b_dp = tl.dot(b_do, tl.trans(b_v))\n            b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale\n            if USE_GATE:\n                b_dg_cumsum_k -= tl.sum(b_dA, axis=0)\n            b_dA = b_dA.to(b_v.dtype)\n            b_dk += tl.dot(tl.trans(b_dA), b_q)\np_dk = tl.make_block_ptr(dk, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_dk, b_dk.to(dk.dtype.element_ty), boundary_check=(0, 1))\ntl.atomic_add(dv + (i_t * BT + tl.arange(0, BT))[:, None] * HQ * K + tl.arange(0, K)[None, :], b_dv, sem='relaxed')\nif USE_GATE:\n    tl.atomic_add(dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_k, sem='relaxed')"
  },
  {
    "name": "chunk_scaled_dot_kkt_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_G': lambda args: args['g_cumsum'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['H', 'K', 'BT', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ag",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_scaled_dot_kkt_fwd_kernel(",
      "    k,",
      "    beta,",
      "    g_cumsum,",
      "    A,",
      "    Ag,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_t = tl.arange(0, BT)",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = b_k * b_beta[:, None]",
      "        b_A += tl.dot(b_kb.to(b_k.dtype), tl.trans(b_k))",
      "",
      "    b_A = tl.where(o_t[:, None] > o_t[None, :], b_A, 0)",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(",
      "            g_cumsum + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_g_diff = b_g[:, None] - b_g[None, :]",
      "        b_Ag = b_A * safe_exp(b_g_diff)",
      "        p_Ag = tl.make_block_ptr(",
      "            Ag + (bos * H + i_h) * BT,",
      "            (T, BT),",
      "            (BT * H, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BT),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_Ag, b_Ag.to(p_Ag.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/373.py",
    "header": "def chunk_scaled_dot_kkt_fwd_kernel(k, beta, g_cumsum, A, Ag, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr, USE_G: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\no_t = tl.arange(0, BT)\np_beta = tl.make_block_ptr(beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kb = b_k * b_beta[:, None]\n    b_A += tl.dot(b_kb.to(b_k.dtype), tl.trans(b_k))\nb_A = tl.where(o_t[:, None] > o_t[None, :], b_A, 0)\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\ntl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\nif USE_G:\n    p_g = tl.make_block_ptr(g_cumsum + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_g = tl.load(p_g, boundary_check=(0,))\n    b_g_diff = b_g[:, None] - b_g[None, :]\n    b_Ag = b_A * safe_exp(b_g_diff)\n    p_Ag = tl.make_block_ptr(Ag + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\n    tl.store(p_Ag, b_Ag.to(p_Ag.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fp8_gemm_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fp8_gemm_configs, key=['N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "a_s_ptr",
        "annotation": null
      },
      {
        "name": "b_s_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fp8_gemm_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    a_s_ptr,",
      "    b_s_ptr,",
      "    M,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_n = tl.program_id(axis=1)",
      "    k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + offs_m[:, None] * K + offs_k[None, :]",
      "    b_ptrs = b_ptr + offs_n[None, :] * K + offs_k[:, None]",
      "    a_s_ptrs = a_s_ptr + offs_m * k",
      "    b_s_ptrs = b_s_ptr + (offs_n // BLOCK_SIZE_K) * k",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for i in range(k):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - i * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - i * BLOCK_SIZE_K, other=0.0)",
      "        a_s = tl.load(a_s_ptrs)",
      "        b_s = tl.load(b_s_ptrs)",
      "        accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]",
      "        a_ptrs += BLOCK_SIZE_K",
      "        b_ptrs += BLOCK_SIZE_K",
      "        a_s_ptrs += 1",
      "        b_s_ptrs += 1",
      "    c = accumulator.to(c_ptr.dtype.element_ty)",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + offs_m[:, None] * N + offs_n[None, :]",
      "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=mask)"
    ],
    "file": "codes/1262.py",
    "header": "def fp8_gemm_kernel(a_ptr, b_ptr, c_ptr, a_s_ptr, b_s_ptr, M, N: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_m = tl.program_id(axis=0)\npid_n = tl.program_id(axis=1)\nk = tl.cdiv(K, BLOCK_SIZE_K)\noffs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + offs_m[:, None] * K + offs_k[None, :]\nb_ptrs = b_ptr + offs_n[None, :] * K + offs_k[:, None]\na_s_ptrs = a_s_ptr + offs_m * k\nb_s_ptrs = b_s_ptr + offs_n // BLOCK_SIZE_K * k\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor i in range(k):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - i * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - i * BLOCK_SIZE_K, other=0.0)\n    a_s = tl.load(a_s_ptrs)\n    b_s = tl.load(b_s_ptrs)\n    accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]\n    a_ptrs += BLOCK_SIZE_K\n    b_ptrs += BLOCK_SIZE_K\n    a_s_ptrs += 1\n    b_s_ptrs += 1\nc = accumulator.to(c_ptr.dtype.element_ty)\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + offs_m[:, None] * N + offs_n[None, :]\nmask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\ntl.store(c_ptrs, c, mask=mask)"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configs)), key=['N_CTX', 'HEAD_DIM'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    HEAD_DIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "    v_order: tl.constexpr = (1, 0)",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, HEAD_DIM),",
      "        order=v_order,",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(HEAD_DIM, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(HEAD_DIM, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0",
      "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)",
      "",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.44269504",
      "",
      "    q = tl.load(Q_block_ptr)",
      "    acc, l_i, m_i = _attn_fwd_inner(",
      "        acc,",
      "        l_i,",
      "        m_i,",
      "        q,",
      "        K_block_ptr,",
      "        V_block_ptr,",
      "        qk_scale,",
      "        BLOCK_N,",
      "        N_CTX,",
      "    )",
      "",
      "    m_i += tl.math.log2(l_i)",
      "    acc = acc / l_i[:, None]",
      "    m_ptrs = M + off_hz * N_CTX + offs_m",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))"
    ],
    "file": "codes/281.py",
    "header": "def _attn_fwd(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, HEAD_DIM: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "tl.static_assert(BLOCK_N <= HEAD_DIM)\nstart_m = tl.program_id(0)\noff_hz = tl.program_id(1)\noff_z = off_hz // H\noff_h = off_hz % H\nqvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\nQ_block_ptr = tl.make_block_ptr(base=Q + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_qm, stride_qk), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\nv_order: tl.constexpr = (1, 0)\nV_block_ptr = tl.make_block_ptr(base=V + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_vk, stride_vn), offsets=(0, 0), block_shape=(BLOCK_N, HEAD_DIM), order=v_order)\nK_block_ptr = tl.make_block_ptr(base=K + qvk_offset, shape=(HEAD_DIM, N_CTX), strides=(stride_kk, stride_kn), offsets=(0, 0), block_shape=(HEAD_DIM, BLOCK_N), order=(0, 1))\nO_block_ptr = tl.make_block_ptr(base=Out + qvk_offset, shape=(N_CTX, HEAD_DIM), strides=(stride_om, stride_on), offsets=(start_m * BLOCK_M, 0), block_shape=(BLOCK_M, HEAD_DIM), order=(1, 0))\noffs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)\nm_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float('inf')\nl_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0\nacc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)\nqk_scale = sm_scale\nqk_scale *= 1.44269504\nq = tl.load(Q_block_ptr)\nacc, l_i, m_i = _attn_fwd_inner(acc, l_i, m_i, q, K_block_ptr, V_block_ptr, qk_scale, BLOCK_N, N_CTX)\nm_i += tl.math.log2(l_i)\nacc = acc / l_i[:, None]\nm_ptrs = M + off_hz * N_CTX + offs_m\ntl.store(m_ptrs, m_i)\ntl.store(O_block_ptr, acc.to(Out.type.element_ty))"
  },
  {
    "name": "chunk_generalized_iplr_delta_rule_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8, 16]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "d",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_generalized_iplr_delta_rule_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    d,",
      "    b,",
      "    u,",
      "    v_new,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_hc = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):",
      "            p_k = tl.make_block_ptr(",
      "                k + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_b = tl.make_block_ptr(",
      "                b + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_d = tl.make_block_ptr(",
      "                d + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_v = tl.make_block_ptr(",
      "                v + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_u = tl.make_block_ptr(",
      "                u + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            b_b = tl.load(p_b, boundary_check=(0, 1))",
      "            b_v2 = tl.dot(b_d, b_h.to(b_d.dtype)) + tl.load(p_u, boundary_check=(0, 1))",
      "            b_hc += tl.dot(b_k, b_v)",
      "            b_hc += tl.dot(b_b, b_v2.to(b_k.dtype))",
      "            tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "        b_h += b_hc",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/390.py",
    "header": "def chunk_generalized_iplr_delta_rule_fwd_kernel_h(k, v, d, b, u, v_new, h, h0, ht, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(NT):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_hc = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):\n        p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n        p_b = tl.make_block_ptr(b + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n        p_d = tl.make_block_ptr(d + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        p_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_b = tl.load(p_b, boundary_check=(0, 1))\n        b_v2 = tl.dot(b_d, b_h.to(b_d.dtype)) + tl.load(p_u, boundary_check=(0, 1))\n        b_hc += tl.dot(b_k, b_v)\n        b_hc += tl.dot(b_b, b_v2.to(b_k.dtype))\n        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n    b_h += b_hc\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_generalized_iplr_delta_rule_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_generalized_iplr_delta_rule_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    u,",
      "    b,",
      "    h,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    u += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_Aqk = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_Aqb = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_b = tl.make_block_ptr(",
      "            b, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_b = tl.load(p_b, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h)",
      "",
      "        b_Aqk += tl.dot(b_q, b_k)",
      "",
      "        b_Aqb += tl.dot(b_q, b_b)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_Aqk = tl.where(m_A, b_Aqk, 0)",
      "    b_Aqb = tl.where(m_A, b_Aqb, 0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_u = tl.make_block_ptr(",
      "        u, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_u = tl.load(p_u, boundary_check=(0, 1))",
      "    b_o = (",
      "        b_o + tl.dot(b_Aqk.to(b_v.dtype), b_v) + tl.dot(b_Aqb.to(b_u.dtype), b_u)",
      "    ) * scale",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/390.py",
    "header": "def chunk_generalized_iplr_delta_rule_fwd_kernel_o(q, k, v, u, b, h, o, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nb += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\nu += (bos * H + i_h) * V\no += (bos * H + i_h) * V\nh += (i_tg * H + i_h) * K * V\nstride_qk = H * K\nstride_vo = H * V\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nb_Aqk = tl.zeros([BT, BT], dtype=tl.float32)\nb_Aqb = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_h = tl.make_block_ptr(h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_b = tl.make_block_ptr(b, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_b = tl.load(p_b, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_o += tl.dot(b_q, b_h)\n    b_Aqk += tl.dot(b_q, b_k)\n    b_Aqb += tl.dot(b_q, b_b)\no_i = tl.arange(0, BT)\nm_A = o_i[:, None] >= o_i[None, :]\nb_Aqk = tl.where(m_A, b_Aqk, 0)\nb_Aqb = tl.where(m_A, b_Aqb, 0)\np_v = tl.make_block_ptr(v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_u = tl.make_block_ptr(u, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_u = tl.load(p_u, boundary_check=(0, 1))\nb_o = (b_o + tl.dot(b_Aqk.to(b_v.dtype), b_v) + tl.dot(b_Aqb.to(b_u.dtype), b_u)) * scale\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_fwd_kernel_h",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORMK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    z,",
      "    h,",
      "    h0,",
      "    ht,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    NORMK: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "    if NORMK:",
      "        p_z0 = tl.make_block_ptr(",
      "            z + i_bh * T * K, (T * K,), (1,), (i_k * BK,), (BK,), (0,)",
      "        )",
      "    else:",
      "        p_z0 = tl.make_block_ptr(",
      "            z + i_bh * T * V, (T * V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "    b_zp = tl.load(p_z0).to(tl.float32)",
      "    for i_t in range(NT):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        if NORMK:",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * K,",
      "                (T * K,),",
      "                (1,),",
      "                ((i_t * BT + BT - 1) * K + i_k * BK,),",
      "                (BK,),",
      "                (0,),",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zp - b_zc), b_zc",
      "",
      "            b_h = b_h * b_r[:, None]",
      "            b_k = exp(b_k - b_zc[:, None]).to(b_k.dtype)",
      "        else:",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * V,",
      "                (T * V,),",
      "                (1,),",
      "                ((i_t * BT + BT - 1) * V + i_v * BV,),",
      "                (BV,),",
      "                (0,),",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zp - b_zc), b_zc",
      "",
      "            b_h = b_h * b_r[None, :]",
      "            b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)",
      "",
      "        b_h += tl.dot(b_k, b_v, allow_tf32=False)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_fwd_kernel_h(k, v, z, h, h0, ht, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr, NORMK: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\nif NORMK:\n    p_z0 = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), (i_k * BK,), (BK,), (0,))\nelse:\n    p_z0 = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), (i_v * BV,), (BV,), (0,))\nb_zp = tl.load(p_z0).to(tl.float32)\nfor i_t in range(NT):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    if NORMK:\n        p_zc = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_r, b_zp = (exp(b_zp - b_zc), b_zc)\n        b_h = b_h * b_r[:, None]\n        b_k = exp(b_k - b_zc[:, None]).to(b_k.dtype)\n    else:\n        p_zc = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_r, b_zp = (exp(b_zp - b_zc), b_zc)\n        b_h = b_h * b_r[None, :]\n        b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)\n    b_h += tl.dot(b_k, b_v, allow_tf32=False)\nif STORE_FINAL_STATE:\n    p_h = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_fwd_kernel_intra_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_intra_K(",
      "    v,",
      "    z,",
      "    o,",
      "    A,",
      "    T,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * V,",
      "        (T * V,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC) * V + i_v * BV,),",
      "        (BV,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_o = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(0, i_i):",
      "        p_A = tl.make_block_ptr(",
      "            A + i_bh * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_A, exp(b_v - b_zn[None, :]).to(b_v.dtype), allow_tf32=False)",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "    b_o *= exp(b_zn[None, :] - b_z)",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    for j in range(0, BC):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "",
      "        b_A = tl.load(A + o_A + j, mask=m_A, other=0)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] >= j",
      "        b_o += tl.where(m_i, b_A[:, None] * exp(b_v[None, :] - b_z), 0)",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_fwd_kernel_intra_K(v, z, o, A, T, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_t, i_i = (i_c // NC, i_c % NC)\np_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_zn = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\nb_zn = tl.load(p_zn, boundary_check=(0,))\nb_o = tl.zeros([BC, BV], dtype=tl.float32)\nfor i_j in range(0, i_i):\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_o += tl.dot(b_A, exp(b_v - b_zn[None, :]).to(b_v.dtype), allow_tf32=False)\nb_z = tl.load(p_z, boundary_check=(0, 1))\nb_o *= exp(b_zn[None, :] - b_z)\no_i = tl.arange(0, BC)\no_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC\nm_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\nfor j in range(0, BC):\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n    b_A = tl.load(A + o_A + j, mask=m_A, other=0)\n    b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)\n    m_i = o_i[:, None] >= j\n    b_o += tl.where(m_i, b_A[:, None] * exp(b_v[None, :] - b_z), 0)\np_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_fwd_kernel_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_K(",
      "    q,",
      "    k,",
      "    z,",
      "    h,",
      "    o,",
      "    A,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h, allow_tf32=False)",
      "",
      "        b_A += tl.dot(b_q, b_k, allow_tf32=False)",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "",
      "    p_zp = tl.make_block_ptr(",
      "        z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)",
      "    )",
      "    b_zp = tl.load(p_zp, boundary_check=(0,))",
      "    b_o = b_o * exp(b_zp[None, :] - b_z)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    if i_v == 0:",
      "        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_fwd_kernel_K(q, k, z, h, o, A, scale, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_p = tl.maximum(i_t * BT - 1, 0)\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_o += tl.dot(b_q, b_h, allow_tf32=False)\n    b_A += tl.dot(b_q, b_k, allow_tf32=False)\np_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\nb_z = tl.load(p_z, boundary_check=(0, 1))\np_zp = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,))\nb_zp = tl.load(p_zp, boundary_check=(0,))\nb_o = b_o * exp(b_zp[None, :] - b_z)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\np_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_A = tl.where(m_s, b_A, 0.0)\nif i_v == 0:\n    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_fwd_kernel_intra_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_intra_V(",
      "    q,",
      "    k,",
      "    z,",
      "    A,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC",
      "    n_bh = tl.num_programs(2)",
      "",
      "    if i_i > i_j:",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (K, T),",
      "            (1, K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_k * n_bh + i_bh) * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        p_zn = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "",
      "        b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_q = (b_q * exp(b_zn[None, :] - b_z) * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k = exp(b_k - b_zn[:, None]).to(b_k.dtype)",
      "",
      "        b_A = tl.dot(b_q, b_k, allow_tf32=False)",
      "        tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))",
      "    elif i_i == i_j:",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_j * BC) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "",
      "        o_i = tl.arange(0, BC)",
      "        o_A = (",
      "            (i_bh + i_k * n_bh) * T * BT",
      "            + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT",
      "            + i_j * BC",
      "        )",
      "        m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "        for j in range(0, BC):",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)",
      "",
      "            b_A = tl.sum(b_q * exp(b_k[None, :] - b_z) * scale, 1)",
      "            b_A = tl.where(o_i >= j, b_A, 0.0)",
      "            tl.store(A + o_A + j, b_A.to(b_q.dtype), mask=m_A)",
      "",
      "            p_k = tl.advance(p_k, (K,))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_fwd_kernel_intra_V(q, k, z, A, scale, T, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr):",
    "body": "i_k, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_t, i_i, i_j = (i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC) % NC)\nn_bh = tl.num_programs(2)\nif i_i > i_j:\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n    p_z = tl.make_block_ptr(z + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_q = (b_q * exp(b_zn[None, :] - b_z) * scale).to(b_q.dtype)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k = exp(b_k - b_zn[:, None]).to(b_k.dtype)\n    b_A = tl.dot(b_q, b_k, allow_tf32=False)\n    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))\nelif i_i == i_j:\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_j * BC) * K + i_k * BK,), (BK,), (0,))\n    p_z = tl.make_block_ptr(z + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    o_i = tl.arange(0, BC)\n    o_A = (i_bh + i_k * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_j * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)\n        b_A = tl.sum(b_q * exp(b_k[None, :] - b_z) * scale, 1)\n        b_A = tl.where(o_i >= j, b_A, 0.0)\n        tl.store(A + o_A + j, b_A.to(b_q.dtype), mask=m_A)\n        p_k = tl.advance(p_k, (K,))"
  },
  {
    "name": "chunk_abc_fwd_kernel_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_V(",
      "    q,",
      "    v,",
      "    z,",
      "    h,",
      "    o,",
      "    A,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_zp = tl.make_block_ptr(",
      "            z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "",
      "        b_zp = tl.load(p_zp, boundary_check=(0,))",
      "        b_q = (b_q * exp(b_zp[None, :] - b_z)).to(b_q.dtype)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        if i_k >= 0:",
      "            b_o += tl.dot(b_q, b_h, allow_tf32=False)",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_o += tl.dot(b_A.to(b_v.dtype), b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_fwd_kernel_V(q, v, z, h, o, A, scale, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr):",
    "body": "i_v, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_p = tl.maximum(i_t * BT - 1, 0)\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_z = tl.make_block_ptr(z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_zp = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_q = (b_q * exp(b_zp[None, :] - b_z)).to(b_q.dtype)\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    if i_k >= 0:\n        b_o += tl.dot(b_q, b_h, allow_tf32=False)\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\np_A = tl.make_block_ptr(A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_o += tl.dot(b_A.to(b_v.dtype), b_v, allow_tf32=False)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_bwd_kernel_dh",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORMK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_dh(",
      "    q,",
      "    z,",
      "    do,",
      "    dh,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    NORMK: tl.constexpr,",
      "):",
      "    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    b_zp = tl.full([BK if NORMK else BV], float(\"inf\"), dtype=tl.float32)",
      "    for i_t in range(NT - 1, -1, -1):",
      "        i_p = tl.maximum(i_t * BT - 1, 0)",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        if NORMK:",
      "            p_z = tl.make_block_ptr(",
      "                z + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "            )",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zc - b_zp), b_zc",
      "",
      "            b_z = tl.load(p_z, boundary_check=(0, 1))",
      "            b_q = (b_q * exp(b_zc[:, None] - b_z)).to(b_q.dtype)",
      "",
      "            b_dh = b_dh * b_r[:, None]",
      "        else:",
      "            p_z = tl.make_block_ptr(",
      "                z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zc - b_zp), b_zc",
      "",
      "            b_z = tl.load(p_z, boundary_check=(0,))",
      "            b_do = (b_do * exp(b_zc[None, :] - b_z)).to(b_do.dtype)",
      "",
      "            b_dh = b_dh * b_r[None, :]",
      "",
      "        b_dh += tl.dot(b_q, b_do, allow_tf32=False)"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_dh(q, z, do, dh, scale, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr, NORMK: tl.constexpr):",
    "body": "i_k, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nb_zp = tl.full([BK if NORMK else BV], float('inf'), dtype=tl.float32)\nfor i_t in range(NT - 1, -1, -1):\n    i_p = tl.maximum(i_t * BT - 1, 0)\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    if NORMK:\n        p_z = tl.make_block_ptr(z + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_zc = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_r, b_zp = (exp(b_zc - b_zp), b_zc)\n        b_z = tl.load(p_z, boundary_check=(0, 1))\n        b_q = (b_q * exp(b_zc[:, None] - b_z)).to(b_q.dtype)\n        b_dh = b_dh * b_r[:, None]\n    else:\n        p_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_zc = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,))\n        b_zc = tl.load(p_zc, boundary_check=(0,))\n        b_r, b_zp = (exp(b_zc - b_zp), b_zc)\n        b_z = tl.load(p_z, boundary_check=(0,))\n        b_do = (b_do * exp(b_zc[None, :] - b_z)).to(b_do.dtype)\n        b_dh = b_dh * b_r[None, :]\n    b_dh += tl.dot(b_q, b_do, allow_tf32=False)"
  },
  {
    "name": "chunk_abc_bwd_kernel_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_V(",
      "    k,",
      "    v,",
      "    z,",
      "    h,",
      "    A,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dA,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "    n_bh = tl.num_programs(2)",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_zc = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T * K,),",
      "        (1,),",
      "        ((i_t * BT + BT - 1) * K + i_k * BK,),",
      "        (BK,),",
      "        (0,),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "",
      "    b_zc = tl.load(p_zc, boundary_check=(0,))",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_k = exp(b_k - b_zc[None, :]).to(b_k.dtype)",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * V * K,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * n_bh + i_bh) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)",
      "        if i_k == 0:",
      "            b_dv += tl.dot(b_A.to(b_do.dtype), b_do, allow_tf32=False)",
      "        b_do = (b_do * scale).to(b_do.dtype)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)",
      "",
      "        b_dq += tl.dot(b_do, b_h, allow_tf32=False)",
      "",
      "        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_zp = tl.make_block_ptr(",
      "        z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)",
      "    )",
      "",
      "    b_zp = tl.load(p_zp, boundary_check=(0,))",
      "",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "    b_z = exp(b_zp[None, :] - b_z)",
      "",
      "    b_dq = b_dq * b_z",
      "    b_dk = b_dk * b_k",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dA = tl.make_block_ptr(",
      "        dA + i_bh * T * BT,",
      "        (",
      "            T,",
      "            BT,",
      "        ),",
      "        (BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_dA = tl.where(m_s, b_dA, 0.0).to(b_k.dtype)",
      "    if i_k == 0:",
      "        tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_V(k, v, z, h, A, do, dh, dq, dk, dv, dA, scale, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_p = tl.maximum(i_t * BT - 1, 0)\nn_bh = tl.num_programs(2)\np_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_zc = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), ((i_t * BT + BT - 1) * K + i_k * BK,), (BK,), (0,))\np_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t * BT), (BT, BT), (0, 1))\nb_zc = tl.load(p_zc, boundary_check=(0,))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_k = exp(b_k - b_zc[None, :]).to(b_k.dtype)\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_dA = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * V * K, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dv = tl.dot(b_k, b_dh, allow_tf32=False)\n    if i_k == 0:\n        b_dv += tl.dot(b_A.to(b_do.dtype), b_do, allow_tf32=False)\n    b_do = (b_do * scale).to(b_do.dtype)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)\n    b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n    b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\np_z = tl.make_block_ptr(z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_zp = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,))\nb_zp = tl.load(p_zp, boundary_check=(0,))\nb_z = tl.load(p_z, boundary_check=(0, 1))\nb_z = exp(b_zp[None, :] - b_z)\nb_dq = b_dq * b_z\nb_dk = b_dk * b_k\np_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\nb_dA = tl.where(m_s, b_dA, 0.0).to(b_k.dtype)\nif i_k == 0:\n    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_bwd_kernel_intra_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_intra_V(",
      "    q,",
      "    k,",
      "    z,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    T,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T * K,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC) * K + i_k * BK,),",
      "        (BK,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "    b_zq = exp(b_zn[None, :] - b_z)",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    for i_j in range(0, i_i):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_j * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_dA = tl.make_block_ptr(",
      "            dA + i_bh * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kz = exp(b_k - b_zn[None, :]).to(b_k.dtype)",
      "",
      "        b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "        b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)",
      "    b_dq *= b_zq",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC",
      "    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    for j in range(0, BC):",
      "        p_kj = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "",
      "        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)",
      "",
      "        b_kj = tl.load(p_kj, boundary_check=(0,)).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] >= j",
      "",
      "        b_dq += tl.where(m_i, b_dA[:, None] * exp(b_kj[None, :] - b_z), 0.0)",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T * K,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,),",
      "        (BK,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_kz = exp(b_k - b_zn[None, :])",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_j * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_j * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_dA = tl.make_block_ptr(",
      "            dA + i_bh * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_j * BC, i_i * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_qz = (b_q * exp(b_zn[None, :] - b_z)).to(b_q.dtype)",
      "",
      "        b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "        b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)",
      "    b_dk *= b_kz",
      "",
      "    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(0, BC)",
      "    for j in range(0, BC):",
      "        p_qj = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "        p_zj = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "",
      "        b_dA = tl.load(dA + o_dA + j * BT, mask=(i_t * BT + i_i * BC + j < T), other=0)",
      "",
      "        b_qj = tl.load(p_qj, boundary_check=(0,)).to(tl.float32)",
      "        b_zj = tl.load(p_zj, boundary_check=(0,)).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dk += tl.where(",
      "            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_k - b_zj[None, :]), 0.0",
      "        )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_intra_V(q, k, z, dA, dq, dk, T, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr):",
    "body": "i_k, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_t, i_i = (i_c // NC, i_c % NC)\np_z = tl.make_block_ptr(z + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_zn = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_i * BC) * K + i_k * BK,), (BK,), (0,))\nb_zn = tl.load(p_zn, boundary_check=(0,))\nb_z = tl.load(p_z, boundary_check=(0, 1))\nb_zq = exp(b_zn[None, :] - b_z)\nb_dq = tl.zeros([BC, BK], dtype=tl.float32)\nfor i_j in range(0, i_i):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kz = exp(b_k - b_zn[None, :]).to(b_k.dtype)\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)\nb_dq *= b_zq\no_i = tl.arange(0, BC)\no_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC\nm_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\nfor j in range(0, BC):\n    p_kj = tl.make_block_ptr(k + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n    b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n    b_kj = tl.load(p_kj, boundary_check=(0,)).to(tl.float32)\n    m_i = o_i[:, None] >= j\n    b_dq += tl.where(m_i, b_dA[:, None] * exp(b_kj[None, :] - b_z), 0.0)\np_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.debug_barrier()\np_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_zn = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,), (BK,), (0,))\nb_zn = tl.load(p_zn, boundary_check=(0,))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_kz = exp(b_k - b_zn[None, :])\nb_dk = tl.zeros([BC, BK], dtype=tl.float32)\nfor i_j in range(i_i + 1, NC):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n    p_z = tl.make_block_ptr(z + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n    p_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT + i_j * BC, i_i * BC), (BC, BC), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_qz = (b_q * exp(b_zn[None, :] - b_z)).to(b_q.dtype)\n    b_dA = tl.load(p_dA, boundary_check=(0, 1))\n    b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)\nb_dk *= b_kz\no_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(0, BC)\nfor j in range(0, BC):\n    p_qj = tl.make_block_ptr(q + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n    p_zj = tl.make_block_ptr(z + i_bh * T * K, (T * K,), (1,), ((i_t * BT + i_i * BC + j) * K + i_k * BK,), (BK,), (0,))\n    b_dA = tl.load(dA + o_dA + j * BT, mask=i_t * BT + i_i * BC + j < T, other=0)\n    b_qj = tl.load(p_qj, boundary_check=(0,)).to(tl.float32)\n    b_zj = tl.load(p_zj, boundary_check=(0,)).to(tl.float32)\n    m_i = o_i[:, None] <= j\n    b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * exp(b_k - b_zj[None, :]), 0.0)\np_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_bwd_kernel_intra_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_intra_K(",
      "    v,",
      "    z,",
      "    do,",
      "    dA,",
      "    scale,",
      "    T,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC",
      "    n_bh = tl.num_programs(2)",
      "",
      "    if i_i > i_j:",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (V, T),",
      "            (1, V),",
      "            (i_v * BV, i_t * BT + i_j * BC),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_zn = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_dA = tl.make_block_ptr(",
      "            dA + (i_bh + i_v * n_bh) * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "",
      "        b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_zn[None, :] - b_z) * scale).to(b_do.dtype)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v = exp(b_v - b_zn[:, None]).to(b_v.dtype)",
      "",
      "        b_dA = tl.dot(b_do, b_v, allow_tf32=False)",
      "        tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))",
      "    elif i_i == i_j:",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_j * BC) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale",
      "",
      "        o_i = tl.arange(0, BC)",
      "        o_A = (",
      "            (i_bh + i_v * n_bh) * T * BT",
      "            + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT",
      "            + i_j * BC",
      "        )",
      "        m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "        for j in range(0, BC):",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)",
      "",
      "            b_dA = tl.sum(b_do * exp(b_v[None, :] - b_z), 1)",
      "            b_dA = tl.where(o_i >= j, b_dA, 0)",
      "            tl.store(dA + o_A + j, b_dA.to(b_do.dtype), mask=m_A)",
      "",
      "            p_v = tl.advance(p_v, (V,))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_intra_K(v, z, do, dA, scale, T, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_t, i_i, i_j = (i_c // (NC * NC), i_c % (NC * NC) // NC, i_c % (NC * NC) % NC)\nn_bh = tl.num_programs(2)\nif i_i > i_j:\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i_t * BT + i_j * BC), (BV, BC), (0, 1))\n    p_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_zn = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_i * BC) * V + i_v * BV,), (BV,), (0,))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_dA = tl.make_block_ptr(dA + (i_bh + i_v * n_bh) * T * BT, (T, BT), (BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n    b_zn = tl.load(p_zn, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_do = (b_do * exp(b_zn[None, :] - b_z) * scale).to(b_do.dtype)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v = exp(b_v - b_zn[:, None]).to(b_v.dtype)\n    b_dA = tl.dot(b_do, b_v, allow_tf32=False)\n    tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))\nelif i_i == i_j:\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_j * BC) * V + i_v * BV,), (BV,), (0,))\n    p_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1)) * scale\n    o_i = tl.arange(0, BC)\n    o_A = (i_bh + i_v * n_bh) * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_j * BC\n    m_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\n    for j in range(0, BC):\n        b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)\n        b_dA = tl.sum(b_do * exp(b_v[None, :] - b_z), 1)\n        b_dA = tl.where(o_i >= j, b_dA, 0)\n        tl.store(dA + o_A + j, b_dA.to(b_do.dtype), mask=m_A)\n        p_v = tl.advance(p_v, (V,))"
  },
  {
    "name": "chunk_abc_bwd_kernel_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_K(",
      "    q,",
      "    k,",
      "    v,",
      "    z,",
      "    h,",
      "    A,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dA,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "    n_bh = tl.num_programs(2)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (i_k * n_bh + i_bh) * T * BT,",
      "        (",
      "            T,",
      "            BT,",
      "        ),",
      "        (BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k), allow_tf32=False)",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_zp = tl.make_block_ptr(",
      "            z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)",
      "        )",
      "        p_zc = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + BT - 1) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * n_bh + i_bh) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_zp = tl.load(p_zp, boundary_check=(0,))",
      "        b_zc = tl.load(p_zc, boundary_check=(0,))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_z = exp(b_zp[None, :] - b_z)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * b_z * scale).to(b_do.dtype)",
      "",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dq += tl.dot(b_do, b_h, allow_tf32=False)",
      "        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)",
      "",
      "        b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dA = tl.make_block_ptr(",
      "        dA + i_bh * T * BT,",
      "        (",
      "            T,",
      "            BT,",
      "        ),",
      "        (BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)",
      "    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q, allow_tf32=False)",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_K(q, k, v, z, h, A, do, dh, dq, dk, dv, dA, scale, T, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NT: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_p = tl.maximum(i_t * BT - 1, 0)\nn_bh = tl.num_programs(2)\no_i = tl.arange(0, BT)\nm_s = o_i[:, None] >= o_i[None, :]\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_A = tl.make_block_ptr(A + (i_k * n_bh + i_bh) * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k), allow_tf32=False)\nb_A = tl.where(m_s, b_A, 0.0)\ntl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_zp = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,))\n    p_zc = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), ((i_t * BT + BT - 1) * V + i_v * BV,), (BV,), (0,))\n    p_h = tl.make_block_ptr(h + i_bh * NT * K * V + i_t * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dh = tl.make_block_ptr(dh + i_bh * NT * K * V + i_t * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_k * n_bh + i_bh) * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_zp = tl.load(p_zp, boundary_check=(0,))\n    b_zc = tl.load(p_zc, boundary_check=(0,))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_z = exp(b_zp[None, :] - b_z)\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_do = (b_do * b_z * scale).to(b_do.dtype)\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dq += tl.dot(b_do, b_h, allow_tf32=False)\n    b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)\n    b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\np_dA = tl.make_block_ptr(dA + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_dA = tl.load(p_dA, boundary_check=(0, 1))\nb_dq += tl.dot(b_dA, b_k, allow_tf32=False)\nb_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q, allow_tf32=False)\np_dq = tl.make_block_ptr(dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_bwd_kernel_intra_KV",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_intra_KV(",
      "    v,",
      "    z,",
      "    A,",
      "    do,",
      "    dv,",
      "    T,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * V,",
      "        (T * V,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC + BC - 1) * V + i_v * BV,),",
      "        (BV,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + i_bh * T * BT,",
      "            (BT, T),",
      "            (1, BT),",
      "            (i_i * BC, i_t * BT + i_j * BC),",
      "            (BC, BC),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_zn[None, :] - b_z)).to(b_do.dtype)",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_A, b_do, allow_tf32=False)",
      "    b_dv *= exp(b_v - b_zn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    for j in range(0, BC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + i_bh * T * BT,",
      "            (T * BT,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * BT + i_i * BC,),",
      "            (BC,),",
      "            (0,),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0,))",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0,))",
      "        b_do = tl.load(p_do, boundary_check=(0,))",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dv += tl.where(",
      "            m_i, exp(b_v - b_z[None, :]) * b_A[:, None] * b_do[None, :], 0.0",
      "        )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_intra_KV(v, z, A, do, dv, T, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BV: tl.constexpr, NC: tl.constexpr):",
    "body": "i_v, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_t, i_i = (i_c // NC, i_c % NC)\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\np_zn = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_i * BC + BC - 1) * V + i_v * BV,), (BV,), (0,))\nb_zn = tl.load(p_zn, boundary_check=(0,))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BC, BV], dtype=tl.float32)\nfor i_j in range(i_i + 1, NC):\n    p_z = tl.make_block_ptr(z + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (BT, T), (1, BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_j * BC, i_v * BV), (BC, BV), (1, 0))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_do = (b_do * exp(b_zn[None, :] - b_z)).to(b_do.dtype)\n    b_A = tl.load(p_A, boundary_check=(0, 1))\n    b_dv += tl.dot(b_A, b_do, allow_tf32=False)\nb_dv *= exp(b_v - b_zn[None, :])\no_i = tl.arange(0, BC)\nfor j in range(0, BC):\n    p_z = tl.make_block_ptr(z + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n    p_A = tl.make_block_ptr(A + i_bh * T * BT, (T * BT,), (1,), ((i_t * BT + i_i * BC + j) * BT + i_i * BC,), (BC,), (0,))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T * V,), (1,), ((i_t * BT + i_i * BC + j) * V + i_v * BV,), (BV,), (0,))\n    b_A = tl.load(p_A, boundary_check=(0,))\n    b_z = tl.load(p_z, boundary_check=(0,))\n    b_do = tl.load(p_do, boundary_check=(0,))\n    m_i = o_i[:, None] <= j\n    b_dv += tl.where(m_i, exp(b_v - b_z[None, :]) * b_A[:, None] * b_do[None, :], 0.0)\np_dv = tl.make_block_ptr(dv + i_bh * T * V, (T, V), (V, 1), (i_t * BT + i_i * BC, i_v * BV), (BC, BV), (1, 0))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_abc_bwd_kernel_rcum_inter",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "ss",
        "annotation": null
      },
      {
        "name": "doo",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_rcum_inter(",
      "    s,",
      "    z,",
      "    ss,",
      "    doo,",
      "    T,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_m, i_bh = tl.program_id(0), tl.program_id(1)",
      "",
      "    b_sp = tl.zeros(",
      "        [",
      "            BS,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    b_zp = tl.full(",
      "        [",
      "            BS,",
      "        ],",
      "        float(\"inf\"),",
      "        dtype=tl.float32,",
      "    )",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_s = tl.make_block_ptr(",
      "            s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "        p_zc = tl.make_block_ptr(",
      "            z + i_bh * T * S, (T * S,), (1,), ((i_t * BT) * S + i_m * BS,), (BS,), (0,)",
      "        )",
      "        p_ss = tl.make_block_ptr(",
      "            ss + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "        p_doo = tl.make_block_ptr(",
      "            doo + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "",
      "        b_zc = tl.load(p_zc, boundary_check=(0,))",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_ss = tl.load(p_ss, boundary_check=(0, 1))",
      "",
      "        b_doo = exp(b_s - b_zp[None, :]) * b_sp[None, :]",
      "        tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_sp = b_sp * exp(b_zc - b_zp) + tl.sum(b_ss * exp(b_zc[None, :] - b_z), 0)",
      "        b_zp = b_zc"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_rcum_inter(s, z, ss, doo, T, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, NT: tl.constexpr):",
    "body": "i_m, i_bh = (tl.program_id(0), tl.program_id(1))\nb_sp = tl.zeros([BS], dtype=tl.float32)\nb_zp = tl.full([BS], float('inf'), dtype=tl.float32)\nfor i_t in range(NT - 1, -1, -1):\n    p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n    p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n    p_zc = tl.make_block_ptr(z + i_bh * T * S, (T * S,), (1,), (i_t * BT * S + i_m * BS,), (BS,), (0,))\n    p_ss = tl.make_block_ptr(ss + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n    p_doo = tl.make_block_ptr(doo + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0))\n    b_zc = tl.load(p_zc, boundary_check=(0,))\n    b_s = tl.load(p_s, boundary_check=(0, 1))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_ss = tl.load(p_ss, boundary_check=(0, 1))\n    b_doo = exp(b_s - b_zp[None, :]) * b_sp[None, :]\n    tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))\n    b_sp = b_sp * exp(b_zc - b_zp) + tl.sum(b_ss * exp(b_zc[None, :] - b_z), 0)\n    b_zp = b_zc"
  },
  {
    "name": "chunk_abc_bwd_kernel_rcum_intra",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "ss",
        "annotation": null
      },
      {
        "name": "doo",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_rcum_intra(",
      "    s,",
      "    z,",
      "    ss,",
      "    doo,",
      "    T,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    o_i = tl.arange(0, BC)",
      "    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)",
      "",
      "    p_s = tl.make_block_ptr(",
      "        s + i_bh * T * S,",
      "        (T, S),",
      "        (S, 1),",
      "        (i_t * BT + i_i * BC, i_s * BS),",
      "        (BC, BS),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * S,",
      "        (T * S,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC + BC - 1) * S + i_s * BS,),",
      "        (BS,),",
      "        (0,),",
      "    )",
      "    p_doo = tl.make_block_ptr(",
      "        doo + i_bh * T * S,",
      "        (T, S),",
      "        (S, 1),",
      "        (i_t * BT + i_i * BC, i_s * BS),",
      "        (BC, BS),",
      "        (1, 0),",
      "    )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1))",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_doo = tl.zeros([BC, BS], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT + i_j * BC, i_s * BS),",
      "            (BC, BS),",
      "            (1, 0),",
      "        )",
      "        p_ss = tl.make_block_ptr(",
      "            ss + i_bh * T * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT + i_j * BC, i_s * BS),",
      "            (BC, BS),",
      "            (1, 0),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_ss = tl.load(p_ss, boundary_check=(0, 1))",
      "",
      "        b_doo += b_ss * exp(b_zn[None, :] - b_z)",
      "    b_doo = exp(b_s - b_zn[None, :]) * tl.dot(",
      "        m_o.to(b_s.dtype), b_doo.to(b_s.dtype), allow_tf32=False",
      "    )",
      "",
      "    for j in range(0, BC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S,",
      "            (T * S,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * S + i_s * BS,),",
      "            (BS,),",
      "            (0,),",
      "        )",
      "        p_ss = tl.make_block_ptr(",
      "            ss + i_bh * T * S,",
      "            (T * S,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * S + i_s * BS,),",
      "            (BS,),",
      "            (0,),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0,))",
      "        b_ss = tl.load(p_ss, boundary_check=(0,))",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_doo += tl.where(m_i, exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)",
      "    b_doo += tl.load(p_doo, boundary_check=(0, 1))",
      "    tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/363.py",
    "header": "def chunk_abc_bwd_kernel_rcum_intra(s, z, ss, doo, T, S: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, NC: tl.constexpr):",
    "body": "i_s, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_t, i_i = (i_c // NC, i_c % NC)\no_i = tl.arange(0, BC)\nm_o = tl.full([BC, BC], 1.0, dtype=tl.float32)\np_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t * BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\np_zn = tl.make_block_ptr(z + i_bh * T * S, (T * S,), (1,), ((i_t * BT + i_i * BC + BC - 1) * S + i_s * BS,), (BS,), (0,))\np_doo = tl.make_block_ptr(doo + i_bh * T * S, (T, S), (S, 1), (i_t * BT + i_i * BC, i_s * BS), (BC, BS), (1, 0))\nb_s = tl.load(p_s, boundary_check=(0, 1))\nb_zn = tl.load(p_zn, boundary_check=(0,))\nb_doo = tl.zeros([BC, BS], dtype=tl.float32)\nfor i_j in range(i_i + 1, NC):\n    p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n    p_ss = tl.make_block_ptr(ss + i_bh * T * S, (T, S), (S, 1), (i_t * BT + i_j * BC, i_s * BS), (BC, BS), (1, 0))\n    b_z = tl.load(p_z, boundary_check=(0, 1))\n    b_ss = tl.load(p_ss, boundary_check=(0, 1))\n    b_doo += b_ss * exp(b_zn[None, :] - b_z)\nb_doo = exp(b_s - b_zn[None, :]) * tl.dot(m_o.to(b_s.dtype), b_doo.to(b_s.dtype), allow_tf32=False)\nfor j in range(0, BC):\n    p_z = tl.make_block_ptr(z + i_bh * T * S, (T * S,), (1,), ((i_t * BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n    p_ss = tl.make_block_ptr(ss + i_bh * T * S, (T * S,), (1,), ((i_t * BT + i_i * BC + j) * S + i_s * BS,), (BS,), (0,))\n    b_z = tl.load(p_z, boundary_check=(0,))\n    b_ss = tl.load(p_ss, boundary_check=(0,))\n    m_i = o_i[:, None] <= j\n    b_doo += tl.where(m_i, exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)\nb_doo += tl.load(p_doo, boundary_check=(0, 1))\ntl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_fwd_kv_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kv_kernel(",
      "    K,",
      "    V,",
      "    S,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + off_bs_head * stride_qk_bh,",
      "        shape=(DK, L),",
      "        strides=(stride_qk_d, stride_qk_l),",
      "        offsets=(start_k * BK, 0),",
      "        block_shape=(BK, BL),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    s = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for _ in range(0, L, BL):",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1))",
      "        v = tl.load(V_block_ptr, boundary_check=(0, 1))",
      "",
      "        v = (v * scale).to(v.dtype)",
      "        s += tl.dot(k, v, allow_tf32=False)",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BL))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))",
      "",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(start_k * BK, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/332.py",
    "header": "def _fwd_kv_kernel(K, V, S, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, scale, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_v, start_k, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nK_block_ptr = tl.make_block_ptr(base=K + off_bs_head * stride_qk_bh, shape=(DK, L), strides=(stride_qk_d, stride_qk_l), offsets=(start_k * BK, 0), block_shape=(BK, BL), order=(0, 1))\nV_block_ptr = tl.make_block_ptr(base=V + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(0, start_v * BV), block_shape=(BL, BV), order=(1, 0))\ns = tl.zeros([BK, BV], dtype=tl.float32)\nfor _ in range(0, L, BL):\n    k = tl.load(K_block_ptr, boundary_check=(0, 1))\n    v = tl.load(V_block_ptr, boundary_check=(0, 1))\n    v = (v * scale).to(v.dtype)\n    s += tl.dot(k, v, allow_tf32=False)\n    K_block_ptr = tl.advance(K_block_ptr, (0, BL))\n    V_block_ptr = tl.advance(V_block_ptr, (BL, 0))\nS_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh, shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(start_k * BK, start_v * BV), block_shape=(BK, BV), order=(1, 0))\ntl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_fwd_qs_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_qs_kernel(",
      "    Q,",
      "    S,",
      "    O,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    o = tl.zeros([BL, BV], dtype=tl.float32)",
      "",
      "    for _ in range(0, DK, BK):",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1))",
      "        s = tl.load(S_block_ptr, boundary_check=(0, 1))",
      "",
      "        o += tl.dot(q, s, allow_tf32=False)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))",
      "        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=O + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/332.py",
    "header": "def _fwd_qs_kernel(Q, S, O, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_v, start_m, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nQ_block_ptr = tl.make_block_ptr(base=Q + off_bs_head * stride_qk_bh, shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, 0), block_shape=(BL, BK), order=(1, 0))\nS_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh, shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(0, start_v * BV), block_shape=(BK, BV), order=(1, 0))\no = tl.zeros([BL, BV], dtype=tl.float32)\nfor _ in range(0, DK, BK):\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    s = tl.load(S_block_ptr, boundary_check=(0, 1))\n    o += tl.dot(q, s, allow_tf32=False)\n    Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))\n    S_block_ptr = tl.advance(S_block_ptr, (BK, 0))\nO_block_ptr = tl.make_block_ptr(base=O + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m * BL, start_v * BV), block_shape=(BL, BV), order=(1, 0))\ntl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_bwd_ds_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DS",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_ds_kernel(",
      "    Q,",
      "    DO,",
      "    DS,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + off_bs_head * stride_qk_bh,",
      "        shape=(DK, L),",
      "        strides=(stride_qk_d, stride_qk_l),",
      "        offsets=(start_k * BK, 0),",
      "        block_shape=(BK, BL),",
      "        order=(0, 1),",
      "    )",
      "    DO_block_ptr = tl.make_block_ptr(",
      "        base=DO + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    ds = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for i in range(0, L, BL):",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1))",
      "        do = tl.load(DO_block_ptr, boundary_check=(0, 1))",
      "",
      "        ds += tl.dot(q, do, allow_tf32=False)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, (0, BL))",
      "        DO_block_ptr = tl.advance(DO_block_ptr, (BL, 0))",
      "",
      "    DS_block_ptr = tl.make_block_ptr(",
      "        base=DS + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(start_k * BK, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(DS_block_ptr, ds.to(DS.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/332.py",
    "header": "def _bwd_ds_kernel(Q, DO, DS, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_v, start_k, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nQ_block_ptr = tl.make_block_ptr(base=Q + off_bs_head * stride_qk_bh, shape=(DK, L), strides=(stride_qk_d, stride_qk_l), offsets=(start_k * BK, 0), block_shape=(BK, BL), order=(0, 1))\nDO_block_ptr = tl.make_block_ptr(base=DO + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(0, start_v * BV), block_shape=(BL, BV), order=(1, 0))\nds = tl.zeros([BK, BV], dtype=tl.float32)\nfor i in range(0, L, BL):\n    q = tl.load(Q_block_ptr, boundary_check=(0, 1))\n    do = tl.load(DO_block_ptr, boundary_check=(0, 1))\n    ds += tl.dot(q, do, allow_tf32=False)\n    Q_block_ptr = tl.advance(Q_block_ptr, (0, BL))\n    DO_block_ptr = tl.advance(DO_block_ptr, (BL, 0))\nDS_block_ptr = tl.make_block_ptr(base=DS + off_bs_head * stride_s_bh, shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(start_k * BK, start_v * BV), block_shape=(BK, BV), order=(1, 0))\ntl.store(DS_block_ptr, ds.to(DS.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_bwd_dqk_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64] for BK in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "dQ",
        "annotation": null
      },
      {
        "name": "dK",
        "annotation": null
      },
      {
        "name": "dS",
        "annotation": null
      },
      {
        "name": "dO",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_dqk_kernel(",
      "    V,",
      "    S,",
      "    dQ,",
      "    dK,",
      "    dS,",
      "    dO,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_k, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    dO_block_ptr = tl.make_block_ptr(",
      "        base=dO + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DV, DK),",
      "        strides=(stride_s_dv, stride_s_dk),",
      "        offsets=(0, start_k * BK),",
      "        block_shape=(BV, BK),",
      "        order=(0, 1),",
      "    )",
      "    dS_block_ptr = tl.make_block_ptr(",
      "        base=dS + off_bs_head * stride_s_bh,",
      "        shape=(DV, DK),",
      "        strides=(stride_s_dv, stride_s_dk),",
      "        offsets=(0, start_k * BK),",
      "        block_shape=(BV, BK),",
      "        order=(0, 1),",
      "    )",
      "",
      "    dq = tl.zeros([BL, BK], dtype=tl.float32)",
      "    dk = tl.zeros([BL, BK], dtype=tl.float32)",
      "",
      "    for _ in range(0, DV, BV):",
      "        v = tl.load(V_block_ptr, boundary_check=(0, 1))",
      "        do = tl.load(dO_block_ptr, boundary_check=(0, 1))",
      "",
      "        s = tl.load(S_block_ptr, boundary_check=(0, 1))",
      "        ds = tl.load(dS_block_ptr, boundary_check=(0, 1))",
      "",
      "        v = (v * scale).to(v.dtype)",
      "        dq += tl.dot(do, s.to(do.dtype), allow_tf32=False)",
      "        dk += tl.dot(v, ds.to(v.dtype), allow_tf32=False)",
      "",
      "        V_block_ptr = tl.advance(V_block_ptr, (0, BV))",
      "        dS_block_ptr = tl.advance(dS_block_ptr, (BV, 0))",
      "",
      "        dO_block_ptr = tl.advance(dO_block_ptr, (0, BV))",
      "        S_block_ptr = tl.advance(S_block_ptr, (BV, 0))",
      "",
      "    dQ_block_ptr = tl.make_block_ptr(",
      "        base=dQ + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, start_k * BK),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    dK_block_ptr = tl.make_block_ptr(",
      "        base=dK + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, start_k * BK),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(dQ_block_ptr, dq.to(dQ.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(dK_block_ptr, dk.to(dK.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/332.py",
    "header": "def _bwd_dqk_kernel(V, S, dQ, dK, dS, dO, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, scale, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_k, start_m, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nV_block_ptr = tl.make_block_ptr(base=V + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m * BL, 0), block_shape=(BL, BV), order=(1, 0))\ndO_block_ptr = tl.make_block_ptr(base=dO + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m * BL, 0), block_shape=(BL, BV), order=(1, 0))\nS_block_ptr = tl.make_block_ptr(base=S + off_bs_head * stride_s_bh, shape=(DV, DK), strides=(stride_s_dv, stride_s_dk), offsets=(0, start_k * BK), block_shape=(BV, BK), order=(0, 1))\ndS_block_ptr = tl.make_block_ptr(base=dS + off_bs_head * stride_s_bh, shape=(DV, DK), strides=(stride_s_dv, stride_s_dk), offsets=(0, start_k * BK), block_shape=(BV, BK), order=(0, 1))\ndq = tl.zeros([BL, BK], dtype=tl.float32)\ndk = tl.zeros([BL, BK], dtype=tl.float32)\nfor _ in range(0, DV, BV):\n    v = tl.load(V_block_ptr, boundary_check=(0, 1))\n    do = tl.load(dO_block_ptr, boundary_check=(0, 1))\n    s = tl.load(S_block_ptr, boundary_check=(0, 1))\n    ds = tl.load(dS_block_ptr, boundary_check=(0, 1))\n    v = (v * scale).to(v.dtype)\n    dq += tl.dot(do, s.to(do.dtype), allow_tf32=False)\n    dk += tl.dot(v, ds.to(v.dtype), allow_tf32=False)\n    V_block_ptr = tl.advance(V_block_ptr, (0, BV))\n    dS_block_ptr = tl.advance(dS_block_ptr, (BV, 0))\n    dO_block_ptr = tl.advance(dO_block_ptr, (0, BV))\n    S_block_ptr = tl.advance(S_block_ptr, (BV, 0))\ndQ_block_ptr = tl.make_block_ptr(base=dQ + off_bs_head * stride_qk_bh, shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, start_k * BK), block_shape=(BL, BK), order=(1, 0))\ndK_block_ptr = tl.make_block_ptr(base=dK + off_bs_head * stride_qk_bh, shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, start_k * BK), block_shape=(BL, BK), order=(1, 0))\ntl.store(dQ_block_ptr, dq.to(dQ.dtype.element_ty), boundary_check=(0, 1))\ntl.store(dK_block_ptr, dk.to(dK.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_bwd_dv_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "dV",
        "annotation": null
      },
      {
        "name": "dS",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_dv_kernel(",
      "    K,",
      "    dV,",
      "    dS,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    dS_block_ptr = tl.make_block_ptr(",
      "        base=dS + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    dv = tl.zeros([BL, BV], dtype=tl.float32)",
      "",
      "    for _ in range(0, DK, BK):",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1))",
      "        ds = tl.load(dS_block_ptr, boundary_check=(0, 1))",
      "",
      "        dv += tl.dot(k, ds.to(k.dtype), allow_tf32=False) * scale",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BK))",
      "        dS_block_ptr = tl.advance(dS_block_ptr, (BK, 0))",
      "",
      "    dV_block_ptr = tl.make_block_ptr(",
      "        base=dV + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(dV_block_ptr, dv.to(dV.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/332.py",
    "header": "def _bwd_dv_kernel(K, dV, dS, stride_qk_bh, stride_qk_l, stride_qk_d, stride_vo_bh, stride_vo_l, stride_vo_d, stride_s_bh, stride_s_dk, stride_s_dv, scale, L: tl.constexpr, DK: tl.constexpr, DV: tl.constexpr, BL: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "start_v, start_m, off_bs_head = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nK_block_ptr = tl.make_block_ptr(base=K + off_bs_head * stride_qk_bh, shape=(L, DK), strides=(stride_qk_l, stride_qk_d), offsets=(start_m * BL, 0), block_shape=(BL, BK), order=(1, 0))\ndS_block_ptr = tl.make_block_ptr(base=dS + off_bs_head * stride_s_bh, shape=(DK, DV), strides=(stride_s_dk, stride_s_dv), offsets=(0, start_v * BV), block_shape=(BK, BV), order=(1, 0))\ndv = tl.zeros([BL, BV], dtype=tl.float32)\nfor _ in range(0, DK, BK):\n    k = tl.load(K_block_ptr, boundary_check=(0, 1))\n    ds = tl.load(dS_block_ptr, boundary_check=(0, 1))\n    dv += tl.dot(k, ds.to(k.dtype), allow_tf32=False) * scale\n    K_block_ptr = tl.advance(K_block_ptr, (0, BK))\n    dS_block_ptr = tl.advance(dS_block_ptr, (BK, 0))\ndV_block_ptr = tl.make_block_ptr(base=dV + off_bs_head * stride_vo_bh, shape=(L, DV), strides=(stride_vo_l, stride_vo_d), offsets=(start_m * BL, start_v * BV), block_shape=(BL, BV), order=(1, 0))\ntl.store(dV_block_ptr, dv.to(dV.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "codes/175.py",
    "header": "def linear_xent_fwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr):",
    "body": "idx = tl.program_id(axis=0)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, 0), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\noffsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + offsets)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(1000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V // V_BLOCK_SIZE):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    local_x_block_ptr = x_block_ptr\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(local_x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == v_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    m = m_new\n    A_block_ptr = tl.advance(A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE])\n    v_range = v_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\nloss += tl.sum(lse) / N\ntl.store(losses_ptr + idx, loss)\ntl.store(lse_ptr + offsets, lse)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr', 'A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "locks_N_ptr",
        "annotation": null
      },
      {
        "name": "locks_V_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    A_grad_ptr,",
      "    locks_N_ptr,",
      "    locks_V_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    linear_xent_bwd_kernel_matmul_t_dA(",
      "        x_ptr,",
      "        y_ptr,",
      "        A_t_ptr,",
      "        lse_global_ptr,",
      "        A_grad_ptr,",
      "        locks_N_ptr,",
      "        stride_x_N,",
      "        stride_x_H,",
      "        stride_A_H,",
      "        stride_A_V,",
      "        V,",
      "        N,",
      "        H,",
      "        V_BLOCK_SIZE,",
      "        N_BLOCK_SIZE,",
      "        H_BLOCK_SIZE,",
      "    )",
      "",
      "    linear_xent_bwd_kernel_matmul_t_dx(",
      "        x_ptr,",
      "        y_ptr,",
      "        A_t_ptr,",
      "        lse_global_ptr,",
      "        x_grad_ptr,",
      "        locks_V_ptr,",
      "        stride_x_N,",
      "        stride_x_H,",
      "        stride_A_H,",
      "        stride_A_V,",
      "        V,",
      "        N,",
      "        H,",
      "        V_BLOCK_SIZE,",
      "        N_BLOCK_SIZE,",
      "        H_BLOCK_SIZE,",
      "    )"
    ],
    "file": "codes/175.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dA_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, A_grad_ptr, locks_N_ptr, locks_V_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "tl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nlinear_xent_bwd_kernel_matmul_t_dA(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, A_grad_ptr, locks_N_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)\nlinear_xent_bwd_kernel_matmul_t_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, locks_V_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "locks_N_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    locks_N_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V, idx_N = tl.program_id(axis=0), tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "    x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "",
      "    mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "    for idx_H in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "",
      "        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "        temp_AgradT = (temp_Agrad.trans() / N).to(tl.float16)",
      "",
      "        while tl.atomic_cas(locks_N_ptr + idx_V, 0, 1) == 1:",
      "            pass",
      "        tl.store(A_grad_block_ptr, temp_AgradT + tl.load(A_grad_block_ptr))",
      "        tl.atomic_xchg(locks_N_ptr + idx_V, 0)",
      "",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])"
    ],
    "file": "codes/175.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dA(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, A_grad_ptr, locks_N_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_V, idx_N = (tl.program_id(axis=0), tl.program_id(axis=1))\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nA_grad_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\ny = tl.load(y_ptr + N_offsets)\nlse = tl.load(lse_global_ptr + N_offsets)\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nx_block_ptr = tl.advance(x_block_ptr, [0, -H])\nmask = (y[:, None] == V_offsets[None, :])[:, :, None]\nsoftmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\nfor idx_H in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)\n    temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)\n    temp_AgradT = (temp_Agrad.trans() / N).to(tl.float16)\n    while tl.atomic_cas(locks_N_ptr + idx_V, 0, 1) == 1:\n        pass\n    tl.store(A_grad_block_ptr, temp_AgradT + tl.load(A_grad_block_ptr))\n    tl.atomic_xchg(locks_N_ptr + idx_V, 0)\n    A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "locks_V_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    locks_V_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V, idx_N = tl.program_id(axis=0), tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "        x_block_ptr = tl.make_block_ptr(",
      "            base=x_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "    mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "    for idx_H in range(H // H_BLOCK_SIZE):",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        A_v = tl.load(A_block_ptr).trans()",
      "",
      "        temp_xgrad = tl.dot(softmax_z, A_v) / N",
      "        temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N",
      "",
      "        while tl.atomic_cas(locks_V_ptr + idx_N, 0, 1) == 1:",
      "            pass",
      "        tl.store(",
      "            x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16)",
      "        )",
      "        tl.atomic_xchg(locks_V_ptr + idx_N, 0)"
    ],
    "file": "codes/175.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_dx(x_ptr, y_ptr, A_t_ptr, lse_global_ptr, x_grad_ptr, locks_V_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr = 16, N_BLOCK_SIZE: tl.constexpr = 16, H_BLOCK_SIZE: tl.constexpr = 16):",
    "body": "idx_V, idx_N = (tl.program_id(axis=0), tl.program_id(axis=1))\ntl.static_assert(N % N_BLOCK_SIZE == 0)\ntl.static_assert(V % V_BLOCK_SIZE == 0)\ntl.static_assert(H % H_BLOCK_SIZE == 0)\nN_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_offsets)\nlse = tl.load(lse_global_ptr + N_offsets)\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor idx_H_1 in range(H // H_BLOCK_SIZE):\n    x_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\nmask = (y[:, None] == V_offsets[None, :])[:, :, None]\nsoftmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)\nfor idx_H in range(H // H_BLOCK_SIZE):\n    x_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    A_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\n    A_v = tl.load(A_block_ptr).trans()\n    temp_xgrad = tl.dot(softmax_z, A_v) / N\n    temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N\n    while tl.atomic_cas(locks_V_ptr + idx_N, 0, 1) == 1:\n        pass\n    tl.store(x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16))\n    tl.atomic_xchg(locks_V_ptr + idx_N, 0)"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])",
      "@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})",
      "@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "X1",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "B1",
        "annotation": null
      },
      {
        "name": "Y1",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "DROPOUT_MASK",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "stride_x1_row",
        "annotation": null
      },
      {
        "name": "stride_y1_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DROPOUT_MASK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_X1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_W1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    X1,",
      "    W1,",
      "    B1,",
      "    Y1,",
      "    RESIDUAL_OUT,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    DROPOUT_MASK,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    stride_x1_row,",
      "    stride_y1_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    STORE_DROPOUT_MASK: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_X1: tl.constexpr,",
      "    HAS_W1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "    if HAS_X1:",
      "        X1 += row * stride_x1_row",
      "    if HAS_W1:",
      "        Y1 += row * stride_y1_row",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_ROWSCALE:",
      "        rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "        x *= rowscale",
      "    if HAS_DROPOUT:",
      "",
      "        keep_mask = (",
      "            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p",
      "        )",
      "        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)",
      "        if STORE_DROPOUT_MASK:",
      "            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)",
      "    if HAS_X1:",
      "        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)",
      "            x1 *= rowscale",
      "        if HAS_DROPOUT:",
      "",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)",
      "            if STORE_DROPOUT_MASK:",
      "                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)",
      "        x += x1",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "",
      "    tl.store(Y + cols, y, mask=mask)",
      "    if HAS_W1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "        if HAS_B1:",
      "            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)",
      "        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1",
      "        tl.store(Y1 + cols, y1, mask=mask)"
    ],
    "file": "codes/124.py",
    "header": "def _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, X1, W1, B1, Y1, RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row, stride_y1_row, M, N, eps, dropout_p, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr, STORE_DROPOUT_MASK: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_X1: tl.constexpr, HAS_W1: tl.constexpr, HAS_B1: tl.constexpr):",
    "body": "row = tl.program_id(0)\nX += row * stride_x_row\nY += row * stride_y_row\nif HAS_RESIDUAL:\n    RESIDUAL += row * stride_res_row\nif STORE_RESIDUAL_OUT:\n    RESIDUAL_OUT += row * stride_res_out_row\nif HAS_X1:\n    X1 += row * stride_x1_row\nif HAS_W1:\n    Y1 += row * stride_y1_row\ncols = tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\nif HAS_ROWSCALE:\n    rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n    x *= rowscale\nif HAS_DROPOUT:\n    keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n    x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n    if STORE_DROPOUT_MASK:\n        tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\nif HAS_X1:\n    x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n        x1 *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n    x += x1\nif HAS_RESIDUAL:\n    residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x += residual\nif STORE_RESIDUAL_OUT:\n    tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\nif not IS_RMS_NORM:\n    mean = tl.sum(x, axis=0) / N\n    tl.store(Mean + row, mean)\n    xbar = tl.where(cols < N, x - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nelse:\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nmask = cols < N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif HAS_BIAS:\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\nx_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\ny = x_hat * w + b if HAS_BIAS else x_hat * w\ntl.store(Y + cols, y, mask=mask)\nif HAS_W1:\n    w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n    if HAS_B1:\n        b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n    y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n    tl.store(Y1 + cols, y1, mask=mask)"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])",
      "@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})",
      "@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})",
      "@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "DY1",
        "annotation": null
      },
      {
        "name": "DX1",
        "annotation": null
      },
      {
        "name": "DW1",
        "annotation": null
      },
      {
        "name": "DB1",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dy1_row",
        "annotation": null
      },
      {
        "name": "stride_dx1_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DY1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DX1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    W1,",
      "    DY1,",
      "    DX1,",
      "    DW1,",
      "    DB1,",
      "    DRESIDUAL_IN,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dy1_row,",
      "    stride_dx1_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_DY1: tl.constexpr,",
      "    HAS_DX1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if HAS_DY1:",
      "        DY1 += row_start * stride_dy1_row",
      "    if HAS_DX1:",
      "        DX1 += row_start * stride_dx1_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_DY1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_DY1:",
      "        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "        if HAS_B1:",
      "            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if HAS_DY1:",
      "            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_DY1:",
      "            wdy += w1 * dy1",
      "            dw1 += dy1 * xhat",
      "            if HAS_B1:",
      "                db1 += dy1",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        if HAS_DX1:",
      "            if HAS_DROPOUT:",
      "                keep_mask = (",
      "                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                    > dropout_p",
      "                )",
      "                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "            else:",
      "                dx1 = dx",
      "            tl.store(DX1 + cols, dx1, mask=mask)",
      "        if HAS_DROPOUT:",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "            dx *= rowscale",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "        if HAS_DY1:",
      "            DY1 += stride_dy1_row",
      "        if HAS_DX1:",
      "            DX1 += stride_dx1_row",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)",
      "    if HAS_DY1:",
      "        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)",
      "        if HAS_B1:",
      "            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)"
    ],
    "file": "codes/124.py",
    "header": "def _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, W1, DY1, DX1, DW1, DB1, DRESIDUAL_IN, ROWSCALE, SEEDS, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row, stride_dy1_row, stride_dx1_row, stride_dres_in_row, M, N, eps, dropout_p, rows_per_program, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_DY1: tl.constexpr, HAS_DX1: tl.constexpr, HAS_B1: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, BLOCK_N)\nmask = cols < N\nX += row_start * stride_x_row\nif HAS_DRESIDUAL:\n    DRESIDUAL += row_start * stride_dres_row\nif STORE_DRESIDUAL:\n    DRESIDUAL_IN += row_start * stride_dres_in_row\nDY += row_start * stride_dy_row\nDX += row_start * stride_dx_row\nif HAS_DY1:\n    DY1 += row_start * stride_dy1_row\nif HAS_DX1:\n    DX1 += row_start * stride_dx1_row\nif RECOMPUTE_OUTPUT:\n    Y += row_start * stride_y_row\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif RECOMPUTE_OUTPUT and HAS_BIAS:\n    b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\nif HAS_DY1:\n    w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\ndw = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_BIAS:\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_DY1:\n    dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_B1:\n        db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\nrow_end = min((row_block_id + 1) * rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    if HAS_DY1:\n        dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    if RECOMPUTE_OUTPUT:\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        tl.store(Y + cols, y, mask=mask)\n    wdy = w * dy\n    dw += dy * xhat\n    if HAS_BIAS:\n        db += dy\n    if HAS_DY1:\n        wdy += w1 * dy1\n        dw1 += dy1 * xhat\n        if HAS_B1:\n            db1 += dy1\n    if not IS_RMS_NORM:\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        c2 = tl.sum(wdy, axis=0) / N\n        dx = (wdy - (xhat * c1 + c2)) * rstd\n    else:\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        dx = (wdy - xhat * c1) * rstd\n    if HAS_DRESIDUAL:\n        dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n        dx += dres\n    if STORE_DRESIDUAL:\n        tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n    if HAS_DX1:\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n            dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        else:\n            dx1 = dx\n        tl.store(DX1 + cols, dx1, mask=mask)\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        dx *= rowscale\n    tl.store(DX + cols, dx, mask=mask)\n    X += stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += stride_dres_in_row\n    if RECOMPUTE_OUTPUT:\n        Y += stride_y_row\n    DY += stride_dy_row\n    DX += stride_dx_row\n    if HAS_DY1:\n        DY1 += stride_dy1_row\n    if HAS_DX1:\n        DX1 += stride_dx1_row\ntl.store(DW + row_block_id * N + cols, dw, mask=mask)\nif HAS_BIAS:\n    tl.store(DB + row_block_id * N + cols, db, mask=mask)\nif HAS_DY1:\n    tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n    if HAS_B1:\n        tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)"
  },
  {
    "name": "grpo_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': BLOCK_SIZE}, num_warps=NUM_WARPS, num_stages=NUM_STAGES) for BLOCK_SIZE in [1024, 2048, 4096, 8192] for NUM_WARPS in [8, 16, 32] for NUM_STAGES in [1, 2, 4]], key=['B', 'N'])"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "ref_logp_ptr",
        "annotation": null
      },
      {
        "name": "input_ids_ptr",
        "annotation": null
      },
      {
        "name": "advantages_ptr",
        "annotation": null
      },
      {
        "name": "completion_mask_ptr",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "save_kl",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "start_idx",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def grpo_fwd_kernel(",
      "    logits_ptr,",
      "    ref_logp_ptr,",
      "    input_ids_ptr,",
      "    advantages_ptr,",
      "    completion_mask_ptr,",
      "    loss_ptr,",
      "    lse_ptr,",
      "    beta,",
      "    save_kl: tl.constexpr,",
      "    B,",
      "    M,",
      "    N,",
      "    L,",
      "    start_idx,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "",
      "    off_b = row_idx // L",
      "    N = tl.cast(N, tl.int64)",
      "",
      "    loss_ptr += row_idx",
      "",
      "    completion_mask_ptr += row_idx",
      "    not_skip = tl.load(completion_mask_ptr).to(tl.int1)",
      "    if not_skip == 1:",
      "        ref_logp_ptr += row_idx",
      "        lse_ptr += row_idx",
      "        advantages_ptr += off_b",
      "        logits_ptr += N * (row_idx + off_b)",
      "        input_ids_ptr += row_idx + (off_b + 1) * start_idx",
      "        base_cols = tl.arange(0, BLOCK_SIZE)",
      "",
      "        m_i = -float(\"inf\")",
      "        l_i = 0.0",
      "        for start_n in tl.range(0, N, BLOCK_SIZE):",
      "            cols = start_n + base_cols",
      "            mask = cols < N",
      "            logits = tl.load(logits_ptr + cols, mask=mask, other=-float(\"inf\")).to(",
      "                tl.float32",
      "            )",
      "            m_ij = tl.max(logits)",
      "            new_m_i = tl.maximum(m_i, m_ij)",
      "            l_i = l_i * exp(m_i - new_m_i) + tl.sum(exp(logits - new_m_i))",
      "            m_i = new_m_i",
      "        lse = log(l_i) + m_i",
      "",
      "        idx = tl.load(input_ids_ptr)",
      "        x = tl.load(logits_ptr + idx).to(tl.float32)",
      "        advantage = tl.load(advantages_ptr).to(tl.float32)",
      "        ref_logp = tl.load(ref_logp_ptr)",
      "        logp = x - lse",
      "        diff = ref_logp - logp",
      "        kl = exp(diff) - diff - 1",
      "        loss = kl * beta - advantage",
      "",
      "        tl.store(loss_ptr, loss.to(loss_ptr.dtype.element_ty))",
      "        tl.store(lse_ptr, lse.to(lse_ptr.dtype.element_ty))",
      "        if save_kl:",
      "            tl.store(loss_ptr + M, kl.to(loss_ptr.dtype.element_ty))",
      "    else:",
      "",
      "        tl.store(loss_ptr, 0.0)",
      "        if save_kl:",
      "            tl.store(loss_ptr + M, 0.0)"
    ],
    "file": "codes/357.py",
    "header": "def grpo_fwd_kernel(logits_ptr, ref_logp_ptr, input_ids_ptr, advantages_ptr, completion_mask_ptr, loss_ptr, lse_ptr, beta, save_kl: tl.constexpr, B, M, N, L, start_idx, BLOCK_SIZE: tl.constexpr):",
    "body": "row_idx = tl.program_id(0)\noff_b = row_idx // L\nN = tl.cast(N, tl.int64)\nloss_ptr += row_idx\ncompletion_mask_ptr += row_idx\nnot_skip = tl.load(completion_mask_ptr).to(tl.int1)\nif not_skip == 1:\n    ref_logp_ptr += row_idx\n    lse_ptr += row_idx\n    advantages_ptr += off_b\n    logits_ptr += N * (row_idx + off_b)\n    input_ids_ptr += row_idx + (off_b + 1) * start_idx\n    base_cols = tl.arange(0, BLOCK_SIZE)\n    m_i = -float('inf')\n    l_i = 0.0\n    for start_n in tl.range(0, N, BLOCK_SIZE):\n        cols = start_n + base_cols\n        mask = cols < N\n        logits = tl.load(logits_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)\n        m_ij = tl.max(logits)\n        new_m_i = tl.maximum(m_i, m_ij)\n        l_i = l_i * exp(m_i - new_m_i) + tl.sum(exp(logits - new_m_i))\n        m_i = new_m_i\n    lse = log(l_i) + m_i\n    idx = tl.load(input_ids_ptr)\n    x = tl.load(logits_ptr + idx).to(tl.float32)\n    advantage = tl.load(advantages_ptr).to(tl.float32)\n    ref_logp = tl.load(ref_logp_ptr)\n    logp = x - lse\n    diff = ref_logp - logp\n    kl = exp(diff) - diff - 1\n    loss = kl * beta - advantage\n    tl.store(loss_ptr, loss.to(loss_ptr.dtype.element_ty))\n    tl.store(lse_ptr, lse.to(lse_ptr.dtype.element_ty))\n    if save_kl:\n        tl.store(loss_ptr + M, kl.to(loss_ptr.dtype.element_ty))\nelse:\n    tl.store(loss_ptr, 0.0)\n    if save_kl:\n        tl.store(loss_ptr + M, 0.0)"
  },
  {
    "name": "grpo_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=NUM_WARPS, num_stages=NUM_STAGES) for NUM_WARPS in [32] for NUM_STAGES in [4]], key=['B', 'N'])"
    ],
    "args": [
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "dlogits_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "ref_logp_ptr",
        "annotation": null
      },
      {
        "name": "input_ids_ptr",
        "annotation": null
      },
      {
        "name": "advantages_ptr",
        "annotation": null
      },
      {
        "name": "completion_mask_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "start_idx",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def grpo_bwd_kernel(",
      "    dloss_ptr,",
      "    dlogits_ptr,",
      "    logits_ptr,",
      "    ref_logp_ptr,",
      "    input_ids_ptr,",
      "    advantages_ptr,",
      "    completion_mask_ptr,",
      "    lse_ptr,",
      "    beta,",
      "    B,",
      "    N,",
      "    L,",
      "    start_idx,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    row_idx = tl.program_id(0)",
      "    off_b = row_idx // L",
      "",
      "    N = tl.cast(N, tl.int64)",
      "",
      "    dlogits_ptr += N * (row_idx + off_b)",
      "    base_cols = tl.arange(0, BLOCK_SIZE)",
      "    completion_mask_ptr += row_idx",
      "    not_skip = tl.load(completion_mask_ptr).to(tl.int1)",
      "",
      "    if not_skip == 1:",
      "        lse_ptr += row_idx",
      "        dloss_ptr += row_idx",
      "        advantages_ptr += off_b",
      "        ref_logp_ptr += row_idx",
      "        logits_ptr += N * (row_idx + off_b)",
      "        input_ids_ptr += row_idx + (off_b + 1) * start_idx",
      "        dloss = tl.load(dloss_ptr).to(tl.float32)",
      "        lse = tl.load(lse_ptr).to(tl.float32)",
      "        idx = tl.load(input_ids_ptr)",
      "        x = tl.load(logits_ptr + idx).to(tl.float32)",
      "        advantage = tl.load(advantages_ptr).to(tl.float32)",
      "        ref_logp = tl.load(ref_logp_ptr)",
      "",
      "        tl.debug_barrier()",
      "        logp = x - lse",
      "",
      "        dlogp = (beta * (-1.0 * exp(ref_logp - logp) + 1) - advantage) * dloss",
      "",
      "        for start_n in tl.range(0, N, BLOCK_SIZE):",
      "            cols = start_n + base_cols",
      "            mask = cols < N",
      "            logits = tl.load(logits_ptr + cols, mask=mask, other=-float(\"inf\")).to(",
      "                tl.float32",
      "            )",
      "            probs = exp(logits - lse)",
      "            dlogits = tl.where(cols == idx, 1 - probs, -probs) * dlogp",
      "",
      "            tl.store(",
      "                dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask",
      "            )",
      "    else:",
      "        dlogits = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "        for start_n in tl.range(0, N, BLOCK_SIZE):",
      "            cols = start_n + base_cols",
      "            mask = cols < N",
      "",
      "            tl.store(",
      "                dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask",
      "            )"
    ],
    "file": "codes/357.py",
    "header": "def grpo_bwd_kernel(dloss_ptr, dlogits_ptr, logits_ptr, ref_logp_ptr, input_ids_ptr, advantages_ptr, completion_mask_ptr, lse_ptr, beta, B, N, L, start_idx, BLOCK_SIZE: tl.constexpr):",
    "body": "row_idx = tl.program_id(0)\noff_b = row_idx // L\nN = tl.cast(N, tl.int64)\ndlogits_ptr += N * (row_idx + off_b)\nbase_cols = tl.arange(0, BLOCK_SIZE)\ncompletion_mask_ptr += row_idx\nnot_skip = tl.load(completion_mask_ptr).to(tl.int1)\nif not_skip == 1:\n    lse_ptr += row_idx\n    dloss_ptr += row_idx\n    advantages_ptr += off_b\n    ref_logp_ptr += row_idx\n    logits_ptr += N * (row_idx + off_b)\n    input_ids_ptr += row_idx + (off_b + 1) * start_idx\n    dloss = tl.load(dloss_ptr).to(tl.float32)\n    lse = tl.load(lse_ptr).to(tl.float32)\n    idx = tl.load(input_ids_ptr)\n    x = tl.load(logits_ptr + idx).to(tl.float32)\n    advantage = tl.load(advantages_ptr).to(tl.float32)\n    ref_logp = tl.load(ref_logp_ptr)\n    tl.debug_barrier()\n    logp = x - lse\n    dlogp = (beta * (-1.0 * exp(ref_logp - logp) + 1) - advantage) * dloss\n    for start_n in tl.range(0, N, BLOCK_SIZE):\n        cols = start_n + base_cols\n        mask = cols < N\n        logits = tl.load(logits_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)\n        probs = exp(logits - lse)\n        dlogits = tl.where(cols == idx, 1 - probs, -probs) * dlogp\n        tl.store(dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask)\nelse:\n    dlogits = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)\n    for start_n in tl.range(0, N, BLOCK_SIZE):\n        cols = start_n + base_cols\n        mask = cols < N\n        tl.store(dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask)"
  },
  {
    "name": "apply_gain_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size, num_warps in itertools.product([32, 64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=['n_audios', 'audio_len'])"
    ],
    "args": [
      {
        "name": "samples_ptr",
        "annotation": null
      },
      {
        "name": "amplitude_ratios_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "n_audios",
        "annotation": null
      },
      {
        "name": "audio_len",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def apply_gain_kernel(",
      "    samples_ptr,",
      "    amplitude_ratios_ptr,",
      "    output_ptr,",
      "    n_audios,",
      "    audio_len,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    audio_idx = tl.program_id(0)",
      "",
      "    if audio_idx >= n_audios:",
      "        return",
      "",
      "    gain = tl.load(amplitude_ratios_ptr + audio_idx)",
      "",
      "    for i in range(0, audio_len, BLOCK_SIZE):",
      "        sample_idx = i + tl.arange(0, BLOCK_SIZE)",
      "        mask = sample_idx < audio_len",
      "        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)",
      "        result = samples * gain",
      "        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)"
    ],
    "file": "codes/186.py",
    "header": "def apply_gain_kernel(samples_ptr, amplitude_ratios_ptr, output_ptr, n_audios, audio_len, BLOCK_SIZE: tl.constexpr):",
    "body": "audio_idx = tl.program_id(0)\nif audio_idx >= n_audios:\n    return\ngain = tl.load(amplitude_ratios_ptr + audio_idx)\nfor i in range(0, audio_len, BLOCK_SIZE):\n    sample_idx = i + tl.arange(0, BLOCK_SIZE)\n    mask = sample_idx < audio_len\n    samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)\n    result = samples * gain\n    tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)"
  },
  {
    "name": "rwkv_seq_mix_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}) for block_size in [128, 256, 512, 1024, 2048, 4096, 8192]], key=['hidden_dim'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "x_prev_ptr",
        "annotation": null
      },
      {
        "name": "mix_k_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "batch_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "token_length",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rwkv_seq_mix_kernel(",
      "    x_ptr,",
      "    x_prev_ptr,",
      "    mix_k_ptr,",
      "    output_ptr,",
      "    batch_size: tl.constexpr,",
      "    token_length,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    block_start = tl.program_id(0) * BLOCK_SIZE",
      "    block_idx = block_start + tl.arange(0, BLOCK_SIZE)[:]",
      "",
      "    total_seq_dim = token_length * hidden_dim",
      "    batch_idx = block_idx // total_seq_dim",
      "    seq_and_feat = block_idx % total_seq_dim",
      "    seq_idx = seq_and_feat // hidden_dim",
      "    feat_idx = seq_and_feat % hidden_dim",
      "",
      "    is_valid = (batch_idx < batch_size) & (seq_idx < token_length)",
      "",
      "    x_idx = batch_idx * total_seq_dim + seq_idx * hidden_dim + feat_idx",
      "",
      "    curr_x = tl.load(x_ptr + x_idx, mask=is_valid, other=0.0).to(tl.float32)",
      "    k_value = tl.load(mix_k_ptr + feat_idx).to(tl.float32)",
      "",
      "    is_first = seq_idx < 1",
      "    prev_state_idx = batch_idx * hidden_dim + feat_idx",
      "    prev_state = tl.load(",
      "        x_prev_ptr + prev_state_idx, mask=(is_first & is_valid), other=0.0",
      "    ).to(tl.float32)",
      "",
      "    prev_x_idx = x_idx - hidden_dim",
      "    prev_x = tl.load(x_ptr + prev_x_idx, mask=(~is_first & is_valid), other=0.0).to(",
      "        tl.float32",
      "    )",
      "",
      "    prev_value = tl.where(is_first, prev_state, prev_x)",
      "    state_diff = prev_value - curr_x",
      "    mixed = state_diff * k_value",
      "    result = tl.cast(",
      "        curr_x + mixed, dtype=output_ptr.dtype.element_ty, fp_downcast_rounding=\"rtne\"",
      "    )",
      "    tl.store(output_ptr + x_idx, result, mask=is_valid)"
    ],
    "file": "codes/419.py",
    "header": "def rwkv_seq_mix_kernel(x_ptr, x_prev_ptr, mix_k_ptr, output_ptr, batch_size: tl.constexpr, token_length, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "block_start = tl.program_id(0) * BLOCK_SIZE\nblock_idx = block_start + tl.arange(0, BLOCK_SIZE)[:]\ntotal_seq_dim = token_length * hidden_dim\nbatch_idx = block_idx // total_seq_dim\nseq_and_feat = block_idx % total_seq_dim\nseq_idx = seq_and_feat // hidden_dim\nfeat_idx = seq_and_feat % hidden_dim\nis_valid = (batch_idx < batch_size) & (seq_idx < token_length)\nx_idx = batch_idx * total_seq_dim + seq_idx * hidden_dim + feat_idx\ncurr_x = tl.load(x_ptr + x_idx, mask=is_valid, other=0.0).to(tl.float32)\nk_value = tl.load(mix_k_ptr + feat_idx).to(tl.float32)\nis_first = seq_idx < 1\nprev_state_idx = batch_idx * hidden_dim + feat_idx\nprev_state = tl.load(x_prev_ptr + prev_state_idx, mask=is_first & is_valid, other=0.0).to(tl.float32)\nprev_x_idx = x_idx - hidden_dim\nprev_x = tl.load(x_ptr + prev_x_idx, mask=~is_first & is_valid, other=0.0).to(tl.float32)\nprev_value = tl.where(is_first, prev_state, prev_x)\nstate_diff = prev_value - curr_x\nmixed = state_diff * k_value\nresult = tl.cast(curr_x + mixed, dtype=output_ptr.dtype.element_ty, fp_downcast_rounding='rtne')\ntl.store(output_ptr + x_idx, result, mask=is_valid)"
  },
  {
    "name": "rwkv_mix_bwd_kenel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}) for block_size in [128, 256, 512, 1024, 2048, 4096, 8192]], key=['hidden_dim'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "dk1_ptr0",
        "annotation": null
      },
      {
        "name": "xk_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "dx_prev_ptr",
        "annotation": null
      },
      {
        "name": "batch_size",
        "annotation": null
      },
      {
        "name": "token_length",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rwkv_mix_bwd_kenel(",
      "    dk1_ptr0,",
      "    xk_ptr,",
      "    dx_ptr,",
      "    dx_prev_ptr,",
      "    batch_size,",
      "    token_length,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "",
      "    batch_idx = offsets // (token_length * hidden_dim)",
      "    seq_feat = offsets % (token_length * hidden_dim)",
      "    seq_idx = seq_feat // hidden_dim",
      "    feat_idx = seq_feat % hidden_dim",
      "",
      "    is_valid = offsets < (batch_size * token_length * hidden_dim)",
      "",
      "    dk1 = tl.load(dk1_ptr0 + offsets, mask=is_valid)",
      "    xk = tl.load(xk_ptr + feat_idx, mask=is_valid)",
      "    prod = dk1 * xk",
      "",
      "    mask_next = seq_idx < (token_length - 1)",
      "    next_offset = offsets + hidden_dim",
      "    dk1_next = tl.load(dk1_ptr0 + next_offset, mask=mask_next & is_valid, other=0.0)",
      "    prod_next = dk1_next * xk",
      "    dx_val = dk1 - prod + tl.where(mask_next, prod_next, 0.0)",
      "    dx_val = tl.cast(dx_val, dtype=dx_ptr.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "    tl.store(dx_ptr + offsets, dx_val, mask=is_valid)",
      "",
      "    dx_prev_offset = batch_idx * hidden_dim + feat_idx",
      "    is_first_step = seq_idx == 0",
      "",
      "    tl.store(",
      "        dx_prev_ptr + dx_prev_offset,",
      "        tl.cast(prod, dtype=dx_prev_ptr.dtype.element_ty),",
      "        mask=is_first_step,",
      "    )"
    ],
    "file": "codes/419.py",
    "header": "def rwkv_mix_bwd_kenel(dk1_ptr0, xk_ptr, dx_ptr, dx_prev_ptr, batch_size, token_length, hidden_dim: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(0)\noffsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nbatch_idx = offsets // (token_length * hidden_dim)\nseq_feat = offsets % (token_length * hidden_dim)\nseq_idx = seq_feat // hidden_dim\nfeat_idx = seq_feat % hidden_dim\nis_valid = offsets < batch_size * token_length * hidden_dim\ndk1 = tl.load(dk1_ptr0 + offsets, mask=is_valid)\nxk = tl.load(xk_ptr + feat_idx, mask=is_valid)\nprod = dk1 * xk\nmask_next = seq_idx < token_length - 1\nnext_offset = offsets + hidden_dim\ndk1_next = tl.load(dk1_ptr0 + next_offset, mask=mask_next & is_valid, other=0.0)\nprod_next = dk1_next * xk\ndx_val = dk1 - prod + tl.where(mask_next, prod_next, 0.0)\ndx_val = tl.cast(dx_val, dtype=dx_ptr.dtype.element_ty, fp_downcast_rounding='rtne')\ntl.store(dx_ptr + offsets, dx_val, mask=is_valid)\ndx_prev_offset = batch_idx * hidden_dim + feat_idx\nis_first_step = seq_idx == 0\ntl.store(dx_prev_ptr + dx_prev_offset, tl.cast(prod, dtype=dx_prev_ptr.dtype.element_ty), mask=is_first_step)"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "mask_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    mask_ptr: tl.tensor,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    mask_block_ptr = tl.make_block_ptr(",
      "        mask_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,))",
      "        mask = tl.load(mask_block_ptr, boundary_check=(1,))",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        mask = tl.where(condition, mask, 1)",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        mask = tl.load(mask_block_ptr)",
      "",
      "    input = tl.where(mask > language.eps, float(\"-inf\"), input)",
      "    max = tl.max(input, 1)",
      "    numerator = tl.math.fast_expf(input - max)",
      "    output = numerator / tl.sum(numerator)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "codes/508.py",
    "header": "def forward(output_ptr: tl.tensor, input_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, y_stride: tl.int32, x_stride: tl.int32, mask_ptr: tl.tensor, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\ninput_block_ptr = tl.make_block_ptr(input_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nmask_block_ptr = tl.make_block_ptr(mask_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nif require_x_boundary_check:\n    input = tl.load(input_block_ptr, boundary_check=(1,))\n    mask = tl.load(mask_block_ptr, boundary_check=(1,))\n    condition = tl.arange(0, x_block_size) < x_size\n    mask = tl.where(condition, mask, 1)\nelse:\n    input = tl.load(input_block_ptr)\n    mask = tl.load(mask_block_ptr)\ninput = tl.where(mask > language.eps, float('-inf'), input)\nmax = tl.max(input, 1)\nnumerator = tl.math.fast_expf(input - max)\noutput = numerator / tl.sum(numerator)\nif require_x_boundary_check:\n    tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))\nelse:\n    tl.store(output_block_ptr, output.to(dtype))"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "delta_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    delta_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    delta_block_ptr = tl.make_block_ptr(",
      "        delta_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        output = tl.load(output_block_ptr, boundary_check=(1,))",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))",
      "    else:",
      "        output = tl.load(output_block_ptr)",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "    delta = tl.load(delta_block_ptr)",
      "    grad_input = output * (grad_output - delta)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))"
    ],
    "file": "codes/508.py",
    "header": "def backward(grad_input_ptr: tl.tensor, grad_output_ptr: tl.tensor, output_ptr: tl.tensor, delta_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, y_stride: tl.int32, x_stride: tl.int32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\ngrad_input_block_ptr = tl.make_block_ptr(grad_input_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\ngrad_output_block_ptr = tl.make_block_ptr(grad_output_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\ndelta_block_ptr = tl.make_block_ptr(delta_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\nif require_x_boundary_check:\n    output = tl.load(output_block_ptr, boundary_check=(1,))\n    grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))\nelse:\n    output = tl.load(output_block_ptr)\n    grad_output = tl.load(grad_output_block_ptr)\ndelta = tl.load(delta_block_ptr)\ngrad_input = output * (grad_output - delta)\nif require_x_boundary_check:\n    tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))\nelse:\n    tl.store(grad_input_block_ptr, grad_input.to(dtype))"
  },
  {
    "name": "backward_delta",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "delta_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_delta(",
      "    delta_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    delta_block_ptr = tl.make_block_ptr(",
      "        delta_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(",
      "            grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"",
      "        )",
      "        output = tl.load(output_block_ptr, boundary_check=(1,))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        output = tl.load(output_block_ptr)",
      "",
      "    delta = tl.sum(grad_output * output, 1)",
      "    tl.store(delta_block_ptr, delta.to(dtype))"
    ],
    "file": "codes/508.py",
    "header": "def backward_delta(delta_ptr: tl.tensor, grad_output_ptr: tl.tensor, output_ptr: tl.tensor, y_size: tl.int32, x_size: tl.int32, y_stride: tl.int32, x_stride: tl.int32, dtype: tl.constexpr, x_block_size: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "y_offset = tl.program_id(0)\ndelta_block_ptr = tl.make_block_ptr(delta_ptr, shape=(y_size,), strides=(1,), offsets=(y_offset,), block_shape=(1,), order=(0,))\ngrad_output_block_ptr = tl.make_block_ptr(grad_output_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\noutput_block_ptr = tl.make_block_ptr(output_ptr, shape=(y_size, x_size), strides=(y_stride, x_stride), offsets=(y_offset, 0), block_shape=(1, x_block_size), order=(1, 0))\nif require_x_boundary_check:\n    grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,), padding_option='zero')\n    output = tl.load(output_block_ptr, boundary_check=(1,))\nelse:\n    grad_output = tl.load(grad_output_block_ptr)\n    output = tl.load(output_block_ptr)\ndelta = tl.sum(grad_output * output, 1)\ntl.store(delta_block_ptr, delta.to(dtype))"
  },
  {
    "name": "recompute_w_u_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def recompute_w_u_fwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    w,",
      "    u,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_u = tl.dot(b_A.to(b_vb.dtype), b_vb, allow_tf32=False)",
      "        tl.store(p_u, (b_u).to(p_u.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_w = tl.dot(b_A.to(b_kb.dtype), b_kb, allow_tf32=False)",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/377.py",
    "header": "def recompute_w_u_fwd_kernel(k, v, beta, w, u, A, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_beta = tl.make_block_ptr(beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)\n    b_u = tl.dot(b_A.to(b_vb.dtype), b_vb, allow_tf32=False)\n    tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_w = tl.dot(b_A.to(b_kb.dtype), b_kb, allow_tf32=False)\n    tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "prepare_wy_repr_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_bwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    A,",
      "    dw,",
      "    du,",
      "    dk,",
      "    dv,",
      "    dbeta,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "    b_dbeta = tl.zeros([BT], dtype=tl.float32)",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_du = tl.make_block_ptr(",
      "            du + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_du = tl.load(p_du, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)",
      "        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)",
      "        b_dv = b_dv_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dv_beta * b_v, 1)",
      "",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_dw = tl.load(p_dw, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)",
      "        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)",
      "        b_dk = b_dk_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)",
      "    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)",
      "    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(",
      "        k.dtype.element_ty",
      "    )",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "",
      "        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)",
      "        b_dk += b_dk_beta * b_beta[:, None]",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_dbeta = tl.make_block_ptr(",
      "        dbeta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/377.py",
    "header": "def prepare_wy_repr_bwd_kernel(k, v, beta, A, dw, du, dk, dv, dbeta, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_beta = tl.make_block_ptr(beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_dbeta = tl.zeros([BT], dtype=tl.float32)\nb_dA = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_du = tl.make_block_ptr(du + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)\n    b_du = tl.load(p_du, boundary_check=(0, 1))\n    b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)\n    b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)\n    b_dv = b_dv_beta * b_beta[:, None]\n    b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dw = tl.make_block_ptr(dw + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_dw = tl.load(p_dw, boundary_check=(0, 1))\n    b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)\n    b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)\n    b_dk = b_dk_beta * b_beta[:, None]\n    b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\nb_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)\nb_dA = tl.dot(b_dA.to(b_A.dtype), b_A)\nb_dA = tl.dot(b_A, b_dA.to(b_A.dtype))\nb_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(k.dtype.element_ty)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.load(p_dk, boundary_check=(0, 1))\n    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n    b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)\n    b_dk += b_dk_beta * b_beta[:, None]\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\np_dbeta = tl.make_block_ptr(dbeta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\ntl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_dplr_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "bg",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_fwd_kernel_h(",
      "    kg,",
      "    v,",
      "    w,",
      "    bg,",
      "    u,",
      "    v_new,",
      "    gk,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_hc = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):",
      "            p_kg = tl.make_block_ptr(",
      "                kg + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_bg = tl.make_block_ptr(",
      "                bg + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_v = tl.make_block_ptr(",
      "                v + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_u = tl.make_block_ptr(",
      "                u + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "",
      "            b_kg = tl.load(p_kg, boundary_check=(0, 1))",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_bg = tl.load(p_bg, boundary_check=(0, 1))",
      "            b_v2 = tl.dot(b_w, b_h.to(b_w.dtype)) + tl.load(p_u, boundary_check=(0, 1))",
      "            b_hc += tl.dot(b_kg, b_v)",
      "            b_hc += tl.dot(b_bg.to(b_hc.dtype), b_v2)",
      "            tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "        b_g_last = tl.load(",
      "            gk + (bos + last_idx) * H * K + i_h * K + o_k, mask=o_k < K",
      "        ).to(tl.float32)",
      "        b_h *= exp(b_g_last[:, None])",
      "        b_h += b_hc",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(",
      "            p_ht,",
      "            b_h.to(p_ht.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            boundary_check=(0, 1),",
      "        )"
    ],
    "file": "codes/384.py",
    "header": "def chunk_dplr_fwd_kernel_h(kg, v, w, bg, u, v_new, gk, h, h0, ht, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\no_k = i_k * BK + tl.arange(0, BK)\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(NT):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_hc = tl.zeros([BK, BV], dtype=tl.float32)\n    for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):\n        p_kg = tl.make_block_ptr(kg + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n        p_bg = tl.make_block_ptr(bg + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_c * BC), (BK, BC), (0, 1))\n        p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_c * BC, i_k * BK), (BC, BK), (1, 0))\n        p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        p_v_new = tl.make_block_ptr(v_new + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT + i_c * BC, i_v * BV), (BC, BV), (1, 0))\n        b_kg = tl.load(p_kg, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_w = tl.load(p_w, boundary_check=(0, 1))\n        b_bg = tl.load(p_bg, boundary_check=(0, 1))\n        b_v2 = tl.dot(b_w, b_h.to(b_w.dtype)) + tl.load(p_u, boundary_check=(0, 1))\n        b_hc += tl.dot(b_kg, b_v)\n        b_hc += tl.dot(b_bg.to(b_hc.dtype), b_v2)\n        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))\n    last_idx = min((i_t + 1) * BT, T) - 1\n    b_g_last = tl.load(gk + (bos + last_idx) * H * K + i_h * K + o_k, mask=o_k < K).to(tl.float32)\n    b_h *= exp(b_g_last[:, None])\n    b_h += b_hc\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "_chunk_cumsum_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}), triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64})], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "dt_out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_out_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_fwd_kernel(",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    dt_out_ptr,",
      "    dA_cumsum_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_dt_out_batch,",
      "    stride_dt_out_chunk,",
      "    stride_dt_out_head,",
      "    stride_dt_out_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk",
      "    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    dt_out_ptrs = dt_out_ptr + (",
      "        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize",
      "    )",
      "    dA_cs_ptrs = dA_cumsum_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize",
      "    )",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt = softplus(dt)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    tl.store(",
      "        dt_out_ptrs,",
      "        dt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    dA = dt * A[:, None]",
      "    dA_cs = tl.cumsum(dA, axis=1)",
      "    tl.store(",
      "        dA_cs_ptrs,",
      "        dA_cs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_cumsum_fwd_kernel(dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr, dA_cumsum_ptr, batch, seqlen, nheads, chunk_size, dt_min, dt_max, stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head, stride_dt_bias_head, stride_dt_out_batch, stride_dt_out_chunk, stride_dt_out_head, stride_dt_out_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, DT_SOFTPLUS: tl.constexpr, HAS_DT_BIAS: tl.constexpr, BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_CHUNK: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=0)\npid_c = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\ndt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\ndt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk\noffs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\noffs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\ndt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen)\nA_ptrs = A_ptr + offs_h * stride_A_head\ndt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize)\ndA_cs_ptrs = dA_cumsum_ptr + (offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nif HAS_DT_BIAS:\n    dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dt += dt_bias[:, None]\nif DT_SOFTPLUS:\n    dt = softplus(dt)\ndt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\ndt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0)\ntl.store(dt_out_ptrs, dt, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\nA = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\ndA = dt * A[:, None]\ndA_cs = tl.cumsum(dA, axis=1)\ntl.store(dA_cs_ptrs, dA_cs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))"
  },
  {
    "name": "_chunk_cumsum_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 2}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 4}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 8}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 16}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 32}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 64}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr']))], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "ddA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_bias_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_ddA_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_dA_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_bias_head",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_bwd_kernel(",
      "    ddA_ptr,",
      "    ddt_out_ptr,",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    ddt_ptr,",
      "    dA_ptr,",
      "    ddt_bias_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_ddA_batch,",
      "    stride_ddA_chunk,",
      "    stride_ddA_head,",
      "    stride_ddA_csize,",
      "    stride_ddt_out_batch,",
      "    stride_ddt_out_chunk,",
      "    stride_ddt_out_head,",
      "    stride_ddt_out_csize,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_ddt_batch,",
      "    stride_ddt_seqlen,",
      "    stride_ddt_head,",
      "    stride_dA_head,",
      "    stride_ddt_bias_head,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk",
      "    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    ddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    ddt_out_ptrs = ddt_out_ptr + (",
      "        offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize",
      "    )",
      "    ddA_ptrs = ddA_ptr + (",
      "        offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize",
      "    )",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    ddt_ptrs = ddt_ptr + (",
      "        offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    ddA = tl.load(",
      "        ddA_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt_out = tl.load(",
      "        ddt_out_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    ddt = ddA * A[:, None] + ddt_out",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt_presoftplus = dt",
      "        dt = softplus(dt)",
      "    clamp_mask = (dt < dt_min) | (dt > dt_max)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    ddt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0",
      "    )",
      "    ddt = tl.where(clamp_mask, 0.0, ddt)",
      "    if DT_SOFTPLUS:",
      "        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)",
      "    tl.store(",
      "        ddt_ptrs,",
      "        ddt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "    )",
      "    dA = tl.sum(ddA * dt, axis=1)",
      "    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)",
      "    if HAS_DT_BIAS:",
      "        ddt_bias = tl.sum(ddt, axis=1)",
      "        tl.atomic_add(",
      "            ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads",
      "        )"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_cumsum_bwd_kernel(ddA_ptr, ddt_out_ptr, dt_ptr, A_ptr, dt_bias_ptr, ddt_ptr, dA_ptr, ddt_bias_ptr, batch, seqlen, nheads, chunk_size, dt_min, dt_max, stride_ddA_batch, stride_ddA_chunk, stride_ddA_head, stride_ddA_csize, stride_ddt_out_batch, stride_ddt_out_chunk, stride_ddt_out_head, stride_ddt_out_csize, stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head, stride_dt_bias_head, stride_ddt_batch, stride_ddt_seqlen, stride_ddt_head, stride_dA_head, stride_ddt_bias_head, DT_SOFTPLUS: tl.constexpr, HAS_DT_BIAS: tl.constexpr, BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_CHUNK: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=0)\npid_c = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\nddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk\nddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk\ndt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\nddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen\noffs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\noffs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\nddt_out_ptrs = ddt_out_ptr + (offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize)\nddA_ptrs = ddA_ptr + (offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize)\ndt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen)\nddt_ptrs = ddt_ptr + (offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen)\nA_ptrs = A_ptr + offs_h * stride_A_head\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nddA = tl.load(ddA_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nddt_out = tl.load(ddt_out_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nA = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\nddt = ddA * A[:, None] + ddt_out\ndt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nif HAS_DT_BIAS:\n    dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dt += dt_bias[:, None]\nif DT_SOFTPLUS:\n    dt_presoftplus = dt\n    dt = softplus(dt)\nclamp_mask = (dt < dt_min) | (dt > dt_max)\ndt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\ndt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0)\nddt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0)\nddt = tl.where(clamp_mask, 0.0, ddt)\nif DT_SOFTPLUS:\n    ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)\ntl.store(ddt_ptrs, ddt, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit))\ndA = tl.sum(ddA * dt, axis=1)\ntl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)\nif HAS_DT_BIAS:\n    ddt_bias = tl.sum(ddt, axis=1)\n    tl.atomic_add(ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads)"
  },
  {
    "name": "_chunk_state_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_fwd_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    states_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    if not REVERSE:",
      "        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "            tl.float32",
      "        )",
      "    else:",
      "        dA_cs_last = tl.load(dA_cumsum_ptr).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(",
      "            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_k = tl.load(",
      "                seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1",
      "            )",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp((dA_cs_last - dA_cs_k)) * dt_k",
      "        else:",
      "            scale = tl.where(",
      "                seq_idx_k == seq_idx_last, tl.exp((dA_cs_last - dA_cs_k)) * dt_k, 0.0",
      "            )",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (",
      "        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_state_fwd_kernel(x_ptr, b_ptr, states_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, hdim, dstate, chunk_size, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_states_batch, stride_states_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, HAS_SEQ_IDX: tl.constexpr, REVERSE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen)\nb_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen)\ndt_ptrs = dt_ptr + offs_k * stride_dt_csize\nif not REVERSE:\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\nelse:\n    dA_cs_last = tl.load(dA_cumsum_ptr).to(tl.float32)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nif HAS_SEQ_IDX:\n    seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nif HAS_SEQ_IDX:\n    seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k), other=0.0)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    if HAS_SEQ_IDX:\n        seq_idx_k = tl.load(seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1)\n    dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_k) * dt_k\n    else:\n        scale = tl.where(seq_idx_k == seq_idx_last, tl.exp(dA_cs_last - dA_cs_k) * dt_k, 0.0)\n    b *= scale[:, None]\n    b = b.to(x_ptr.dtype.element_ty)\n    acc += tl.dot(x, b)\n    x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n    b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n    dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\n    if HAS_SEQ_IDX:\n        seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen\nstates = acc.to(states_ptr.dtype.element_ty)\nstates_ptr += pid_b * stride_states_batch + pid_c * stride_states_chunk + pid_h * stride_states_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nstates_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate)\nc_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\ntl.store(states_ptrs, states, mask=c_mask)"
  },
  {
    "name": "_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)",
      "    ddA_cs = -(ddt * dt_m)",
      "    ddA_cs_last = -tl.sum(ddA_cs)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)",
      "    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)",
      "",
      "    dx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_state_bwd_dx_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr, dA_cumsum_ptr, dx_ptr, ddt_ptr, ddA_cumsum_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch, stride_dstates_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_dx_batch, stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\ndstates_ptr += pid_b * stride_dstates_batch + pid_c * stride_dstates_chunk + pid_h * stride_states_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\noffs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\nb_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate)\ndstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate)\nif BLOCK_SIZE_DSTATE <= 128:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate), other=0.0)\n    dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates = dstates.to(b_ptr.dtype.element_ty)\n    acc = tl.dot(b, dstates)\nelse:\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, dstate, BLOCK_SIZE_K):\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate - k), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates)\n        b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n        dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\ndA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nacc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nddt = tl.sum(acc * x, axis=1)\nddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\ntl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\nddA_cs = -(ddt * dt_m)\nddA_cs_last = -tl.sum(ddA_cs)\nddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\ntl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\ntl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)\ndx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)\ndx_ptr += pid_b * stride_dx_batch + pid_c * chunk_size * stride_dx_seqlen + pid_h * stride_dx_head\ndx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim)\ntl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))"
  },
  {
    "name": "_chunk_state_bwd_db_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_split",
        "annotation": null
      },
      {
        "name": "stride_db_group",
        "annotation": null
      },
      {
        "name": "stride_db_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_db_kernel(",
      "    x_ptr,",
      "    dstates_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    db_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_split,",
      "    stride_db_group,",
      "    stride_db_dstate,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_g * stride_db_group",
      "        + pid_s * stride_db_split",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_states_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        b_ptr += (",
      "            pid_b * stride_b_batch",
      "            + pid_c * chunk_size * stride_b_seqlen",
      "            + pid_g * stride_b_head",
      "        )",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_dstate + offs_k[:, None] * stride_states_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    if HAS_DDA_CS:",
      "        b_ptrs = b_ptr + (",
      "            offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate",
      "        )",
      "        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(x_ptrs.dtype.element_ty)",
      "        db = tl.dot(x, dstates)",
      "        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "            tl.float32",
      "        )",
      "        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp(dA_cs_last - dA_cs_m)",
      "        else:",
      "            scale = tl.where(",
      "                seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0",
      "            )",
      "        db *= (scale * dt_m)[:, None]",
      "        if HAS_DDA_CS:",
      "",
      "            ddA_cs = tl.sum(db * b, axis=1)",
      "            tl.atomic_add(",
      "                ddA_cumsum_ptrs + stride_ddA_cs_csize,",
      "                ddA_cs,",
      "                mask=offs_m < chunk_size - 1,",
      "            )",
      "        acc += db",
      "        x_ptrs += stride_x_head",
      "        dstates_ptrs += stride_states_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_ptr += stride_dA_cs_head",
      "        dA_cumsum_ptrs += stride_dA_cs_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_ptrs += stride_ddA_cs_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate",
      "    )",
      "    tl.store(",
      "        db_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_state_bwd_db_kernel(x_ptr, dstates_ptr, b_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, db_ptr, ddA_cumsum_ptr, chunk_size, dstate, hdim, batch, seqlen, nheads, nheads_per_program, ngroups, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dstates_batch, stride_dstates_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_db_batch, stride_db_seqlen, stride_db_split, stride_db_group, stride_db_dstate, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_DDA_CS: tl.constexpr, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_sg = tl.program_id(axis=2)\npid_s = pid_sg // ngroups\npid_g = pid_sg - pid_s * ngroups\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\ndb_ptr += pid_b * stride_db_batch + pid_c * chunk_size * stride_db_seqlen + pid_g * stride_db_group + pid_s * stride_db_split\ndstates_ptr += pid_b * stride_dstates_batch + pid_c * stride_dstates_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_states_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head\nif HAS_DDA_CS:\n    b_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_g * stride_b_head\n    ddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim)\ndstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_dstate + offs_k[:, None] * stride_states_hdim)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\nif HAS_DDA_CS:\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate)\n    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif HAS_DDA_CS:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\nif HAS_SEQ_IDX:\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen)\nnheads_iter = min(nheads_per_program, nheads // ngroups - pid_s * nheads_per_program)\nfor h in range(nheads_iter):\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\n    dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\n    dstates = dstates.to(x_ptrs.dtype.element_ty)\n    db = tl.dot(x, dstates)\n    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\n    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    if not HAS_SEQ_IDX:\n        scale = tl.exp(dA_cs_last - dA_cs_m)\n    else:\n        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\n    db *= (scale * dt_m)[:, None]\n    if HAS_DDA_CS:\n        ddA_cs = tl.sum(db * b, axis=1)\n        tl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1)\n    acc += db\n    x_ptrs += stride_x_head\n    dstates_ptrs += stride_states_head\n    dt_ptrs += stride_dt_head\n    dA_cumsum_ptr += stride_dA_cs_head\n    dA_cumsum_ptrs += stride_dA_cs_head\n    if HAS_DDA_CS:\n        ddA_cumsum_ptrs += stride_ddA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndb_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate)\ntl.store(db_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate))"
  },
  {
    "name": "_chunk_state_bwd_ddAcs_stable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_ddAcs_stable_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_last - dA_cs_m)",
      "    else:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)",
      "    acc *= scale[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "",
      "    ddA_cs = ddt * dt_m",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    tl.atomic_add(",
      "        ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1",
      "    )"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_state_bwd_ddAcs_stable_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch, stride_dstates_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\ndstates_ptr += pid_b * stride_dstates_batch + pid_c * stride_dstates_chunk + pid_h * stride_states_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\noffs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\nb_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate)\ndstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate)\nif BLOCK_SIZE_DSTATE <= 128:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate), other=0.0)\n    dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates = dstates.to(b_ptr.dtype.element_ty)\n    acc = tl.dot(b, dstates)\nelse:\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, dstate, BLOCK_SIZE_K):\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate - k), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates)\n        b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n        dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\nif not HAS_SEQ_IDX:\n    scale = tl.exp(dA_cs_last - dA_cs_m)\nelse:\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen)\n    scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\nacc *= scale[:, None]\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nddt = tl.sum(acc * x, axis=1)\nddA_cs = ddt * dt_m\nddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\ntl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1)"
  },
  {
    "name": "_chunk_state_varlen_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_states_ptr",
        "annotation": null
      },
      {
        "name": "cu_seqlens_ptr",
        "annotation": null
      },
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_head",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_varlen_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    chunk_states_ptr,",
      "    cu_seqlens_ptr,",
      "    states_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_chunk_states_chunk,",
      "    stride_chunk_states_head,",
      "    stride_chunk_states_hdim,",
      "    stride_chunk_states_dstate,",
      "    stride_states_batch,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)",
      "    pid_c = (end_idx - 1) // chunk_size",
      "    b_ptr += (",
      "        pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    chunk_states_ptr += (",
      "        pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_last = tl.load(",
      "        dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize",
      "    ).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = end_idx - pid_c * chunk_size",
      "    start_idx = tl.load(cu_seqlens_ptr + pid_b)",
      "    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim)",
      "            & (offs_k[None, :] < chunk_size_limit - k)",
      "            & (offs_k[None, :] >= start_idx_cur - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k)",
      "            & (offs_n[None, :] < dstate)",
      "            & (offs_k[:, None] >= start_idx_cur - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(",
      "            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale = tl.where(",
      "            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),",
      "            tl.exp((dA_cs_last - dA_cs_k)) * dt_k,",
      "            0.0,",
      "        )",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    if start_idx < pid_c * chunk_size:",
      "        chunk_states_ptrs = chunk_states_ptr + (",
      "            offs_m[:, None] * stride_chunk_states_hdim",
      "            + offs_n[None, :] * stride_chunk_states_dstate",
      "        )",
      "        chunk_states = tl.load(",
      "            chunk_states_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "",
      "        scale = tl.exp(dA_cs_last)",
      "        acc += chunk_states * scale",
      "",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (",
      "        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)"
    ],
    "file": "codes/130.py",
    "header": "def _chunk_state_varlen_kernel(x_ptr, b_ptr, dt_ptr, dA_cumsum_ptr, chunk_states_ptr, cu_seqlens_ptr, states_ptr, hdim, dstate, chunk_size, seqlen, nheads_ngroups_ratio, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_chunk_states_chunk, stride_chunk_states_head, stride_chunk_states_hdim, stride_chunk_states_dstate, stride_states_batch, stride_states_head, stride_states_hdim, stride_states_dstate, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nend_idx = tl.load(cu_seqlens_ptr + pid_b + 1)\npid_c = (end_idx - 1) // chunk_size\nb_ptr += pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\nx_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nchunk_states_ptr += pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen)\nb_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen)\ndt_ptrs = dt_ptr + offs_k * stride_dt_csize\ndA_cs_last = tl.load(dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nchunk_size_limit = end_idx - pid_c * chunk_size\nstart_idx = tl.load(cu_seqlens_ptr + pid_b)\nstart_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k) & (offs_k[None, :] >= start_idx_cur - k), other=0.0)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate) & (offs_k[:, None] >= start_idx_cur - k), other=0.0).to(tl.float32)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    scale = tl.where((offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k), tl.exp(dA_cs_last - dA_cs_k) * dt_k, 0.0)\n    b *= scale[:, None]\n    b = b.to(x_ptr.dtype.element_ty)\n    acc += tl.dot(x, b)\n    x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n    b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n    dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\nif start_idx < pid_c * chunk_size:\n    chunk_states_ptrs = chunk_states_ptr + (offs_m[:, None] * stride_chunk_states_hdim + offs_n[None, :] * stride_chunk_states_dstate)\n    chunk_states = tl.load(chunk_states_ptrs, mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    scale = tl.exp(dA_cs_last)\n    acc += chunk_states * scale\nstates = acc.to(states_ptr.dtype.element_ty)\nstates_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nstates_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate)\nc_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\ntl.store(states_ptrs, states, mask=c_mask)"
  },
  {
    "name": "_chunk_cumsum_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}), triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64})], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "dt_out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_out_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_fwd_kernel(",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    dt_out_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_dt_out_batch,",
      "    stride_dt_out_chunk,",
      "    stride_dt_out_head,",
      "    stride_dt_out_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk",
      "    dA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk",
      "    dA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    dt_out_ptrs = dt_out_ptr + (",
      "        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize",
      "    )",
      "    dA_cs_f_ptrs = dA_cumsum_f_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_f_head + offs_c[None, :] * stride_dA_cs_f_csize",
      "    )",
      "    dA_cs_b_ptrs = dA_cumsum_b_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_b_head + offs_c[None, :] * stride_dA_cs_b_csize",
      "    )",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt = softplus(dt)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    tl.store(",
      "        dt_out_ptrs,",
      "        dt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    dA = dt * A[:, None]",
      "    dA_cs_f = tl.cumsum(dA, axis=1)",
      "    tl.store(",
      "        dA_cs_f_ptrs,",
      "        dA_cs_f,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "",
      "    dA_cs_b = tl.flip(tl.cumsum(tl.flip(dA, dim=1), axis=1))",
      "",
      "    tl.store(",
      "        dA_cs_b_ptrs,",
      "        dA_cs_b,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_cumsum_fwd_kernel(dt_ptr, A_ptr, dt_bias_ptr, dt_out_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, batch, seqlen, nheads, chunk_size, dt_min, dt_max, stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head, stride_dt_bias_head, stride_dt_out_batch, stride_dt_out_chunk, stride_dt_out_head, stride_dt_out_csize, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, DT_SOFTPLUS: tl.constexpr, HAS_DT_BIAS: tl.constexpr, BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_CHUNK: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=0)\npid_c = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\ndt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\ndt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk\noffs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\noffs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\ndt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen)\nA_ptrs = A_ptr + offs_h * stride_A_head\ndt_out_ptrs = dt_out_ptr + (offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize)\ndA_cs_f_ptrs = dA_cumsum_f_ptr + (offs_h[:, None] * stride_dA_cs_f_head + offs_c[None, :] * stride_dA_cs_f_csize)\ndA_cs_b_ptrs = dA_cumsum_b_ptr + (offs_h[:, None] * stride_dA_cs_b_head + offs_c[None, :] * stride_dA_cs_b_csize)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\ndt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nif HAS_DT_BIAS:\n    dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dt += dt_bias[:, None]\nif DT_SOFTPLUS:\n    dt = softplus(dt)\ndt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\ndt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0)\ntl.store(dt_out_ptrs, dt, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\nA = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\ndA = dt * A[:, None]\ndA_cs_f = tl.cumsum(dA, axis=1)\ntl.store(dA_cs_f_ptrs, dA_cs_f, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))\ndA_cs_b = tl.flip(tl.cumsum(tl.flip(dA, dim=1), axis=1))\ntl.store(dA_cs_b_ptrs, dA_cs_b, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size))"
  },
  {
    "name": "_chunk_cumsum_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 2}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 4}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 8}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 16}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 32}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 64}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr']))], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "ddA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_bias_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_ddA_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_dA_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_bias_head",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_bwd_kernel(",
      "    ddA_ptr,",
      "    ddt_out_ptr,",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    ddt_ptr,",
      "    dA_ptr,",
      "    ddt_bias_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_ddA_batch,",
      "    stride_ddA_chunk,",
      "    stride_ddA_head,",
      "    stride_ddA_csize,",
      "    stride_ddt_out_batch,",
      "    stride_ddt_out_chunk,",
      "    stride_ddt_out_head,",
      "    stride_ddt_out_csize,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_ddt_batch,",
      "    stride_ddt_seqlen,",
      "    stride_ddt_head,",
      "    stride_dA_head,",
      "    stride_ddt_bias_head,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk",
      "    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    ddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    ddt_out_ptrs = ddt_out_ptr + (",
      "        offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize",
      "    )",
      "    ddA_ptrs = ddA_ptr + (",
      "        offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize",
      "    )",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    ddt_ptrs = ddt_ptr + (",
      "        offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    ddA = tl.load(",
      "        ddA_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt_out = tl.load(",
      "        ddt_out_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    ddt = ddA * A[:, None] + ddt_out",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt_presoftplus = dt",
      "        dt = softplus(dt)",
      "    clamp_mask = (dt < dt_min) | (dt > dt_max)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    ddt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0",
      "    )",
      "    ddt = tl.where(clamp_mask, 0.0, ddt)",
      "    if DT_SOFTPLUS:",
      "        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)",
      "    tl.store(",
      "        ddt_ptrs,",
      "        ddt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "    )",
      "    dA = tl.sum(ddA * dt, axis=1)",
      "    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)",
      "    if HAS_DT_BIAS:",
      "        ddt_bias = tl.sum(ddt, axis=1)",
      "        tl.atomic_add(",
      "            ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads",
      "        )"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_cumsum_bwd_kernel(ddA_ptr, ddt_out_ptr, dt_ptr, A_ptr, dt_bias_ptr, ddt_ptr, dA_ptr, ddt_bias_ptr, batch, seqlen, nheads, chunk_size, dt_min, dt_max, stride_ddA_batch, stride_ddA_chunk, stride_ddA_head, stride_ddA_csize, stride_ddt_out_batch, stride_ddt_out_chunk, stride_ddt_out_head, stride_ddt_out_csize, stride_dt_batch, stride_dt_seqlen, stride_dt_head, stride_A_head, stride_dt_bias_head, stride_ddt_batch, stride_ddt_seqlen, stride_ddt_head, stride_dA_head, stride_ddt_bias_head, DT_SOFTPLUS: tl.constexpr, HAS_DT_BIAS: tl.constexpr, BLOCK_SIZE_H: tl.constexpr, BLOCK_SIZE_CHUNK: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=0)\npid_c = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\nddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk\nddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk\ndt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen\nddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen\noffs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)\noffs_c = tl.arange(0, BLOCK_SIZE_CHUNK)\nddt_out_ptrs = ddt_out_ptr + (offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize)\nddA_ptrs = ddA_ptr + (offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize)\ndt_ptrs = dt_ptr + (offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen)\nddt_ptrs = ddt_ptr + (offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen)\nA_ptrs = A_ptr + offs_h * stride_A_head\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nddA = tl.load(ddA_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nddt_out = tl.load(ddt_out_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nA = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)\nddt = ddA * A[:, None] + ddt_out\ndt = tl.load(dt_ptrs, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), other=0.0).to(tl.float32)\nif HAS_DT_BIAS:\n    dt_bias = tl.load(dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0).to(tl.float32)\n    dt += dt_bias[:, None]\nif DT_SOFTPLUS:\n    dt_presoftplus = dt\n    dt = softplus(dt)\nclamp_mask = (dt < dt_min) | (dt > dt_max)\ndt = tl.minimum(tl.maximum(dt, dt_min), dt_max)\ndt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0)\nddt = tl.where((offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0)\nddt = tl.where(clamp_mask, 0.0, ddt)\nif DT_SOFTPLUS:\n    ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)\ntl.store(ddt_ptrs, ddt, mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit))\ndA = tl.sum(ddA * dt, axis=1)\ntl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)\nif HAS_DT_BIAS:\n    ddt_bias = tl.sum(ddt, axis=1)\n    tl.atomic_add(ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads)"
  },
  {
    "name": "_chunk_state_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "states_f_ptr",
        "annotation": null
      },
      {
        "name": "states_b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_fwd_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    states_f_ptr,",
      "    states_b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_states_f_batch,",
      "    stride_states_f_chunk,",
      "    stride_states_f_head,",
      "    stride_states_f_hdim,",
      "    stride_states_f_dstate,",
      "    stride_states_b_batch,",
      "    stride_states_b_chunk,",
      "    stride_states_b_head,",
      "    stride_states_b_hdim,",
      "    stride_states_b_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_f_last = tl.load(",
      "        dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize",
      "    ).to(tl.float32)",
      "    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    acc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_f_k = tl.load(",
      "            dA_cumsum_f_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dA_cs_b_k = tl.load(",
      "            dA_cumsum_b_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale_f = tl.exp((dA_cs_f_last - dA_cs_f_k)) * dt_k",
      "        scale_b = tl.exp((dA_cs_b_last - dA_cs_b_k)) * dt_k",
      "        b_f = b * scale_f[:, None]",
      "        b_b = b * scale_b[:, None]",
      "        b_f = b_f.to(x_ptr.dtype.element_ty)",
      "        b_b = b_b.to(x_ptr.dtype.element_ty)",
      "        acc_f += tl.dot(x, b_f)",
      "        acc_b += tl.dot(x, b_b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "    states_f = acc_f.to(states_f_ptr.dtype.element_ty)",
      "    states_b = acc_b.to(states_b_ptr.dtype.element_ty)",
      "",
      "    states_f_ptr += (",
      "        pid_b * stride_states_f_batch",
      "        + pid_c * stride_states_f_chunk",
      "        + pid_h * stride_states_f_head",
      "    )",
      "    states_b_ptr += (",
      "        pid_b * stride_states_b_batch",
      "        + pid_c * stride_states_b_chunk",
      "        + pid_h * stride_states_b_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_f_ptrs = states_f_ptr + (",
      "        offs_m[:, None] * stride_states_f_hdim",
      "        + offs_n[None, :] * stride_states_f_dstate",
      "    )",
      "    states_b_ptrs = states_b_ptr + (",
      "        offs_m[:, None] * stride_states_b_hdim",
      "        + offs_n[None, :] * stride_states_b_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_f_ptrs, states_f, mask=c_mask)",
      "    tl.store(states_b_ptrs, states_b, mask=c_mask)"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_state_fwd_kernel(x_ptr, b_ptr, states_f_ptr, states_b_ptr, dt_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, hdim, dstate, chunk_size, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_states_f_batch, stride_states_f_chunk, stride_states_f_head, stride_states_f_hdim, stride_states_f_dstate, stride_states_b_batch, stride_states_b_chunk, stride_states_b_head, stride_states_b_hdim, stride_states_b_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + pid_h * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + pid_h * stride_dA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen)\nb_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen)\ndt_ptrs = dt_ptr + offs_k * stride_dt_csize\ndA_cs_f_last = tl.load(dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize).to(tl.float32)\ndA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)\ndA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize\ndA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nacc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k), other=0.0)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    dA_cs_f_k = tl.load(dA_cumsum_f_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    dA_cs_b_k = tl.load(dA_cumsum_b_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    scale_f = tl.exp(dA_cs_f_last - dA_cs_f_k) * dt_k\n    scale_b = tl.exp(dA_cs_b_last - dA_cs_b_k) * dt_k\n    b_f = b * scale_f[:, None]\n    b_b = b * scale_b[:, None]\n    b_f = b_f.to(x_ptr.dtype.element_ty)\n    b_b = b_b.to(x_ptr.dtype.element_ty)\n    acc_f += tl.dot(x, b_f)\n    acc_b += tl.dot(x, b_b)\n    x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n    b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n    dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n    dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize\n    dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize\nstates_f = acc_f.to(states_f_ptr.dtype.element_ty)\nstates_b = acc_b.to(states_b_ptr.dtype.element_ty)\nstates_f_ptr += pid_b * stride_states_f_batch + pid_c * stride_states_f_chunk + pid_h * stride_states_f_head\nstates_b_ptr += pid_b * stride_states_b_batch + pid_c * stride_states_b_chunk + pid_h * stride_states_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nstates_f_ptrs = states_f_ptr + (offs_m[:, None] * stride_states_f_hdim + offs_n[None, :] * stride_states_f_dstate)\nstates_b_ptrs = states_b_ptr + (offs_m[:, None] * stride_states_b_hdim + offs_n[None, :] * stride_states_b_dstate)\nc_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\ntl.store(states_f_ptrs, states_f, mask=c_mask)\ntl.store(states_b_ptrs, states_b, mask=c_mask)"
  },
  {
    "name": "_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)",
      "    ddA_cs = -(ddt * dt_m)",
      "    ddA_cs_last = -tl.sum(ddA_cs)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)",
      "    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)",
      "",
      "    dx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_state_bwd_dx_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr, dA_cumsum_ptr, dx_ptr, ddt_ptr, ddA_cumsum_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch, stride_dstates_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_dx_batch, stride_dx_seqlen, stride_dx_head, stride_dx_hdim, stride_ddt_batch, stride_ddt_chunk, stride_ddt_head, stride_ddt_csize, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\ndstates_ptr += pid_b * stride_dstates_batch + pid_c * stride_dstates_chunk + pid_h * stride_states_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddt_ptr += pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\noffs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\nb_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate)\ndstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate)\nif BLOCK_SIZE_DSTATE <= 128:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate), other=0.0)\n    dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates = dstates.to(b_ptr.dtype.element_ty)\n    acc = tl.dot(b, dstates)\nelse:\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, dstate, BLOCK_SIZE_K):\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate - k), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates)\n        b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n        dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize\ndA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nacc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\nddt = tl.sum(acc * x, axis=1)\nddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize\ntl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)\nddA_cs = -(ddt * dt_m)\nddA_cs_last = -tl.sum(ddA_cs)\nddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\ntl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)\ntl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)\ndx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)\ndx_ptr += pid_b * stride_dx_batch + pid_c * chunk_size * stride_dx_seqlen + pid_h * stride_dx_head\ndx_ptrs = dx_ptr + (offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim)\ntl.store(dx_ptrs, dx, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim))"
  },
  {
    "name": "_chunk_state_bwd_db_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dstates_f_ptr",
        "annotation": null
      },
      {
        "name": "dstates_b_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_split",
        "annotation": null
      },
      {
        "name": "stride_db_group",
        "annotation": null
      },
      {
        "name": "stride_db_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_db_kernel(",
      "    x_ptr,",
      "    dstates_f_ptr,",
      "    dstates_b_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    db_ptr,",
      "    ddA_cumsum_f_ptr,",
      "    ddA_cumsum_b_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dstates_f_batch,",
      "    stride_dstates_f_chunk,",
      "    stride_dstates_f_head,",
      "    stride_dstates_f_hdim,",
      "    stride_dstates_f_dstate,",
      "    stride_dstates_b_batch,",
      "    stride_dstates_b_chunk,",
      "    stride_dstates_b_head,",
      "    stride_dstates_b_hdim,",
      "    stride_dstates_b_dstate,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_split,",
      "    stride_db_group,",
      "    stride_db_dstate,",
      "    stride_ddA_cs_f_batch,",
      "    stride_ddA_cs_f_chunk,",
      "    stride_ddA_cs_f_head,",
      "    stride_ddA_cs_f_csize,",
      "    stride_ddA_cs_b_batch,",
      "    stride_ddA_cs_b_chunk,",
      "    stride_ddA_cs_b_head,",
      "    stride_ddA_cs_b_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_g * stride_db_group",
      "        + pid_s * stride_db_split",
      "    )",
      "    dstates_f_ptr += (",
      "        pid_b * stride_dstates_f_batch",
      "        + pid_c * stride_dstates_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dstates_f_head",
      "    )",
      "    dstates_b_ptr += (",
      "        pid_b * stride_dstates_b_batch",
      "        + pid_c * stride_dstates_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dstates_b_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_b_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        b_ptr += (",
      "            pid_b * stride_b_batch",
      "            + pid_c * chunk_size * stride_b_seqlen",
      "            + pid_g * stride_b_head",
      "        )",
      "        ddA_cumsum_f_ptr += (",
      "            pid_b * stride_ddA_cs_f_batch",
      "            + pid_c * stride_ddA_cs_f_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_f_head",
      "        )",
      "        ddA_cumsum_b_ptr += (",
      "            pid_b * stride_ddA_cs_b_batch",
      "            + pid_c * stride_ddA_cs_b_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_b_head",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim",
      "    )",
      "    dstates_f_ptrs = dstates_f_ptr + (",
      "        offs_n[None, :] * stride_dstates_f_dstate",
      "        + offs_k[:, None] * stride_dstates_f_hdim",
      "    )",
      "    dstates_b_ptrs = dstates_b_ptr + (",
      "        offs_n[None, :] * stride_dstates_b_dstate",
      "        + offs_k[:, None] * stride_dstates_b_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize",
      "    if HAS_DDA_CS:",
      "        b_ptrs = b_ptr + (",
      "            offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate",
      "        )",
      "        ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize",
      "        ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        dstates_f = tl.load(",
      "            dstates_f_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates_f = dstates_f.to(x_ptrs.dtype.element_ty)",
      "        db_f = tl.dot(x, dstates_f)",
      "        dA_cs_f_last = tl.load(",
      "            dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize",
      "        ).to(tl.float32)",
      "        dA_cs_f_m = tl.load(dA_cumsum_f_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "        scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)",
      "        db_f *= (scale_f * dt_m)[:, None]",
      "        if HAS_DDA_CS:",
      "",
      "            ddA_cs_f = tl.sum(db_f * b, axis=1)",
      "",
      "            tl.atomic_add(",
      "                ddA_cumsum_f_ptrs + stride_ddA_cs_f_csize,",
      "                ddA_cs_f,",
      "                mask=offs_m < chunk_size - 1,",
      "            )",
      "",
      "        dstates_b = tl.load(",
      "            dstates_b_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates_b = dstates_b.to(x_ptrs.dtype.element_ty)",
      "        db_b = tl.dot(x, dstates_b)",
      "        dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)",
      "        dA_cs_b_m = tl.load(dA_cumsum_b_ptrs, mask=(offs_m < chunk_size), other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)",
      "        db_b *= (scale_b * dt_m)[:, None]",
      "        if HAS_DDA_CS:",
      "",
      "            ddA_cs_b = tl.sum(db_b * b, axis=1)",
      "",
      "            tl.atomic_add(",
      "                ddA_cumsum_b_ptrs - stride_ddA_cs_b_csize,",
      "                ddA_cs_b,",
      "                mask=(offs_m >= 1) & (offs_m < chunk_size),",
      "            )",
      "",
      "        acc += db_f + db_b",
      "        x_ptrs += stride_x_head",
      "        dstates_f_ptrs += stride_dstates_f_head",
      "        dstates_b_ptrs += stride_dstates_b_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_f_ptr += stride_dA_cs_f_head",
      "        dA_cumsum_f_ptrs += stride_dA_cs_f_head",
      "        dA_cumsum_b_ptr += stride_dA_cs_b_head",
      "        dA_cumsum_b_ptrs += stride_dA_cs_b_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_f_ptrs += stride_ddA_cs_f_head",
      "            ddA_cumsum_b_ptrs += stride_ddA_cs_b_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate",
      "    )",
      "    tl.store(",
      "        db_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_state_bwd_db_kernel(x_ptr, dstates_f_ptr, dstates_b_ptr, b_ptr, dt_ptr, dA_cumsum_f_ptr, dA_cumsum_b_ptr, db_ptr, ddA_cumsum_f_ptr, ddA_cumsum_b_ptr, chunk_size, dstate, hdim, batch, seqlen, nheads, nheads_per_program, ngroups, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_dstates_f_batch, stride_dstates_f_chunk, stride_dstates_f_head, stride_dstates_f_hdim, stride_dstates_f_dstate, stride_dstates_b_batch, stride_dstates_b_chunk, stride_dstates_b_head, stride_dstates_b_hdim, stride_dstates_b_dstate, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_f_batch, stride_dA_cs_f_chunk, stride_dA_cs_f_head, stride_dA_cs_f_csize, stride_dA_cs_b_batch, stride_dA_cs_b_chunk, stride_dA_cs_b_head, stride_dA_cs_b_csize, stride_db_batch, stride_db_seqlen, stride_db_split, stride_db_group, stride_db_dstate, stride_ddA_cs_f_batch, stride_ddA_cs_f_chunk, stride_ddA_cs_f_head, stride_ddA_cs_f_csize, stride_ddA_cs_b_batch, stride_ddA_cs_b_chunk, stride_ddA_cs_b_head, stride_ddA_cs_b_csize, HAS_DDA_CS: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_sg = tl.program_id(axis=2)\npid_s = pid_sg // ngroups\npid_g = pid_sg - pid_s * ngroups\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head\ndb_ptr += pid_b * stride_db_batch + pid_c * chunk_size * stride_db_seqlen + pid_g * stride_db_group + pid_s * stride_db_split\ndstates_f_ptr += pid_b * stride_dstates_f_batch + pid_c * stride_dstates_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dstates_f_head\ndstates_b_ptr += pid_b * stride_dstates_b_batch + pid_c * stride_dstates_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dstates_b_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head\ndA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_f_head\ndA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_b_head\nif HAS_DDA_CS:\n    b_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_g * stride_b_head\n    ddA_cumsum_f_ptr += pid_b * stride_ddA_cs_f_batch + pid_c * stride_ddA_cs_f_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_f_head\n    ddA_cumsum_b_ptr += pid_b * stride_ddA_cs_b_batch + pid_c * stride_ddA_cs_b_chunk + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_ddA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim)\ndstates_f_ptrs = dstates_f_ptr + (offs_n[None, :] * stride_dstates_f_dstate + offs_k[:, None] * stride_dstates_f_hdim)\ndstates_b_ptrs = dstates_b_ptr + (offs_n[None, :] * stride_dstates_b_dstate + offs_k[:, None] * stride_dstates_b_hdim)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize\ndA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize\nif HAS_DDA_CS:\n    b_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate)\n    ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize\n    ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nif HAS_DDA_CS:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\nnheads_iter = min(nheads_per_program, nheads // ngroups - pid_s * nheads_per_program)\nfor h in range(nheads_iter):\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim), other=0.0)\n    dstates_f = tl.load(dstates_f_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\n    dstates_f = dstates_f.to(x_ptrs.dtype.element_ty)\n    db_f = tl.dot(x, dstates_f)\n    dA_cs_f_last = tl.load(dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize).to(tl.float32)\n    dA_cs_f_m = tl.load(dA_cumsum_f_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)\n    db_f *= (scale_f * dt_m)[:, None]\n    if HAS_DDA_CS:\n        ddA_cs_f = tl.sum(db_f * b, axis=1)\n        tl.atomic_add(ddA_cumsum_f_ptrs + stride_ddA_cs_f_csize, ddA_cs_f, mask=offs_m < chunk_size - 1)\n    dstates_b = tl.load(dstates_b_ptrs, mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0)\n    dstates_b = dstates_b.to(x_ptrs.dtype.element_ty)\n    db_b = tl.dot(x, dstates_b)\n    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)\n    dA_cs_b_m = tl.load(dA_cumsum_b_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\n    scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)\n    db_b *= (scale_b * dt_m)[:, None]\n    if HAS_DDA_CS:\n        ddA_cs_b = tl.sum(db_b * b, axis=1)\n        tl.atomic_add(ddA_cumsum_b_ptrs - stride_ddA_cs_b_csize, ddA_cs_b, mask=(offs_m >= 1) & (offs_m < chunk_size))\n    acc += db_f + db_b\n    x_ptrs += stride_x_head\n    dstates_f_ptrs += stride_dstates_f_head\n    dstates_b_ptrs += stride_dstates_b_head\n    dt_ptrs += stride_dt_head\n    dA_cumsum_f_ptr += stride_dA_cs_f_head\n    dA_cumsum_f_ptrs += stride_dA_cs_f_head\n    dA_cumsum_b_ptr += stride_dA_cs_b_head\n    dA_cumsum_b_ptrs += stride_dA_cs_b_head\n    if HAS_DDA_CS:\n        ddA_cumsum_f_ptrs += stride_ddA_cs_f_head\n        ddA_cumsum_b_ptrs += stride_ddA_cs_b_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndb_ptrs = db_ptr + (offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate)\ntl.store(db_ptrs, acc, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate))"
  },
  {
    "name": "_chunk_state_bwd_ddAcs_stable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_ddAcs_stable_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_last - dA_cs_m)",
      "    else:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)",
      "    acc *= scale[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "",
      "    ddA_cs = ddt * dt_m",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    tl.atomic_add(",
      "        ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1",
      "    )"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_state_bwd_ddAcs_stable_kernel(x_ptr, b_ptr, dstates_ptr, dt_ptr, dA_cumsum_ptr, seq_idx_ptr, ddA_cumsum_ptr, chunk_size, hdim, dstate, batch, seqlen, nheads_ngroups_ratio, stride_x_batch, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_batch, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dstates_batch, stride_dstates_chunk, stride_states_head, stride_states_hdim, stride_states_dstate, stride_dt_batch, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_batch, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_seq_idx_batch, stride_seq_idx_seqlen, stride_ddA_cs_batch, stride_ddA_cs_chunk, stride_ddA_cs_head, stride_ddA_cs_csize, HAS_SEQ_IDX: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, BLOCK_SIZE_DSTATE: tl.constexpr):",
    "body": "pid_bc = tl.program_id(axis=1)\npid_c = pid_bc // batch\npid_b = pid_bc - pid_c * batch\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nx_ptr += pid_b * stride_x_batch + pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\nb_ptr += pid_b * stride_b_batch + pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\ndstates_ptr += pid_b * stride_dstates_batch + pid_c * stride_dstates_chunk + pid_h * stride_states_head\ndt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head\nddA_cumsum_ptr += pid_b * stride_ddA_cs_batch + pid_c * stride_ddA_cs_chunk + pid_h * stride_ddA_cs_head\ndA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nif HAS_SEQ_IDX:\n    seq_idx_ptr += pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nchunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)\noffs_k = tl.arange(0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K)\nb_ptrs = b_ptr + (offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate)\ndstates_ptrs = dstates_ptr + (offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate)\nif BLOCK_SIZE_DSTATE <= 128:\n    b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate), other=0.0)\n    dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim), other=0.0)\n    dstates = dstates.to(b_ptr.dtype.element_ty)\n    acc = tl.dot(b, dstates)\nelse:\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, dstate, BLOCK_SIZE_K):\n        b = tl.load(b_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate - k), other=0.0)\n        dstates = tl.load(dstates_ptrs, mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim), other=0.0)\n        dstates = dstates.to(b_ptr.dtype.element_ty)\n        acc += tl.dot(b, dstates)\n        b_ptrs += BLOCK_SIZE_K * stride_b_dstate\n        dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\ndA_cs_m = tl.load(dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\ndA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\nif not HAS_SEQ_IDX:\n    scale = tl.exp(dA_cs_last - dA_cs_m)\nelse:\n    seq_idx_m = tl.load(seq_idx_ptr + offs_m * stride_seq_idx_seqlen, mask=offs_m < chunk_size_limit, other=-1)\n    seq_idx_last = tl.load(seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen)\n    scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)\nacc *= scale[:, None]\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim)\nx = tl.load(x_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim), other=0.0).to(tl.float32)\ndt_ptrs = dt_ptr + offs_m * stride_dt_csize\ndt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)\nddt = tl.sum(acc * x, axis=1)\nddA_cs = ddt * dt_m\nddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize\ntl.atomic_add(ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1)"
  },
  {
    "name": "_chunk_state_varlen_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_states_ptr",
        "annotation": null
      },
      {
        "name": "cu_seqlens_ptr",
        "annotation": null
      },
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_head",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_varlen_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    chunk_states_ptr,",
      "    cu_seqlens_ptr,",
      "    states_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_chunk_states_chunk,",
      "    stride_chunk_states_head,",
      "    stride_chunk_states_hdim,",
      "    stride_chunk_states_dstate,",
      "    stride_states_batch,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)",
      "    pid_c = (end_idx - 1) // chunk_size",
      "    b_ptr += (",
      "        pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    chunk_states_ptr += (",
      "        pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_last = tl.load(",
      "        dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize",
      "    ).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = end_idx - pid_c * chunk_size",
      "    start_idx = tl.load(cu_seqlens_ptr + pid_b)",
      "    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim)",
      "            & (offs_k[None, :] < chunk_size_limit - k)",
      "            & (offs_k[None, :] >= start_idx_cur - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k)",
      "            & (offs_n[None, :] < dstate)",
      "            & (offs_k[:, None] >= start_idx_cur - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(",
      "            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale = tl.where(",
      "            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),",
      "            tl.exp((dA_cs_last - dA_cs_k)) * dt_k,",
      "            0.0,",
      "        )",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    if start_idx < pid_c * chunk_size:",
      "        chunk_states_ptrs = chunk_states_ptr + (",
      "            offs_m[:, None] * stride_chunk_states_hdim",
      "            + offs_n[None, :] * stride_chunk_states_dstate",
      "        )",
      "        chunk_states = tl.load(",
      "            chunk_states_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "",
      "        scale = tl.exp(dA_cs_last)",
      "        acc += chunk_states * scale",
      "",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (",
      "        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)"
    ],
    "file": "codes/120.py",
    "header": "def _chunk_state_varlen_kernel(x_ptr, b_ptr, dt_ptr, dA_cumsum_ptr, chunk_states_ptr, cu_seqlens_ptr, states_ptr, hdim, dstate, chunk_size, seqlen, nheads_ngroups_ratio, stride_x_seqlen, stride_x_head, stride_x_hdim, stride_b_seqlen, stride_b_head, stride_b_dstate, stride_dt_chunk, stride_dt_head, stride_dt_csize, stride_dA_cs_chunk, stride_dA_cs_head, stride_dA_cs_csize, stride_chunk_states_chunk, stride_chunk_states_head, stride_chunk_states_hdim, stride_chunk_states_dstate, stride_states_batch, stride_states_head, stride_states_hdim, stride_states_dstate, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid_b = tl.program_id(axis=1)\npid_h = tl.program_id(axis=2)\nnum_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)\npid_m = tl.program_id(axis=0) // num_pid_n\npid_n = tl.program_id(axis=0) % num_pid_n\nend_idx = tl.load(cu_seqlens_ptr + pid_b + 1)\npid_c = (end_idx - 1) // chunk_size\nb_ptr += pid_c * chunk_size * stride_b_seqlen + pid_h // nheads_ngroups_ratio * stride_b_head\nx_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head\ndt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head\ndA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head\nchunk_states_ptr += pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\nx_ptrs = x_ptr + (offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen)\nb_ptrs = b_ptr + (offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen)\ndt_ptrs = dt_ptr + offs_k * stride_dt_csize\ndA_cs_last = tl.load(dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize).to(tl.float32)\ndA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize\nchunk_size_limit = end_idx - pid_c * chunk_size\nstart_idx = tl.load(cu_seqlens_ptr + pid_b)\nstart_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)\nacc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, chunk_size_limit, BLOCK_SIZE_K):\n    x = tl.load(x_ptrs, mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k) & (offs_k[None, :] >= start_idx_cur - k), other=0.0)\n    b = tl.load(b_ptrs, mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate) & (offs_k[:, None] >= start_idx_cur - k), other=0.0).to(tl.float32)\n    dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(tl.float32)\n    scale = tl.where((offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k), tl.exp(dA_cs_last - dA_cs_k) * dt_k, 0.0)\n    b *= scale[:, None]\n    b = b.to(x_ptr.dtype.element_ty)\n    acc += tl.dot(x, b)\n    x_ptrs += BLOCK_SIZE_K * stride_x_seqlen\n    b_ptrs += BLOCK_SIZE_K * stride_b_seqlen\n    dt_ptrs += BLOCK_SIZE_K * stride_dt_csize\n    dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize\nif start_idx < pid_c * chunk_size:\n    chunk_states_ptrs = chunk_states_ptr + (offs_m[:, None] * stride_chunk_states_hdim + offs_n[None, :] * stride_chunk_states_dstate)\n    chunk_states = tl.load(chunk_states_ptrs, mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate), other=0.0).to(tl.float32)\n    scale = tl.exp(dA_cs_last)\n    acc += chunk_states * scale\nstates = acc.to(states_ptr.dtype.element_ty)\nstates_ptr += pid_b * stride_states_batch + pid_h * stride_states_head\noffs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nstates_ptrs = states_ptr + (offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate)\nc_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)\ntl.store(states_ptrs, states, mask=c_mask)"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    V_TILES: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N, V // 64),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = (",
      "        losses_ptr",
      "        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb",
      "        + idx_V_group * stride_loss_B",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "codes/171.py",
    "header": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, V_TILES: tl.constexpr = 1):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nV_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N, V // 64), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb + idx_V_group * stride_loss_B\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nm = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10000000.0)\ns = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)\nloss = 0.0\nfor _ in range(V_TILES):\n    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\n    for _ in range(H // H_BLOCK_SIZE):\n        x_chunk = tl.load(x_block_ptr)\n        A_v = tl.load(A_block_ptr)\n        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\n    m_new = tl.maximum(m, tl.max(z_j_to_k, 1))\n    s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)\n    s = s * tl.exp(m - m_new) + s_update\n    mask = y[:, None] == V_range[None, :]\n    loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\n    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\n    m = m_new\n    x_block_ptr = tl.advance(x_block_ptr, [0, -H])\n    A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    V_range = V_range + V_BLOCK_SIZE\nlse = m + tl.log(s)\ntl.store(loss_val_ptr, loss)\ntl.store(lse_row_ptr, lse[:, None])"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "codes/171.py",
    "header": "def linear_xent_mini_bwd_prologue_kernel(z_nv_ptr, y_ptr, lse_ptr, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V = tl.program_id(axis=1)\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nlse = tl.load(lse_ptr + N_range)\nz_j_to_k = tl.load(z_block_ptr)\nmask = y[:, None] == v_range[None, :]\nsoftmax_z = (z_j_to_k - lse[:, None]).exp()\nz_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N\ntl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
  },
  {
    "name": "cross_entropy_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "z_loss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "label_smoothing",
        "annotation": null
      },
      {
        "name": "logit_scale",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "n_rows",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_fwd_kernel(",
      "    loss_ptr,",
      "    lse_ptr,",
      "    z_loss_ptr,",
      "    logits_ptr,",
      "    labels_ptr,",
      "    label_smoothing,",
      "    logit_scale,",
      "    lse_square_scale,",
      "    ignore_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    n_rows,",
      "    logits_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "    SPLIT: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    logits = tl.load(",
      "        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "    )",
      "    logits = logits.to(tl.float32) * logit_scale",
      "    max_logits = tl.max(logits, 0)",
      "    if HAS_SMOOTHING:",
      "        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)",
      "    lse = log(tl.sum(exp(logits - max_logits), 0)) + max_logits",
      "    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)",
      "    if label_idx == ignore_index:",
      "        loss = 0.0",
      "        z_loss = 0.0",
      "    else:",
      "        label_idx -= class_start_idx",
      "        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(",
      "            n_cols, (col_block_idx + 1) * BLOCK_SIZE",
      "        ):",
      "            logits_label = tl.load(logits_ptr + label_idx) * logit_scale",
      "            if HAS_SMOOTHING:",
      "                loss = (",
      "                    (lse if not SPLIT else 0.0)",
      "                    - label_smoothing * sum_logits / total_classes",
      "                    - (1 - label_smoothing) * logits_label",
      "                )",
      "            else:",
      "                loss = (lse if not SPLIT else 0.0) - logits_label",
      "        else:",
      "",
      "            if HAS_SMOOTHING:",
      "                loss = label_smoothing * (",
      "                    (lse if not SPLIT else 0.0) - sum_logits / total_classes",
      "                )",
      "            else:",
      "                loss = 0.0",
      "        if not SPLIT:",
      "            z_loss = lse_square_scale * lse * lse",
      "            loss += z_loss",
      "        else:",
      "            z_loss = 0.0",
      "    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)",
      "    if not SPLIT:",
      "        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)"
    ],
    "file": "codes/353.py",
    "header": "def cross_entropy_fwd_kernel(loss_ptr, lse_ptr, z_loss_ptr, logits_ptr, labels_ptr, label_smoothing, logit_scale, lse_square_scale, ignore_index, total_classes, class_start_idx, n_cols, n_rows, logits_row_stride, BLOCK_SIZE: tl.constexpr, HAS_SMOOTHING: tl.constexpr, SPLIT: tl.constexpr):",
    "body": "row_idx = tl.program_id(0)\ncol_block_idx = tl.program_id(1)\nlogits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\ncol_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nlabel_idx = tl.load(labels_ptr + row_idx)\nlogits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float('inf'))\nlogits = logits.to(tl.float32) * logit_scale\nmax_logits = tl.max(logits, 0)\nif HAS_SMOOTHING:\n    sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)\nlse = log(tl.sum(exp(logits - max_logits), 0)) + max_logits\ntl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)\nif label_idx == ignore_index:\n    loss = 0.0\n    z_loss = 0.0\nelse:\n    label_idx -= class_start_idx\n    if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(n_cols, (col_block_idx + 1) * BLOCK_SIZE):\n        logits_label = tl.load(logits_ptr + label_idx) * logit_scale\n        if HAS_SMOOTHING:\n            loss = (lse if not SPLIT else 0.0) - label_smoothing * sum_logits / total_classes - (1 - label_smoothing) * logits_label\n        else:\n            loss = (lse if not SPLIT else 0.0) - logits_label\n    elif HAS_SMOOTHING:\n        loss = label_smoothing * ((lse if not SPLIT else 0.0) - sum_logits / total_classes)\n    else:\n        loss = 0.0\n    if not SPLIT:\n        z_loss = lse_square_scale * lse * lse\n        loss += z_loss\n    else:\n        z_loss = 0.0\ntl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)\nif not SPLIT:\n    tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)"
  },
  {
    "name": "cross_entropy_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "dlogits_ptr",
        "annotation": null
      },
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "label_smoothing",
        "annotation": null
      },
      {
        "name": "logit_scale",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "dlogits_row_stride",
        "annotation": null
      },
      {
        "name": "dloss_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_bwd_kernel(",
      "    dlogits_ptr,",
      "    dloss_ptr,",
      "    logits_ptr,",
      "    lse_ptr,",
      "    labels_ptr,",
      "    label_smoothing,",
      "    logit_scale,",
      "    lse_square_scale,",
      "    ignore_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    logits_row_stride,",
      "    dlogits_row_stride,",
      "    dloss_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    if label_idx != ignore_index:",
      "        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)",
      "    else:",
      "        dloss = 0.0",
      "    logits = (",
      "        tl.load(",
      "            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "        ).to(tl.float32)",
      "        * logit_scale",
      "    )",
      "    lse = tl.load(lse_ptr + row_idx)",
      "    probs = exp(logits - lse)",
      "    probs += 2.0 * lse_square_scale * lse * probs",
      "    label_idx -= class_start_idx",
      "    if HAS_SMOOTHING:",
      "        smooth_negative = label_smoothing / total_classes",
      "        probs = (",
      "            tl.where(col_offsets == label_idx, probs - (1 - label_smoothing), probs)",
      "            - smooth_negative",
      "        )",
      "    else:",
      "        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)",
      "    tl.store(",
      "        dlogits_ptr + col_offsets,",
      "        (dloss * logit_scale) * probs,",
      "        mask=col_offsets < n_cols,",
      "    )"
    ],
    "file": "codes/353.py",
    "header": "def cross_entropy_bwd_kernel(dlogits_ptr, dloss_ptr, logits_ptr, lse_ptr, labels_ptr, label_smoothing, logit_scale, lse_square_scale, ignore_index, total_classes, class_start_idx, n_cols, logits_row_stride, dlogits_row_stride, dloss_row_stride, BLOCK_SIZE: tl.constexpr, HAS_SMOOTHING: tl.constexpr):",
    "body": "row_idx = tl.program_id(0)\ncol_block_idx = tl.program_id(1)\nlogits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)\ndlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)\ncol_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nlabel_idx = tl.load(labels_ptr + row_idx)\nif label_idx != ignore_index:\n    dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)\nelse:\n    dloss = 0.0\nlogits = tl.load(logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float('inf')).to(tl.float32) * logit_scale\nlse = tl.load(lse_ptr + row_idx)\nprobs = exp(logits - lse)\nprobs += 2.0 * lse_square_scale * lse * probs\nlabel_idx -= class_start_idx\nif HAS_SMOOTHING:\n    smooth_negative = label_smoothing / total_classes\n    probs = tl.where(col_offsets == label_idx, probs - (1 - label_smoothing), probs) - smooth_negative\nelse:\n    probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)\ntl.store(dlogits_ptr + col_offsets, dloss * logit_scale * probs, mask=col_offsets < n_cols)"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 2}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 4}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=2)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "codes/182.py",
    "header": "def linear_xent_fwd_prep_bwd_kernel_matmul_t(x_ptr, y_ptr, A_t_ptr, z_nv_ptr, losses_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, stride_lse_N, stride_lse_B, stride_loss_Nb, stride_loss_B, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_V_group = tl.program_id(axis=1)\nnum_idx_N, num_idx_V_group = (tl.num_programs(0), tl.num_programs(1))\nidx_N, idx_V_group = tl.swizzle2d(idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE)\nV_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nA_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(0, idx_V_group * V_GROUP_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nz_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)\nfor _ in range(H // H_BLOCK_SIZE):\n    x_chunk = tl.load(x_block_ptr)\n    A_v = tl.load(A_block_ptr)\n    z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)\n    x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])\n    A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])\nm = tl.max(z_j_to_k, 1)\ns = tl.sum(tl.exp(z_j_to_k - m[:, None]), axis=1)\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range)\nmask = y[:, None] == V_range[None, :]\nloss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N\ntl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))\nlse = m + tl.log(s)\nlse_row_ptr = tl.make_block_ptr(base=lse_ptr, shape=(N_group, V // 128), strides=(stride_lse_N, stride_lse_B), offsets=(idx_N * N_BLOCK_SIZE, idx_V_group), block_shape=(N_BLOCK_SIZE, 1), order=(1, 0))\nloss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B\ntl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)\ntl.store(lse_row_ptr, lse[:, None])"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=2)",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (V // V_BLOCK_SIZE), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(",
      "            x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "codes/182.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(z_nv_ptr, y_ptr, A_t_ptr, x_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_V: tl.constexpr):",
    "body": "idx_N = tl.program_id(axis=0)\nidx_H = tl.program_id(axis=1)\nidx_V_tile = tl.program_id(axis=2)\nnum_idx_N, num_idx_H = (tl.num_programs(0) - V // V_BLOCK_SIZE, tl.num_programs(1))\nidx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)\nV_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)\nA_t_block_ptr = tl.make_block_ptr(base=A_t_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, V_split_offset), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(idx_N * N_BLOCK_SIZE, V_split_offset), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\nv_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)\ny = tl.load(y_ptr + N_range, eviction_policy='evict_last')\nlse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), eviction_policy='evict_last')\nx_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)\nfor _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):\n    mask = y[:, None] == v_range[None, :]\n    A_v = tl.load(A_t_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)\n    x_grad_acc = tl.dot(z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty)\n    A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])\n    z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])\n    v_range += V_BLOCK_SIZE\nif SPLIT_V == 1:\n    x_grad_block_ptr = tl.make_block_ptr(base=x_grad_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\n    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))\nelse:\n    row_n = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    x_grad_simple_ptr = x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H\n    tl.atomic_add(x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "):",
      "    idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = tl.program_id(axis=2)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(N, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr)",
      "                + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(",
      "                A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "            )",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(",
      "            A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "codes/182.py",
    "header": "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(z_nv_ptr, y_ptr, x_ptr, A_grad_ptr, lse_ptr, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group: tl.constexpr, V: tl.constexpr, N: tl.constexpr, H: tl.constexpr, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_N: tl.constexpr):",
    "body": "idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE\nidx_H = tl.program_id(axis=1)\nidx_N_tile = tl.program_id(axis=2)\nnum_idx_V, num_idx_H = (tl.num_programs(0) - N_group // N_BLOCK_SIZE, tl.num_programs(1))\nidx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)\nN_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)\nx_block_ptr = tl.make_block_ptr(base=x_ptr, shape=(N, H), strides=(stride_x_N, stride_x_H), offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE), order=(1, 0))\nz_block_ptr = tl.make_block_ptr(base=z_nv_ptr, shape=(N_group, V), strides=(stride_z_N, stride_z_V), offsets=(N_split_offset, idx_V * V_BLOCK_SIZE), block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE), order=(1, 0))\nN_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)\nV_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\nA_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)\nfor _ in range(0, tl.cdiv(N, N_BLOCK_SIZE * SPLIT_N)):\n    y = tl.load(y_ptr + idx_N_group * N_group + N_range, eviction_policy='evict_last')\n    lse = tl.load(lse_ptr + N_range, eviction_policy='evict_last')\n    mask = y[:, None] == V_range[None, :]\n    x_chunk = tl.load(x_block_ptr, eviction_policy='evict_first')\n    z_j_to_k = tl.load(z_block_ptr, eviction_policy='evict_last')\n    softmax_z = (z_j_to_k - lse[:, None]).exp()\n    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)\n    A_grad_acc = tl.dot(x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty)\n    x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])\n    z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])\n    N_range += N_BLOCK_SIZE\nif SPLIT_N == 1:\n    A_grad_T_block_ptr = tl.make_block_ptr(base=A_grad_ptr, shape=(H, V), strides=(stride_A_H, stride_A_V), offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE), block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE), order=(0, 1))\n    if idx_N_group > 0:\n        tl.store(A_grad_T_block_ptr, tl.load(A_grad_T_block_ptr) + (A_grad_acc / N).to(A_grad_ptr.type.element_ty))\n    else:\n        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))\nelse:\n    row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)\n    row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)\n    A_grad_T_simple_ptr = A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V\n    tl.atomic_add(A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
  },
  {
    "name": "linear_xent_bwd_dispatcher",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=bwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune})",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "logits",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "At",
        "annotation": null
      },
      {
        "name": "x_grad",
        "annotation": null
      },
      {
        "name": "At_grad",
        "annotation": null
      },
      {
        "name": "lse_global",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_NV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_dispatcher(",
      "    logits,",
      "    y,",
      "    x,",
      "    At,",
      "    x_grad,",
      "    At_grad,",
      "    lse_global,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group,",
      "    V,",
      "    N,",
      "    H,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_NV: tl.constexpr,",
      "):",
      "    idx_NV = tl.program_id(axis=0)",
      "    if idx_NV < N_group // N_BLOCK_SIZE:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "            logits,",
      "            y,",
      "            At,",
      "            x_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_V=SPLIT_NV,",
      "        )",
      "    else:",
      "        linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "            logits,",
      "            y,",
      "            x,",
      "            At_grad,",
      "            lse_global,",
      "            stride_x_N,",
      "            stride_x_H,",
      "            stride_A_H,",
      "            stride_A_V,",
      "            stride_z_N,",
      "            stride_z_V,",
      "            idx_N_group,",
      "            N_group,",
      "            V,",
      "            N,",
      "            H,",
      "            V_BLOCK_SIZE,",
      "            N_BLOCK_SIZE,",
      "            H_BLOCK_SIZE,",
      "            GROUP_SIZE,",
      "            SPLIT_N=SPLIT_NV,",
      "        )"
    ],
    "file": "codes/182.py",
    "header": "def linear_xent_bwd_dispatcher(logits, y, x, At, x_grad, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE: tl.constexpr, N_BLOCK_SIZE: tl.constexpr, H_BLOCK_SIZE: tl.constexpr, GROUP_SIZE: tl.constexpr, SPLIT_NV: tl.constexpr):",
    "body": "idx_NV = tl.program_id(axis=0)\nif idx_NV < N_group // N_BLOCK_SIZE:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dx(logits, y, At, x_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_V=SPLIT_NV)\nelse:\n    linear_xent_bwd_kernel_matmul_t_epilogue_dA(logits, y, x, At_grad, lse_global, stride_x_N, stride_x_H, stride_A_H, stride_A_V, stride_z_N, stride_z_V, idx_N_group, N_group, V, N, H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE, SPLIT_N=SPLIT_NV)"
  },
  {
    "name": "tuned_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_FWD, key=['max_seqlen_q', 'max_seqlen_k', 'CAUSAL'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "num_head_q",
        "annotation": null
      },
      {
        "name": "num_head_k",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed_ptr",
        "annotation": null
      },
      {
        "name": "philox_offset1",
        "annotation": null
      },
      {
        "name": "philox_offset2",
        "annotation": null
      },
      {
        "name": "philox_seed_output",
        "annotation": null
      },
      {
        "name": "philox_offset_output",
        "annotation": null
      },
      {
        "name": "encoded_softmax",
        "annotation": null
      },
      {
        "name": "CAUSAL_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "Window_left",
        "annotation": null
      },
      {
        "name": "Window_right",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "pre_load_v",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RETURN_ENCODED_SOFTMAX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    num_head_q,",
      "    num_head_k,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed_ptr,",
      "    philox_offset1,",
      "    philox_offset2,",
      "    philox_seed_output,",
      "    philox_offset_output,",
      "    encoded_softmax,",
      "    CAUSAL_TYPE: tl.constexpr,",
      "    Window_left,",
      "    Window_right,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    pre_load_v: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "):",
      "    bare_attn_fwd(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        num_head_q,",
      "        num_head_k,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed_ptr,",
      "        philox_offset1,",
      "        philox_offset2,",
      "        philox_seed_output,",
      "        philox_offset_output,",
      "        encoded_softmax,",
      "        CAUSAL_TYPE,",
      "        Window_left,",
      "        Window_right,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        pre_load_v,",
      "        ENABLE_DROPOUT,",
      "        RETURN_ENCODED_SOFTMAX,",
      "        PADDED_HEAD,",
      "        BIAS_TYPE=BIAS_TYPE,",
      "    )"
    ],
    "file": "codes/242.py",
    "header": "def tuned_attn_fwd(Q, K, V, B, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh, stride_om, stride_on, num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr, philox_offset1, philox_offset2, philox_seed_output, philox_offset_output, encoded_softmax, CAUSAL_TYPE: tl.constexpr, Window_left, Window_right, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr, pre_load_v: tl.constexpr, ENABLE_DROPOUT: tl.constexpr, RETURN_ENCODED_SOFTMAX: tl.constexpr, PADDED_HEAD: tl.constexpr, BIAS_TYPE: tl.constexpr):",
    "body": "bare_attn_fwd(Q, K, V, B, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh, stride_om, stride_on, num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr, philox_offset1, philox_offset2, philox_seed_output, philox_offset_output, encoded_softmax, CAUSAL_TYPE, Window_left, Window_right, BLOCK_M, BLOCK_DMODEL, BLOCK_N, pre_load_v, ENABLE_DROPOUT, RETURN_ENCODED_SOFTMAX, PADDED_HEAD, BIAS_TYPE=BIAS_TYPE)"
  },
  {
    "name": "tuned_attn_bwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_FUSED, key=['max_seqlen_q', 'max_seqlen_k', 'head_dim'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_ok",
        "annotation": null
      },
      {
        "name": "stride_dkz",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dkk",
        "annotation": null
      },
      {
        "name": "stride_dvz",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvk",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "stride_dqz",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dqk",
        "annotation": null
      },
      {
        "name": "stride_dbz",
        "annotation": null
      },
      {
        "name": "stride_dbh",
        "annotation": null
      },
      {
        "name": "stride_dbm",
        "annotation": null
      },
      {
        "name": "stride_dbn",
        "annotation": null
      },
      {
        "name": "num_head_q",
        "annotation": null
      },
      {
        "name": "num_head_k",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed_ptr",
        "annotation": null
      },
      {
        "name": "philox_offset1",
        "annotation": null
      },
      {
        "name": "philox_offset2",
        "annotation": null
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLK_SLICE_FACTOR",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_attn_bwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    Out,",
      "    DO,",
      "    DK,",
      "    DV,",
      "    DQ,",
      "    DB,",
      "    L,",
      "    D,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_ok,",
      "    stride_dkz,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dkk,",
      "    stride_dvz,",
      "    stride_dvh,",
      "    stride_dvk,",
      "    stride_dvn,",
      "    stride_dqz,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dqk,",
      "    stride_dbz,",
      "    stride_dbh,",
      "    stride_dbm,",
      "    stride_dbn,",
      "    num_head_q,",
      "    num_head_k,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed_ptr,",
      "    philox_offset1,",
      "    philox_offset2,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "    BLOCK_M1: tl.constexpr,",
      "    BLOCK_N1: tl.constexpr,",
      "    BLOCK_M2: tl.constexpr,",
      "    BLOCK_N2: tl.constexpr,",
      "    BLK_SLICE_FACTOR: tl.constexpr,",
      "):",
      "    bare_attn_bwd(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        Out,",
      "        DO,",
      "        DK,",
      "        DV,",
      "        DQ,",
      "        DB,",
      "        L,",
      "        D,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_ok,",
      "        stride_dkz,",
      "        stride_dkh,",
      "        stride_dkn,",
      "        stride_dkk,",
      "        stride_dvz,",
      "        stride_dvh,",
      "        stride_dvk,",
      "        stride_dvn,",
      "        stride_dqz,",
      "        stride_dqh,",
      "        stride_dqm,",
      "        stride_dqk,",
      "        stride_dbz,",
      "        stride_dbh,",
      "        stride_dbm,",
      "        stride_dbn,",
      "        num_head_q,",
      "        num_head_k,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed_ptr,",
      "        philox_offset_base,",
      "        BLOCK_DMODEL,",
      "        CAUSAL,",
      "        ENABLE_DROPOUT,",
      "        PADDED_HEAD,",
      "        BIAS_TYPE,",
      "        BLOCK_M1,",
      "        BLOCK_N1,",
      "        BLOCK_M2,",
      "        BLOCK_N2,",
      "        BLK_SLICE_FACTOR,",
      "    )"
    ],
    "file": "codes/242.py",
    "header": "def tuned_attn_bwd(Q, K, V, B, sm_scale, Out, DO, DK, DV, DQ, DB, L, D, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh, stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn, stride_dkk, stride_dvz, stride_dvh, stride_dvk, stride_dvn, stride_dqz, stride_dqh, stride_dqm, stride_dqk, stride_dbz, stride_dbh, stride_dbm, stride_dbn, num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr, philox_offset1, philox_offset2, BLOCK_DMODEL: tl.constexpr, CAUSAL: tl.constexpr, ENABLE_DROPOUT: tl.constexpr, PADDED_HEAD: tl.constexpr, BIAS_TYPE: tl.constexpr, BLOCK_M1: tl.constexpr, BLOCK_N1: tl.constexpr, BLOCK_M2: tl.constexpr, BLOCK_N2: tl.constexpr, BLK_SLICE_FACTOR: tl.constexpr):",
    "body": "bare_attn_bwd(Q, K, V, B, sm_scale, Out, DO, DK, DV, DQ, DB, L, D, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_bz, stride_bh, stride_bm, stride_bn, stride_oz, stride_oh, stride_om, stride_ok, stride_dkz, stride_dkh, stride_dkn, stride_dkk, stride_dvz, stride_dvh, stride_dvk, stride_dvn, stride_dqz, stride_dqh, stride_dqm, stride_dqk, stride_dbz, stride_dbh, stride_dbm, stride_dbn, num_head_q, num_head_k, cu_seqlens_q, cu_seqlens_k, num_seqlens, max_seqlen_q, max_seqlen_k, head_dim, dropout_p, philox_seed_ptr, philox_offset_base, BLOCK_DMODEL, CAUSAL, ENABLE_DROPOUT, PADDED_HEAD, BIAS_TYPE, BLOCK_M1, BLOCK_N1, BLOCK_M2, BLOCK_N2, BLK_SLICE_FACTOR)"
  },
  {
    "name": "prepare_qg_kg",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_qg_kg(",
      "    q, k, g, qg, kg, scale, T, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    p_q = q + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_g = g + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_qg = qg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_kg = kg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "",
      "    last_decay = tl.load(",
      "        g + i_bh * T * K + (i_c * BT + BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "    )",
      "",
      "    for _ in range(BT):",
      "        b_q = tl.load(p_q, mask=mask, other=0)",
      "        b_k = tl.load(p_k, mask=mask, other=0)",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_q *= exp(b_g) * scale",
      "        b_k *= exp(last_decay - b_g)",
      "        tl.store(p_kg, b_k.to(p_kg.dtype.element_ty), mask=mask)",
      "        tl.store(p_qg, b_q.to(p_qg.dtype.element_ty), mask=mask)",
      "        p_q += K",
      "        p_g += K",
      "        p_k += K",
      "        p_kg += K",
      "        p_qg += K"
    ],
    "file": "codes/394.py",
    "header": "def prepare_qg_kg(q, k, g, qg, kg, scale, T, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr):",
    "body": "i_k, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\np_q = q + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\np_g = g + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\np_k = k + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\np_qg = qg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\np_kg = kg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)\nmask = i_k * BK + tl.arange(0, BK) < K\nlast_decay = tl.load(g + i_bh * T * K + (i_c * BT + BT - 1) * K + i_k * BK + tl.arange(0, BK))\nfor _ in range(BT):\n    b_q = tl.load(p_q, mask=mask, other=0)\n    b_k = tl.load(p_k, mask=mask, other=0)\n    b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n    b_q *= exp(b_g) * scale\n    b_k *= exp(last_decay - b_g)\n    tl.store(p_kg, b_k.to(p_kg.dtype.element_ty), mask=mask)\n    tl.store(p_qg, b_q.to(p_qg.dtype.element_ty), mask=mask)\n    p_q += K\n    p_g += K\n    p_k += K\n    p_kg += K\n    p_qg += K"
  },
  {
    "name": "bwd_decay_global_cumsum",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "dq_inner",
        "annotation": null
      },
      {
        "name": "dq_inter",
        "annotation": null
      },
      {
        "name": "dk_inner",
        "annotation": null
      },
      {
        "name": "dk_inter",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bwd_decay_global_cumsum(",
      "    dq_inner,",
      "    dq_inter,",
      "    dk_inner,",
      "    dk_inter,",
      "    q,",
      "    k,",
      "    g,",
      "    dg,",
      "    T,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_g = g + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_dg = dg + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_dq_inner = (",
      "        dq_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    p_dk_inner = (",
      "        dk_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    p_dq_inter = (",
      "        dq_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    p_dk_inter = (",
      "        dk_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "    last_g = tl.zeros([BK], dtype=tl.float32)",
      "    for j in range(BT - 1, -1, -1):",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        if j == (BT - 1):",
      "            last_g = b_g",
      "        b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)",
      "        b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)",
      "        b_dq2 *= exp(b_g)",
      "        b_dq = b_dq1 + b_dq2",
      "        tl.store(p_dq_inter, b_dq, mask=mask)",
      "        b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)",
      "        b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)",
      "        b_dk2 *= safe_exp(last_g - b_g)",
      "        b_dk = b_dk1 + b_dk2",
      "        tl.store(p_dk_inter, b_dk, mask=mask)",
      "        b_q = tl.load(p_q, mask=mask, other=0)",
      "        b_k = tl.load(p_k, mask=mask, other=0)",
      "        b_dg = b_dq * b_q - b_dk * b_k",
      "        cum_grad_dg += b_dg",
      "        tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)",
      "        p_g -= K",
      "        p_k -= K",
      "        p_q -= K",
      "        p_dq_inner -= K",
      "        p_dk_inner -= K",
      "        p_dq_inter -= K",
      "        p_dk_inter -= K",
      "        p_dg -= K"
    ],
    "file": "codes/394.py",
    "header": "def bwd_decay_global_cumsum(dq_inner, dq_inter, dk_inner, dk_inter, q, k, g, dg, T, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr):",
    "body": "i_k, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\np_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_g = g + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_dg = dg + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_dq_inner = dq_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_dk_inner = dk_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_dq_inter = dq_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\np_dk_inter = dk_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K\ncum_grad_dg = tl.zeros([BK], dtype=tl.float32)\nmask = i_k * BK + tl.arange(0, BK) < K\nlast_g = tl.zeros([BK], dtype=tl.float32)\nfor j in range(BT - 1, -1, -1):\n    b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)\n    if j == BT - 1:\n        last_g = b_g\n    b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)\n    b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)\n    b_dq2 *= exp(b_g)\n    b_dq = b_dq1 + b_dq2\n    tl.store(p_dq_inter, b_dq, mask=mask)\n    b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)\n    b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)\n    b_dk2 *= safe_exp(last_g - b_g)\n    b_dk = b_dk1 + b_dk2\n    tl.store(p_dk_inter, b_dk, mask=mask)\n    b_q = tl.load(p_q, mask=mask, other=0)\n    b_k = tl.load(p_k, mask=mask, other=0)\n    b_dg = b_dq * b_q - b_dk * b_k\n    cum_grad_dg += b_dg\n    tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)\n    p_g -= K\n    p_k -= K\n    p_q -= K\n    p_dq_inner -= K\n    p_dk_inner -= K\n    p_dq_inter -= K\n    p_dk_inter -= K\n    p_dg -= K"
  },
  {
    "name": "fused_chunk_gla_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_gla_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    o,",
      "    h0,",
      "    ht,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_gn = g + i_bh * T * K + (BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + i_k * B * H) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (0, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)",
      "        if CHECK and i == 0:",
      "            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[:, None] + tl.dot(",
      "                b_k.to(b_v.dtype), b_v, allow_tf32=False",
      "            )",
      "        else:",
      "            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[:, None] + tl.dot(",
      "                b_k.to(b_v.dtype), b_v, allow_tf32=False",
      "            )",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "        p_q = tl.advance(p_q, (BT, 0))",
      "        p_k = tl.advance(p_k, (0, BT))",
      "        p_v = tl.advance(p_v, (BT, 0))",
      "        p_o = tl.advance(p_o, (BT, 0))",
      "        p_gn += BT * K",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_final = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_final, b_h.to(p_final.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/394.py",
    "header": "def fused_chunk_gla_fwd_kernel(q, k, v, g, o, h0, ht, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, CHECK: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0))\np_gn = g + i_bh * T * K + (BT - 1) * K + i_k * BK + tl.arange(0, BK)\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0))\np_o = tl.make_block_ptr(o + (i_bh + i_k * B * H) * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0))\nif USE_INITIAL_STATE:\n    p_h = tl.make_block_ptr(h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\nmask = i_k * BK + tl.arange(0, BK) < K\nfor i in range(0, tl.cdiv(T, BT)):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)\n    if CHECK and i == 0:\n        b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)\n        b_h = b_h * exp(b_gn)[:, None] + tl.dot(b_k.to(b_v.dtype), b_v, allow_tf32=False)\n    else:\n        b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)\n        b_h = b_h * exp(b_gn)[:, None] + tl.dot(b_k.to(b_v.dtype), b_v, allow_tf32=False)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\n    p_q = tl.advance(p_q, (BT, 0))\n    p_k = tl.advance(p_k, (0, BT))\n    p_v = tl.advance(p_v, (BT, 0))\n    p_o = tl.advance(p_o, (BT, 0))\n    p_gn += BT * K\nif STORE_FINAL_STATE:\n    p_final = tl.make_block_ptr(ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_final, b_h.to(p_final.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_chunk_gla_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_gla_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_gn = g + i_bh * T * K + ((i + 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (i_bh + i_v * B * H) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        if CHECK and i == 0:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[None, :] + tl.dot(",
      "                b_v, b_k.to(b_v.dtype), allow_tf32=False",
      "            )",
      "        else:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[None, :] + tl.dot(",
      "                b_v, b_k.to(b_v.dtype), allow_tf32=False",
      "            )",
      "        b_dq *= scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_h = None",
      "    tl.debug_barrier()",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for i in range(1, tl.cdiv(T, BT) + 1):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_gn = (",
      "            g + i_bh * T * K + (T - (i - 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (i_bh + i_v * B * H) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (T - i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_bh + i_k * B * H) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (T - i * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_db = tl.load(p_gn, mask=mask, other=0).to(tl.float32)",
      "",
      "        if CHECK and i == 1:",
      "            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))",
      "            b_dv = tl.dot((b_k).to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)",
      "            b_dh = b_dh * exp(b_db)[:, None] + tl.dot(",
      "                b_q.to(b_do.dtype), b_do, allow_tf32=False",
      "            )",
      "        else:",
      "            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))",
      "            b_dv = tl.dot((b_k).to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)",
      "            b_dh = b_dh * exp(b_db)[:, None] + tl.dot(",
      "                b_q.to(b_do.dtype), b_do, allow_tf32=False",
      "            )",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/394.py",
    "header": "def fused_chunk_gla_bwd_kernel(q, k, v, g, do, dq, dk, dv, h0, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, CHECK: tl.constexpr):",
    "body": "i_v, i_k, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h = tl.make_block_ptr(h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)\nmask = i_k * BK + tl.arange(0, BK) < K\nfor i in range(0, tl.cdiv(T, BT)):\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0))\n    p_gn = g + i_bh * T * K + ((i + 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dq = tl.make_block_ptr(dq + (i_bh + i_v * B * H) * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0))\n    b_dq = tl.zeros([BT, BK], dtype=tl.float32)\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    if CHECK and i == 0:\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n        b_h = b_h * exp(b_gn)[None, :] + tl.dot(b_v, b_k.to(b_v.dtype), allow_tf32=False)\n    else:\n        b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)\n        b_h = b_h * exp(b_gn)[None, :] + tl.dot(b_v, b_k.to(b_v.dtype), allow_tf32=False)\n    b_dq *= scale\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\nb_h = None\ntl.debug_barrier()\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nfor i in range(1, tl.cdiv(T, BT) + 1):\n    p_q = tl.make_block_ptr(q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n    p_gn = g + i_bh * T * K + (T - (i - 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)\n    p_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (i_bh + i_v * B * H) * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (i_bh + i_k * B * H) * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_db = tl.load(p_gn, mask=mask, other=0).to(tl.float32)\n    if CHECK and i == 1:\n        b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))\n        b_dv = tl.dot(b_k.to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)\n        b_dh = b_dh * exp(b_db)[:, None] + tl.dot(b_q.to(b_do.dtype), b_do, allow_tf32=False)\n    else:\n        b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))\n        b_dv = tl.dot(b_k.to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)\n        b_dh = b_dh * exp(b_db)[:, None] + tl.dot(b_q.to(b_do.dtype), b_do, allow_tf32=False)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])",
      "@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})",
      "@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "X1",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "B1",
        "annotation": null
      },
      {
        "name": "Y1",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "DROPOUT_MASK",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "stride_x1_row",
        "annotation": null
      },
      {
        "name": "stride_y1_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DROPOUT_MASK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_X1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_W1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    X1,",
      "    W1,",
      "    B1,",
      "    Y1,",
      "    RESIDUAL_OUT,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    DROPOUT_MASK,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    stride_x1_row,",
      "    stride_y1_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    STORE_DROPOUT_MASK: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_X1: tl.constexpr,",
      "    HAS_W1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "    if HAS_X1:",
      "        X1 += row * stride_x1_row",
      "    if HAS_W1:",
      "        Y1 += row * stride_y1_row",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_ROWSCALE:",
      "        rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "        x *= rowscale",
      "    if HAS_DROPOUT:",
      "",
      "        keep_mask = (",
      "            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p",
      "        )",
      "        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)",
      "        if STORE_DROPOUT_MASK:",
      "            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)",
      "    if HAS_X1:",
      "        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)",
      "            x1 *= rowscale",
      "        if HAS_DROPOUT:",
      "",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)",
      "            if STORE_DROPOUT_MASK:",
      "                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)",
      "        x += x1",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "",
      "    tl.store(Y + cols, y, mask=mask)",
      "    if HAS_W1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "        if HAS_B1:",
      "            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)",
      "        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1",
      "        tl.store(Y1 + cols, y1, mask=mask)"
    ],
    "file": "codes/114.py",
    "header": "def _layer_norm_fwd_1pass_kernel(X, Y, W, B, RESIDUAL, X1, W1, B1, Y1, RESIDUAL_OUT, ROWSCALE, SEEDS, DROPOUT_MASK, Mean, Rstd, stride_x_row, stride_y_row, stride_res_row, stride_res_out_row, stride_x1_row, stride_y1_row, M, N, eps, dropout_p, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr, STORE_DROPOUT_MASK: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_X1: tl.constexpr, HAS_W1: tl.constexpr, HAS_B1: tl.constexpr):",
    "body": "row = tl.program_id(0)\nX += row * stride_x_row\nY += row * stride_y_row\nif HAS_RESIDUAL:\n    RESIDUAL += row * stride_res_row\nif STORE_RESIDUAL_OUT:\n    RESIDUAL_OUT += row * stride_res_out_row\nif HAS_X1:\n    X1 += row * stride_x1_row\nif HAS_W1:\n    Y1 += row * stride_y1_row\ncols = tl.arange(0, BLOCK_N)\nx = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)\nif HAS_ROWSCALE:\n    rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n    x *= rowscale\nif HAS_DROPOUT:\n    keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n    x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)\n    if STORE_DROPOUT_MASK:\n        tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)\nif HAS_X1:\n    x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)\n        x1 *= rowscale\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)\n        if STORE_DROPOUT_MASK:\n            tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)\n    x += x1\nif HAS_RESIDUAL:\n    residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)\n    x += residual\nif STORE_RESIDUAL_OUT:\n    tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)\nif not IS_RMS_NORM:\n    mean = tl.sum(x, axis=0) / N\n    tl.store(Mean + row, mean)\n    xbar = tl.where(cols < N, x - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nelse:\n    xbar = tl.where(cols < N, x, 0.0)\n    var = tl.sum(xbar * xbar, axis=0) / N\nrstd = 1 / tl.sqrt(var + eps)\ntl.store(Rstd + row, rstd)\nmask = cols < N\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif HAS_BIAS:\n    b = tl.load(B + cols, mask=mask).to(tl.float32)\nx_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\ny = x_hat * w + b if HAS_BIAS else x_hat * w\ntl.store(Y + cols, y, mask=mask)\nif HAS_W1:\n    w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\n    if HAS_B1:\n        b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)\n    y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1\n    tl.store(Y1 + cols, y1, mask=mask)"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])",
      "@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})",
      "@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})",
      "@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "DY1",
        "annotation": null
      },
      {
        "name": "DX1",
        "annotation": null
      },
      {
        "name": "DW1",
        "annotation": null
      },
      {
        "name": "DB1",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dy1_row",
        "annotation": null
      },
      {
        "name": "stride_dx1_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DY1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DX1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    W1,",
      "    DY1,",
      "    DX1,",
      "    DW1,",
      "    DB1,",
      "    DRESIDUAL_IN,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dy1_row,",
      "    stride_dx1_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_DY1: tl.constexpr,",
      "    HAS_DX1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if HAS_DY1:",
      "        DY1 += row_start * stride_dy1_row",
      "    if HAS_DX1:",
      "        DX1 += row_start * stride_dx1_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_DY1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_DY1:",
      "        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "        if HAS_B1:",
      "            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if HAS_DY1:",
      "            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_DY1:",
      "            wdy += w1 * dy1",
      "            dw1 += dy1 * xhat",
      "            if HAS_B1:",
      "                db1 += dy1",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        if HAS_DX1:",
      "            if HAS_DROPOUT:",
      "                keep_mask = (",
      "                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                    > dropout_p",
      "                )",
      "                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "            else:",
      "                dx1 = dx",
      "            tl.store(DX1 + cols, dx1, mask=mask)",
      "        if HAS_DROPOUT:",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "            dx *= rowscale",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "        if HAS_DY1:",
      "            DY1 += stride_dy1_row",
      "        if HAS_DX1:",
      "            DX1 += stride_dx1_row",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)",
      "    if HAS_DY1:",
      "        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)",
      "        if HAS_B1:",
      "            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)"
    ],
    "file": "codes/114.py",
    "header": "def _layer_norm_bwd_kernel(X, W, B, Y, DY, DX, DW, DB, DRESIDUAL, W1, DY1, DX1, DW1, DB1, DRESIDUAL_IN, ROWSCALE, SEEDS, Mean, Rstd, stride_x_row, stride_y_row, stride_dy_row, stride_dx_row, stride_dres_row, stride_dy1_row, stride_dx1_row, stride_dres_in_row, M, N, eps, dropout_p, rows_per_program, IS_RMS_NORM: tl.constexpr, BLOCK_N: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_BIAS: tl.constexpr, HAS_DROPOUT: tl.constexpr, HAS_ROWSCALE: tl.constexpr, HAS_DY1: tl.constexpr, HAS_DX1: tl.constexpr, HAS_B1: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "row_block_id = tl.program_id(0)\nrow_start = row_block_id * rows_per_program\ncols = tl.arange(0, BLOCK_N)\nmask = cols < N\nX += row_start * stride_x_row\nif HAS_DRESIDUAL:\n    DRESIDUAL += row_start * stride_dres_row\nif STORE_DRESIDUAL:\n    DRESIDUAL_IN += row_start * stride_dres_in_row\nDY += row_start * stride_dy_row\nDX += row_start * stride_dx_row\nif HAS_DY1:\n    DY1 += row_start * stride_dy1_row\nif HAS_DX1:\n    DX1 += row_start * stride_dx1_row\nif RECOMPUTE_OUTPUT:\n    Y += row_start * stride_y_row\nw = tl.load(W + cols, mask=mask).to(tl.float32)\nif RECOMPUTE_OUTPUT and HAS_BIAS:\n    b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)\nif HAS_DY1:\n    w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)\ndw = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_BIAS:\n    db = tl.zeros((BLOCK_N,), dtype=tl.float32)\nif HAS_DY1:\n    dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\n    if HAS_B1:\n        db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)\nrow_end = min((row_block_id + 1) * rows_per_program, M)\nfor row in range(row_start, row_end):\n    x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)\n    dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)\n    if HAS_DY1:\n        dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        mean = tl.load(Mean + row)\n    rstd = tl.load(Rstd + row)\n    xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd\n    xhat = tl.where(mask, xhat, 0.0)\n    if RECOMPUTE_OUTPUT:\n        y = xhat * w + b if HAS_BIAS else xhat * w\n        tl.store(Y + cols, y, mask=mask)\n    wdy = w * dy\n    dw += dy * xhat\n    if HAS_BIAS:\n        db += dy\n    if HAS_DY1:\n        wdy += w1 * dy1\n        dw1 += dy1 * xhat\n        if HAS_B1:\n            db1 += dy1\n    if not IS_RMS_NORM:\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        c2 = tl.sum(wdy, axis=0) / N\n        dx = (wdy - (xhat * c1 + c2)) * rstd\n    else:\n        c1 = tl.sum(xhat * wdy, axis=0) / N\n        dx = (wdy - xhat * c1) * rstd\n    if HAS_DRESIDUAL:\n        dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)\n        dx += dres\n    if STORE_DRESIDUAL:\n        tl.store(DRESIDUAL_IN + cols, dx, mask=mask)\n    if HAS_DX1:\n        if HAS_DROPOUT:\n            keep_mask = tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n            dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n        else:\n            dx1 = dx\n        tl.store(DX1 + cols, dx1, mask=mask)\n    if HAS_DROPOUT:\n        keep_mask = tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p\n        dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)\n    if HAS_ROWSCALE:\n        rowscale = tl.load(ROWSCALE + row).to(tl.float32)\n        dx *= rowscale\n    tl.store(DX + cols, dx, mask=mask)\n    X += stride_x_row\n    if HAS_DRESIDUAL:\n        DRESIDUAL += stride_dres_row\n    if STORE_DRESIDUAL:\n        DRESIDUAL_IN += stride_dres_in_row\n    if RECOMPUTE_OUTPUT:\n        Y += stride_y_row\n    DY += stride_dy_row\n    DX += stride_dx_row\n    if HAS_DY1:\n        DY1 += stride_dy1_row\n    if HAS_DX1:\n        DX1 += stride_dx1_row\ntl.store(DW + row_block_id * N + cols, dw, mask=mask)\nif HAS_BIAS:\n    tl.store(DB + row_block_id * N + cols, db, mask=mask)\nif HAS_DY1:\n    tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)\n    if HAS_B1:\n        tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)"
  },
  {
    "name": "parallel_based_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_based_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    z,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % (NV)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_o = tl.zeros([BTL, BV], dtype=tl.float32)",
      "    b_z = tl.zeros([BTL], dtype=tl.float32)",
      "",
      "    for _ in range(0, i_c * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, (b_k), allow_tf32=False)",
      "        b_s = 1 + b_s + 0.5 * b_s * b_s",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "",
      "    tl.debug_barrier()",
      "    o_q = tl.arange(0, BTL)",
      "",
      "    o_k = tl.arange(0, BTS)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = 1 + b_s + 0.5 * b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "        o_k += BTS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + B * H * i_k) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_c * BTL, i_v * BV),",
      "        (BTL, BV),",
      "        (1, 0),",
      "    )",
      "    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(",
      "        p_z, b_z.to(p_z.dtype.element_ty), mask=((i_c * BTL + tl.arange(0, BTL)) < T)",
      "    )"
    ],
    "file": "codes/367.py",
    "header": "def parallel_based_fwd_kernel(q, k, v, o, z, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "i_kv, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNV = tl.cdiv(V, BV)\ni_k = i_kv // NV\ni_v = i_kv % NV\np_q = tl.make_block_ptr(q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0))\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nb_o = tl.zeros([BTL, BV], dtype=tl.float32)\nb_z = tl.zeros([BTL], dtype=tl.float32)\nfor _ in range(0, i_c * BTL, BTS):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = 1 + b_s + 0.5 * b_s * b_s\n    b_z += tl.sum(b_s, axis=1)\n    b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)\n    p_k = tl.advance(p_k, (0, BTS))\n    p_v = tl.advance(p_v, (BTS, 0))\ntl.debug_barrier()\no_q = tl.arange(0, BTL)\no_k = tl.arange(0, BTS)\np_k = tl.make_block_ptr(k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1))\np_v = tl.make_block_ptr(v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0))\nfor _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    m_s = o_q[:, None] >= o_k[None, :]\n    b_s = tl.dot(b_q, b_k, allow_tf32=False)\n    b_s = 1 + b_s + 0.5 * b_s * b_s\n    b_s = tl.where(m_s, b_s, 0)\n    b_z += tl.sum(b_s, axis=1)\n    b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)\n    p_k = tl.advance(p_k, (0, BTS))\n    p_v = tl.advance(p_v, (BTS, 0))\n    o_k += BTS\np_o = tl.make_block_ptr(o + (i_bh + B * H * i_k) * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0))\np_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_z, b_z.to(p_z.dtype.element_ty), mask=i_c * BTL + tl.arange(0, BTL) < T)"
  },
  {
    "name": "parallel_based_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_based_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % NV",
      "    _parallel_based_bwd_dq(",
      "        i_bh, i_c, i_k, i_v, q, k, v, do, dz, dq, scale, T, B, H, BTL, BTS, BK, BV, K, V",
      "    )",
      "    tl.debug_barrier()",
      "    _parallel_based_bwd_dkv(",
      "        i_bh,",
      "        i_c,",
      "        i_k,",
      "        i_v,",
      "        q,",
      "        k,",
      "        v,",
      "        do,",
      "        dz,",
      "        dk,",
      "        dv,",
      "        scale,",
      "        T,",
      "        B,",
      "        H,",
      "        BTL,",
      "        BTS,",
      "        BK,",
      "        BV,",
      "        K,",
      "        V,",
      "    )"
    ],
    "file": "codes/367.py",
    "header": "def parallel_based_bwd_kernel(q, k, v, do, dz, dq, dk, dv, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BTL: tl.constexpr, BTS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr):",
    "body": "i_kv, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nNV = tl.cdiv(V, BV)\ni_k = i_kv // NV\ni_v = i_kv % NV\n_parallel_based_bwd_dq(i_bh, i_c, i_k, i_v, q, k, v, do, dz, dq, scale, T, B, H, BTL, BTS, BK, BV, K, V)\ntl.debug_barrier()\n_parallel_based_bwd_dkv(i_bh, i_c, i_k, i_v, q, k, v, do, dz, dk, dv, scale, T, B, H, BTL, BTS, BK, BV, K, V)"
  },
  {
    "name": "logcumsumexp_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for BT in [16, 32, 64] for num_warps in [2, 4, 8]], key=['S'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logcumsumexp_fwd_kernel(s, z, T, S: tl.constexpr, BT: tl.constexpr):",
      "    i_bh = tl.program_id(0)",
      "    o_i = tl.arange(0, BT)",
      "    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "",
      "    b_mp = tl.full(",
      "        [",
      "            S,",
      "        ],",
      "        float(\"-inf\"),",
      "        dtype=tl.float32,",
      "    )",
      "    b_zp = tl.zeros(",
      "        [",
      "            S,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    for i_t in range(tl.cdiv(T, BT)):",
      "        p_s = tl.make_block_ptr(",
      "            s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0)",
      "        )",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        b_mc = tl.max(b_s, 0)",
      "        b_mc = tl.maximum(b_mp, b_mc)",
      "        b_zp = b_zp * exp(b_mp - b_mc)",
      "",
      "        b_s = exp(b_s - b_mc)",
      "        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp",
      "",
      "        b_zc = tl.max(b_z, 0)",
      "        b_mp = b_mc",
      "        b_zp = b_zc",
      "",
      "        b_z = log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc",
      "        tl.store(p_z, b_z.to(p_z.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/429.py",
    "header": "def logcumsumexp_fwd_kernel(s, z, T, S: tl.constexpr, BT: tl.constexpr):",
    "body": "i_bh = tl.program_id(0)\no_i = tl.arange(0, BT)\nm_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\nb_mp = tl.full([S], float('-inf'), dtype=tl.float32)\nb_zp = tl.zeros([S], dtype=tl.float32)\nfor i_t in range(tl.cdiv(T, BT)):\n    p_s = tl.make_block_ptr(s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0))\n    p_z = tl.make_block_ptr(z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_mc = tl.max(b_s, 0)\n    b_mc = tl.maximum(b_mp, b_mc)\n    b_zp = b_zp * exp(b_mp - b_mc)\n    b_s = exp(b_s - b_mc)\n    b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp\n    b_zc = tl.max(b_z, 0)\n    b_mp = b_mc\n    b_zp = b_zc\n    b_z = log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc\n    tl.store(p_z, b_z.to(p_z.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "act_func_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dropout",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def act_func_forward_kernel(",
      "    input_pointer,",
      "    output_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    dropout: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input = tl.load(input_pointer + offset, mask=mask)",
      "    tl.store(",
      "        output_pointer + offset,",
      "        apply_act_func(input, drop_p, seed, offset, param, act_func, dropout),",
      "        mask=mask,",
      "    )"
    ],
    "file": "codes/18.py",
    "header": "def act_func_forward_kernel(input_pointer, output_pointer, size, drop_p, seed, param, act_func: tl.constexpr, dropout: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\noffset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = offset < size\ninput = tl.load(input_pointer + offset, mask=mask)\ntl.store(output_pointer + offset, apply_act_func(input, drop_p, seed, offset, param, act_func, dropout), mask=mask)"
  },
  {
    "name": "act_func_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dropout",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def act_func_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    input_grad_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    dropout: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad = tl.load(output_grad_pointer + offset, mask=mask)",
      "    input = tl.load(input_pointer + offset, mask=mask)",
      "",
      "    tl.store(",
      "        input_grad_pointer + offset,",
      "        apply_act_func_grad(",
      "            output_grad, input, drop_p, seed, offset, param, act_func, dropout",
      "        ),",
      "        mask=mask,",
      "    )"
    ],
    "file": "codes/18.py",
    "header": "def act_func_backward_kernel(output_grad_pointer, input_pointer, input_grad_pointer, size, drop_p, seed, param, act_func: tl.constexpr, dropout: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\noffset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = offset < size\noutput_grad = tl.load(output_grad_pointer + offset, mask=mask)\ninput = tl.load(input_pointer + offset, mask=mask)\ntl.store(input_grad_pointer + offset, apply_act_func_grad(output_grad, input, drop_p, seed, offset, param, act_func, dropout), mask=mask)"
  },
  {
    "name": "intra_chunk_preprocess_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "AT",
        "annotation": null
      },
      {
        "name": "dA_local",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dq_new",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dk_new",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def intra_chunk_preprocess_bwd_kernel(",
      "    q,",
      "    k,",
      "    w,",
      "    beta,",
      "    AT,",
      "    dA_local,",
      "    dq,",
      "    dq_new,",
      "    dk,",
      "    dk_new,",
      "    dw,",
      "    dbeta,",
      "    dh,",
      "    T,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dw_beta = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dw = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dT = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_w = tl.make_block_ptr(",
      "        w + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_beta = tl.make_block_ptr(",
      "        beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_T = tl.make_block_ptr(",
      "        AT + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_w = tl.load(p_w, boundary_check=(0, 1))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_k.dtype)",
      "    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    b_qw = tl.where(o_i[:, None] >= o_i[None, :], tl.dot(b_q, tl.trans(b_w)), 0).to(",
      "        b_q.dtype",
      "    )",
      "    b_wbk = tl.where(",
      "        o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, tl.trans(b_k)), 0",
      "    ).to(b_k.dtype)",
      "    b_Twb = tl.dot(b_T, b_w_beta).to(b_w.dtype)",
      "    b_Twbk = tl.dot(b_T, b_wbk).to(b_w.dtype)",
      "",
      "    p_dA_local = tl.make_block_ptr(",
      "        dA_local + (bos * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (BT * HQ, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    b_dA_local = tl.load(p_dA_local, boundary_check=(0, 1))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(b_w.dtype)",
      "",
      "    p_dh = tl.make_block_ptr(",
      "        dh + ((boh + i_t) * HQ + i_hq) * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "    )",
      "    b_dh = tl.load(p_dh, boundary_check=(0, 1)).to(b_w.dtype)",
      "    b_dw += tl.dot(b_Twb, tl.trans(b_dh))",
      "    b_dqw = -tl.dot(b_dA_local, tl.trans(b_Twbk)) - tl.dot(",
      "        b_dq.to(b_Twb.dtype), tl.trans(b_Twb)",
      "    )",
      "    b_dTwb = (-tl.dot(tl.trans(b_qw), b_dq) + tl.dot(b_w, b_dh)).to(b_w.dtype)",
      "    b_dT += tl.dot(b_dTwb, tl.trans(b_w_beta))",
      "    b_dw_beta += tl.dot(tl.trans(b_T), b_dTwb)",
      "",
      "    b_dqw = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dqw, 0)",
      "    b_dq += tl.dot(b_dA_local.to(b_k.dtype), b_k)",
      "    b_dq += tl.dot(b_dqw.to(b_w.dtype), b_w)",
      "    b_dw += tl.dot(tl.trans(b_dqw.to(b_q.dtype)), b_q)",
      "    p_q_new = tl.make_block_ptr(",
      "        dq_new + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (K * HQ, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_q_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "    b_dTwbk = -tl.dot(tl.trans(b_qw), b_dA_local.to(b_qw.dtype)) - tl.dot(",
      "        b_w, tl.trans(b_dk.to(b_w.dtype))",
      "    )",
      "    b_dw -= tl.dot(b_Twbk, b_dk.to(b_w.dtype))",
      "    b_dT += tl.dot(b_dTwbk.to(b_wbk.dtype), tl.trans(b_wbk))",
      "    b_dwbk = tl.where(",
      "        o_i[:, None] > o_i[None, :], tl.dot(tl.trans(b_T), b_dTwbk.to(b_T.dtype)), 0",
      "    ).to(b_w.dtype)",
      "    b_dw_beta += tl.dot(b_dwbk, b_k)",
      "",
      "    b_dk += tl.dot(tl.trans(b_dwbk), b_w_beta)",
      "    b_dk += tl.dot(tl.trans(b_dA_local), b_q)",
      "    p_dk_new = tl.make_block_ptr(",
      "        dk_new + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (K * HQ, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dk_new, b_dk.to(dk_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_T = tl.make_block_ptr(",
      "        AT + (bos * H + i_h) * BT, (BT, T), (1, BT * H), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    b_Tt = tl.load(p_T, boundary_check=(0, 1))",
      "    b_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dT, 0).to(",
      "        b_w.dtype",
      "    )",
      "    b_dT = tl.dot(b_Tt, b_dT).to(b_w.dtype)",
      "    b_dT = tl.dot(b_dT, b_Tt)",
      "    b_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dT, 0).to(",
      "        b_k.dtype",
      "    )",
      "",
      "    b_dw_beta += tl.dot(b_dT, b_w)",
      "    b_dw += tl.dot(tl.trans(b_dT), b_w_beta)",
      "    b_dw += b_dw_beta * b_beta[:, None]",
      "    b_dbeta = tl.sum(b_dw_beta * b_w, axis=1)",
      "",
      "    p_dw = tl.make_block_ptr(",
      "        dw + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dw, b_dw.to(dw.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dbeta = tl.make_block_ptr(",
      "        dbeta + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dbeta, b_dbeta.to(dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/406.py",
    "header": "def intra_chunk_preprocess_bwd_kernel(q, k, w, beta, AT, dA_local, dq, dq_new, dk, dk_new, dw, dbeta, dh, T, offsets, indices, chunk_offsets, HQ: tl.constexpr, G: tl.constexpr, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_dw_beta = tl.zeros([BT, BK], dtype=tl.float32)\nb_dw = tl.zeros([BT, BK], dtype=tl.float32)\nb_dT = tl.zeros([BT, BT], dtype=tl.float32)\np_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_beta = tl.make_block_ptr(beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\np_T = tl.make_block_ptr(AT + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_w = tl.load(p_w, boundary_check=(0, 1))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_T = tl.load(p_T, boundary_check=(0, 1)).to(b_k.dtype)\nb_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)\no_i = tl.arange(0, BT)\nb_qw = tl.where(o_i[:, None] >= o_i[None, :], tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)\nb_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, tl.trans(b_k)), 0).to(b_k.dtype)\nb_Twb = tl.dot(b_T, b_w_beta).to(b_w.dtype)\nb_Twbk = tl.dot(b_T, b_wbk).to(b_w.dtype)\np_dA_local = tl.make_block_ptr(dA_local + (bos * HQ + i_hq) * BT, (T, BT), (BT * HQ, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_dA_local = tl.load(p_dA_local, boundary_check=(0, 1))\np_dq = tl.make_block_ptr(dq + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_dq = tl.load(p_dq, boundary_check=(0, 1)).to(b_w.dtype)\np_dh = tl.make_block_ptr(dh + ((boh + i_t) * HQ + i_hq) * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\nb_dh = tl.load(p_dh, boundary_check=(0, 1)).to(b_w.dtype)\nb_dw += tl.dot(b_Twb, tl.trans(b_dh))\nb_dqw = -tl.dot(b_dA_local, tl.trans(b_Twbk)) - tl.dot(b_dq.to(b_Twb.dtype), tl.trans(b_Twb))\nb_dTwb = (-tl.dot(tl.trans(b_qw), b_dq) + tl.dot(b_w, b_dh)).to(b_w.dtype)\nb_dT += tl.dot(b_dTwb, tl.trans(b_w_beta))\nb_dw_beta += tl.dot(tl.trans(b_T), b_dTwb)\nb_dqw = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dqw, 0)\nb_dq += tl.dot(b_dA_local.to(b_k.dtype), b_k)\nb_dq += tl.dot(b_dqw.to(b_w.dtype), b_w)\nb_dw += tl.dot(tl.trans(b_dqw.to(b_q.dtype)), b_q)\np_q_new = tl.make_block_ptr(dq_new + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_q_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))\np_dk = tl.make_block_ptr(dk + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_dk = tl.load(p_dk, boundary_check=(0, 1))\nb_dTwbk = -tl.dot(tl.trans(b_qw), b_dA_local.to(b_qw.dtype)) - tl.dot(b_w, tl.trans(b_dk.to(b_w.dtype)))\nb_dw -= tl.dot(b_Twbk, b_dk.to(b_w.dtype))\nb_dT += tl.dot(b_dTwbk.to(b_wbk.dtype), tl.trans(b_wbk))\nb_dwbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(tl.trans(b_T), b_dTwbk.to(b_T.dtype)), 0).to(b_w.dtype)\nb_dw_beta += tl.dot(b_dwbk, b_k)\nb_dk += tl.dot(tl.trans(b_dwbk), b_w_beta)\nb_dk += tl.dot(tl.trans(b_dA_local), b_q)\np_dk_new = tl.make_block_ptr(dk_new + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_dk_new, b_dk.to(dk_new.dtype.element_ty), boundary_check=(0, 1))\np_T = tl.make_block_ptr(AT + (bos * H + i_h) * BT, (BT, T), (1, BT * H), (0, i_t * BT), (BT, BT), (0, 1))\nb_Tt = tl.load(p_T, boundary_check=(0, 1))\nb_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dT, 0).to(b_w.dtype)\nb_dT = tl.dot(b_Tt, b_dT).to(b_w.dtype)\nb_dT = tl.dot(b_dT, b_Tt)\nb_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dT, 0).to(b_k.dtype)\nb_dw_beta += tl.dot(b_dT, b_w)\nb_dw += tl.dot(tl.trans(b_dT), b_w_beta)\nb_dw += b_dw_beta * b_beta[:, None]\nb_dbeta = tl.sum(b_dw_beta * b_w, axis=1)\np_dw = tl.make_block_ptr(dw + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_dw, b_dw.to(dw.dtype.element_ty), boundary_check=(0, 1))\np_dbeta = tl.make_block_ptr(dbeta + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,))\ntl.store(p_dbeta, b_dbeta.to(dbeta.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "copy_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank'])"
    ],
    "args": [
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "local_buf_ptr",
        "annotation": null
      },
      {
        "name": "global_buf_ptr",
        "annotation": null
      },
      {
        "name": "M_per_rank",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_local_m",
        "annotation": null
      },
      {
        "name": "stride_local_n",
        "annotation": null
      },
      {
        "name": "stride_global_m",
        "annotation": null
      },
      {
        "name": "stride_global_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def copy_kernel(",
      "    rank,",
      "    local_buf_ptr,",
      "    global_buf_ptr,",
      "    M_per_rank,",
      "    N,",
      "    stride_local_m,",
      "    stride_local_n,",
      "    stride_global_m,",
      "    stride_global_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    sm_id = tl.program_id(axis=0)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    pid_m = sm_id // num_pid_n",
      "    pid_n = sm_id % num_pid_n",
      "",
      "    offs_m = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    data_ptr = (",
      "        local_buf_ptr",
      "        + (pid_m * BLOCK_SIZE_M + offs_m[:, None]) * stride_local_m",
      "        + (pid_n * BLOCK_SIZE_N + offs_n[None, :]) * stride_local_n",
      "    )",
      "    dst_ptr = (",
      "        global_buf_ptr",
      "        + (rank * M_per_rank + pid_m * BLOCK_SIZE_M + offs_m[:, None]) * stride_global_m",
      "        + (pid_n * BLOCK_SIZE_N + offs_n[None, :]) * stride_global_n",
      "    )",
      "    mask_data = (pid_m * BLOCK_SIZE_M + offs_m[:, None] < M_per_rank) & (",
      "        pid_n * BLOCK_SIZE_N + offs_n[None, :] < N",
      "    )",
      "    mask_dst = (pid_m * BLOCK_SIZE_M + offs_m[:, None] < M_per_rank) & (",
      "        pid_n * BLOCK_SIZE_N + offs_n[None, :] < N",
      "    )",
      "",
      "    data = tl.load(data_ptr, mask=mask_data)",
      "    tl.store(dst_ptr, data, mask=mask_dst)"
    ],
    "file": "codes/37.py",
    "header": "def copy_kernel(rank, local_buf_ptr, global_buf_ptr, M_per_rank, N, stride_local_m, stride_local_n, stride_global_m, stride_global_n, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):",
    "body": "sm_id = tl.program_id(axis=0)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\npid_m = sm_id // num_pid_n\npid_n = sm_id % num_pid_n\noffs_m = tl.arange(0, BLOCK_SIZE_M)\noffs_n = tl.arange(0, BLOCK_SIZE_N)\ndata_ptr = local_buf_ptr + (pid_m * BLOCK_SIZE_M + offs_m[:, None]) * stride_local_m + (pid_n * BLOCK_SIZE_N + offs_n[None, :]) * stride_local_n\ndst_ptr = global_buf_ptr + (rank * M_per_rank + pid_m * BLOCK_SIZE_M + offs_m[:, None]) * stride_global_m + (pid_n * BLOCK_SIZE_N + offs_n[None, :]) * stride_global_n\nmask_data = (pid_m * BLOCK_SIZE_M + offs_m[:, None] < M_per_rank) & (pid_n * BLOCK_SIZE_N + offs_n[None, :] < N)\nmask_dst = (pid_m * BLOCK_SIZE_M + offs_m[:, None] < M_per_rank) & (pid_n * BLOCK_SIZE_N + offs_n[None, :] < N)\ndata = tl.load(data_ptr, mask=mask_data)\ntl.store(dst_ptr, data, mask=mask_dst)"
  },
  {
    "name": "copy_and_barrier_all_intra_node_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['local_rank', 'rank', 'num_ranks', 'flag_value'])"
    ],
    "args": [
      {
        "name": "local_rank",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "num_ranks",
        "annotation": null
      },
      {
        "name": "local_buf_ptr",
        "annotation": null
      },
      {
        "name": "global_buf_ptr",
        "annotation": null
      },
      {
        "name": "symm_barrier_ptr",
        "annotation": null
      },
      {
        "name": "symm_sync_ptr",
        "annotation": null
      },
      {
        "name": "M_per_rank",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_local_m",
        "annotation": null
      },
      {
        "name": "stride_local_n",
        "annotation": null
      },
      {
        "name": "stride_global_m",
        "annotation": null
      },
      {
        "name": "stride_global_n",
        "annotation": null
      },
      {
        "name": "flag_value",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def copy_and_barrier_all_intra_node_kernel(",
      "    local_rank,",
      "    rank,",
      "    num_ranks,",
      "    local_buf_ptr,",
      "    global_buf_ptr,",
      "    symm_barrier_ptr,",
      "    symm_sync_ptr,",
      "    M_per_rank,",
      "    N,",
      "    stride_local_m,",
      "    stride_local_n,",
      "    stride_global_m,",
      "    stride_global_n,",
      "    flag_value,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    barrier_all_intra_node_non_atomic(",
      "        local_rank, rank, num_ranks, symm_sync_ptr, flag_value",
      "    )",
      "    copy_kernel(",
      "        rank,",
      "        local_buf_ptr,",
      "        global_buf_ptr,",
      "        M_per_rank,",
      "        N,",
      "        stride_local_m,",
      "        stride_local_n,",
      "        stride_global_m,",
      "        stride_global_n,",
      "        BLOCK_SIZE_M,",
      "        BLOCK_SIZE_N,",
      "    )",
      "    thread_idx = tid(0)",
      "    if thread_idx < num_ranks:",
      "        st(symm_barrier_ptr + thread_idx, 1 if thread_idx == rank else 0)",
      "    barrier_all_intra_node_non_atomic(",
      "        local_rank, rank, num_ranks, symm_sync_ptr, flag_value + 1",
      "    )"
    ],
    "file": "codes/37.py",
    "header": "def copy_and_barrier_all_intra_node_kernel(local_rank, rank, num_ranks, local_buf_ptr, global_buf_ptr, symm_barrier_ptr, symm_sync_ptr, M_per_rank, N, stride_local_m, stride_local_n, stride_global_m, stride_global_n, flag_value, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr):",
    "body": "barrier_all_intra_node_non_atomic(local_rank, rank, num_ranks, symm_sync_ptr, flag_value)\ncopy_kernel(rank, local_buf_ptr, global_buf_ptr, M_per_rank, N, stride_local_m, stride_local_n, stride_global_m, stride_global_n, BLOCK_SIZE_M, BLOCK_SIZE_N)\nthread_idx = tid(0)\nif thread_idx < num_ranks:\n    st(symm_barrier_ptr + thread_idx, 1 if thread_idx == rank else 0)\nbarrier_all_intra_node_non_atomic(local_rank, rank, num_ranks, symm_sync_ptr, flag_value + 1)"
  },
  {
    "name": "kernel_consumer_gemm_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_ranks",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ready_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EPILOGUE_SUBTILE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ready_value",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOCAL_WORLD_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_consumer_gemm_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    rank: tl.constexpr,",
      "    num_ranks: tl.constexpr,",
      "    ready_ptr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    EPILOGUE_SUBTILE: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    ready_value: tl.constexpr = 1,",
      "    LOCAL_WORLD_SIZE: tl.constexpr = 8,",
      "):",
      "",
      "    dtype = c_ptr.dtype.element_ty",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "    node_id = rank // LOCAL_WORLD_SIZE",
      "    nnodes = num_ranks // LOCAL_WORLD_SIZE",
      "",
      "    a_desc = tl.make_tensor_descriptor(",
      "        a_ptr,",
      "        shape=[M, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],",
      "    )",
      "    b_desc = tl.make_tensor_descriptor(",
      "        b_ptr,",
      "        shape=[N, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],",
      "    )",
      "    c_desc = tl.make_tensor_descriptor(",
      "        c_ptr,",
      "        shape=[M, N],",
      "        strides=[N, 1],",
      "        block_shape=[",
      "            BLOCK_SIZE_M,",
      "            BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2,",
      "        ],",
      "    )",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    M_per_rank = M // num_ranks",
      "    pid_ms_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            if nnodes == 1:",
      "                alpha = 0",
      "                beta = 0",
      "                pid_m = (",
      "                    pid_m + ((((rank ^ alpha) + beta) % num_ranks) * pid_ms_per_rank)",
      "                ) % num_pid_m",
      "            else:",
      "                m_rank = pid_m // pid_ms_per_rank",
      "                pid_m_intra_rank = pid_m - m_rank * pid_ms_per_rank",
      "                m_node_id = m_rank // LOCAL_WORLD_SIZE",
      "                m_local_rank = m_rank % LOCAL_WORLD_SIZE",
      "                swizzle_m_node_id = (m_node_id + node_id) % nnodes",
      "                swizzle_m_local_rank = (m_local_rank + rank) % LOCAL_WORLD_SIZE",
      "                swizzle_m_rank = (",
      "                    swizzle_m_node_id * LOCAL_WORLD_SIZE + swizzle_m_local_rank",
      "                )",
      "",
      "                pid_m = swizzle_m_rank * pid_ms_per_rank + pid_m_intra_rank",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "            rank_beg = offs_am // M_per_rank",
      "            rank_end = (min(offs_am + BLOCK_SIZE_M, M) - 1) // M_per_rank",
      "            token = dl.wait(",
      "                ready_ptr + rank_beg,",
      "                rank_end - rank_beg + 1,",
      "                \"gpu\",",
      "                \"acquire\",",
      "                waitValue=ready_value,",
      "            )",
      "            a_desc = dl.consume_token(a_desc, token)",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = a_desc.load([offs_am, offs_k])",
      "        b = b_desc.load([offs_bn, offs_k])",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            if EPILOGUE_SUBTILE:",
      "                acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))",
      "                acc = tl.permute(acc, (0, 2, 1))",
      "                acc0, acc1 = tl.split(acc)",
      "                c0 = acc0.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c0)",
      "                c1 = acc1.to(dtype)",
      "                c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)",
      "            else:",
      "                c = accumulator.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c)",
      "",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/37.py",
    "header": "def kernel_consumer_gemm_persistent(a_ptr, b_ptr, c_ptr, M, N, K, rank: tl.constexpr, num_ranks: tl.constexpr, ready_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, EPILOGUE_SUBTILE: tl.constexpr, NUM_SMS: tl.constexpr, ready_value: tl.constexpr = 1, LOCAL_WORLD_SIZE: tl.constexpr = 8):",
    "body": "dtype = c_ptr.dtype.element_ty\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\nnode_id = rank // LOCAL_WORLD_SIZE\nnnodes = num_ranks // LOCAL_WORLD_SIZE\na_desc = tl.make_tensor_descriptor(a_ptr, shape=[M, K], strides=[K, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K])\nb_desc = tl.make_tensor_descriptor(b_ptr, shape=[N, K], strides=[K, 1], block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K])\nc_desc = tl.make_tensor_descriptor(c_ptr, shape=[M, N], strides=[N, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2])\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\npid_m = 0\npid_n = 0\noffs_am = 0\noffs_bn = 0\nM_per_rank = M // num_ranks\npid_ms_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        if nnodes == 1:\n            alpha = 0\n            beta = 0\n            pid_m = (pid_m + ((rank ^ alpha) + beta) % num_ranks * pid_ms_per_rank) % num_pid_m\n        else:\n            m_rank = pid_m // pid_ms_per_rank\n            pid_m_intra_rank = pid_m - m_rank * pid_ms_per_rank\n            m_node_id = m_rank // LOCAL_WORLD_SIZE\n            m_local_rank = m_rank % LOCAL_WORLD_SIZE\n            swizzle_m_node_id = (m_node_id + node_id) % nnodes\n            swizzle_m_local_rank = (m_local_rank + rank) % LOCAL_WORLD_SIZE\n            swizzle_m_rank = swizzle_m_node_id * LOCAL_WORLD_SIZE + swizzle_m_local_rank\n            pid_m = swizzle_m_rank * pid_ms_per_rank + pid_m_intra_rank\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n        rank_beg = offs_am // M_per_rank\n        rank_end = (min(offs_am + BLOCK_SIZE_M, M) - 1) // M_per_rank\n        token = dl.wait(ready_ptr + rank_beg, rank_end - rank_beg + 1, 'gpu', 'acquire', waitValue=ready_value)\n        a_desc = dl.consume_token(a_desc, token)\n    offs_k = ki * BLOCK_SIZE_K\n    a = a_desc.load([offs_am, offs_k])\n    b = b_desc.load([offs_bn, offs_k])\n    accumulator = tl.dot(a, b.T, accumulator)\n    if ki == k_tiles - 1:\n        if EPILOGUE_SUBTILE:\n            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n            acc = tl.permute(acc, (0, 2, 1))\n            acc0, acc1 = tl.split(acc)\n            c0 = acc0.to(dtype)\n            c_desc.store([offs_am, offs_bn], c0)\n            c1 = acc1.to(dtype)\n            c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)\n        else:\n            c = accumulator.to(dtype)\n            c_desc.store([offs_am, offs_bn], c)\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "kernel_consumer_gemm_non_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank'], launch_metadata=_matmul_launch_metadata, repr=_kernel_consumer_gemm_non_persistent_repr)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "barrier_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_consumer_gemm_non_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    rank,",
      "    WORLD_SIZE: tl.constexpr,",
      "    barrier_ptr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    a_dtype = a_ptr.dtype.element_ty",
      "    b_dtype = b_ptr.dtype.element_ty",
      "    c_dtype = c_ptr.dtype.element_ty",
      "",
      "    tl.static_assert(a_dtype == b_dtype, \"A and B must have the same dtype\")",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    m_per_rank = M // WORLD_SIZE",
      "    m_offset = m_per_rank * rank",
      "    pid_m_offset = tl.cdiv(m_offset, BLOCK_SIZE_M)",
      "    pid_m = (pid_m + pid_m_offset) % num_pid_m",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M",
      "    rank_beg = offs_am // m_per_rank",
      "    rank_end = (min(offs_am + BLOCK_SIZE_M, M) - 1) // m_per_rank",
      "    token = dl.wait(",
      "        barrier_ptr + rank_beg, rank_end - rank_beg + 1, \"gpu\", \"acquire\", waitValue=1",
      "    )",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    a_ptrs = dl.consume_token(a_ptrs, token)",
      "",
      "    if a_dtype == tl.int8:",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)",
      "    else:",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "",
      "    tl.store(c_ptrs, accumulator.to(c_dtype), mask=c_mask)"
    ],
    "file": "codes/37.py",
    "header": "def kernel_consumer_gemm_non_persistent(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, rank, WORLD_SIZE: tl.constexpr, barrier_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "a_dtype = a_ptr.dtype.element_ty\nb_dtype = b_ptr.dtype.element_ty\nc_dtype = c_ptr.dtype.element_ty\ntl.static_assert(a_dtype == b_dtype, 'A and B must have the same dtype')\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\nm_per_rank = M // WORLD_SIZE\nm_offset = m_per_rank * rank\npid_m_offset = tl.cdiv(m_offset, BLOCK_SIZE_M)\npid_m = (pid_m + pid_m_offset) % num_pid_m\noffs_am = pid_m * BLOCK_SIZE_M\nrank_beg = offs_am // m_per_rank\nrank_end = (min(offs_am + BLOCK_SIZE_M, M) - 1) // m_per_rank\ntoken = dl.wait(barrier_ptr + rank_beg, rank_end - rank_beg + 1, 'gpu', 'acquire', waitValue=1)\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\na_ptrs = dl.consume_token(a_ptrs, token)\nif a_dtype == tl.int8:\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\nelse:\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator += tl.dot(a, b)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, accumulator.to(c_dtype), mask=c_mask)"
  },
  {
    "name": "triton_sum_kernel_1D_result_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_NON_REDUCE_DIM': b_nr, 'BLOCK_SIZE_REDUCE_DIM': b_r}, num_warps=w) for b_nr, b_r, w in itertools.product([2, 4, 8, 16], [2, 4, 8, 16], [2, 4, 8])], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_NON_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dim",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_1D_result_sum_then_buffer(",
      "    input_ptr,",
      "    output_ptr,",
      "    M,",
      "    N,",
      "    BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr,",
      "    BLOCK_SIZE_REDUCE_DIM: tl.constexpr,",
      "    dim: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    reduce_dim_len = M if dim == 0 else N",
      "    non_reduce_dim_len = N if dim == 0 else M",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32)",
      "",
      "    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM",
      "    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(",
      "        0, BLOCK_SIZE_NON_REDUCE_DIM",
      "    )",
      "    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len",
      "",
      "    for block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):",
      "        offsets_reduce_dim = block_start_reduce_dim + tl.arange(",
      "            0, BLOCK_SIZE_REDUCE_DIM",
      "        )",
      "        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len",
      "",
      "        idxs, mask = None, None",
      "        if dim == 0:",
      "            idxs = (",
      "                offsets_reduce_dim[:, None] * non_reduce_dim_len",
      "            ) + offsets_non_reduce_dim",
      "            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim",
      "        elif dim == 1:",
      "            idxs = (",
      "                offsets_non_reduce_dim[:, None] * reduce_dim_len",
      "            ) + offsets_reduce_dim",
      "            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim",
      "",
      "        input = tl.load(input_ptr + idxs, mask=mask, other=mask)",
      "",
      "        buffer += tl.sum(input, axis=dim)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_NON_REDUCE_DIM,),",
      "    )",
      "",
      "    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)"
    ],
    "file": "codes/675.py",
    "header": "def triton_sum_kernel_1D_result_sum_then_buffer(input_ptr, output_ptr, M, N, BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr, BLOCK_SIZE_REDUCE_DIM: tl.constexpr, dim: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nreduce_dim_len = M if dim == 0 else N\nnon_reduce_dim_len = N if dim == 0 else M\nbuffer = tl.zeros((1, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32)\nblock_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM\noffsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(0, BLOCK_SIZE_NON_REDUCE_DIM)\nmask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len\nfor block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):\n    offsets_reduce_dim = block_start_reduce_dim + tl.arange(0, BLOCK_SIZE_REDUCE_DIM)\n    mask_reduce_dim = offsets_reduce_dim < reduce_dim_len\n    idxs, mask = (None, None)\n    if dim == 0:\n        idxs = offsets_reduce_dim[:, None] * non_reduce_dim_len + offsets_non_reduce_dim\n        mask = mask_reduce_dim[:, None] & mask_non_reduce_dim\n    elif dim == 1:\n        idxs = offsets_non_reduce_dim[:, None] * reduce_dim_len + offsets_reduce_dim\n        mask = mask_non_reduce_dim[:, None] & mask_reduce_dim\n    input = tl.load(input_ptr + idxs, mask=mask, other=mask)\n    buffer += tl.sum(input, axis=dim)\nbuffer_view = buffer.reshape((BLOCK_SIZE_NON_REDUCE_DIM,))\ntl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)"
  },
  {
    "name": "triton_sum_kernel_1D_result_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_NON_REDUCE_DIM': b, 'BLOCK_SIZE_REDUCE_DIM': b}, num_warps=w) for b, w in itertools.product([2, 4, 8, 16], [2, 4, 8])], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_NON_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dim",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_1D_result_buffer_then_sum(",
      "    input_ptr,",
      "    output_ptr,",
      "    M,",
      "    N,",
      "    BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr,",
      "    BLOCK_SIZE_REDUCE_DIM: tl.constexpr,",
      "    dim: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    reduce_dim_len = M if dim == 0 else N",
      "    non_reduce_dim_len = N if dim == 0 else M",
      "",
      "    buffer = tl.zeros(",
      "        (BLOCK_SIZE_REDUCE_DIM, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32",
      "    )",
      "",
      "    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM",
      "    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(",
      "        0, BLOCK_SIZE_NON_REDUCE_DIM",
      "    )",
      "    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len",
      "",
      "    for block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):",
      "        offsets_reduce_dim = block_start_reduce_dim + tl.arange(",
      "            0, BLOCK_SIZE_REDUCE_DIM",
      "        )",
      "        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len",
      "",
      "        idxs, mask = None, None",
      "        if dim == 0:",
      "            idxs = (",
      "                offsets_reduce_dim[:, None] * non_reduce_dim_len",
      "            ) + offsets_non_reduce_dim",
      "            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim",
      "        elif dim == 1:",
      "            idxs = (",
      "                offsets_non_reduce_dim[:, None] * reduce_dim_len",
      "            ) + offsets_reduce_dim",
      "            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim",
      "",
      "        buffer += tl.load(input_ptr + idxs, mask=mask, other=mask)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=dim)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_NON_REDUCE_DIM,),",
      "    )",
      "",
      "    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)"
    ],
    "file": "codes/675.py",
    "header": "def triton_sum_kernel_1D_result_buffer_then_sum(input_ptr, output_ptr, M, N, BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr, BLOCK_SIZE_REDUCE_DIM: tl.constexpr, dim: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nreduce_dim_len = M if dim == 0 else N\nnon_reduce_dim_len = N if dim == 0 else M\nbuffer = tl.zeros((BLOCK_SIZE_REDUCE_DIM, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32)\nblock_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM\noffsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(0, BLOCK_SIZE_NON_REDUCE_DIM)\nmask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len\nfor block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):\n    offsets_reduce_dim = block_start_reduce_dim + tl.arange(0, BLOCK_SIZE_REDUCE_DIM)\n    mask_reduce_dim = offsets_reduce_dim < reduce_dim_len\n    idxs, mask = (None, None)\n    if dim == 0:\n        idxs = offsets_reduce_dim[:, None] * non_reduce_dim_len + offsets_non_reduce_dim\n        mask = mask_reduce_dim[:, None] & mask_non_reduce_dim\n    elif dim == 1:\n        idxs = offsets_non_reduce_dim[:, None] * reduce_dim_len + offsets_reduce_dim\n        mask = mask_non_reduce_dim[:, None] & mask_reduce_dim\n    buffer += tl.load(input_ptr + idxs, mask=mask, other=mask)\nbuffer_sum = tl.sum(buffer, axis=dim)\nbuffer_view = buffer_sum.reshape((BLOCK_SIZE_NON_REDUCE_DIM,))\ntl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)"
  },
  {
    "name": "triton_sum_kernel_2D_result_dim_1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_K': b}, num_warps=w) for b, w in itertools.product([2, 4, 16, 32, 128, 256], [2, 4, 8])], key=['N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_2D_result_dim_1(",
      "    input_ptr,",
      "    output_ptr,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)",
      "    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    block_start_n = 0",
      "    block_start_k = pid_k * BLOCK_SIZE_K",
      "",
      "    offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)",
      "    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    mask_n = offsets_n < N",
      "    mask_k = offsets_k < K",
      "",
      "    idxs_base = (offsets_n[:, None] * K) + offsets_k",
      "    idxs = idxs_base + (pid_m * N * K)",
      "",
      "    mask = mask_n[:, None] & mask_k",
      "",
      "    input = tl.load(input_ptr + idxs, mask=mask, other=0)",
      "",
      "    output = tl.sum(input, axis=0)",
      "",
      "    output_offsets = (pid_m * K) + offsets_k",
      "",
      "    tl.store(output_ptr + output_offsets, output, mask=mask_k)"
    ],
    "file": "codes/675.py",
    "header": "def triton_sum_kernel_2D_result_dim_1(input_ptr, output_ptr, M: tl.constexpr, N: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\npid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\nblock_start_n = 0\nblock_start_k = pid_k * BLOCK_SIZE_K\noffsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\noffsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\nmask_n = offsets_n < N\nmask_k = offsets_k < K\nidxs_base = offsets_n[:, None] * K + offsets_k\nidxs = idxs_base + pid_m * N * K\nmask = mask_n[:, None] & mask_k\ninput = tl.load(input_ptr + idxs, mask=mask, other=0)\noutput = tl.sum(input, axis=0)\noutput_offsets = pid_m * K + offsets_k\ntl.store(output_ptr + output_offsets, output, mask=mask_k)"
  },
  {
    "name": "triton_sum_kernel_2D_result_dim_1_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': b_n, 'BLOCK_SIZE_K': b_k}, num_warps=w) for b_n, b_k, w in itertools.product([4 ** n for n in range(6)], [4 ** n for n in range(4)], [2, 4, 8])], key=['N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_2D_result_dim_1_sum_then_buffer(",
      "    input_ptr,",
      "    output_ptr,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)",
      "    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_K), dtype=tl.float32)",
      "",
      "    block_start_k = pid_k * BLOCK_SIZE_K",
      "    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)",
      "    mask_k = offsets_k < K",
      "",
      "    for block_start_n in range(0, N, BLOCK_SIZE_N):",
      "        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)",
      "        mask_n = offsets_n < N",
      "",
      "        idxs_base = (offsets_n[:, None] * K) + offsets_k",
      "        idxs = idxs_base + (pid_m * N * K)",
      "",
      "        mask = mask_n[:, None] & mask_k",
      "",
      "        input = tl.load(input_ptr + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_K,),",
      "    )",
      "",
      "    output_offsets = (pid_m * K) + offsets_k",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=mask_k)"
    ],
    "file": "codes/675.py",
    "header": "def triton_sum_kernel_2D_result_dim_1_sum_then_buffer(input_ptr, output_ptr, M: tl.constexpr, N: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\npid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\nbuffer = tl.zeros((1, BLOCK_SIZE_K), dtype=tl.float32)\nblock_start_k = pid_k * BLOCK_SIZE_K\noffsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\nmask_k = offsets_k < K\nfor block_start_n in range(0, N, BLOCK_SIZE_N):\n    offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < N\n    idxs_base = offsets_n[:, None] * K + offsets_k\n    idxs = idxs_base + pid_m * N * K\n    mask = mask_n[:, None] & mask_k\n    input = tl.load(input_ptr + idxs, mask=mask, other=0)\n    buffer += tl.sum(input, axis=0)\nbuffer_view = buffer.reshape((BLOCK_SIZE_K,))\noutput_offsets = pid_m * K + offsets_k\ntl.store(output_ptr + output_offsets, buffer_view, mask=mask_k)"
  },
  {
    "name": "triton_sum_kernel_2D_result_dim_1_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': b_n, 'BLOCK_SIZE_K': b_k}, num_warps=w) for b_n, b_k, w in itertools.product([4 ** n for n in range(7)], [4 ** n for n in range(4)], [2, 4, 8])], key=['N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_2D_result_dim_1_buffer_then_sum(",
      "    input_ptr,",
      "    output_ptr,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)",
      "    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)",
      "",
      "    block_start_k = pid_k * BLOCK_SIZE_K",
      "    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)",
      "    mask_k = offsets_k < K",
      "",
      "    for block_start_n in range(0, N, BLOCK_SIZE_N):",
      "        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)",
      "        mask_n = offsets_n < N",
      "",
      "        idxs_base = (offsets_n[:, None] * K) + offsets_k",
      "        idxs = idxs_base + (pid_m * N * K)",
      "",
      "        mask = mask_n[:, None] & mask_k",
      "",
      "        input = tl.load(input_ptr + idxs, mask=mask, other=0)",
      "",
      "        buffer += input",
      "",
      "    output = tl.sum(buffer, axis=0)",
      "",
      "    output_offsets = (pid_m * K) + offsets_k",
      "",
      "    tl.store(output_ptr + output_offsets, output, mask=mask_k)"
    ],
    "file": "codes/675.py",
    "header": "def triton_sum_kernel_2D_result_dim_1_buffer_then_sum(input_ptr, output_ptr, M: tl.constexpr, N: tl.constexpr, K: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)\npid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)\nbuffer = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)\nblock_start_k = pid_k * BLOCK_SIZE_K\noffsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)\nmask_k = offsets_k < K\nfor block_start_n in range(0, N, BLOCK_SIZE_N):\n    offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)\n    mask_n = offsets_n < N\n    idxs_base = offsets_n[:, None] * K + offsets_k\n    idxs = idxs_base + pid_m * N * K\n    mask = mask_n[:, None] & mask_k\n    input = tl.load(input_ptr + idxs, mask=mask, other=0)\n    buffer += input\noutput = tl.sum(buffer, axis=0)\noutput_offsets = pid_m * K + offsets_k\ntl.store(output_ptr + output_offsets, output, mask=mask_k)"
  },
  {
    "name": "update_fn_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_warps=4), triton.Config({'BLOCK_SIZE': 1024}, num_warps=8)], key=['n_elements'], restore_value=['p_ptr', 'exp_avg_ptr'])"
    ],
    "args": [
      {
        "name": "p_ptr",
        "annotation": null
      },
      {
        "name": "grad_ptr",
        "annotation": null
      },
      {
        "name": "exp_avg_ptr",
        "annotation": null
      },
      {
        "name": "lr",
        "annotation": null
      },
      {
        "name": "wd",
        "annotation": null
      },
      {
        "name": "beta1",
        "annotation": null
      },
      {
        "name": "beta2",
        "annotation": null
      },
      {
        "name": "n_elements",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def update_fn_kernel(",
      "    p_ptr,",
      "    grad_ptr,",
      "    exp_avg_ptr,",
      "    lr,",
      "    wd,",
      "    beta1,",
      "    beta2,",
      "    n_elements,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "",
      "    block_start = pid * BLOCK_SIZE",
      "    offsets = block_start + tl.arange(0, BLOCK_SIZE)",
      "",
      "    mask = offsets < n_elements",
      "",
      "    offset_p_ptr = p_ptr + offsets",
      "    offset_grad_ptr = grad_ptr + offsets",
      "    offset_exp_avg_ptr = exp_avg_ptr + offsets",
      "",
      "    p = tl.load(offset_p_ptr, mask=mask)",
      "    grad = tl.load(offset_grad_ptr, mask=mask)",
      "    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)",
      "",
      "    p = p * (1 - lr * wd)",
      "",
      "    diff = exp_avg - grad",
      "",
      "    update = diff * beta1 + grad",
      "",
      "    can_update = update != 0",
      "    update_sign = tl.where(update > 0, -lr, lr)",
      "",
      "    p = p + update_sign * can_update",
      "",
      "    exp_avg = diff * beta2 + grad",
      "",
      "    tl.store(offset_p_ptr, p, mask=mask)",
      "    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)"
    ],
    "file": "codes/735.py",
    "header": "def update_fn_kernel(p_ptr, grad_ptr, exp_avg_ptr, lr, wd, beta1, beta2, n_elements, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nblock_start = pid * BLOCK_SIZE\noffsets = block_start + tl.arange(0, BLOCK_SIZE)\nmask = offsets < n_elements\noffset_p_ptr = p_ptr + offsets\noffset_grad_ptr = grad_ptr + offsets\noffset_exp_avg_ptr = exp_avg_ptr + offsets\np = tl.load(offset_p_ptr, mask=mask)\ngrad = tl.load(offset_grad_ptr, mask=mask)\nexp_avg = tl.load(offset_exp_avg_ptr, mask=mask)\np = p * (1 - lr * wd)\ndiff = exp_avg - grad\nupdate = diff * beta1 + grad\ncan_update = update != 0\nupdate_sign = tl.where(update > 0, -lr, lr)\np = p + update_sign * can_update\nexp_avg = diff * beta2 + grad\ntl.store(offset_p_ptr, p, mask=mask)\ntl.store(offset_exp_avg_ptr, exp_avg, mask=mask)"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        accumulator = leaky_relu(accumulator)",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/641.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, ACTIVATION: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nif ACTIVATION == 'leaky_relu':\n    accumulator = leaky_relu(accumulator)\nc = accumulator.to(tl.float16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=tuning_configs, key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_BUFFER_OPS_ASSUMES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,",
      "):",
      "",
      "    if ENABLE_BUFFER_OPS_ASSUMES:",
      "        tl.assume(M >= 0)",
      "        tl.assume(N >= 0)",
      "        tl.assume(K >= 0)",
      "        tl.assume(stride_am >= 0)",
      "        tl.assume(stride_ak >= 0)",
      "        tl.assume(stride_bn >= 0)",
      "        tl.assume(stride_bk >= 0)",
      "        tl.assume(stride_cm >= 0)",
      "        tl.assume(stride_cn >= 0)",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    num_pid_in_group = GROUP_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    tl.assume(pid_m >= 0)",
      "    tl.assume(pid_n >= 0)",
      "",
      "    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_K * stride_ak",
      "        b_ptrs += BLOCK_K * stride_bk",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        accumulator = leaky_relu(accumulator)",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/651.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr, ACTIVATION: tl.constexpr, ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr):",
    "body": "if ENABLE_BUFFER_OPS_ASSUMES:\n    tl.assume(M >= 0)\n    tl.assume(N >= 0)\n    tl.assume(K >= 0)\n    tl.assume(stride_am >= 0)\n    tl.assume(stride_ak >= 0)\n    tl.assume(stride_bn >= 0)\n    tl.assume(stride_bk >= 0)\n    tl.assume(stride_cm >= 0)\n    tl.assume(stride_cn >= 0)\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_M)\nnum_pid_n = tl.cdiv(N, BLOCK_N)\nnum_pid_in_group = GROUP_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\ntl.assume(pid_m >= 0)\ntl.assume(pid_n >= 0)\noffs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\noffs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\noffs_k = tl.arange(0, BLOCK_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)\n    accumulator += tl.dot(a, b)\n    a_ptrs += BLOCK_K * stride_ak\n    b_ptrs += BLOCK_K * stride_bk\nif ACTIVATION == 'leaky_relu':\n    accumulator = leaky_relu(accumulator)\nc = accumulator.to(tl.float16)\noffs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\noffs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "swizzle_tile",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def swizzle_tile(",
      "    tile_id,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = tile_id // width",
      "    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (tile_id % group_size)",
      "    pid_n = (tile_id % width) // group_size",
      "    return pid_m, pid_n"
    ],
    "file": "codes/562.py",
    "header": "def swizzle_tile(tile_id, M, N, K, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr):",
    "body": "grid_m = tl.cdiv(M, BLOCK_M)\ngrid_n = tl.cdiv(N, BLOCK_N)\nwidth = GROUP_M * grid_n\ngroup_id = tile_id // width\ngroup_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)\npid_m = group_id * GROUP_M + tile_id % group_size\npid_n = tile_id % width // group_size\nreturn (pid_m, pid_n)"
  },
  {
    "name": "linear_tile",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_tile(",
      "    tile_id,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    pid_m = tile_id // tl.cdiv(N, BLOCK_N)",
      "    pid_n = tile_id % tl.cdiv(N, BLOCK_N)",
      "    return pid_m, pid_n"
    ],
    "file": "codes/562.py",
    "header": "def linear_tile(tile_id, M, N, K, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr):",
    "body": "pid_m = tile_id // tl.cdiv(N, BLOCK_N)\npid_n = tile_id % tl.cdiv(N, BLOCK_N)\nreturn (pid_m, pid_n)"
  },
  {
    "name": "mac_loop",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "locks",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "iters_per_tile",
        "annotation": null
      },
      {
        "name": "start_iter",
        "annotation": null
      },
      {
        "name": "end_iter",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mac_loop(",
      "    A,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    locks,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    iters_per_tile,",
      "    start_iter,",
      "    end_iter,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "",
      "    tile_id = start_iter // iters_per_tile",
      "    if GROUP_M > 0:",
      "        pid_m, pid_n = swizzle_tile(",
      "            tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M",
      "        )",
      "    else:",
      "        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    rk = tl.arange(0, BLOCK_K)",
      "    A = (",
      "        A",
      "        + (rm[:, None] * stride_am + rk[None, :] * stride_ak)",
      "        + BLOCK_K * stride_ak * (start_iter % iters_per_tile)",
      "    )",
      "    B = (",
      "        B",
      "        + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)",
      "        + BLOCK_K * stride_bk * (start_iter % iters_per_tile)",
      "    )",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)",
      "",
      "    for current_iter in range(start_iter, end_iter):",
      "        a = tl.load(A)",
      "        b = tl.load(B)",
      "        acc += tl.dot(a, b)",
      "        A += BLOCK_K * stride_ak",
      "        B += BLOCK_K * stride_bk",
      "",
      "    if end_iter % iters_per_tile == 0:",
      "        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "        tl.store(C_, acc)",
      "        if start_iter % iters_per_tile != 0:",
      "            tl.atomic_xchg(locks + tile_id, 1)",
      "    else:",
      "        while tl.atomic_cas(locks + tile_id, 1, 1) != 1:",
      "            pass",
      "        C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "        tl.atomic_add(C_, acc)"
    ],
    "file": "codes/562.py",
    "header": "def mac_loop(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, iters_per_tile, start_iter, end_iter, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr):",
    "body": "tile_id = start_iter // iters_per_tile\nif GROUP_M > 0:\n    pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nelse:\n    pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nrm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nrn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nrk = tl.arange(0, BLOCK_K)\nA = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak) + BLOCK_K * stride_ak * (start_iter % iters_per_tile)\nB = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn) + BLOCK_K * stride_bk * (start_iter % iters_per_tile)\nacc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\nfor current_iter in range(start_iter, end_iter):\n    a = tl.load(A)\n    b = tl.load(B)\n    acc += tl.dot(a, b)\n    A += BLOCK_K * stride_ak\n    B += BLOCK_K * stride_bk\nif end_iter % iters_per_tile == 0:\n    C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    tl.store(C_, acc)\n    if start_iter % iters_per_tile != 0:\n        tl.atomic_xchg(locks + tile_id, 1)\nelse:\n    while tl.atomic_cas(locks + tile_id, 1, 1) != 1:\n        pass\n    C_ = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\n    tl.atomic_add(C_, acc)"
  },
  {
    "name": "first_wave",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "locks",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "total_full_tiles_streamk",
        "annotation": null
      },
      {
        "name": "total_partial_tiles_streamk",
        "annotation": null
      },
      {
        "name": "iters_per_tile",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def first_wave(",
      "    A,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    locks,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    total_full_tiles_streamk,",
      "    total_partial_tiles_streamk,",
      "    iters_per_tile,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    start_iter = pid * total_full_tiles_streamk + tl.minimum(",
      "        pid, total_partial_tiles_streamk",
      "    )",
      "    last_iter = (pid + 1) * total_full_tiles_streamk + tl.minimum(",
      "        pid + 1, total_partial_tiles_streamk",
      "    )",
      "",
      "    while start_iter < last_iter:",
      "        end_iter = tl.minimum(",
      "            start_iter + (iters_per_tile - start_iter % iters_per_tile), last_iter",
      "        )",
      "        mac_loop(",
      "            A,",
      "            B,",
      "            C,",
      "            M,",
      "            N,",
      "            K,",
      "            locks,",
      "            stride_am,",
      "            stride_ak,",
      "            stride_bk,",
      "            stride_bn,",
      "            stride_cm,",
      "            stride_cn,",
      "            iters_per_tile,",
      "            start_iter,",
      "            end_iter,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            BLOCK_K,",
      "            ACC_TYPE,",
      "            GROUP_M,",
      "        )",
      "",
      "        start_iter = end_iter"
    ],
    "file": "codes/562.py",
    "header": "def first_wave(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_full_tiles_streamk, total_partial_tiles_streamk, iters_per_tile, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nstart_iter = pid * total_full_tiles_streamk + tl.minimum(pid, total_partial_tiles_streamk)\nlast_iter = (pid + 1) * total_full_tiles_streamk + tl.minimum(pid + 1, total_partial_tiles_streamk)\nwhile start_iter < last_iter:\n    end_iter = tl.minimum(start_iter + (iters_per_tile - start_iter % iters_per_tile), last_iter)\n    mac_loop(A, B, C, M, N, K, locks, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, iters_per_tile, start_iter, end_iter, BLOCK_M, BLOCK_N, BLOCK_K, ACC_TYPE, GROUP_M)\n    start_iter = end_iter"
  },
  {
    "name": "full_tiles",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "total_tiles_streamk",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def full_tiles(",
      "    A,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    total_tiles_streamk,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "",
      "    tile_id = tl.program_id(0) + total_tiles_streamk",
      "    if GROUP_M > 0:",
      "        pid_m, pid_n = swizzle_tile(",
      "            tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M",
      "        )",
      "    else:",
      "        pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    rk = tl.arange(0, BLOCK_K)",
      "",
      "    A = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)",
      "    B = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)",
      "    for k in range(0, tl.cdiv(K, BLOCK_K)):",
      "        a = tl.load(A)",
      "        b = tl.load(B)",
      "        acc += tl.dot(a, b)",
      "        A += BLOCK_K * stride_ak",
      "        B += BLOCK_K * stride_bk",
      "    acc = acc.to(tl.float16)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "    tl.store(C, acc)"
    ],
    "file": "codes/562.py",
    "header": "def full_tiles(A, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, total_tiles_streamk, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, ACC_TYPE: tl.constexpr, GROUP_M: tl.constexpr):",
    "body": "tile_id = tl.program_id(0) + total_tiles_streamk\nif GROUP_M > 0:\n    pid_m, pid_n = swizzle_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nelse:\n    pid_m, pid_n = linear_tile(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M)\nrm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nrn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nrk = tl.arange(0, BLOCK_K)\nA = A + (rm[:, None] * stride_am + rk[None, :] * stride_ak)\nB = B + (rk[:, None] * stride_bk + rn[None, :] * stride_bn)\nacc = tl.zeros((BLOCK_M, BLOCK_N), dtype=ACC_TYPE)\nfor k in range(0, tl.cdiv(K, BLOCK_K)):\n    a = tl.load(A)\n    b = tl.load(B)\n    acc += tl.dot(a, b)\n    A += BLOCK_K * stride_ak\n    B += BLOCK_K * stride_bk\nacc = acc.to(tl.float16)\nrm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nrn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\nC = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)\ntl.store(C, acc)"
  },
  {
    "name": "fused_recurrent_rwkv7_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BV in [16, 32, 64] for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "kk",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_DECODE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv7_fwd_kernel(",
      "    r,",
      "    w,",
      "    k,",
      "    v,",
      "    kk,",
      "    a,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    IS_DECODE: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_k = tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_r = r + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_a = a + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_kk = kk + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "",
      "    p_o = o + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    if IS_DECODE:",
      "        b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)",
      "        b_act_a = -b_kk",
      "        b_b = b_kk * b_a",
      "",
      "        tmp = tl.sum(b_h * b_act_a[None, :], axis=1)",
      "        b_h = exp(b_w)[None, :] * b_h + (",
      "            tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "        )",
      "        b_o = tl.sum(b_h * b_r[None, :], axis=1)",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "    else:",
      "        for _ in range(0, T):",
      "            b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale",
      "            b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "            b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "            b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "            b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "            b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)",
      "            b_act_a = -b_kk",
      "            b_b = b_kk * b_a",
      "",
      "            tmp = tl.sum(b_h * b_act_a[None, :], axis=1)",
      "            b_h = exp(b_w)[None, :] * b_h + (",
      "                tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "            )",
      "            b_o = tl.sum(b_h * b_r[None, :], axis=1)",
      "",
      "            tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "            p_r += (-1 if REVERSE else 1) * H * K",
      "            p_w += (-1 if REVERSE else 1) * H * K",
      "            p_k += (-1 if REVERSE else 1) * H * K",
      "            p_v += (-1 if REVERSE else 1) * H * V",
      "            p_a += (-1 if REVERSE else 1) * H * K",
      "            p_kk += (-1 if REVERSE else 1) * H * K",
      "            p_o += (-1 if REVERSE else 1) * H * V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/422.py",
    "header": "def fused_recurrent_rwkv7_fwd_kernel(r, w, k, v, kk, a, o, h0, ht, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, REVERSE: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr, IS_DECODE: tl.constexpr):",
    "body": "i_v, i_nh = (tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\no_k = tl.arange(0, BK)\no_v = i_v * BV + tl.arange(0, BV)\np_r = r + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_w = w + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_k = k + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_v = v + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\np_a = a + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_kk = kk + (bos + (T - 1 if REVERSE else 0)) * H * K + i_h * K + o_k\np_o = o + (bos + (T - 1 if REVERSE else 0)) * H * V + i_h * V + o_v\nmask_k = o_k < K\nmask_v = o_v < V\nmask_h = mask_k[None, :] & mask_v[:, None]\nb_h = tl.zeros([BV, BK], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nif IS_DECODE:\n    b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale\n    b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n    b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)\n    b_act_a = -b_kk\n    b_b = b_kk * b_a\n    tmp = tl.sum(b_h * b_act_a[None, :], axis=1)\n    b_h = exp(b_w)[None, :] * b_h + (tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None])\n    b_o = tl.sum(b_h * b_r[None, :], axis=1)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\nelse:\n    for _ in range(0, T):\n        b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale\n        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)\n        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)\n        b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)\n        b_act_a = -b_kk\n        b_b = b_kk * b_a\n        tmp = tl.sum(b_h * b_act_a[None, :], axis=1)\n        b_h = exp(b_w)[None, :] * b_h + (tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None])\n        b_o = tl.sum(b_h * b_r[None, :], axis=1)\n        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n        p_r += (-1 if REVERSE else 1) * H * K\n        p_w += (-1 if REVERSE else 1) * H * K\n        p_k += (-1 if REVERSE else 1) * H * K\n        p_v += (-1 if REVERSE else 1) * H * V\n        p_a += (-1 if REVERSE else 1) * H * K\n        p_kk += (-1 if REVERSE else 1) * H * K\n        p_o += (-1 if REVERSE else 1) * H * V\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'], 'require_n_boundary_check': lambda args: args['n_size'] % args['n_block_size'], 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_n_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_n_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    input_batch_stride: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    weight_n_stride: tl.int32,",
      "    weight_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    n_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_n_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_m_blocks = tl.cdiv(m_size, m_block_size)",
      "    num_n_blocks = tl.cdiv(n_size, n_block_size)",
      "    num_blocks = num_m_blocks * num_n_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    m_block = block // num_n_blocks",
      "    n_block = block % num_n_blocks",
      "    m_offset = m_block * m_block_size",
      "    n_offset = n_block * n_block_size",
      "",
      "    output = language.Linear.forward(",
      "        input_ptr + batch * input_batch_stride,",
      "        weight_ptr,",
      "        bias_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        n_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        n_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_n_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr + batch * m_size * n_size,",
      "        shape=(m_size, n_size),",
      "        strides=(n_size, 1),",
      "        offsets=(m_offset, n_offset),",
      "        block_shape=(m_block_size, n_block_size),",
      "        order=(1, 0),",
      "    )",
      "    if require_m_boundary_check | require_n_boundary_check:",
      "        tl.store(output_block_ptr, output, boundary_check=(0, 1))",
      "    else:",
      "        tl.store(output_block_ptr, output)"
    ],
    "file": "codes/507.py",
    "header": "def forward(output_ptr: tl.tensor, input_ptr: tl.tensor, weight_ptr: tl.tensor, bias_ptr: tl.tensor, m_size: tl.int32, n_size: tl.int32, k_size: tl.int32, input_batch_stride: tl.int32, input_m_stride: tl.int32, input_k_stride: tl.int32, weight_n_stride: tl.int32, weight_k_stride: tl.int32, use_accelerator: tl.constexpr, dtype: tl.constexpr, m_block_size: tl.constexpr, n_block_size: tl.constexpr, k_block_size: tl.constexpr, require_m_boundary_check: tl.constexpr, require_n_boundary_check: tl.constexpr, require_k_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_m_blocks = tl.cdiv(m_size, m_block_size)\nnum_n_blocks = tl.cdiv(n_size, n_block_size)\nnum_blocks = num_m_blocks * num_n_blocks\nbatch = pid // num_blocks\nblock = pid % num_blocks\nm_block = block // num_n_blocks\nn_block = block % num_n_blocks\nm_offset = m_block * m_block_size\nn_offset = n_block * n_block_size\noutput = language.Linear.forward(input_ptr + batch * input_batch_stride, weight_ptr, bias_ptr, m_size, n_size, k_size, input_m_stride, input_k_stride, weight_n_stride, weight_k_stride, m_offset, n_offset, use_accelerator, m_block_size, n_block_size, k_block_size, require_m_boundary_check, require_n_boundary_check, require_k_boundary_check, dtype)\noutput_block_ptr = tl.make_block_ptr(output_ptr + batch * m_size * n_size, shape=(m_size, n_size), strides=(n_size, 1), offsets=(m_offset, n_offset), block_shape=(m_block_size, n_block_size), order=(1, 0))\nif require_m_boundary_check | require_n_boundary_check:\n    tl.store(output_block_ptr, output, boundary_check=(0, 1))\nelse:\n    tl.store(output_block_ptr, output)"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'], 'require_n_boundary_check': lambda args: args['n_size'] % args['n_block_size'], 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_n_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_n_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    weight_n_stride: tl.int32,",
      "    weight_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    n_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_n_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_m_blocks = tl.cdiv(m_size, m_block_size)",
      "    num_k_blocks = tl.cdiv(k_size, k_block_size)",
      "    num_blocks = num_m_blocks * num_k_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    m_block = block // num_k_blocks",
      "    k_block = block % num_k_blocks",
      "    m_offset = m_block * m_block_size",
      "    k_offset = k_block * k_block_size",
      "",
      "    grad_input = language.Linear.backward(",
      "        grad_output_ptr + batch * m_size * n_size,",
      "        weight_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        k_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        n_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_n_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr + batch * m_size * k_size,",
      "        shape=(m_size, k_size),",
      "        strides=(input_m_stride, input_k_stride),",
      "        offsets=(m_offset, k_offset),",
      "        block_shape=(m_block_size, k_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_m_boundary_check | require_k_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input, boundary_check=(0, 1))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input)"
    ],
    "file": "codes/507.py",
    "header": "def backward(grad_input_ptr: tl.tensor, grad_output_ptr: tl.tensor, weight_ptr: tl.tensor, m_size: tl.int32, n_size: tl.int32, k_size: tl.int32, input_m_stride: tl.int32, input_k_stride: tl.int32, weight_n_stride: tl.int32, weight_k_stride: tl.int32, use_accelerator: tl.constexpr, dtype: tl.constexpr, m_block_size: tl.constexpr, n_block_size: tl.constexpr, k_block_size: tl.constexpr, require_m_boundary_check: tl.constexpr, require_n_boundary_check: tl.constexpr, require_k_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_m_blocks = tl.cdiv(m_size, m_block_size)\nnum_k_blocks = tl.cdiv(k_size, k_block_size)\nnum_blocks = num_m_blocks * num_k_blocks\nbatch = pid // num_blocks\nblock = pid % num_blocks\nm_block = block // num_k_blocks\nk_block = block % num_k_blocks\nm_offset = m_block * m_block_size\nk_offset = k_block * k_block_size\ngrad_input = language.Linear.backward(grad_output_ptr + batch * m_size * n_size, weight_ptr, m_size, n_size, k_size, weight_n_stride, weight_k_stride, m_offset, k_offset, use_accelerator, m_block_size, n_block_size, k_block_size, require_m_boundary_check, require_n_boundary_check, require_k_boundary_check, dtype)\ngrad_input_block_ptr = tl.make_block_ptr(grad_input_ptr + batch * m_size * k_size, shape=(m_size, k_size), strides=(input_m_stride, input_k_stride), offsets=(m_offset, k_offset), block_shape=(m_block_size, k_block_size), order=(1, 0))\nif require_m_boundary_check | require_k_boundary_check:\n    tl.store(grad_input_block_ptr, grad_input, boundary_check=(0, 1))\nelse:\n    tl.store(grad_input_block_ptr, grad_input)"
  },
  {
    "name": "backward_weight",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'], 'require_n_boundary_check': lambda args: args['n_size'] % args['n_block_size'], 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size']})"
    ],
    "args": [
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_n_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_weight(",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    input_batch_stride: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    n_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_n_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_n_blocks = tl.cdiv(n_size, n_block_size)",
      "    num_k_blocks = tl.cdiv(k_size, k_block_size)",
      "    num_blocks = num_n_blocks * num_k_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    n_block = block // num_k_blocks",
      "    k_block = block % num_k_blocks",
      "    n_offset = n_block * n_block_size",
      "    k_offset = k_block * k_block_size",
      "",
      "    grad_weight = language.Linear.backward_weight(",
      "        grad_output_ptr + batch * m_size * n_size,",
      "        input_ptr + batch * input_batch_stride,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        n_offset,",
      "        k_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        n_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_n_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "        grad_weight_staging_ptr + batch * n_size * k_size,",
      "        shape=(n_size, k_size),",
      "        strides=(k_size, 1),",
      "        offsets=(n_offset, k_offset),",
      "        block_shape=(n_block_size, k_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_n_boundary_check | require_k_boundary_check:",
      "        tl.store(grad_weight_staging_block_ptr, grad_weight, boundary_check=(0, 1))",
      "    else:",
      "        tl.store(grad_weight_staging_block_ptr, grad_weight)"
    ],
    "file": "codes/507.py",
    "header": "def backward_weight(grad_weight_staging_ptr: tl.tensor, grad_output_ptr: tl.tensor, input_ptr: tl.tensor, m_size: tl.int32, n_size: tl.int32, k_size: tl.int32, input_batch_stride: tl.int32, input_m_stride: tl.int32, input_k_stride: tl.int32, use_accelerator: tl.constexpr, dtype: tl.constexpr, m_block_size: tl.constexpr, n_block_size: tl.constexpr, k_block_size: tl.constexpr, require_m_boundary_check: tl.constexpr, require_n_boundary_check: tl.constexpr, require_k_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_n_blocks = tl.cdiv(n_size, n_block_size)\nnum_k_blocks = tl.cdiv(k_size, k_block_size)\nnum_blocks = num_n_blocks * num_k_blocks\nbatch = pid // num_blocks\nblock = pid % num_blocks\nn_block = block // num_k_blocks\nk_block = block % num_k_blocks\nn_offset = n_block * n_block_size\nk_offset = k_block * k_block_size\ngrad_weight = language.Linear.backward_weight(grad_output_ptr + batch * m_size * n_size, input_ptr + batch * input_batch_stride, m_size, n_size, k_size, input_m_stride, input_k_stride, n_offset, k_offset, use_accelerator, m_block_size, n_block_size, k_block_size, require_m_boundary_check, require_n_boundary_check, require_k_boundary_check, dtype)\ngrad_weight_staging_block_ptr = tl.make_block_ptr(grad_weight_staging_ptr + batch * n_size * k_size, shape=(n_size, k_size), strides=(k_size, 1), offsets=(n_offset, k_offset), block_shape=(n_block_size, k_block_size), order=(1, 0))\nif require_n_boundary_check | require_k_boundary_check:\n    tl.store(grad_weight_staging_block_ptr, grad_weight, boundary_check=(0, 1))\nelse:\n    tl.store(grad_weight_staging_block_ptr, grad_weight)"
  },
  {
    "name": "backward_bias",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size']})"
    ],
    "args": [
      {
        "name": "grad_bias_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_bias(",
      "    grad_bias_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    batch = pid // n_size",
      "    n_offset = pid % n_size",
      "    grad_bias = language.Linear.backward_bias(",
      "        grad_output_ptr + batch * m_size * n_size,",
      "        m_size,",
      "        n_size,",
      "        n_offset,",
      "        m_block_size,",
      "        require_m_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    grad_bias_staging_block_ptr = tl.make_block_ptr(",
      "        grad_bias_staging_ptr + batch * n_size,",
      "        shape=(n_size,),",
      "        strides=(1,),",
      "        offsets=(n_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    tl.store(grad_bias_staging_block_ptr, grad_bias)"
    ],
    "file": "codes/507.py",
    "header": "def backward_bias(grad_bias_staging_ptr: tl.tensor, grad_output_ptr: tl.tensor, m_size: tl.int32, n_size: tl.int32, dtype: tl.constexpr, m_block_size: tl.constexpr, require_m_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nbatch = pid // n_size\nn_offset = pid % n_size\ngrad_bias = language.Linear.backward_bias(grad_output_ptr + batch * m_size * n_size, m_size, n_size, n_offset, m_block_size, require_m_boundary_check, dtype)\ngrad_bias_staging_block_ptr = tl.make_block_ptr(grad_bias_staging_ptr + batch * n_size, shape=(n_size,), strides=(1,), offsets=(n_offset,), block_shape=(1,), order=(0,))\ntl.store(grad_bias_staging_block_ptr, grad_bias)"
  },
  {
    "name": "l2norm_fwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "eps",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_fwd_kernel1(",
      "    x,",
      "    y,",
      "    D,",
      "    BD: tl.constexpr,",
      "    eps,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    y += i_t * D",
      "",
      "    cols = tl.arange(0, BD)",
      "    mask = cols < D",
      "    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=0)",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "",
      "    b_y = b_x * b_rstd",
      "    tl.store(y + cols, b_y, mask=mask)"
    ],
    "file": "codes/358.py",
    "header": "def l2norm_fwd_kernel1(x, y, D, BD: tl.constexpr, eps):",
    "body": "i_t = tl.program_id(0)\nx += i_t * D\ny += i_t * D\ncols = tl.arange(0, BD)\nmask = cols < D\nb_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)\nb_var = tl.sum(b_x * b_x, axis=0)\nb_rstd = 1 / tl.sqrt(b_var + eps)\nb_y = b_x * b_rstd\ntl.store(y + cols, b_y, mask=mask)"
  },
  {
    "name": "l2norm_bwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_bwd_kernel1(",
      "    x,",
      "    dy,",
      "    dx,",
      "    eps,",
      "    D,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    dx += i_t * D",
      "    dy += i_t * D",
      "",
      "    cols = tl.arange(0, BD)",
      "    mask = cols < D",
      "    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x)",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    b_dy = tl.load(dy + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_dx = b_dy * b_rstd - tl.sum(b_dy * b_x) * (1 / (b_var + eps)) * b_rstd * b_x",
      "    tl.store(dx + cols, b_dx, mask=mask)"
    ],
    "file": "codes/358.py",
    "header": "def l2norm_bwd_kernel1(x, dy, dx, eps, D, BD: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\nx += i_t * D\ndx += i_t * D\ndy += i_t * D\ncols = tl.arange(0, BD)\nmask = cols < D\nb_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)\nb_var = tl.sum(b_x * b_x)\nb_rstd = 1 / tl.sqrt(b_var + eps)\nb_dy = tl.load(dy + cols, mask=mask, other=0.0).to(tl.float32)\nb_dx = b_dy * b_rstd - tl.sum(b_dy * b_x) * (1 / (b_var + eps)) * b_rstd * b_x\ntl.store(dx + cols, b_dx, mask=mask)"
  },
  {
    "name": "l2norm_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16] for BT in BT_LIST], key=['D', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_fwd_kernel(",
      "    x,",
      "    y,",
      "    eps,",
      "    NB: tl.constexpr,",
      "    T: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=1)",
      "    b_y = b_x / tl.sqrt(b_var + eps)[:, None]",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/358.py",
    "header": "def l2norm_fwd_kernel(x, y, eps, NB: tl.constexpr, T: tl.constexpr, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\np_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\nb_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\nb_var = tl.sum(b_x * b_x, axis=1)\nb_y = b_x / tl.sqrt(b_var + eps)[:, None]\np_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\ntl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "l2norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16] for BT in BT_LIST], key=['D', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_bwd_kernel(",
      "    x,",
      "    dy,",
      "    dx,",
      "    eps,",
      "    NB: tl.constexpr,",
      "    T: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=1)[:, None]",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    b_dx = (",
      "        b_dy * b_rstd",
      "        - tl.sum(b_dy * b_x, axis=1)[:, None] / (b_var + eps) * b_rstd * b_x",
      "    )",
      "    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/358.py",
    "header": "def l2norm_bwd_kernel(x, dy, dx, eps, NB: tl.constexpr, T: tl.constexpr, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\np_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\np_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\np_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\nb_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\nb_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\nb_var = tl.sum(b_x * b_x, axis=1)[:, None]\nb_rstd = 1 / tl.sqrt(b_var + eps)\nb_dx = b_dy * b_rstd - tl.sum(b_dy * b_x, axis=1)[:, None] / (b_var + eps) * b_rstd * b_x\ntl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_recurrent_gated_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_BETA_HEADWISE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_QK_L2NORM_IN_KERNEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_gated_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    beta,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_BETA_HEADWISE: tl.constexpr,",
      "    USE_QK_L2NORM_IN_KERNEL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "",
      "    p_q = q + (bos * H + i_h) * K + o_k",
      "    p_k = k + (bos * H + i_h) * K + o_k",
      "    p_v = v + (bos * H + i_h) * V + o_v",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * H + i_h) * V + o_v",
      "    else:",
      "        p_beta = beta + bos * H + i_h",
      "    p_g = g + bos * H + i_h",
      "    p_o = o + ((i_k * all + bos) * H + i_h) * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g).to(tl.float32)",
      "",
      "        if USE_QK_L2NORM_IN_KERNEL:",
      "            b_q = b_q / (tl.sqrt(tl.sum(b_q * b_q)) + 1e-6)",
      "            b_k = b_k / (tl.sqrt(tl.sum(b_k * b_k)) + 1e-6)",
      "        b_q = b_q * scale",
      "",
      "        b_h *= exp(b_g)",
      "",
      "        b_v -= tl.sum(b_h * b_k[:, None], 0)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_v *= b_beta",
      "",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "",
      "        b_o = tl.sum(b_h * b_q[:, None], 0)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "",
      "        p_q += H * K",
      "        p_k += H * K",
      "        p_o += H * V",
      "        p_v += H * V",
      "        p_g += H",
      "        p_beta += H * (V if IS_BETA_HEADWISE else 1)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "codes/378.py",
    "header": "def fused_recurrent_gated_delta_rule_fwd_kernel(q, k, v, g, beta, o, h0, ht, cu_seqlens, scale, T, B: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_BETA_HEADWISE: tl.constexpr, USE_QK_L2NORM_IN_KERNEL: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(cu_seqlens + i_n + 1).to(tl.int64))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    all = B * T\no_k = i_k * BK + tl.arange(0, BK)\no_v = i_v * BV + tl.arange(0, BV)\np_q = q + (bos * H + i_h) * K + o_k\np_k = k + (bos * H + i_h) * K + o_k\np_v = v + (bos * H + i_h) * V + o_v\nif IS_BETA_HEADWISE:\n    p_beta = beta + (bos * H + i_h) * V + o_v\nelse:\n    p_beta = beta + bos * H + i_h\np_g = g + bos * H + i_h\np_o = o + ((i_k * all + bos) * H + i_h) * V + o_v\nmask_k = o_k < K\nmask_v = o_v < V\nmask_h = mask_k[:, None] & mask_v[None, :]\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n    b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)\nfor _ in range(0, T):\n    b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32)\n    b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)\n    b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)\n    b_g = tl.load(p_g).to(tl.float32)\n    if USE_QK_L2NORM_IN_KERNEL:\n        b_q = b_q / (tl.sqrt(tl.sum(b_q * b_q)) + 1e-06)\n        b_k = b_k / (tl.sqrt(tl.sum(b_k * b_k)) + 1e-06)\n    b_q = b_q * scale\n    b_h *= exp(b_g)\n    b_v -= tl.sum(b_h * b_k[:, None], 0)\n    if IS_BETA_HEADWISE:\n        b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)\n    else:\n        b_beta = tl.load(p_beta).to(tl.float32)\n    b_v *= b_beta\n    b_h += b_k[:, None] * b_v[None, :]\n    b_o = tl.sum(b_h * b_q[:, None], 0)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)\n    p_q += H * K\n    p_k += H * K\n    p_o += H * V\n    p_v += H * V\n    p_g += H\n    p_beta += H * (V if IS_BETA_HEADWISE else 1)\nif STORE_FINAL_STATE:\n    p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
  },
  {
    "name": "solve_tril_16x16_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4, 5]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ad",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def solve_tril_16x16_kernel(",
      "    A,",
      "    Ad,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A = A + (bos * H + i_h) * BT",
      "    Ad = Ad + (bos * H + i_h) * 16",
      "",
      "    offset = (i_t * 16) % BT",
      "    p_A = tl.make_block_ptr(",
      "        A, (T, BT), (H * BT, 1), (i_t * 16, offset), (16, 16), (1, 0)",
      "    )",
      "    p_Ai = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 16, 0), (16, 16), (1, 0))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1)).to(tl.float32)",
      "    b_A = -tl.where(tl.arange(0, 16)[:, None] > tl.arange(0, 16)[None, :], b_A, 0)",
      "",
      "    o_i = tl.arange(0, 16)",
      "    for i in range(1, min(16, T - i_t * 16)):",
      "        b_a = -tl.load(A + (i_t * 16 + i) * H * BT + o_i + offset)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0)",
      "        mask = o_i == i",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "    b_A += o_i[:, None] == o_i[None, :]",
      "    tl.store(",
      "        p_Ai,",
      "        b_A.to(p_Ai.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/436.py",
    "header": "def solve_tril_16x16_kernel(A, Ad, cu_seqlens, chunk_indices, T, H: tl.constexpr, BT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nA = A + (bos * H + i_h) * BT\nAd = Ad + (bos * H + i_h) * 16\noffset = i_t * 16 % BT\np_A = tl.make_block_ptr(A, (T, BT), (H * BT, 1), (i_t * 16, offset), (16, 16), (1, 0))\np_Ai = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 16, 0), (16, 16), (1, 0))\nb_A = tl.load(p_A, boundary_check=(0, 1)).to(tl.float32)\nb_A = -tl.where(tl.arange(0, 16)[:, None] > tl.arange(0, 16)[None, :], b_A, 0)\no_i = tl.arange(0, 16)\nfor i in range(1, min(16, T - i_t * 16)):\n    b_a = -tl.load(A + (i_t * 16 + i) * H * BT + o_i + offset)\n    b_a = b_a + tl.sum(b_a[:, None] * b_A, 0)\n    mask = o_i == i\n    b_A = tl.where(mask[:, None], b_a, b_A)\nb_A += o_i[:, None] == o_i[None, :]\ntl.store(p_Ai, b_A.to(p_Ai.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "merge_16x16_to_32x32_inverse_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4, 5]], key=['H', 'BT', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ad",
        "annotation": null
      },
      {
        "name": "Ai",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def merge_16x16_to_32x32_inverse_kernel(",
      "    A,",
      "    Ad,",
      "    Ai,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A += (bos * H + i_h) * 32",
      "    Ad += (bos * H + i_h) * 16",
      "    Ai += (bos * H + i_h) * 32",
      "",
      "    p_A_21 = tl.make_block_ptr(",
      "        A, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_11 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_22 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_11 = tl.make_block_ptr(",
      "        Ai, (T, 32), (H * 32, 1), (i_t * 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_22 = tl.make_block_ptr(",
      "        Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_21 = tl.make_block_ptr(",
      "        Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "",
      "    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_21 = -tl.dot(",
      "        tl.dot(Ai_22, A_21, input_precision=\"ieee\"), Ai_11, input_precision=\"ieee\"",
      "    )",
      "    tl.store(",
      "        p_Ai_11,",
      "        Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_22,",
      "        Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_21,",
      "        Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/436.py",
    "header": "def merge_16x16_to_32x32_inverse_kernel(A, Ad, Ai, cu_seqlens, chunk_indices, T, H: tl.constexpr, BT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nA += (bos * H + i_h) * 32\nAd += (bos * H + i_h) * 16\nAi += (bos * H + i_h) * 32\np_A_21 = tl.make_block_ptr(A, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0))\np_Ad_11 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 32, 0), (16, 16), (1, 0))\np_Ad_22 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0))\np_Ai_11 = tl.make_block_ptr(Ai, (T, 32), (H * 32, 1), (i_t * 32, 0), (16, 16), (1, 0))\np_Ai_22 = tl.make_block_ptr(Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 16), (16, 16), (1, 0))\np_Ai_21 = tl.make_block_ptr(Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0))\nA_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)\nAi_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)\nAi_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)\nAi_21 = -tl.dot(tl.dot(Ai_22, A_21, input_precision='ieee'), Ai_11, input_precision='ieee')\ntl.store(p_Ai_11, Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_22, Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_21, Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "merge_16x16_to_64x64_inverse_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4, 5]], key=['H', 'BT', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ad",
        "annotation": null
      },
      {
        "name": "Ai",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def merge_16x16_to_64x64_inverse_kernel(",
      "    A,",
      "    Ad,",
      "    Ai,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A += (bos * H + i_h) * 64",
      "    Ad += (bos * H + i_h) * 16",
      "    Ai += (bos * H + i_h) * 64",
      "",
      "    p_A_21 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_A_32 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0)",
      "    )",
      "    p_A_31 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_A_43 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0)",
      "    )",
      "    p_A_42 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0)",
      "    )",
      "    p_A_41 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_11 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_22 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_33 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_44 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)",
      "    )",
      "",
      "    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)",
      "    A_32 = tl.load(p_A_32, boundary_check=(0, 1)).to(tl.float32)",
      "    A_31 = tl.load(p_A_31, boundary_check=(0, 1)).to(tl.float32)",
      "    A_43 = tl.load(p_A_43, boundary_check=(0, 1)).to(tl.float32)",
      "    A_42 = tl.load(p_A_42, boundary_check=(0, 1)).to(tl.float32)",
      "    A_41 = tl.load(p_A_41, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_33 = tl.load(p_Ad_33, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_44 = tl.load(p_Ad_44, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    Ai_21 = -tl.dot(",
      "        tl.dot(Ai_22, A_21, input_precision=\"ieee\"), Ai_11, input_precision=\"ieee\"",
      "    )",
      "    Ai_32 = -tl.dot(",
      "        tl.dot(Ai_33, A_32, input_precision=\"ieee\"), Ai_22, input_precision=\"ieee\"",
      "    )",
      "    Ai_43 = -tl.dot(",
      "        tl.dot(Ai_44, A_43, input_precision=\"ieee\"), Ai_33, input_precision=\"ieee\"",
      "    )",
      "",
      "    Ai_31 = -tl.dot(",
      "        Ai_33,",
      "        tl.dot(A_31, Ai_11, input_precision=\"ieee\")",
      "        + tl.dot(A_32, Ai_21, input_precision=\"ieee\"),",
      "        input_precision=\"ieee\",",
      "    )",
      "    Ai_42 = -tl.dot(",
      "        Ai_44,",
      "        tl.dot(A_42, Ai_22, input_precision=\"ieee\")",
      "        + tl.dot(A_43, Ai_32, input_precision=\"ieee\"),",
      "        input_precision=\"ieee\",",
      "    )",
      "    Ai_41 = -tl.dot(",
      "        Ai_44,",
      "        tl.dot(A_41, Ai_11, input_precision=\"ieee\")",
      "        + tl.dot(A_42, Ai_21, input_precision=\"ieee\")",
      "        + tl.dot(A_43, Ai_31, input_precision=\"ieee\"),",
      "        input_precision=\"ieee\",",
      "    )",
      "",
      "    p_Ai_11 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_22 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_33 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 32), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_44 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 48), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_21 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_31 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_32 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_41 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_42 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_43 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_Ai_11,",
      "        Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_22,",
      "        Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_33,",
      "        Ai_33.to(p_Ai_33.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_44,",
      "        Ai_44.to(p_Ai_44.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_21,",
      "        Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_31,",
      "        Ai_31.to(p_Ai_31.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_32,",
      "        Ai_32.to(p_Ai_32.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_41,",
      "        Ai_41.to(p_Ai_41.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_42,",
      "        Ai_42.to(p_Ai_42.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_43,",
      "        Ai_43.to(p_Ai_43.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/436.py",
    "header": "def merge_16x16_to_64x64_inverse_kernel(A, Ad, Ai, cu_seqlens, chunk_indices, T, H: tl.constexpr, BT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nA += (bos * H + i_h) * 64\nAd += (bos * H + i_h) * 16\nAi += (bos * H + i_h) * 64\np_A_21 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0))\np_A_32 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0))\np_A_31 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0))\np_A_43 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0))\np_A_42 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0))\np_A_41 = tl.make_block_ptr(A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0))\np_Ad_11 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64, 0), (16, 16), (1, 0))\np_Ad_22 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0))\np_Ad_33 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0))\np_Ad_44 = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0))\nA_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)\nA_32 = tl.load(p_A_32, boundary_check=(0, 1)).to(tl.float32)\nA_31 = tl.load(p_A_31, boundary_check=(0, 1)).to(tl.float32)\nA_43 = tl.load(p_A_43, boundary_check=(0, 1)).to(tl.float32)\nA_42 = tl.load(p_A_42, boundary_check=(0, 1)).to(tl.float32)\nA_41 = tl.load(p_A_41, boundary_check=(0, 1)).to(tl.float32)\nAi_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)\nAi_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)\nAi_33 = tl.load(p_Ad_33, boundary_check=(0, 1)).to(tl.float32)\nAi_44 = tl.load(p_Ad_44, boundary_check=(0, 1)).to(tl.float32)\nAi_21 = -tl.dot(tl.dot(Ai_22, A_21, input_precision='ieee'), Ai_11, input_precision='ieee')\nAi_32 = -tl.dot(tl.dot(Ai_33, A_32, input_precision='ieee'), Ai_22, input_precision='ieee')\nAi_43 = -tl.dot(tl.dot(Ai_44, A_43, input_precision='ieee'), Ai_33, input_precision='ieee')\nAi_31 = -tl.dot(Ai_33, tl.dot(A_31, Ai_11, input_precision='ieee') + tl.dot(A_32, Ai_21, input_precision='ieee'), input_precision='ieee')\nAi_42 = -tl.dot(Ai_44, tl.dot(A_42, Ai_22, input_precision='ieee') + tl.dot(A_43, Ai_32, input_precision='ieee'), input_precision='ieee')\nAi_41 = -tl.dot(Ai_44, tl.dot(A_41, Ai_11, input_precision='ieee') + tl.dot(A_42, Ai_21, input_precision='ieee') + tl.dot(A_43, Ai_31, input_precision='ieee'), input_precision='ieee')\np_Ai_11 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64, 0), (16, 16), (1, 0))\np_Ai_22 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 16), (16, 16), (1, 0))\np_Ai_33 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 32), (16, 16), (1, 0))\np_Ai_44 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 48), (16, 16), (1, 0))\np_Ai_21 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0))\np_Ai_31 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0))\np_Ai_32 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0))\np_Ai_41 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0))\np_Ai_42 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0))\np_Ai_43 = tl.make_block_ptr(Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0))\ntl.store(p_Ai_11, Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_22, Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_33, Ai_33.to(p_Ai_33.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_44, Ai_44.to(p_Ai_44.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_21, Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_31, Ai_31.to(p_Ai_31.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_32, Ai_32.to(p_Ai_32.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_41, Ai_41.to(p_Ai_41.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_42, Ai_42.to(p_Ai_42.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_Ai_43, Ai_43.to(p_Ai_43.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "kernel_consumer_gemm_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "use_cuda_graph": true
        }
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2, 'kpack': 1, 'matrix_instr_nonkdim': 16}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2, 'kpack': 1, 'matrix_instr_nonkdim': 16}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0, 'kpack': 1}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0, 'kpack': 1}, num_warps=8, num_stages=2)], key=['M', 'N', 'K'], use_cuda_graph=True)",
      "@triton.heuristics({'EVEN_K': lambda args: args['K'] % args['BLOCK_SIZE_K'] == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "localA",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "world_size",
        "annotation": null
      },
      {
        "name": "barrier_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M_PER_CHUNK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_XCDS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_consumer_gemm_persistent(",
      "    A,",
      "    localA,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    rank,",
      "    world_size,",
      "    barrier_ptr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    M_PER_CHUNK: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    NUM_XCDS: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    if NUM_XCDS != 1:",
      "        pid = (pid % NUM_XCDS) * (NUM_SMS // NUM_XCDS) + (pid // NUM_XCDS)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    total_tiles = num_pid_m * num_pid_n",
      "",
      "    tl.assume(stride_am > 0)",
      "    tl.assume(stride_ak > 0)",
      "    tl.assume(stride_bn > 0)",
      "    tl.assume(stride_bk > 0)",
      "    tl.assume(stride_cm > 0)",
      "    tl.assume(stride_cn > 0)",
      "",
      "    M_per_rank = M // world_size",
      "    pid_m_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)",
      "",
      "    acc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32",
      "    for tile_id in range(pid, total_tiles, NUM_SMS):",
      "        num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "        group_id = tile_id // num_pid_in_group",
      "        first_pid_m = group_id * GROUP_SIZE_M",
      "        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)",
      "        pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "        num_pid_m_per_copy_chunk = M_PER_CHUNK // BLOCK_SIZE_M",
      "        chunk_offset = pid_m // (num_pid_m_per_copy_chunk * world_size)",
      "        rank_offset = (",
      "            pid_m % (num_pid_m_per_copy_chunk * world_size) // num_pid_m_per_copy_chunk",
      "        )",
      "        block_offset = pid_m % num_pid_m_per_copy_chunk",
      "",
      "        rank_offset = (rank_offset + rank) % world_size",
      "        pid_m = (",
      "            rank_offset * M_per_rank",
      "            + chunk_offset * M_PER_CHUNK",
      "            + block_offset * BLOCK_SIZE_M",
      "        ) // BLOCK_SIZE_M",
      "",
      "        offs_am = pid_m * BLOCK_SIZE_M",
      "        offs_sig = offs_am // M_PER_CHUNK",
      "        offs_rank = pid_m // pid_m_per_rank",
      "",
      "        if offs_rank != rank:",
      "            wait_eq_sys(barrier_ptr + offs_sig, 1)",
      "",
      "        rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "        rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "        rk = tl.arange(0, BLOCK_SIZE_K)",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "",
      "        if offs_rank == rank:",
      "            rm = rm % M_per_rank",
      "            A_BASE = localA + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "        else:",
      "            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "",
      "        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "        tl.assume(pid_m > 0)",
      "        tl.assume(pid_n > 0)",
      "",
      "        loop_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "        if not EVEN_K:",
      "            loop_k -= 1",
      "",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)",
      "        for k in range(0, loop_k):",
      "            a = tl.load(tl.multiple_of(A_BASE, (1, 16)))",
      "            b = tl.load(tl.multiple_of(B_BASE, (16, 1)))",
      "            acc += tl.dot(a, b)",
      "            A_BASE += BLOCK_SIZE_K * stride_ak",
      "            B_BASE += BLOCK_SIZE_K * stride_bk",
      "",
      "        if not EVEN_K:",
      "            k = loop_k",
      "            rk = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "            B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "            A_BASE = tl.multiple_of(A_BASE, (1, 16))",
      "            B_BASE = tl.multiple_of(B_BASE, (16, 1))",
      "            a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)",
      "            b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)",
      "            acc += tl.dot(a, b)",
      "",
      "        c = acc.to(C.type.element_ty)",
      "        rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "        rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "        c_mask = (rm[:, None] < M) & (rn[None, :] < N)",
      "        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn",
      "        tl.store(C_, c, c_mask)"
    ],
    "file": "codes/33.py",
    "header": "def kernel_consumer_gemm_persistent(A, localA, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, rank, world_size, barrier_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, M_PER_CHUNK: tl.constexpr, NUM_SMS: tl.constexpr, NUM_XCDS: tl.constexpr, EVEN_K: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nif NUM_XCDS != 1:\n    pid = pid % NUM_XCDS * (NUM_SMS // NUM_XCDS) + pid // NUM_XCDS\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\ntotal_tiles = num_pid_m * num_pid_n\ntl.assume(stride_am > 0)\ntl.assume(stride_ak > 0)\ntl.assume(stride_bn > 0)\ntl.assume(stride_bk > 0)\ntl.assume(stride_cm > 0)\ntl.assume(stride_cn > 0)\nM_per_rank = M // world_size\npid_m_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)\nacc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32\nfor tile_id in range(pid, total_tiles, NUM_SMS):\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = tile_id // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + tile_id % num_pid_in_group % group_size_m\n    pid_n = tile_id % num_pid_in_group // group_size_m\n    num_pid_m_per_copy_chunk = M_PER_CHUNK // BLOCK_SIZE_M\n    chunk_offset = pid_m // (num_pid_m_per_copy_chunk * world_size)\n    rank_offset = pid_m % (num_pid_m_per_copy_chunk * world_size) // num_pid_m_per_copy_chunk\n    block_offset = pid_m % num_pid_m_per_copy_chunk\n    rank_offset = (rank_offset + rank) % world_size\n    pid_m = (rank_offset * M_per_rank + chunk_offset * M_PER_CHUNK + block_offset * BLOCK_SIZE_M) // BLOCK_SIZE_M\n    offs_am = pid_m * BLOCK_SIZE_M\n    offs_sig = offs_am // M_PER_CHUNK\n    offs_rank = pid_m // pid_m_per_rank\n    if offs_rank != rank:\n        wait_eq_sys(barrier_ptr + offs_sig, 1)\n    rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    if offs_rank == rank:\n        rm = rm % M_per_rank\n        A_BASE = localA + rm[:, None] * stride_am + rk[None, :] * stride_ak\n    else:\n        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n    B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n    tl.assume(pid_m > 0)\n    tl.assume(pid_n > 0)\n    loop_k = tl.cdiv(K, BLOCK_SIZE_K)\n    if not EVEN_K:\n        loop_k -= 1\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)\n    for k in range(0, loop_k):\n        a = tl.load(tl.multiple_of(A_BASE, (1, 16)))\n        b = tl.load(tl.multiple_of(B_BASE, (16, 1)))\n        acc += tl.dot(a, b)\n        A_BASE += BLOCK_SIZE_K * stride_ak\n        B_BASE += BLOCK_SIZE_K * stride_bk\n    if not EVEN_K:\n        k = loop_k\n        rk = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n        A_BASE = tl.multiple_of(A_BASE, (1, 16))\n        B_BASE = tl.multiple_of(B_BASE, (16, 1))\n        a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)\n        b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)\n        acc += tl.dot(a, b)\n    c = acc.to(C.type.element_ty)\n    rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    c_mask = (rm[:, None] < M) & (rn[None, :] < N)\n    C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    tl.store(C_, c, c_mask)"
  },
  {
    "name": "triton_red_fused_native_layer_norm_0",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_out_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "out_ptr1",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_native_layer_norm_0(",
      "    in_out_ptr0,",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr0,",
      "    out_ptr1,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :]",
      "    x0 = xindex",
      "    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp0 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp1 = tmp0.to(tl.float32)",
      "        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])",
      "        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(",
      "            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0",
      "        )",
      "        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)",
      "        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)",
      "        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)",
      "    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(",
      "        tmp3_mean, tmp3_m2, tmp3_weight, 1",
      "    )",
      "    tmp3 = tmp3_tmp[:, None]",
      "    tmp4 = tmp4_tmp[:, None]",
      "    tmp5 = tmp5_tmp[:, None]",
      "    tl.store(out_ptr0 + (x0), tmp3, None)",
      "    tmp6 = rnumel",
      "    tmp7 = tmp4 / tmp6",
      "    tmp8 = 1e-05",
      "    tmp9 = tmp7 + tmp8",
      "    tmp10 = libdevice.rsqrt(tmp9)",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr0 + (x0), tmp10, None)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp11 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"",
      "        ).to(tl.float32)",
      "        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp12 = tmp11.to(tl.float32)",
      "        tmp13 = tmp12 - tmp3",
      "        tmp14 = tmp13 * tmp10",
      "        tmp16 = tmp15.to(tl.float32)",
      "        tmp17 = tmp14 * tmp16",
      "        tmp19 = tmp18.to(tl.float32)",
      "        tmp20 = tmp17 + tmp19",
      "        tmp21 = tmp20.to(tl.float32)",
      "        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)"
    ],
    "file": "codes/682.py",
    "header": "def triton_red_fused_native_layer_norm_0(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):",
    "body": "xoffset = tl.program_id(0) * XBLOCK\nxindex = xoffset + tl.arange(0, XBLOCK)[:, None]\nxmask = xindex < xnumel\nrbase = tl.arange(0, RBLOCK)[None, :]\nx0 = xindex\ntmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)\ntmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)\ntmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)\nfor roffset in range(0, rnumel, RBLOCK):\n    rindex = roffset + rbase\n    rmask = rindex < rnumel\n    r1 = rindex\n    tmp0 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp1 = tmp0.to(tl.float32)\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0)\n    tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)\n    tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)\n    tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)\ntmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(tmp3_mean, tmp3_m2, tmp3_weight, 1)\ntmp3 = tmp3_tmp[:, None]\ntmp4 = tmp4_tmp[:, None]\ntmp5 = tmp5_tmp[:, None]\ntl.store(out_ptr0 + x0, tmp3, None)\ntmp6 = rnumel\ntmp7 = tmp4 / tmp6\ntmp8 = 1e-05\ntmp9 = tmp7 + tmp8\ntmp10 = libdevice.rsqrt(tmp9)\ntl.debug_barrier()\ntl.store(in_out_ptr0 + x0, tmp10, None)\nfor roffset in range(0, rnumel, RBLOCK):\n    rindex = roffset + rbase\n    rmask = rindex < rnumel\n    r1 = rindex\n    tmp11 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy='evict_first').to(tl.float32)\n    tmp15 = tl.load(in_ptr1 + r1, rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp18 = tl.load(in_ptr2 + r1, rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp12 = tmp11.to(tl.float32)\n    tmp13 = tmp12 - tmp3\n    tmp14 = tmp13 * tmp10\n    tmp16 = tmp15.to(tl.float32)\n    tmp17 = tmp14 * tmp16\n    tmp19 = tmp18.to(tl.float32)\n    tmp20 = tmp17 + tmp19\n    tmp21 = tmp20.to(tl.float32)\n    tl.store(out_ptr1 + (r1 + rnumel * x0), tmp21, rmask)"
  },
  {
    "name": "triton_red_fused_native_layer_norm_no_welford",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_out_ptr0",
        "annotation": null
      },
      {
        "name": "in_out_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_native_layer_norm_no_welford(",
      "    in_out_ptr0,",
      "    in_out_ptr1,",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr0,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :]",
      "    x0 = xindex",
      "    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp0 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp1 = tmp0.to(tl.float32)",
      "        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])",
      "        tmp4 = _tmp3 + tmp2",
      "        _tmp3 = tmp4",
      "    tmp3 = tl.sum(_tmp3, 1)[:, None]",
      "    tmp5 = rnumel",
      "    tmp6 = tmp3 / tmp5",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr0 + (x0), tmp6, None)",
      "    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp7 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp8 = tmp7.to(tl.float32)",
      "        tmp9 = tmp8 - tmp6",
      "        tmp10 = tmp9 * tmp9",
      "        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])",
      "        tmp13 = _tmp12 + tmp11",
      "        _tmp12 = tmp13",
      "    tmp12 = tl.sum(_tmp12, 1)[:, None]",
      "    tmp14 = rnumel",
      "    tmp15 = tmp12 / tmp14",
      "    tmp16 = 1e-05",
      "    tmp17 = tmp15 + tmp16",
      "    tmp18 = libdevice.rsqrt(tmp17)",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr1 + (x0), tmp18, None)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp19 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"",
      "        ).to(tl.float32)",
      "        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp20 = tmp19.to(tl.float32)",
      "        tmp21 = tmp20 - tmp6",
      "        tmp22 = tmp21 * tmp18",
      "        tmp24 = tmp23.to(tl.float32)",
      "        tmp25 = tmp22 * tmp24",
      "        tmp27 = tmp26.to(tl.float32)",
      "        tmp28 = tmp25 + tmp27",
      "        tmp29 = tmp28.to(tl.float32)",
      "        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)"
    ],
    "file": "codes/682.py",
    "header": "def triton_red_fused_native_layer_norm_no_welford(in_out_ptr0, in_out_ptr1, in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK: tl.constexpr, RBLOCK: tl.constexpr):",
    "body": "xoffset = tl.program_id(0) * XBLOCK\nxindex = xoffset + tl.arange(0, XBLOCK)[:, None]\nxmask = xindex < xnumel\nrbase = tl.arange(0, RBLOCK)[None, :]\nx0 = xindex\n_tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\nfor roffset in range(0, rnumel, RBLOCK):\n    rindex = roffset + rbase\n    rmask = rindex < rnumel\n    r1 = rindex\n    tmp0 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp1 = tmp0.to(tl.float32)\n    tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])\n    tmp4 = _tmp3 + tmp2\n    _tmp3 = tmp4\ntmp3 = tl.sum(_tmp3, 1)[:, None]\ntmp5 = rnumel\ntmp6 = tmp3 / tmp5\ntl.debug_barrier()\ntl.store(in_out_ptr0 + x0, tmp6, None)\n_tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)\nfor roffset in range(0, rnumel, RBLOCK):\n    rindex = roffset + rbase\n    rmask = rindex < rnumel\n    r1 = rindex\n    tmp7 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp8 = tmp7.to(tl.float32)\n    tmp9 = tmp8 - tmp6\n    tmp10 = tmp9 * tmp9\n    tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])\n    tmp13 = _tmp12 + tmp11\n    _tmp12 = tmp13\ntmp12 = tl.sum(_tmp12, 1)[:, None]\ntmp14 = rnumel\ntmp15 = tmp12 / tmp14\ntmp16 = 1e-05\ntmp17 = tmp15 + tmp16\ntmp18 = libdevice.rsqrt(tmp17)\ntl.debug_barrier()\ntl.store(in_out_ptr1 + x0, tmp18, None)\nfor roffset in range(0, rnumel, RBLOCK):\n    rindex = roffset + rbase\n    rmask = rindex < rnumel\n    r1 = rindex\n    tmp19 = tl.load(in_ptr0 + (r1 + rnumel * x0), rmask, eviction_policy='evict_first').to(tl.float32)\n    tmp23 = tl.load(in_ptr1 + r1, rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp26 = tl.load(in_ptr2 + r1, rmask, eviction_policy='evict_last').to(tl.float32)\n    tmp20 = tmp19.to(tl.float32)\n    tmp21 = tmp20 - tmp6\n    tmp22 = tmp21 * tmp18\n    tmp24 = tmp23.to(tl.float32)\n    tmp25 = tmp22 * tmp24\n    tmp27 = tmp26.to(tl.float32)\n    tmp28 = tmp25 + tmp27\n    tmp29 = tmp28.to(tl.float32)\n    tl.store(out_ptr0 + (r1 + rnumel * x0), tmp29, rmask)"
  },
  {
    "name": "parallel_nsa_kernel_topk",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_topk(",
      "    q,",
      "    k,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    cu_seqlens,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    if lse is not None:",
      "        b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))",
      "    else:",
      "",
      "        b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "        b_acc = tl.zeros([G], dtype=tl.float32)",
      "        for i_c in range(0, NC, BC):",
      "            o_c = i_c + tl.arange(0, BC)",
      "",
      "            p_k = tl.make_block_ptr(",
      "                k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = exp(b_mp - b_m)",
      "",
      "            b_p = exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_mp = b_m",
      "        if NC == 0:",
      "            b_lse = tl.zeros([G], dtype=tl.float32)",
      "        else:",
      "            b_lse = b_m + log(b_acc)",
      "",
      "    b_i = tl.full([BC], -1, dtype=tl.float32)",
      "    o_i = tl.zeros([BC], dtype=tl.int32)",
      "    m_i = tl.arange(0, BC) < BC // 2",
      "    for i_c in range(0, i_t // BS + 1, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_p = tl.where(",
      "            (i_t // BS == o_c)[None, :], float(1.0), exp(b_s - b_lse[:, None])",
      "        )",
      "",
      "        b_i, b_ip = tl.sum(b_p, 0), b_i",
      "        o_i, o_ip = tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i",
      "",
      "        n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])",
      "        for i in tl.static_range(1, n_dims):",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)",
      "",
      "        if i_c != 0:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)",
      "            b_i_new = b_ip * m_i + b_i * (1 - m_i)",
      "            o_i_new = o_ip * m_i + o_i * (1 - m_i)",
      "            b_i, o_i = _bitonic_merge(",
      "                b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims",
      "            )",
      "        else:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)",
      "",
      "    m_top = tl.arange(0, BC // S) == 0",
      "    b_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)",
      "",
      "    p_b = tl.make_block_ptr(",
      "        block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,)",
      "    )",
      "    tl.store(p_b, b_top.to(p_b.dtype.element_ty))"
    ],
    "file": "codes/402.py",
    "header": "def parallel_nsa_kernel_topk(q, k, lse, scale, block_indices, cu_seqlens, token_indices, chunk_offsets, T, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, S: tl.constexpr, BC: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boc = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    boc = i_b * tl.cdiv(T, BS)\np_q = tl.make_block_ptr(q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\nTC = tl.cdiv(T, BS)\nNC = (i_t + 1) // BS\nif lse is not None:\n    b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))\nelse:\n    b_m = tl.full([G], float('-inf'), dtype=tl.float32)\n    b_acc = tl.zeros([G], dtype=tl.float32)\n    for i_c in range(0, NC, BC):\n        o_c = i_c + tl.arange(0, BC)\n        p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((o_c < NC)[None, :], b_s, float('-inf'))\n        b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n        b_r = exp(b_mp - b_m)\n        b_p = exp(b_s - b_m[:, None])\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n        b_mp = b_m\n    if NC == 0:\n        b_lse = tl.zeros([G], dtype=tl.float32)\n    else:\n        b_lse = b_m + log(b_acc)\nb_i = tl.full([BC], -1, dtype=tl.float32)\no_i = tl.zeros([BC], dtype=tl.int32)\nm_i = tl.arange(0, BC) < BC // 2\nfor i_c in range(0, i_t // BS + 1, BC):\n    o_c = i_c + tl.arange(0, BC)\n    p_k = tl.make_block_ptr(k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_s = tl.dot(b_q, b_k)\n    b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float('-inf'))\n    b_p = tl.where((i_t // BS == o_c)[None, :], float(1.0), exp(b_s - b_lse[:, None]))\n    b_i, b_ip = (tl.sum(b_p, 0), b_i)\n    o_i, o_ip = (tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i)\n    n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])\n    for i in tl.static_range(1, n_dims):\n        b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)\n    if i_c != 0:\n        b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)\n        b_i_new = b_ip * m_i + b_i * (1 - m_i)\n        o_i_new = o_ip * m_i + o_i * (1 - m_i)\n        b_i, o_i = _bitonic_merge(b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims)\n    else:\n        b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)\nm_top = tl.arange(0, BC // S) == 0\nb_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)\np_b = tl.make_block_ptr(block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,))\ntl.store(p_b, b_top.to(p_b.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    cu_seqlens,",
      "    token_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "    p_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where(",
      "                (i_t >= (i_s + tl.arange(0, BS)))[None, :], b_s, float(\"-inf\")",
      "            )",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = exp(b_mp - b_m)",
      "",
      "            b_p = exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "            b_mp = b_m",
      "    b_o = b_o / b_acc[:, None]",
      "    b_m += log(b_acc)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty))"
    ],
    "file": "codes/402.py",
    "header": "def parallel_nsa_fwd_kernel(q, k, v, o, lse, scale, block_indices, block_counts, cu_seqlens, token_indices, T, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, S: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr, USE_BLOCK_COUNTS: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\nblock_indices += (bos + i_t) * H * S + i_h * S\nif USE_BLOCK_COUNTS:\n    NS = tl.load(block_counts + (bos + i_t) * H + i_h)\nelse:\n    NS = S\np_q = tl.make_block_ptr(q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\np_o = tl.make_block_ptr(o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\np_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)\nb_o = tl.zeros([G, BV], dtype=tl.float32)\nb_m = tl.full([G], float('-inf'), dtype=tl.float32)\nb_acc = tl.zeros([G], dtype=tl.float32)\nfor i in range(NS):\n    i_s = tl.load(block_indices + i).to(tl.int32) * BS\n    if i_s <= i_t and i_s >= 0:\n        p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        b_s = tl.where((i_t >= i_s + tl.arange(0, BS))[None, :], b_s, float('-inf'))\n        b_m, b_mp = (tl.maximum(b_m, tl.max(b_s, 1)), b_m)\n        b_r = exp(b_mp - b_m)\n        b_p = exp(b_s - b_m[:, None])\n        b_acc = b_acc * b_r + tl.sum(b_p, 1)\n        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)\n        b_mp = b_m\nb_o = b_o / b_acc[:, None]\nb_m += log(b_acc)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_lse, b_m.to(p_lse.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_kernel_mask",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_mask(",
      "    block_indices,",
      "    block_counts,",
      "    block_mask,",
      "    T,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_b, i_hs = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h, i_s = i_hs // S, i_hs % S",
      "",
      "    b_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)",
      "    if USE_BLOCK_COUNTS:",
      "        b_m = b_i * BS <= i_t and i_s < tl.load(",
      "            block_counts + i_b * T * H + i_t * H + i_h",
      "        )",
      "    else:",
      "        b_m = b_i * BS <= i_t",
      "",
      "    if b_i < NS and b_i >= 0:",
      "        tl.store(",
      "            block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i,",
      "            b_m.to(block_mask.dtype.element_ty),",
      "        )"
    ],
    "file": "codes/402.py",
    "header": "def parallel_nsa_kernel_mask(block_indices, block_counts, block_mask, T, H: tl.constexpr, S: tl.constexpr, BS: tl.constexpr, NS: tl.constexpr, USE_BLOCK_COUNTS: tl.constexpr):",
    "body": "i_t, i_b, i_hs = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_h, i_s = (i_hs // S, i_hs % S)\nb_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)\nif USE_BLOCK_COUNTS:\n    b_m = b_i * BS <= i_t and i_s < tl.load(block_counts + i_b * T * H + i_t * H + i_h)\nelse:\n    b_m = b_i * BS <= i_t\nif b_i < NS and b_i >= 0:\n    tl.store(block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i, b_m.to(block_mask.dtype.element_ty))"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    cu_seqlens,",
      "    token_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_p = exp(b_s - b_lse[:, None])",
      "            b_p = tl.where((i_t >= (i_s + tl.arange(0, BS)))[None, :], b_p, 0)",
      "",
      "            b_dp = tl.dot(b_do, b_v)",
      "            b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "            b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/402.py",
    "header": "def parallel_nsa_bwd_kernel_dq(q, k, v, lse, delta, do, dq, scale, block_indices, block_counts, cu_seqlens, token_indices, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, S: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr, USE_BLOCK_COUNTS: tl.constexpr):",
    "body": "i_t, i_v, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(token_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nq += (bos + i_t) * HQ * K\ndo += (bos + i_t) * HQ * V\nlse += (bos + i_t) * HQ\ndelta += (bos + i_t) * HQ\ndq += (i_v * B * T + bos + i_t) * HQ * K\nblock_indices += (bos + i_t) * H * S + i_h * S\nif USE_BLOCK_COUNTS:\n    NS = tl.load(block_counts + (bos + i_t) * H + i_h)\nelse:\n    NS = S\nk += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\np_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\np_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_q = (b_q * scale).to(b_q.dtype)\np_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\np_lse = lse + i_h * G + tl.arange(0, G)\np_delta = delta + i_h * G + tl.arange(0, G)\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_lse = tl.load(p_lse)\nb_delta = tl.load(p_delta)\nb_dq = tl.zeros([G, BK], dtype=tl.float32)\nfor i in range(NS):\n    i_s = tl.load(block_indices + i).to(tl.int32) * BS\n    if i_s <= i_t and i_s >= 0:\n        p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))\n        p_v = tl.make_block_ptr(v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_v = tl.load(p_v, boundary_check=(0, 1))\n        b_s = tl.dot(b_q, b_k)\n        b_p = exp(b_s - b_lse[:, None])\n        b_p = tl.where((i_t >= i_s + tl.arange(0, BS))[None, :], b_p, 0)\n        b_dp = tl.dot(b_do, b_v)\n        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])\n        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))\nb_dq *= scale\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    block_mask,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    M: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_s, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_s * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_s * BS, 0),",
      "        (BS, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BS, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BS, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_s * BS, T):",
      "        b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)",
      "        if b_m:",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "            p_do = tl.make_block_ptr(",
      "                do + (bos + i) * HQ * V,",
      "                (HQ, V),",
      "                (V, 1),",
      "                (i_h * G, i_v * BV),",
      "                (G, BV),",
      "                (1, 0),",
      "            )",
      "            p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "            p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "            b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "            b_lse = tl.load(p_lse)",
      "            b_delta = tl.load(p_delta)",
      "",
      "            b_s = tl.dot(b_k, tl.trans(b_q))",
      "            b_p = exp(b_s - b_lse[None, :])",
      "            b_p = tl.where((i >= (i_s * BS + tl.arange(0, BS)))[:, None], b_p, 0)",
      "",
      "            b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "            b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "            b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "            b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/402.py",
    "header": "def parallel_nsa_bwd_kernel_dkv(q, k, v, lse, delta, do, dk, dv, block_mask, cu_seqlens, chunk_indices, scale, T, B: tl.constexpr, H: tl.constexpr, HQ: tl.constexpr, G: tl.constexpr, K: tl.constexpr, V: tl.constexpr, M: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_s, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_s = (tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(chunk_indices + i_s * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0))\np_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s * BS, i_v * BV), (BS, BV), (1, 0))\np_dk = tl.make_block_ptr(dk + (i_v * B * T * H + bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0))\np_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_s * BS, i_v * BV), (BS, BV), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dk = tl.zeros([BS, BK], dtype=tl.float32)\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dv = tl.zeros([BS, BV], dtype=tl.float32)\nfor i in range(i_s * BS, T):\n    b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)\n    if b_m:\n        p_q = tl.make_block_ptr(q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        p_do = tl.make_block_ptr(do + (bos + i) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))\n        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)\n        b_do = tl.load(p_do, boundary_check=(0, 1))\n        b_lse = tl.load(p_lse)\n        b_delta = tl.load(p_delta)\n        b_s = tl.dot(b_k, tl.trans(b_q))\n        b_p = exp(b_s - b_lse[None, :])\n        b_p = tl.where((i >= i_s * BS + tl.arange(0, BS))[:, None], b_p, 0)\n        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)\n        b_dp = tl.dot(b_v, tl.trans(b_do))\n        b_ds = b_p * (b_dp - b_delta[None, :])\n        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "_score_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args['BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args['BLOCK_N'] == 0})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "H_KV",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "ROUND_CTX",
        "annotation": null
      },
      {
        "name": "NKV_CTX",
        "annotation": null
      },
      {
        "name": "sliding_window_offset",
        "annotation": null
      },
      {
        "name": "sliding_window_size",
        "annotation": null
      },
      {
        "name": "SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "COMPLEMENT_SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _score_kernel(",
      "    Q,",
      "    K,",
      "    M,",
      "    sm_scale,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    H_KV,",
      "    N_CTX,",
      "    ROUND_CTX,",
      "    NKV_CTX,",
      "    sliding_window_offset,",
      "    sliding_window_size,",
      "    SLIDING_WINDOW: tl.constexpr,",
      "    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,",
      "    IS_EVEN_M: tl.constexpr,",
      "    IS_EVEN_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_n = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    off_hkv = off_h // (H // H_KV)",
      "    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh",
      "    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)",
      "    o = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + q_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + k_offset,",
      "        shape=(BLOCK_DMODEL, NKV_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, start_n * BLOCK_N),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "",
      "    if IS_EVEN_N:",
      "        k = tl.load(K_block_ptr)",
      "    else:",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    lo = 0",
      "    hi = ROUND_CTX",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.4426950408889634",
      "",
      "    for start_m in range(lo, hi, BLOCK_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_M)",
      "        if IS_EVEN_M:",
      "            q = tl.load(Q_block_ptr)",
      "        else:",
      "            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        m = tl.load(m_ptrs)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k)",
      "        qk = qk * qk_scale",
      "",
      "        if SLIDING_WINDOW:",
      "            dist = (",
      "                tl.arange(0, BLOCK_M)[:, None]",
      "                - tl.arange(0, BLOCK_N)[None, :]",
      "                + start_m",
      "                - start_n * BLOCK_N",
      "                + sliding_window_offset",
      "            )",
      "",
      "            if COMPLEMENT_SLIDING_WINDOW:",
      "                mask = dist >= sliding_window_size",
      "            else:",
      "                mask = (dist >= 0) & (dist < sliding_window_size)",
      "",
      "        qk = qk - m[:, None]",
      "        p = tl.math.exp2(qk)",
      "",
      "        if SLIDING_WINDOW:",
      "            p = tl.where(mask, p, 0)",
      "",
      "        if not IS_EVEN_N:",
      "            p = tl.where(((tl.arange(0, BLOCK_M) + start_m) < N_CTX)[:, None], p, 0)",
      "",
      "        o += tl.sum(p, axis=0)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))",
      "        m_ptrs = m_ptrs + BLOCK_M",
      "",
      "    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh",
      "    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N",
      "    o_ptrs = Out + o_offset + o_range",
      "    tl.store(o_ptrs, o.to(Out.type.element_ty), mask=o_range < NKV_CTX)"
    ],
    "file": "codes/731.py",
    "header": "def _score_kernel(Q, K, M, sm_scale, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_oz, stride_oh, stride_on, Z, H, H_KV, N_CTX, ROUND_CTX, NKV_CTX, sliding_window_offset, sliding_window_size, SLIDING_WINDOW: tl.constexpr, COMPLEMENT_SLIDING_WINDOW: tl.constexpr, IS_EVEN_M: tl.constexpr, IS_EVEN_N: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "start_n = tl.program_id(0)\noff_hz = tl.program_id(1)\noff_z = off_hz // H\noff_h = off_hz % H\noff_hkv = off_h // (H // H_KV)\nq_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh\nk_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh\nm_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)\no = tl.zeros([BLOCK_M], dtype=tl.float32)\nQ_block_ptr = tl.make_block_ptr(base=Q + q_offset, shape=(N_CTX, BLOCK_DMODEL), strides=(stride_qm, stride_qk), offsets=(0, 0), block_shape=(BLOCK_M, BLOCK_DMODEL), order=(1, 0))\nK_block_ptr = tl.make_block_ptr(base=K + k_offset, shape=(BLOCK_DMODEL, NKV_CTX), strides=(stride_kk, stride_kn), offsets=(0, start_n * BLOCK_N), block_shape=(BLOCK_DMODEL, BLOCK_N), order=(0, 1))\nif IS_EVEN_N:\n    k = tl.load(K_block_ptr)\nelse:\n    k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option='zero')\nlo = 0\nhi = ROUND_CTX\nqk_scale = sm_scale\nqk_scale *= 1.4426950408889634\nfor start_m in range(lo, hi, BLOCK_M):\n    start_m = tl.multiple_of(start_m, BLOCK_M)\n    if IS_EVEN_M:\n        q = tl.load(Q_block_ptr)\n    else:\n        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option='zero')\n    m = tl.load(m_ptrs)\n    qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n    qk += tl.dot(q, k)\n    qk = qk * qk_scale\n    if SLIDING_WINDOW:\n        dist = tl.arange(0, BLOCK_M)[:, None] - tl.arange(0, BLOCK_N)[None, :] + start_m - start_n * BLOCK_N + sliding_window_offset\n        if COMPLEMENT_SLIDING_WINDOW:\n            mask = dist >= sliding_window_size\n        else:\n            mask = (dist >= 0) & (dist < sliding_window_size)\n    qk = qk - m[:, None]\n    p = tl.math.exp2(qk)\n    if SLIDING_WINDOW:\n        p = tl.where(mask, p, 0)\n    if not IS_EVEN_N:\n        p = tl.where((tl.arange(0, BLOCK_M) + start_m < N_CTX)[:, None], p, 0)\n    o += tl.sum(p, axis=0)\n    Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))\n    m_ptrs = m_ptrs + BLOCK_M\no_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh\no_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N\no_ptrs = Out + o_offset + o_range\ntl.store(o_ptrs, o.to(Out.type.element_ty), mask=o_range < NKV_CTX)"
  },
  {
    "name": "parallel_path_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_stages=num_stages, num_warps=4) for num_stages in [2, 3, 4, 5, 6]], key=['BK', 'BS', 'BT', 'USE_GATE', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "o_new",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "L_new",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    o_new,",
      "    g_cumsum,",
      "    h,",
      "    scale,",
      "    L,",
      "    L_new,",
      "    M,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh = i_n * tl.cdiv(T, BS)",
      "    sm_scale = scale * 1.44269504",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_q = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_q += tl.load(p_q, boundary_check=(0, 1))",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_o += tl.load(p_o, boundary_check=(0, 1))",
      "",
      "    p_L = tl.make_block_ptr(L + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    p_M = tl.make_block_ptr(M + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_L, boundary_check=(0,))",
      "    b_m = tl.load(p_M, boundary_check=(0,))",
      "",
      "    if USE_GATE:",
      "        p_g_cumsum_q = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "    else:",
      "        b_g_cumsum_q = None",
      "",
      "    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):",
      "        i_tk = tl.cdiv(offset, BS)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_tk) * H + i_h) * K * K,",
      "            (K, K),",
      "            (K, 1),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        m_s = i_t * BT + tl.arange(0, BT) >= (offset + BS)",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k)",
      "        b_q_minus = tl.dot(b_q.to(b_h.dtype), b_h)",
      "        b_q = tl.where(m_s[:, None], b_q - b_q_minus, b_q)",
      "        if USE_GATE:",
      "            p_g_cumsum_k = tl.make_block_ptr(",
      "                g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,)",
      "            )",
      "            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "            b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "        b_s = tl.where(m_s[:, None], b_s * sm_scale, float(\"-inf\"))",
      "        b_m_new = tl.maximum(b_m, tl.max(b_s, 1))",
      "        alpha = tl.math.exp2((b_m - b_m_new))",
      "        b_s = tl.math.exp2(b_s - b_m_new[:, None])",
      "        b_s = tl.where(m_s[:, None], b_s, 0)",
      "        b_o *= alpha[:, None]",
      "        b_l = b_l * alpha + tl.sum(b_s, 1)",
      "        b_m = b_m_new",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v)",
      "",
      "    tl.debug_barrier()",
      "",
      "    for offset in range(i_t * BT - BS, -BS, -BS):",
      "        i_tk = tl.cdiv(offset, BS)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_tk) * H + i_h) * K * K,",
      "            (K, K),",
      "            (K, 1),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k)",
      "        b_q -= tl.dot(b_q.to(b_h.dtype), b_h)",
      "        if USE_GATE:",
      "            p_g_cumsum_k = tl.make_block_ptr(",
      "                g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,)",
      "            )",
      "            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "            b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "        b_s = b_s * sm_scale",
      "        b_m_new = tl.maximum(b_m, tl.max(b_s, 1))",
      "        alpha = tl.math.exp2((b_m - b_m_new))",
      "        b_s = tl.math.exp2(b_s - b_m_new[:, None])",
      "        b_o *= alpha[:, None]",
      "        b_l = b_l * alpha + tl.sum(b_s, 1)",
      "        b_m = b_m_new",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v)",
      "",
      "    b_o = b_o / b_l[:, None]",
      "    p_o_new = tl.make_block_ptr(",
      "        o_new + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_o_new, b_o.to(p_o_new.dtype.element_ty), boundary_check=(0, 1))",
      "    b_l = tl.math.log2(b_l) + b_m",
      "    p_L_new = tl.make_block_ptr(",
      "        L_new + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_L_new, b_l.to(p_L_new.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/412.py",
    "header": "def parallel_path_fwd_kernel(q, k, v, o, o_new, g_cumsum, h, scale, L, L_new, M, offsets, indices, chunk_offsets, T, G: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr, USE_GATE: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_hq = (i_bh // HQ, i_bh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\n    boh = i_n * tl.cdiv(T, BS)\nsm_scale = scale * 1.44269504\np_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_q = tl.zeros([BT, BK], dtype=tl.float32)\nb_q += tl.load(p_q, boundary_check=(0, 1))\nb_o = tl.zeros([BT, BV], dtype=tl.float32)\np_o = tl.make_block_ptr(o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_o += tl.load(p_o, boundary_check=(0, 1))\np_L = tl.make_block_ptr(L + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\np_M = tl.make_block_ptr(M + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_l = tl.load(p_L, boundary_check=(0,))\nb_m = tl.load(p_M, boundary_check=(0,))\nif USE_GATE:\n    p_g_cumsum_q = tl.make_block_ptr(g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))\nelse:\n    b_g_cumsum_q = None\nfor offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):\n    i_tk = tl.cdiv(offset, BS)\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + ((boh + i_tk) * H + i_h) * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    m_s = i_t * BT + tl.arange(0, BT) >= offset + BS\n    b_s = tl.dot(b_q.to(b_k.dtype), b_k)\n    b_q_minus = tl.dot(b_q.to(b_h.dtype), b_h)\n    b_q = tl.where(m_s[:, None], b_q - b_q_minus, b_q)\n    if USE_GATE:\n        p_g_cumsum_k = tl.make_block_ptr(g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,))\n        b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))\n        b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n    b_s = tl.where(m_s[:, None], b_s * sm_scale, float('-inf'))\n    b_m_new = tl.maximum(b_m, tl.max(b_s, 1))\n    alpha = tl.math.exp2(b_m - b_m_new)\n    b_s = tl.math.exp2(b_s - b_m_new[:, None])\n    b_s = tl.where(m_s[:, None], b_s, 0)\n    b_o *= alpha[:, None]\n    b_l = b_l * alpha + tl.sum(b_s, 1)\n    b_m = b_m_new\n    b_o += tl.dot(b_s.to(b_v.dtype), b_v)\ntl.debug_barrier()\nfor offset in range(i_t * BT - BS, -BS, -BS):\n    i_tk = tl.cdiv(offset, BS)\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + ((boh + i_tk) * H + i_h) * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_s = tl.dot(b_q.to(b_k.dtype), b_k)\n    b_q -= tl.dot(b_q.to(b_h.dtype), b_h)\n    if USE_GATE:\n        p_g_cumsum_k = tl.make_block_ptr(g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,))\n        b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))\n        b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]\n    b_s = b_s * sm_scale\n    b_m_new = tl.maximum(b_m, tl.max(b_s, 1))\n    alpha = tl.math.exp2(b_m - b_m_new)\n    b_s = tl.math.exp2(b_s - b_m_new[:, None])\n    b_o *= alpha[:, None]\n    b_l = b_l * alpha + tl.sum(b_s, 1)\n    b_m = b_m_new\n    b_o += tl.dot(b_s.to(b_v.dtype), b_v)\nb_o = b_o / b_l[:, None]\np_o_new = tl.make_block_ptr(o_new + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\ntl.store(p_o_new, b_o.to(p_o_new.dtype.element_ty), boundary_check=(0, 1))\nb_l = tl.math.log2(b_l) + b_m\np_L_new = tl.make_block_ptr(L_new + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,))\ntl.store(p_L_new, b_l.to(p_L_new.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "all_to_all_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config(kwargs={'BM': BM}, num_warps=w) for BM in [16] for w in [16]], key=[])"
    ],
    "args": [
      {
        "name": "send_tensor",
        "annotation": null
      },
      {
        "name": "data_src",
        "annotation": null
      },
      {
        "name": "data_dst",
        "annotation": null
      },
      {
        "name": "scale_src",
        "annotation": null
      },
      {
        "name": "scale_dst",
        "annotation": null
      },
      {
        "name": "splits_src",
        "annotation": null
      },
      {
        "name": "splits_dst",
        "annotation": null
      },
      {
        "name": "signal",
        "annotation": null
      },
      {
        "name": "send_splits_cumsum",
        "annotation": null
      },
      {
        "name": "recv_offset",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": "int"
      },
      {
        "name": "call_count",
        "annotation": "int"
      },
      {
        "name": "act_pos",
        "annotation": "int"
      },
      {
        "name": "MODE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONLINE_QUANT_FP8",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_GSIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HIDDEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "MAX_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_TOT_EXPERTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def all_to_all_kernel(",
      "    send_tensor,",
      "    data_src,",
      "    data_dst,",
      "    scale_src,",
      "    scale_dst,",
      "    splits_src,",
      "    splits_dst,",
      "    signal,",
      "    send_splits_cumsum,",
      "    recv_offset,",
      "    rank: int,",
      "    call_count: int,",
      "    act_pos: int,",
      "    MODE: tl.constexpr,",
      "    ONLINE_QUANT_FP8: tl.constexpr,",
      "    FP8_GSIZE: tl.constexpr,",
      "    WORLD_SIZE: tl.constexpr,",
      "    HIDDEN: tl.constexpr,",
      "    MAX_M: tl.constexpr,",
      "    NUM_TOT_EXPERTS: tl.constexpr,",
      "    BM: tl.constexpr,",
      "    BN: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(0)",
      "",
      "    threadidx = tid(axis=0)",
      "    NUM_GROUPS: tl.constexpr = HIDDEN // FP8_GSIZE",
      "    EXPERTS_PER_RANK: tl.constexpr = NUM_TOT_EXPERTS // WORLD_SIZE",
      "",
      "    exp_st = pid * EXPERTS_PER_RANK",
      "    exp_ed = exp_st + EXPERTS_PER_RANK",
      "    m_st = tl.load(send_splits_cumsum + exp_st)",
      "    m_ed = tl.load(send_splits_cumsum + exp_ed)",
      "    num_rows_cur_block = m_ed - m_st",
      "",
      "    signal_ptr = signal + act_pos * WORLD_SIZE + rank",
      "    if MODE == 0:",
      "",
      "        split_src_ptr = splits_src + (exp_st + pid)",
      "        split_dst_ptr = (",
      "            splits_dst",
      "            + act_pos * (NUM_TOT_EXPERTS + WORLD_SIZE)",
      "            + rank * (EXPERTS_PER_RANK + 1)",
      "        )",
      "",
      "        off0 = exp_st + tl.arange(0, EXPERTS_PER_RANK)",
      "        off1 = exp_st + tl.arange(0, EXPERTS_PER_RANK) + 1",
      "        cumsum_sts = tl.load(send_splits_cumsum + off0)",
      "        cumsum_eds = tl.load(send_splits_cumsum + off1)",
      "        tl.store(",
      "            split_src_ptr + tl.arange(0, EXPERTS_PER_RANK), cumsum_eds - cumsum_sts",
      "        )",
      "        tl.store(split_src_ptr + EXPERTS_PER_RANK, m_st)",
      "",
      "        src_off = m_st",
      "        dst_off = rank * MAX_M",
      "        data_src_ptr = data_src + src_off * HIDDEN",
      "        data_dst_ptr = (",
      "            data_dst + act_pos * WORLD_SIZE * MAX_M * HIDDEN + dst_off * HIDDEN",
      "        )",
      "        scale_src_ptr = scale_src + src_off * NUM_GROUPS",
      "        scale_dst_ptr = (",
      "            scale_dst + act_pos * WORLD_SIZE * MAX_M * NUM_GROUPS + dst_off * NUM_GROUPS",
      "        )",
      "    else:",
      "",
      "        src_off = pid * MAX_M",
      "        dst_off = tl.load(recv_offset + pid)",
      "        data_src_ptr = (",
      "            data_src + act_pos * WORLD_SIZE * MAX_M * HIDDEN + src_off * HIDDEN",
      "        )",
      "        data_dst_ptr = data_dst + dst_off * HIDDEN",
      "        scale_src_ptr = (",
      "            scale_src + act_pos * WORLD_SIZE * MAX_M * NUM_GROUPS + src_off * NUM_GROUPS",
      "        )",
      "        scale_dst_ptr = scale_dst + dst_off * NUM_GROUPS",
      "",
      "    off_m = tl.arange(0, BM)",
      "    if ONLINE_QUANT_FP8 and MODE == 0:",
      "",
      "        UNROLL_FACTOR: tl.constexpr = 4",
      "        group_offs = (",
      "            off_m[:, None] * HIDDEN + tl.arange(0, FP8_GSIZE * UNROLL_FACTOR)[None, :]",
      "        )",
      "        send_tensor_ptrs = send_tensor + m_st * HIDDEN + group_offs",
      "        data_src_ptrs = (",
      "            tl.cast(data_src_ptr, tl.pointer_type(tl.float8e4nv)) + group_offs",
      "        )",
      "        scale_src_ptrs = (",
      "            scale_src_ptr",
      "            + off_m[:, None] * NUM_GROUPS",
      "            + tl.arange(0, UNROLL_FACTOR)[None, :]",
      "        )",
      "",
      "        for i in tl.range(ceil_div(num_rows_cur_block, BM)):",
      "            group_mask = off_m[:, None] < num_rows_cur_block - i * BM",
      "            for _ in tl.static_range(0, NUM_GROUPS, UNROLL_FACTOR):",
      "                group = tl.reshape(",
      "                    tl.load(send_tensor_ptrs, group_mask),",
      "                    (BM * UNROLL_FACTOR, FP8_GSIZE),",
      "                )",
      "                scale = (",
      "                    tl.max(tl.abs(group), 1, keep_dims=True).to(tl.float32)",
      "                    * FP8_MAX_INV",
      "                )",
      "                quant = tl.reshape(",
      "                    (group.to(tl.float32) / scale).to(tl.float8e4nv),",
      "                    (BM, UNROLL_FACTOR * FP8_GSIZE),",
      "                )",
      "                tl.store(data_src_ptrs, quant, group_mask)",
      "                tl.store(",
      "                    scale_src_ptrs, tl.reshape(scale, (BM, UNROLL_FACTOR)), group_mask",
      "                )",
      "                send_tensor_ptrs += UNROLL_FACTOR * FP8_GSIZE",
      "                data_src_ptrs += UNROLL_FACTOR * FP8_GSIZE",
      "                scale_src_ptrs += UNROLL_FACTOR",
      "            send_tensor_ptrs += (BM - 1) * HIDDEN",
      "            data_src_ptrs += (BM - 1) * HIDDEN",
      "            scale_src_ptrs += (BM - 1) * NUM_GROUPS",
      "    else:",
      "        off_n = tl.arange(0, BN)",
      "        send_tensor_ptrs = (",
      "            send_tensor + m_st * HIDDEN + off_m[:, None] * HIDDEN + off_n[None, :]",
      "        )",
      "        data_src_ptrs = data_src_ptr + off_m[:, None] * HIDDEN + off_n[None, :]",
      "        for i in tl.range(ceil_div(num_rows_cur_block, BM)):",
      "            data_mask = (off_m[:, None] < num_rows_cur_block - i * BM) & (",
      "                off_n[None, :] < HIDDEN",
      "            )",
      "            tl.store(data_src_ptrs, tl.load(send_tensor_ptrs, data_mask), data_mask)",
      "            send_tensor_ptrs += BM * HIDDEN",
      "            data_src_ptrs += BM * HIDDEN",
      "",
      "    libshmem_device.putmem_nbi_block(",
      "        data_dst_ptr,",
      "        data_src_ptr,",
      "        num_rows_cur_block * HIDDEN * (1 if (ONLINE_QUANT_FP8 and MODE == 0) else 2),",
      "        pid,",
      "    )",
      "    if MODE == 0:",
      "",
      "        libshmem_device.putmem_nbi_block(",
      "            split_dst_ptr,",
      "            split_src_ptr,",
      "            (EXPERTS_PER_RANK + 1) * 4,",
      "            pid,",
      "        )",
      "",
      "    if ONLINE_QUANT_FP8:",
      "        libshmem_device.putmem_signal_nbi_block(",
      "            scale_dst_ptr,",
      "            scale_src_ptr,",
      "            num_rows_cur_block * NUM_GROUPS * 4,",
      "            signal_ptr,",
      "            call_count,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            pid,",
      "        )",
      "",
      "    libshmem_device.fence()",
      "    if threadidx == 0:",
      "",
      "        if not ONLINE_QUANT_FP8:",
      "            libshmem_device.signal_op(",
      "                signal_ptr,",
      "                call_count,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                pid,",
      "            )",
      "",
      "        libshmem_device.signal_wait_until(",
      "            signal + act_pos * WORLD_SIZE + pid,",
      "            libshmem_device.NVSHMEM_CMP_EQ,",
      "            call_count,",
      "        )"
    ],
    "file": "codes/72.py",
    "header": "def all_to_all_kernel(send_tensor, data_src, data_dst, scale_src, scale_dst, splits_src, splits_dst, signal, send_splits_cumsum, recv_offset, rank: int, call_count: int, act_pos: int, MODE: tl.constexpr, ONLINE_QUANT_FP8: tl.constexpr, FP8_GSIZE: tl.constexpr, WORLD_SIZE: tl.constexpr, HIDDEN: tl.constexpr, MAX_M: tl.constexpr, NUM_TOT_EXPERTS: tl.constexpr, BM: tl.constexpr, BN: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nthreadidx = tid(axis=0)\nNUM_GROUPS: tl.constexpr = HIDDEN // FP8_GSIZE\nEXPERTS_PER_RANK: tl.constexpr = NUM_TOT_EXPERTS // WORLD_SIZE\nexp_st = pid * EXPERTS_PER_RANK\nexp_ed = exp_st + EXPERTS_PER_RANK\nm_st = tl.load(send_splits_cumsum + exp_st)\nm_ed = tl.load(send_splits_cumsum + exp_ed)\nnum_rows_cur_block = m_ed - m_st\nsignal_ptr = signal + act_pos * WORLD_SIZE + rank\nif MODE == 0:\n    split_src_ptr = splits_src + (exp_st + pid)\n    split_dst_ptr = splits_dst + act_pos * (NUM_TOT_EXPERTS + WORLD_SIZE) + rank * (EXPERTS_PER_RANK + 1)\n    off0 = exp_st + tl.arange(0, EXPERTS_PER_RANK)\n    off1 = exp_st + tl.arange(0, EXPERTS_PER_RANK) + 1\n    cumsum_sts = tl.load(send_splits_cumsum + off0)\n    cumsum_eds = tl.load(send_splits_cumsum + off1)\n    tl.store(split_src_ptr + tl.arange(0, EXPERTS_PER_RANK), cumsum_eds - cumsum_sts)\n    tl.store(split_src_ptr + EXPERTS_PER_RANK, m_st)\n    src_off = m_st\n    dst_off = rank * MAX_M\n    data_src_ptr = data_src + src_off * HIDDEN\n    data_dst_ptr = data_dst + act_pos * WORLD_SIZE * MAX_M * HIDDEN + dst_off * HIDDEN\n    scale_src_ptr = scale_src + src_off * NUM_GROUPS\n    scale_dst_ptr = scale_dst + act_pos * WORLD_SIZE * MAX_M * NUM_GROUPS + dst_off * NUM_GROUPS\nelse:\n    src_off = pid * MAX_M\n    dst_off = tl.load(recv_offset + pid)\n    data_src_ptr = data_src + act_pos * WORLD_SIZE * MAX_M * HIDDEN + src_off * HIDDEN\n    data_dst_ptr = data_dst + dst_off * HIDDEN\n    scale_src_ptr = scale_src + act_pos * WORLD_SIZE * MAX_M * NUM_GROUPS + src_off * NUM_GROUPS\n    scale_dst_ptr = scale_dst + dst_off * NUM_GROUPS\noff_m = tl.arange(0, BM)\nif ONLINE_QUANT_FP8 and MODE == 0:\n    UNROLL_FACTOR: tl.constexpr = 4\n    group_offs = off_m[:, None] * HIDDEN + tl.arange(0, FP8_GSIZE * UNROLL_FACTOR)[None, :]\n    send_tensor_ptrs = send_tensor + m_st * HIDDEN + group_offs\n    data_src_ptrs = tl.cast(data_src_ptr, tl.pointer_type(tl.float8e4nv)) + group_offs\n    scale_src_ptrs = scale_src_ptr + off_m[:, None] * NUM_GROUPS + tl.arange(0, UNROLL_FACTOR)[None, :]\n    for i in tl.range(ceil_div(num_rows_cur_block, BM)):\n        group_mask = off_m[:, None] < num_rows_cur_block - i * BM\n        for _ in tl.static_range(0, NUM_GROUPS, UNROLL_FACTOR):\n            group = tl.reshape(tl.load(send_tensor_ptrs, group_mask), (BM * UNROLL_FACTOR, FP8_GSIZE))\n            scale = tl.max(tl.abs(group), 1, keep_dims=True).to(tl.float32) * FP8_MAX_INV\n            quant = tl.reshape((group.to(tl.float32) / scale).to(tl.float8e4nv), (BM, UNROLL_FACTOR * FP8_GSIZE))\n            tl.store(data_src_ptrs, quant, group_mask)\n            tl.store(scale_src_ptrs, tl.reshape(scale, (BM, UNROLL_FACTOR)), group_mask)\n            send_tensor_ptrs += UNROLL_FACTOR * FP8_GSIZE\n            data_src_ptrs += UNROLL_FACTOR * FP8_GSIZE\n            scale_src_ptrs += UNROLL_FACTOR\n        send_tensor_ptrs += (BM - 1) * HIDDEN\n        data_src_ptrs += (BM - 1) * HIDDEN\n        scale_src_ptrs += (BM - 1) * NUM_GROUPS\nelse:\n    off_n = tl.arange(0, BN)\n    send_tensor_ptrs = send_tensor + m_st * HIDDEN + off_m[:, None] * HIDDEN + off_n[None, :]\n    data_src_ptrs = data_src_ptr + off_m[:, None] * HIDDEN + off_n[None, :]\n    for i in tl.range(ceil_div(num_rows_cur_block, BM)):\n        data_mask = (off_m[:, None] < num_rows_cur_block - i * BM) & (off_n[None, :] < HIDDEN)\n        tl.store(data_src_ptrs, tl.load(send_tensor_ptrs, data_mask), data_mask)\n        send_tensor_ptrs += BM * HIDDEN\n        data_src_ptrs += BM * HIDDEN\nlibshmem_device.putmem_nbi_block(data_dst_ptr, data_src_ptr, num_rows_cur_block * HIDDEN * (1 if ONLINE_QUANT_FP8 and MODE == 0 else 2), pid)\nif MODE == 0:\n    libshmem_device.putmem_nbi_block(split_dst_ptr, split_src_ptr, (EXPERTS_PER_RANK + 1) * 4, pid)\nif ONLINE_QUANT_FP8:\n    libshmem_device.putmem_signal_nbi_block(scale_dst_ptr, scale_src_ptr, num_rows_cur_block * NUM_GROUPS * 4, signal_ptr, call_count, libshmem_device.NVSHMEM_SIGNAL_SET, pid)\nlibshmem_device.fence()\nif threadidx == 0:\n    if not ONLINE_QUANT_FP8:\n        libshmem_device.signal_op(signal_ptr, call_count, libshmem_device.NVSHMEM_SIGNAL_SET, pid)\n    libshmem_device.signal_wait_until(signal + act_pos * WORLD_SIZE + pid, libshmem_device.NVSHMEM_CMP_EQ, call_count)"
  },
  {
    "name": "conv2d_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[conv2d_forward_config(128, 32, 128, n_warps=8, n_stages=2), conv2d_forward_config(256, 32, 64, n_warps=8, n_stages=2), conv2d_forward_config(256, 32, 32, n_warps=4, n_stages=4), conv2d_forward_config(256, 64, 32, n_warps=4, n_stages=4), conv2d_forward_config(256, 32, 16, n_warps=2, n_stages=4), conv2d_forward_config(64, 32, 128, n_warps=8, n_stages=4), conv2d_forward_config(128, 32, 64, n_warps=4, n_stages=4), conv2d_forward_config(64, 32, 64, n_warps=4, n_stages=4), conv2d_forward_config(128, 32, 16, n_warps=4, n_stages=4), conv2d_forward_config(128, 128, 128, n_warps=8, n_stages=3), conv2d_forward_config(256, 128, 64, n_warps=8, n_stages=3), conv2d_forward_config(256, 128, 32, n_warps=4, n_stages=4), conv2d_forward_config(64, 128, 128, n_warps=4, n_stages=4), conv2d_forward_config(128, 128, 64, n_warps=4, n_stages=4), conv2d_forward_config(128, 64, 32, n_warps=2, n_stages=4), conv2d_forward_config(64, 64, 64, n_warps=2, n_stages=4)], key=['batch_dim', 'in_feat_dim', 'in_height', 'in_width', 'out_feat_dim', 'out_height', 'out_width', 'kernel_height', 'kernel_width', 'stride_height', 'stride_width', 'padding_height', 'padding_width', 'groups', 'fp16'])",
      "@triton.heuristics({'tf32': lambda _: allow_tf32()})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "in_feat_dim",
        "annotation": null
      },
      {
        "name": "in_height",
        "annotation": null
      },
      {
        "name": "in_width",
        "annotation": null
      },
      {
        "name": "out_feat_dim",
        "annotation": null
      },
      {
        "name": "out_height",
        "annotation": null
      },
      {
        "name": "out_width",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_in_feat_stride",
        "annotation": null
      },
      {
        "name": "input_height_stride",
        "annotation": null
      },
      {
        "name": "input_width_stride",
        "annotation": null
      },
      {
        "name": "weight_out_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_in_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_height_stride",
        "annotation": null
      },
      {
        "name": "weight_width_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_out_feat_stride",
        "annotation": null
      },
      {
        "name": "output_height_stride",
        "annotation": null
      },
      {
        "name": "output_width_stride",
        "annotation": null
      },
      {
        "name": "kernel_height",
        "annotation": "tl.constexpr"
      },
      {
        "name": "kernel_width",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_height",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_width",
        "annotation": "tl.constexpr"
      },
      {
        "name": "padding_height",
        "annotation": "tl.constexpr"
      },
      {
        "name": "padding_width",
        "annotation": "tl.constexpr"
      },
      {
        "name": "groups",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp16",
        "annotation": "tl.constexpr"
      },
      {
        "name": "tf32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH_HEIGHT_WIDTH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_IN_FEAT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_OUT_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def conv2d_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    in_feat_dim,",
      "    in_height,",
      "    in_width,",
      "    out_feat_dim,",
      "    out_height,",
      "    out_width,",
      "    input_batch_stride,",
      "    input_in_feat_stride,",
      "    input_height_stride,",
      "    input_width_stride,",
      "    weight_out_feat_stride,",
      "    weight_in_feat_stride,",
      "    weight_height_stride,",
      "    weight_width_stride,",
      "    output_batch_stride,",
      "    output_out_feat_stride,",
      "    output_height_stride,",
      "    output_width_stride,",
      "    kernel_height: tl.constexpr,",
      "    kernel_width: tl.constexpr,",
      "    stride_height: tl.constexpr,",
      "    stride_width: tl.constexpr,",
      "    padding_height: tl.constexpr,",
      "    padding_width: tl.constexpr,",
      "    groups: tl.constexpr,",
      "    fp16: tl.constexpr,",
      "    tf32: tl.constexpr,",
      "    BLOCK_SIZE_BATCH_HEIGHT_WIDTH: tl.constexpr,",
      "    BLOCK_SIZE_IN_FEAT: tl.constexpr,",
      "    BLOCK_SIZE_OUT_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_height_width_pid = tl.program_id(0)",
      "    out_feat_pid = tl.program_id(1)",
      "    group_pid = tl.program_id(2)",
      "",
      "    in_group_dim = in_feat_dim // groups",
      "    out_group_dim = out_feat_dim // groups",
      "",
      "    batch_height_width_offset = (",
      "        batch_height_width_pid * BLOCK_SIZE_BATCH_HEIGHT_WIDTH",
      "        + tl.arange(0, BLOCK_SIZE_BATCH_HEIGHT_WIDTH)",
      "    )",
      "    batch_height_offset = batch_height_width_offset // out_width",
      "    batch_offset = batch_height_offset // out_height",
      "",
      "    output_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(",
      "        0, BLOCK_SIZE_OUT_FEAT",
      "    )",
      "    output_height_offset = batch_height_offset % out_height",
      "    output_width_offset = batch_height_width_offset % out_width",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset",
      "        + input_in_feat_stride * group_pid * in_group_dim",
      "    )[:, None]",
      "    weight_pointer += (",
      "        weight_out_feat_stride * output_feat_offset",
      "        + weight_out_feat_stride * group_pid * out_group_dim",
      "    )[None, :]",
      "",
      "    accum = tl.zeros(",
      "        (BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32",
      "    )",
      "",
      "    for h in range(kernel_height):",
      "        for w in range(kernel_width):",
      "            for c in range(0, in_group_dim, BLOCK_SIZE_IN_FEAT):",
      "                input_feat_offset = c + tl.arange(0, BLOCK_SIZE_IN_FEAT)",
      "                input_height_offset = (",
      "                    h - padding_height + stride_height * output_height_offset",
      "                )",
      "                input_width_offset = (",
      "                    w - padding_width + stride_width * output_width_offset",
      "                )",
      "",
      "                curr_input_pointer = (",
      "                    input_pointer",
      "                    + (input_in_feat_stride * input_feat_offset)[None, :]",
      "                    + (input_height_stride * input_height_offset)[:, None]",
      "                    + (input_width_stride * input_width_offset)[:, None]",
      "                )",
      "                curr_weight_pointer = (",
      "                    weight_pointer",
      "                    + (weight_in_feat_stride * input_feat_offset)[:, None]",
      "                    + (weight_height_stride * h)",
      "                    + (weight_width_stride * w)",
      "                )",
      "",
      "                input_mask = (",
      "                    (batch_offset < batch_dim)[:, None]",
      "                    & (input_feat_offset < in_group_dim)[None, :]",
      "                    & (0 <= input_height_offset)[:, None]",
      "                    & (input_height_offset < in_height)[:, None]",
      "                    & (0 <= input_width_offset)[:, None]",
      "                    & (input_width_offset < in_width)[:, None]",
      "                )",
      "                weight_mask = (input_feat_offset < in_group_dim)[:, None] & (",
      "                    output_feat_offset < out_group_dim",
      "                )[None, :]",
      "",
      "                input_block = tl.load(curr_input_pointer, mask=input_mask)",
      "                weight_block = tl.load(curr_weight_pointer, mask=weight_mask)",
      "",
      "                if fp16:",
      "                    input_block = input_block.to(tl.float16)",
      "                    weight_block = weight_block.to(tl.float16)",
      "",
      "                accum += tl.dot(input_block, weight_block, allow_tf32=tf32)",
      "",
      "    output_pointer += (",
      "        (output_batch_stride * batch_offset)[:, None]",
      "        + (output_out_feat_stride * (group_pid * out_group_dim + output_feat_offset))[",
      "            None, :",
      "        ]",
      "        + (output_height_stride * output_height_offset)[:, None]",
      "        + (output_width_stride * output_width_offset)[:, None]",
      "    )",
      "    output_mask = (",
      "        (batch_offset < batch_dim)[:, None]",
      "        & (output_feat_offset < out_group_dim)[None, :]",
      "        & (output_height_offset < out_height)[:, None]",
      "        & (output_width_offset < out_width)[:, None]",
      "    )",
      "",
      "    tl.store(output_pointer, accum, mask=output_mask)"
    ],
    "file": "codes/23.py",
    "header": "def conv2d_forward_kernel(input_pointer, weight_pointer, output_pointer, batch_dim, in_feat_dim, in_height, in_width, out_feat_dim, out_height, out_width, input_batch_stride, input_in_feat_stride, input_height_stride, input_width_stride, weight_out_feat_stride, weight_in_feat_stride, weight_height_stride, weight_width_stride, output_batch_stride, output_out_feat_stride, output_height_stride, output_width_stride, kernel_height: tl.constexpr, kernel_width: tl.constexpr, stride_height: tl.constexpr, stride_width: tl.constexpr, padding_height: tl.constexpr, padding_width: tl.constexpr, groups: tl.constexpr, fp16: tl.constexpr, tf32: tl.constexpr, BLOCK_SIZE_BATCH_HEIGHT_WIDTH: tl.constexpr, BLOCK_SIZE_IN_FEAT: tl.constexpr, BLOCK_SIZE_OUT_FEAT: tl.constexpr):",
    "body": "batch_height_width_pid = tl.program_id(0)\nout_feat_pid = tl.program_id(1)\ngroup_pid = tl.program_id(2)\nin_group_dim = in_feat_dim // groups\nout_group_dim = out_feat_dim // groups\nbatch_height_width_offset = batch_height_width_pid * BLOCK_SIZE_BATCH_HEIGHT_WIDTH + tl.arange(0, BLOCK_SIZE_BATCH_HEIGHT_WIDTH)\nbatch_height_offset = batch_height_width_offset // out_width\nbatch_offset = batch_height_offset // out_height\noutput_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(0, BLOCK_SIZE_OUT_FEAT)\noutput_height_offset = batch_height_offset % out_height\noutput_width_offset = batch_height_width_offset % out_width\ninput_pointer += (input_batch_stride * batch_offset + input_in_feat_stride * group_pid * in_group_dim)[:, None]\nweight_pointer += (weight_out_feat_stride * output_feat_offset + weight_out_feat_stride * group_pid * out_group_dim)[None, :]\naccum = tl.zeros((BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32)\nfor h in range(kernel_height):\n    for w in range(kernel_width):\n        for c in range(0, in_group_dim, BLOCK_SIZE_IN_FEAT):\n            input_feat_offset = c + tl.arange(0, BLOCK_SIZE_IN_FEAT)\n            input_height_offset = h - padding_height + stride_height * output_height_offset\n            input_width_offset = w - padding_width + stride_width * output_width_offset\n            curr_input_pointer = input_pointer + (input_in_feat_stride * input_feat_offset)[None, :] + (input_height_stride * input_height_offset)[:, None] + (input_width_stride * input_width_offset)[:, None]\n            curr_weight_pointer = weight_pointer + (weight_in_feat_stride * input_feat_offset)[:, None] + weight_height_stride * h + weight_width_stride * w\n            input_mask = (batch_offset < batch_dim)[:, None] & (input_feat_offset < in_group_dim)[None, :] & (0 <= input_height_offset)[:, None] & (input_height_offset < in_height)[:, None] & (0 <= input_width_offset)[:, None] & (input_width_offset < in_width)[:, None]\n            weight_mask = (input_feat_offset < in_group_dim)[:, None] & (output_feat_offset < out_group_dim)[None, :]\n            input_block = tl.load(curr_input_pointer, mask=input_mask)\n            weight_block = tl.load(curr_weight_pointer, mask=weight_mask)\n            if fp16:\n                input_block = input_block.to(tl.float16)\n                weight_block = weight_block.to(tl.float16)\n            accum += tl.dot(input_block, weight_block, allow_tf32=tf32)\noutput_pointer += (output_batch_stride * batch_offset)[:, None] + (output_out_feat_stride * (group_pid * out_group_dim + output_feat_offset))[None, :] + (output_height_stride * output_height_offset)[:, None] + (output_width_stride * output_width_offset)[:, None]\noutput_mask = (batch_offset < batch_dim)[:, None] & (output_feat_offset < out_group_dim)[None, :] & (output_height_offset < out_height)[:, None] & (output_width_offset < out_width)[:, None]\ntl.store(output_pointer, accum, mask=output_mask)"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K: tl.constexpr,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    tl.static_assert(",
      "        K % (4 * BLOCK_SIZE_K) == 0,",
      "        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",",
      "    )",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)",
      "",
      "    for i in range(4):",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):",
      "            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j",
      "",
      "            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)",
      "",
      "            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)",
      "",
      "            mask = 3 << (2 * i)",
      "",
      "            b = (b_uint8 & mask) >> (2 * i)",
      "",
      "            tensor_full = tl.full((1,), 1, dtype=tl.int8)",
      "",
      "            accumulator += tl.dot(a, (b.to(tl.int8) - tensor_full), out_dtype=tl.int32)",
      "",
      "            a_ptrs += BLOCK_SIZE_K * stride_ak",
      "            b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/595.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K: tl.constexpr, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "tl.static_assert(K % (4 * BLOCK_SIZE_K) == 0, 'K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K')\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\nfor i in range(4):\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):\n        k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j\n        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)\n        b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)\n        mask = 3 << 2 * i\n        b = (b_uint8 & mask) >> 2 * i\n        tensor_full = tl.full((1,), 1, dtype=tl.int8)\n        accumulator += tl.dot(a, b.to(tl.int8) - tensor_full, out_dtype=tl.int32)\n        a_ptrs += BLOCK_SIZE_K * stride_ak\n        b_ptrs += BLOCK_SIZE_K * stride_bk\nc = accumulator\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "grouped_matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'NUM_SM': 84}), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'NUM_SM': 128}), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'NUM_SM': 84}), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'NUM_SM': 128})], key=['group_size'])"
    ],
    "args": [
      {
        "name": "group_a_ptrs",
        "annotation": null
      },
      {
        "name": "group_b_ptrs",
        "annotation": null
      },
      {
        "name": "group_c_ptrs",
        "annotation": null
      },
      {
        "name": "group_gemm_sizes",
        "annotation": null
      },
      {
        "name": "g_lds",
        "annotation": null
      },
      {
        "name": "group_size",
        "annotation": null
      },
      {
        "name": "NUM_SM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def grouped_matmul_kernel(",
      "    group_a_ptrs,",
      "    group_b_ptrs,",
      "    group_c_ptrs,",
      "    group_gemm_sizes,",
      "    g_lds,",
      "    group_size,",
      "    NUM_SM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    tile_idx = tl.program_id(0)",
      "    last_problem_end = 0",
      "    for g in range(group_size):",
      "",
      "        gm = tl.load(group_gemm_sizes + g * 3)",
      "        gn = tl.load(group_gemm_sizes + g * 3 + 1)",
      "        gk = tl.load(group_gemm_sizes + g * 3 + 2)",
      "        num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)",
      "        num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)",
      "        num_tiles = num_m_tiles * num_n_tiles",
      "",
      "        while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:",
      "",
      "            k = gk",
      "            lda = tl.load(g_lds + g * 3)",
      "            ldb = tl.load(g_lds + g * 3 + 1)",
      "            ldc = tl.load(g_lds + g * 3 + 2)",
      "            a_ptr = tl.load(group_a_ptrs + g).to(tl.pointer_type(tl.float16))",
      "            b_ptr = tl.load(group_b_ptrs + g).to(tl.pointer_type(tl.float16))",
      "            c_ptr = tl.load(group_c_ptrs + g).to(tl.pointer_type(tl.float16))",
      "",
      "            tile_idx_in_gemm = tile_idx - last_problem_end",
      "            tile_m_idx = tile_idx_in_gemm // num_n_tiles",
      "            tile_n_idx = tile_idx_in_gemm % num_n_tiles",
      "",
      "            offs_am = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_bn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "            a_ptrs = a_ptr + offs_am[:, None] * lda + offs_k[None, :]",
      "            b_ptrs = b_ptr + offs_k[:, None] * ldb + offs_bn[None, :]",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "            for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):",
      "",
      "                tl.multiple_of(a_ptrs, [16, 16])",
      "                tl.multiple_of(b_ptrs, [16, 16])",
      "",
      "                a = tl.load(a_ptrs)",
      "                b = tl.load(b_ptrs)",
      "                accumulator += tl.dot(a, b)",
      "                a_ptrs += BLOCK_SIZE_K",
      "                b_ptrs += BLOCK_SIZE_K * ldb",
      "            c = accumulator.to(tl.float16)",
      "",
      "            offs_cm = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_cn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            c_ptrs = c_ptr + ldc * offs_cm[:, None] + offs_cn[None, :]",
      "",
      "            tl.store(c_ptrs, c)",
      "",
      "            tile_idx += NUM_SM",
      "",
      "        last_problem_end = last_problem_end + num_tiles"
    ],
    "file": "codes/655.py",
    "header": "def grouped_matmul_kernel(group_a_ptrs, group_b_ptrs, group_c_ptrs, group_gemm_sizes, g_lds, group_size, NUM_SM: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr):",
    "body": "tile_idx = tl.program_id(0)\nlast_problem_end = 0\nfor g in range(group_size):\n    gm = tl.load(group_gemm_sizes + g * 3)\n    gn = tl.load(group_gemm_sizes + g * 3 + 1)\n    gk = tl.load(group_gemm_sizes + g * 3 + 2)\n    num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)\n    num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)\n    num_tiles = num_m_tiles * num_n_tiles\n    while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:\n        k = gk\n        lda = tl.load(g_lds + g * 3)\n        ldb = tl.load(g_lds + g * 3 + 1)\n        ldc = tl.load(g_lds + g * 3 + 2)\n        a_ptr = tl.load(group_a_ptrs + g).to(tl.pointer_type(tl.float16))\n        b_ptr = tl.load(group_b_ptrs + g).to(tl.pointer_type(tl.float16))\n        c_ptr = tl.load(group_c_ptrs + g).to(tl.pointer_type(tl.float16))\n        tile_idx_in_gemm = tile_idx - last_problem_end\n        tile_m_idx = tile_idx_in_gemm // num_n_tiles\n        tile_n_idx = tile_idx_in_gemm % num_n_tiles\n        offs_am = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        offs_k = tl.arange(0, BLOCK_SIZE_K)\n        a_ptrs = a_ptr + offs_am[:, None] * lda + offs_k[None, :]\n        b_ptrs = b_ptr + offs_k[:, None] * ldb + offs_bn[None, :]\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n        for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):\n            tl.multiple_of(a_ptrs, [16, 16])\n            tl.multiple_of(b_ptrs, [16, 16])\n            a = tl.load(a_ptrs)\n            b = tl.load(b_ptrs)\n            accumulator += tl.dot(a, b)\n            a_ptrs += BLOCK_SIZE_K\n            b_ptrs += BLOCK_SIZE_K * ldb\n        c = accumulator.to(tl.float16)\n        offs_cm = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_cn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        c_ptrs = c_ptr + ldc * offs_cm[:, None] + offs_cn[None, :]\n        tl.store(c_ptrs, c)\n        tile_idx += NUM_SM\n    last_problem_end = last_problem_end + num_tiles"
  },
  {
    "name": "glu_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input1_pointer",
        "annotation": null
      },
      {
        "name": "input2_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def glu_forward_kernel(",
      "    input1_pointer,",
      "    input2_pointer,",
      "    output_pointer,",
      "    size,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input1 = tl.load(input1_pointer + offset, mask=mask)",
      "    input2 = tl.load(input2_pointer + offset, mask=mask)",
      "",
      "    output = input1 * apply_act_func(input2, None, None, None, param, act_func, False)",
      "    tl.store(output_pointer + offset, output, mask=mask)"
    ],
    "file": "codes/17.py",
    "header": "def glu_forward_kernel(input1_pointer, input2_pointer, output_pointer, size, param, act_func: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\noffset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = offset < size\ninput1 = tl.load(input1_pointer + offset, mask=mask)\ninput2 = tl.load(input2_pointer + offset, mask=mask)\noutput = input1 * apply_act_func(input2, None, None, None, param, act_func, False)\ntl.store(output_pointer + offset, output, mask=mask)"
  },
  {
    "name": "glu_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input1_pointer",
        "annotation": null
      },
      {
        "name": "input2_pointer",
        "annotation": null
      },
      {
        "name": "input1_grad_pointer",
        "annotation": null
      },
      {
        "name": "input2_grad_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def glu_backward_kernel(",
      "    output_grad_pointer,",
      "    input1_pointer,",
      "    input2_pointer,",
      "    input1_grad_pointer,",
      "    input2_grad_pointer,",
      "    size,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad = tl.load(output_grad_pointer + offset, mask=mask)",
      "    input1 = tl.load(input1_pointer + offset, mask=mask)",
      "    input2 = tl.load(input2_pointer + offset, mask=mask)",
      "",
      "    input1_grad = output_grad * apply_act_func(",
      "        input2, None, None, None, param, act_func, False",
      "    )",
      "    input2_grad = (",
      "        output_grad",
      "        * input1",
      "        * apply_act_func_grad(1, input2, None, None, None, param, act_func, False)",
      "    )",
      "",
      "    tl.store(input1_grad_pointer + offset, input1_grad, mask=mask)",
      "    tl.store(input2_grad_pointer + offset, input2_grad, mask=mask)"
    ],
    "file": "codes/17.py",
    "header": "def glu_backward_kernel(output_grad_pointer, input1_pointer, input2_pointer, input1_grad_pointer, input2_grad_pointer, size, param, act_func: tl.constexpr, BLOCK_SIZE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\noffset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\nmask = offset < size\noutput_grad = tl.load(output_grad_pointer + offset, mask=mask)\ninput1 = tl.load(input1_pointer + offset, mask=mask)\ninput2 = tl.load(input2_pointer + offset, mask=mask)\ninput1_grad = output_grad * apply_act_func(input2, None, None, None, param, act_func, False)\ninput2_grad = output_grad * input1 * apply_act_func_grad(1, input2, None, None, None, param, act_func, False)\ntl.store(input1_grad_pointer + offset, input1_grad, mask=mask)\ntl.store(input2_grad_pointer + offset, input2_grad, mask=mask)"
  },
  {
    "name": "_forward_pull_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "world_size",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_pull_kernel(",
      "    symm_ptr, bytes_per_rank, symm_flag, world_size, rank, signal_target",
      "):",
      "    pid = tl.program_id(0)",
      "    thread_idx = tid(0)",
      "    if pid == rank:",
      "        if thread_idx != rank and thread_idx < world_size:",
      "            libshmem_device.signal_op(",
      "                symm_flag + rank,",
      "                signal_target,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                thread_idx,",
      "            )",
      "        __syncthreads()",
      "    else:",
      "        peer = pid",
      "        if thread_idx == 0:",
      "            libshmem_device.signal_wait_until(",
      "                symm_flag + peer, libshmem_device.NVSHMEM_CMP_EQ, signal_target",
      "            )",
      "        __syncthreads()",
      "        libshmem_device.getmem_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + peer * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + peer * bytes_per_rank,",
      "            bytes_per_rank,",
      "            peer,",
      "        )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_pull_kernel(symm_ptr, bytes_per_rank, symm_flag, world_size, rank, signal_target):",
    "body": "pid = tl.program_id(0)\nthread_idx = tid(0)\nif pid == rank:\n    if thread_idx != rank and thread_idx < world_size:\n        libshmem_device.signal_op(symm_flag + rank, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, thread_idx)\n    __syncthreads()\nelse:\n    peer = pid\n    if thread_idx == 0:\n        libshmem_device.signal_wait_until(symm_flag + peer, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n    __syncthreads()\n    libshmem_device.getmem_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + peer * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + peer * bytes_per_rank, bytes_per_rank, peer)"
  },
  {
    "name": "_forward_push_numa_2d_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "n_numa_nodes",
        "annotation": "tl.constexpr"
      },
      {
        "name": "world_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_numa_2d_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_flag,",
      "    n_numa_nodes: tl.constexpr,",
      "    world_size: tl.constexpr,",
      "    rank,",
      "    signal_target,",
      "):",
      "    tl.static_assert(n_numa_nodes == 2, \"only support NUMA node == 2\")",
      "    numa_world_size = world_size // n_numa_nodes",
      "    local_rank = rank % numa_world_size",
      "    nid = rank // numa_world_size",
      "",
      "    pid = tl.program_id(0)",
      "    peer_nid = pid // numa_world_size",
      "    peer_local_rank = pid % numa_world_size",
      "    thread_idx = tid(0)",
      "",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "",
      "    if peer_local_rank == local_rank:",
      "        if peer_nid != nid:",
      "            peer_to = peer_nid * numa_world_size + local_rank",
      "            libshmem_device.putmem_signal_block(",
      "                symm_ptr + rank * bytes_per_rank,",
      "                symm_ptr + rank * bytes_per_rank,",
      "                bytes_per_rank,",
      "                symm_flag + rank,",
      "                signal_target,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                peer_to,",
      "            )",
      "        else:",
      "",
      "            if thread_idx < world_size and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "    else:",
      "        peer = nid * numa_world_size + peer_local_rank",
      "        segment = peer_nid * numa_world_size + local_rank",
      "        if peer_nid != nid:",
      "            if thread_idx == 0:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target",
      "                )",
      "            __syncthreads()",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_numa_2d_kernel(symm_ptr, bytes_per_rank, symm_flag, n_numa_nodes: tl.constexpr, world_size: tl.constexpr, rank, signal_target):",
    "body": "tl.static_assert(n_numa_nodes == 2, 'only support NUMA node == 2')\nnuma_world_size = world_size // n_numa_nodes\nlocal_rank = rank % numa_world_size\nnid = rank // numa_world_size\npid = tl.program_id(0)\npeer_nid = pid // numa_world_size\npeer_local_rank = pid % numa_world_size\nthread_idx = tid(0)\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nif peer_local_rank == local_rank:\n    if peer_nid != nid:\n        peer_to = peer_nid * numa_world_size + local_rank\n        libshmem_device.putmem_signal_block(symm_ptr + rank * bytes_per_rank, symm_ptr + rank * bytes_per_rank, bytes_per_rank, symm_flag + rank, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer_to)\n    else:\n        if thread_idx < world_size and thread_idx != rank:\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\nelse:\n    peer = nid * numa_world_size + peer_local_rank\n    segment = peer_nid * numa_world_size + local_rank\n    if peer_nid != nid:\n        if thread_idx == 0:\n            libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "_forward_push_numa_2d_ll_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "symm_ll_buffer",
        "annotation": null
      },
      {
        "name": "n_numa_nodes",
        "annotation": "tl.constexpr"
      },
      {
        "name": "world_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_numa_2d_ll_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_flag,",
      "    symm_ll_buffer,",
      "    n_numa_nodes: tl.constexpr,",
      "    world_size: tl.constexpr,",
      "    rank,",
      "    signal_target,",
      "):",
      "    tl.static_assert(n_numa_nodes == 2, \"only support NUMA node == 2\")",
      "    numa_world_size = world_size // n_numa_nodes",
      "    local_rank = rank % numa_world_size",
      "    nid = rank // numa_world_size",
      "",
      "    pid = tl.program_id(0)",
      "    peer_nid = pid // numa_world_size",
      "    peer_local_rank = pid % numa_world_size",
      "    thread_idx = tid(0)",
      "    num_ints = bytes_per_rank // 4",
      "",
      "    symm_ll_buffer = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "",
      "    if peer_local_rank == local_rank:",
      "        if peer_nid != nid:",
      "            segment = peer_nid * numa_world_size + local_rank",
      "            _recv_ll_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                symm_ll_buffer + segment * bytes_per_rank * 2,",
      "                num_ints,",
      "                signal_target,",
      "            )",
      "            __syncthreads()",
      "            if thread_idx == 0:",
      "                st(symm_flag + segment, signal_target, scope=\"gpu\", semantic=\"release\")",
      "        else:",
      "            _pack_ll_block(",
      "                symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                symm_ptr + rank * bytes_per_rank,",
      "                num_ints,",
      "                signal_target,",
      "                2048,",
      "            )",
      "            __syncthreads()",
      "",
      "            peer_to_nid = 1 - nid",
      "            peer_to = peer_to_nid * numa_world_size + local_rank",
      "            libshmem_device.putmem_block(",
      "                symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                bytes_per_rank * 2,",
      "                peer_to,",
      "            )",
      "",
      "            if thread_idx < world_size and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "",
      "    else:",
      "        peer = nid * numa_world_size + peer_local_rank",
      "        segment = peer_nid * numa_world_size + local_rank",
      "        if peer_nid != nid:",
      "            if thread_idx == 0:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target",
      "                )",
      "            __syncthreads()",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_numa_2d_ll_kernel(symm_ptr, bytes_per_rank, symm_flag, symm_ll_buffer, n_numa_nodes: tl.constexpr, world_size: tl.constexpr, rank, signal_target):",
    "body": "tl.static_assert(n_numa_nodes == 2, 'only support NUMA node == 2')\nnuma_world_size = world_size // n_numa_nodes\nlocal_rank = rank % numa_world_size\nnid = rank // numa_world_size\npid = tl.program_id(0)\npeer_nid = pid // numa_world_size\npeer_local_rank = pid % numa_world_size\nthread_idx = tid(0)\nnum_ints = bytes_per_rank // 4\nsymm_ll_buffer = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nif peer_local_rank == local_rank:\n    if peer_nid != nid:\n        segment = peer_nid * numa_world_size + local_rank\n        _recv_ll_block(symm_ptr + segment * bytes_per_rank, symm_ll_buffer + segment * bytes_per_rank * 2, num_ints, signal_target)\n        __syncthreads()\n        if thread_idx == 0:\n            st(symm_flag + segment, signal_target, scope='gpu', semantic='release')\n    else:\n        _pack_ll_block(symm_ll_buffer + rank * bytes_per_rank * 2, symm_ptr + rank * bytes_per_rank, num_ints, signal_target, 2048)\n        __syncthreads()\n        peer_to_nid = 1 - nid\n        peer_to = peer_to_nid * numa_world_size + local_rank\n        libshmem_device.putmem_block(symm_ll_buffer + rank * bytes_per_rank * 2, symm_ll_buffer + rank * bytes_per_rank * 2, bytes_per_rank * 2, peer_to)\n        if thread_idx < world_size and thread_idx != rank:\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\nelse:\n    peer = nid * numa_world_size + peer_local_rank\n    segment = peer_nid * numa_world_size + local_rank\n    if peer_nid != nid:\n        if thread_idx == 0:\n            libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "_forward_push_numa_2d_ll_multinode_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "symm_ll_buffer",
        "annotation": null
      },
      {
        "name": "nnodes",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_numa_nodes",
        "annotation": "tl.constexpr"
      },
      {
        "name": "world_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_numa_2d_ll_multinode_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_flag,",
      "    symm_ll_buffer,",
      "    nnodes: tl.constexpr,",
      "    n_numa_nodes: tl.constexpr,",
      "    world_size: tl.constexpr,",
      "    rank,",
      "    signal_target,",
      "):",
      "",
      "    tl.static_assert(n_numa_nodes == 2, \"only support NUMA node == 2\")",
      "    local_world_size = world_size // nnodes",
      "    node_id = rank // local_world_size",
      "    local_rank = rank % local_world_size",
      "    numa_world_size = local_world_size // n_numa_nodes",
      "    numa_rank = local_rank % numa_world_size",
      "    local_numa_id = local_rank // numa_world_size",
      "    global_numa_id = rank // numa_world_size",
      "",
      "    pid = tl.program_id(0)",
      "    peer_node_id = pid // local_world_size",
      "    peer_local_rank = pid % local_world_size",
      "    peer_numa_rank = peer_local_rank % numa_world_size",
      "    peer_local_numa_id = peer_local_rank // numa_world_size",
      "    peer_global_numa_id = pid // numa_world_size",
      "    thread_idx = tid(0)",
      "    num_ints = bytes_per_rank // 4",
      "",
      "    symm_ll_buffer = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "",
      "    is_intra_numa = numa_rank != peer_numa_rank",
      "    is_inter_numa = node_id == peer_node_id and (",
      "        local_numa_id != peer_local_numa_id and numa_rank == peer_numa_rank",
      "    )",
      "",
      "    if is_intra_numa and global_numa_id == peer_global_numa_id:",
      "        peer = global_numa_id * numa_world_size + peer_numa_rank",
      "        segment = rank",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )",
      "    elif is_intra_numa and global_numa_id != peer_global_numa_id:",
      "        peer = global_numa_id * numa_world_size + peer_numa_rank",
      "        segment = (",
      "            peer_node_id * local_world_size",
      "            + peer_local_numa_id * numa_world_size",
      "            + numa_rank",
      "        )",
      "",
      "        if thread_idx == 0:",
      "            libshmem_device.signal_wait_until(",
      "                symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target",
      "            )",
      "        __syncthreads()",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )",
      "    elif is_inter_numa:",
      "        peer = (",
      "            node_id * local_world_size",
      "            + peer_local_numa_id * numa_world_size",
      "            + peer_numa_rank",
      "        )",
      "        segment = rank",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )",
      "    else:",
      "        if peer_node_id != node_id:",
      "            segment = peer_global_numa_id * numa_world_size + peer_numa_rank",
      "            _recv_ll_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                symm_ll_buffer + segment * bytes_per_rank * 2,",
      "                num_ints,",
      "                signal_target,",
      "            )",
      "            __syncthreads()",
      "",
      "            if thread_idx == 0:",
      "                st(symm_flag + segment, signal_target, scope=\"gpu\", semantic=\"release\")",
      "        else:",
      "            _pack_ll_block(",
      "                symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                symm_ptr + rank * bytes_per_rank,",
      "                num_ints,",
      "                signal_target,",
      "                2048,",
      "            )",
      "            __syncthreads()",
      "",
      "            for i in range(world_size // numa_world_size):",
      "                if i // n_numa_nodes != node_id:",
      "                    peer_to = numa_rank + i * numa_world_size",
      "                    libshmem_device.putmem_nbi_warp(",
      "                        symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                        symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                        bytes_per_rank * 2,",
      "                        peer_to,",
      "                    )",
      "",
      "            if thread_idx < world_size and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_numa_2d_ll_multinode_kernel(symm_ptr, bytes_per_rank, symm_flag, symm_ll_buffer, nnodes: tl.constexpr, n_numa_nodes: tl.constexpr, world_size: tl.constexpr, rank, signal_target):",
    "body": "tl.static_assert(n_numa_nodes == 2, 'only support NUMA node == 2')\nlocal_world_size = world_size // nnodes\nnode_id = rank // local_world_size\nlocal_rank = rank % local_world_size\nnuma_world_size = local_world_size // n_numa_nodes\nnuma_rank = local_rank % numa_world_size\nlocal_numa_id = local_rank // numa_world_size\nglobal_numa_id = rank // numa_world_size\npid = tl.program_id(0)\npeer_node_id = pid // local_world_size\npeer_local_rank = pid % local_world_size\npeer_numa_rank = peer_local_rank % numa_world_size\npeer_local_numa_id = peer_local_rank // numa_world_size\npeer_global_numa_id = pid // numa_world_size\nthread_idx = tid(0)\nnum_ints = bytes_per_rank // 4\nsymm_ll_buffer = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nis_intra_numa = numa_rank != peer_numa_rank\nis_inter_numa = node_id == peer_node_id and (local_numa_id != peer_local_numa_id and numa_rank == peer_numa_rank)\nif is_intra_numa and global_numa_id == peer_global_numa_id:\n    peer = global_numa_id * numa_world_size + peer_numa_rank\n    segment = rank\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\nelif is_intra_numa and global_numa_id != peer_global_numa_id:\n    peer = global_numa_id * numa_world_size + peer_numa_rank\n    segment = peer_node_id * local_world_size + peer_local_numa_id * numa_world_size + numa_rank\n    if thread_idx == 0:\n        libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n    __syncthreads()\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\nelif is_inter_numa:\n    peer = node_id * local_world_size + peer_local_numa_id * numa_world_size + peer_numa_rank\n    segment = rank\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\nelif peer_node_id != node_id:\n    segment = peer_global_numa_id * numa_world_size + peer_numa_rank\n    _recv_ll_block(symm_ptr + segment * bytes_per_rank, symm_ll_buffer + segment * bytes_per_rank * 2, num_ints, signal_target)\n    __syncthreads()\n    if thread_idx == 0:\n        st(symm_flag + segment, signal_target, scope='gpu', semantic='release')\nelse:\n    _pack_ll_block(symm_ll_buffer + rank * bytes_per_rank * 2, symm_ptr + rank * bytes_per_rank, num_ints, signal_target, 2048)\n    __syncthreads()\n    for i in range(world_size // numa_world_size):\n        if i // n_numa_nodes != node_id:\n            peer_to = numa_rank + i * numa_world_size\n            libshmem_device.putmem_nbi_warp(symm_ll_buffer + rank * bytes_per_rank * 2, symm_ll_buffer + rank * bytes_per_rank * 2, bytes_per_rank * 2, peer_to)\n    if thread_idx < world_size and thread_idx != rank:\n        libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n    __syncthreads()"
  },
  {
    "name": "_forward_push_2d_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "NNODES",
        "annotation": null
      },
      {
        "name": "WORLD_SIZE",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_2d_kernel(",
      "    symm_ptr, bytes_per_rank, symm_flag, NNODES, WORLD_SIZE, rank, signal_target",
      "):",
      "    LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES",
      "    local_rank = rank % LOCAL_WORLD_SIZE",
      "    node_id = rank // LOCAL_WORLD_SIZE",
      "    rank_base = node_id * LOCAL_WORLD_SIZE",
      "",
      "    pid = tl.program_id(0)",
      "    peer_rank = pid",
      "    peer_node_id = peer_rank // LOCAL_WORLD_SIZE",
      "    peer_local_rank = peer_rank % LOCAL_WORLD_SIZE",
      "    thread_idx = tid(0)",
      "    if peer_local_rank == local_rank:",
      "        if peer_rank != rank:",
      "            peer = peer_node_id * LOCAL_WORLD_SIZE + local_rank",
      "            segment = rank",
      "            libshmem_device.putmem_signal_nbi_block(",
      "                tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "                tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "                bytes_per_rank,",
      "                symm_flag + segment,",
      "                signal_target,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                peer,",
      "            )",
      "        else:",
      "            if thread_idx < WORLD_SIZE and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "    else:",
      "        peer = rank_base + peer_local_rank",
      "        segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank",
      "        if peer_node_id != node_id:",
      "            if thread_idx == 0:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + segment,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_2d_kernel(symm_ptr, bytes_per_rank, symm_flag, NNODES, WORLD_SIZE, rank, signal_target):",
    "body": "LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES\nlocal_rank = rank % LOCAL_WORLD_SIZE\nnode_id = rank // LOCAL_WORLD_SIZE\nrank_base = node_id * LOCAL_WORLD_SIZE\npid = tl.program_id(0)\npeer_rank = pid\npeer_node_id = peer_rank // LOCAL_WORLD_SIZE\npeer_local_rank = peer_rank % LOCAL_WORLD_SIZE\nthread_idx = tid(0)\nif peer_local_rank == local_rank:\n    if peer_rank != rank:\n        peer = peer_node_id * LOCAL_WORLD_SIZE + local_rank\n        segment = rank\n        libshmem_device.putmem_signal_nbi_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\n    else:\n        if thread_idx < WORLD_SIZE and thread_idx != rank:\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\nelse:\n    peer = rank_base + peer_local_rank\n    segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank\n    if peer_node_id != node_id:\n        if thread_idx == 0:\n            libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "_forward_push_3d_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_ll_buffer",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "NNODES",
        "annotation": null
      },
      {
        "name": "N_NUMA_NODES",
        "annotation": null
      },
      {
        "name": "WORLD_SIZE",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      },
      {
        "name": "INTER_NODE_WITH_LL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_3d_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_ll_buffer,",
      "    symm_flag,",
      "    NNODES,",
      "    N_NUMA_NODES,",
      "    WORLD_SIZE,",
      "    rank,",
      "    signal_target,",
      "    INTER_NODE_WITH_LL: tl.constexpr = False,",
      "):",
      "",
      "    LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES",
      "    NUMA_WORLD_SIZE = LOCAL_WORLD_SIZE // N_NUMA_NODES",
      "    local_rank = rank % LOCAL_WORLD_SIZE",
      "    node_id = rank // LOCAL_WORLD_SIZE",
      "    numa_rank = local_rank % NUMA_WORLD_SIZE",
      "    local_numa_id = local_rank // NUMA_WORLD_SIZE",
      "",
      "    pid = tl.program_id(0)",
      "    peer_rank = pid",
      "    peer_node_id = peer_rank // LOCAL_WORLD_SIZE",
      "    peer_local_rank = peer_rank % LOCAL_WORLD_SIZE",
      "    peer_numa_rank = peer_local_rank % NUMA_WORLD_SIZE",
      "    peer_local_numa_id = peer_local_rank // NUMA_WORLD_SIZE",
      "",
      "    thread_idx = tid(0)",
      "    num_ints = bytes_per_rank // 4",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "    symm_ll_buffer = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))",
      "    if peer_local_rank == local_rank:",
      "        if peer_node_id != node_id:",
      "            if INTER_NODE_WITH_LL:",
      "                segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank",
      "                _recv_ll_block(",
      "                    symm_ptr + segment * bytes_per_rank,",
      "                    symm_ll_buffer + segment * bytes_per_rank * 2,",
      "                    num_ints,",
      "                    signal_target,",
      "                )",
      "                __syncthreads()",
      "                if thread_idx == 0:",
      "                    st(",
      "                        symm_flag + segment,",
      "                        signal_target,",
      "                        scope=\"gpu\",",
      "                        semantic=\"release\",",
      "                    )",
      "        else:",
      "            wid = thread_idx // 32",
      "            if INTER_NODE_WITH_LL:",
      "                segment = rank",
      "                _pack_ll_block(",
      "                    symm_ll_buffer + rank * bytes_per_rank * 2,",
      "                    symm_ptr + rank * bytes_per_rank,",
      "                    num_ints,",
      "                    signal_target,",
      "                    2048,",
      "                )",
      "                __syncthreads()",
      "",
      "                if wid < NNODES and wid != node_id:",
      "                    peer = wid * LOCAL_WORLD_SIZE + local_rank",
      "                    libshmem_device.putmem_nbi_warp(",
      "                        symm_ll_buffer + segment * bytes_per_rank * 2,",
      "                        symm_ll_buffer + segment * bytes_per_rank * 2,",
      "                        bytes_per_rank * 2,",
      "                        peer,",
      "                    )",
      "            else:",
      "                if wid < NNODES and wid != node_id:",
      "                    peer = wid * LOCAL_WORLD_SIZE + local_rank",
      "                    segment = rank",
      "                    libshmem_device.putmem_signal_nbi_warp(",
      "                        symm_ptr + segment * bytes_per_rank,",
      "                        symm_ptr + segment * bytes_per_rank,",
      "                        bytes_per_rank,",
      "                        symm_flag + segment,",
      "                        signal_target,",
      "                        libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                        peer,",
      "                    )",
      "",
      "            __syncthreads()",
      "            if thread_idx < WORLD_SIZE and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "    else:",
      "",
      "        if NNODES > 1:",
      "            if thread_idx < WORLD_SIZE and (",
      "                thread_idx % LOCAL_WORLD_SIZE == local_rank and thread_idx != rank",
      "            ):",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "",
      "        if peer_numa_rank == numa_rank:",
      "            peer = (",
      "                node_id * LOCAL_WORLD_SIZE",
      "                + peer_local_numa_id * NUMA_WORLD_SIZE",
      "                + numa_rank",
      "            )",
      "            segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank",
      "            libshmem_device.putmem_signal_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                symm_ptr + segment * bytes_per_rank,",
      "                bytes_per_rank,",
      "                symm_flag + segment,",
      "                signal_target,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                peer,",
      "            )",
      "        else:",
      "            peer = (",
      "                node_id * LOCAL_WORLD_SIZE",
      "                + local_numa_id * NUMA_WORLD_SIZE",
      "                + peer_numa_rank",
      "            )",
      "            segment = (",
      "                peer_node_id * LOCAL_WORLD_SIZE",
      "                + peer_local_numa_id * NUMA_WORLD_SIZE",
      "                + numa_rank",
      "            )",
      "",
      "            if peer_local_numa_id != local_numa_id:",
      "                if thread_idx == 0:",
      "                    libshmem_device.signal_wait_until(",
      "                        symm_flag + segment,",
      "                        libshmem_device.NVSHMEM_CMP_EQ,",
      "                        signal_target,",
      "                    )",
      "                __syncthreads()",
      "",
      "            libshmem_device.putmem_signal_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                symm_ptr + segment * bytes_per_rank,",
      "                bytes_per_rank,",
      "                symm_flag + segment,",
      "                signal_target,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                peer,",
      "            )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_3d_kernel(symm_ptr, bytes_per_rank, symm_ll_buffer, symm_flag, NNODES, N_NUMA_NODES, WORLD_SIZE, rank, signal_target, INTER_NODE_WITH_LL: tl.constexpr = False):",
    "body": "LOCAL_WORLD_SIZE = WORLD_SIZE // NNODES\nNUMA_WORLD_SIZE = LOCAL_WORLD_SIZE // N_NUMA_NODES\nlocal_rank = rank % LOCAL_WORLD_SIZE\nnode_id = rank // LOCAL_WORLD_SIZE\nnuma_rank = local_rank % NUMA_WORLD_SIZE\nlocal_numa_id = local_rank // NUMA_WORLD_SIZE\npid = tl.program_id(0)\npeer_rank = pid\npeer_node_id = peer_rank // LOCAL_WORLD_SIZE\npeer_local_rank = peer_rank % LOCAL_WORLD_SIZE\npeer_numa_rank = peer_local_rank % NUMA_WORLD_SIZE\npeer_local_numa_id = peer_local_rank // NUMA_WORLD_SIZE\nthread_idx = tid(0)\nnum_ints = bytes_per_rank // 4\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nsymm_ll_buffer = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))\nif peer_local_rank == local_rank:\n    if peer_node_id != node_id:\n        if INTER_NODE_WITH_LL:\n            segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank\n            _recv_ll_block(symm_ptr + segment * bytes_per_rank, symm_ll_buffer + segment * bytes_per_rank * 2, num_ints, signal_target)\n            __syncthreads()\n            if thread_idx == 0:\n                st(symm_flag + segment, signal_target, scope='gpu', semantic='release')\n    else:\n        wid = thread_idx // 32\n        if INTER_NODE_WITH_LL:\n            segment = rank\n            _pack_ll_block(symm_ll_buffer + rank * bytes_per_rank * 2, symm_ptr + rank * bytes_per_rank, num_ints, signal_target, 2048)\n            __syncthreads()\n            if wid < NNODES and wid != node_id:\n                peer = wid * LOCAL_WORLD_SIZE + local_rank\n                libshmem_device.putmem_nbi_warp(symm_ll_buffer + segment * bytes_per_rank * 2, symm_ll_buffer + segment * bytes_per_rank * 2, bytes_per_rank * 2, peer)\n        elif wid < NNODES and wid != node_id:\n            peer = wid * LOCAL_WORLD_SIZE + local_rank\n            segment = rank\n            libshmem_device.putmem_signal_nbi_warp(symm_ptr + segment * bytes_per_rank, symm_ptr + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\n        __syncthreads()\n        if thread_idx < WORLD_SIZE and thread_idx != rank:\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\nelse:\n    if NNODES > 1:\n        if thread_idx < WORLD_SIZE and (thread_idx % LOCAL_WORLD_SIZE == local_rank and thread_idx != rank):\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\n    if peer_numa_rank == numa_rank:\n        peer = node_id * LOCAL_WORLD_SIZE + peer_local_numa_id * NUMA_WORLD_SIZE + numa_rank\n        segment = peer_node_id * LOCAL_WORLD_SIZE + local_rank\n        libshmem_device.putmem_signal_block(symm_ptr + segment * bytes_per_rank, symm_ptr + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\n    else:\n        peer = node_id * LOCAL_WORLD_SIZE + local_numa_id * NUMA_WORLD_SIZE + peer_numa_rank\n        segment = peer_node_id * LOCAL_WORLD_SIZE + peer_local_numa_id * NUMA_WORLD_SIZE + numa_rank\n        if peer_local_numa_id != local_numa_id:\n            if thread_idx == 0:\n                libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n            __syncthreads()\n        libshmem_device.putmem_signal_block(symm_ptr + segment * bytes_per_rank, symm_ptr + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "_pack_ll_block",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['ll_flag'])"
    ],
    "args": [
      {
        "name": "dest_ptr",
        "annotation": null
      },
      {
        "name": "src_ptr",
        "annotation": null
      },
      {
        "name": "num_ints",
        "annotation": null
      },
      {
        "name": "ll_flag",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _pack_ll_block(dest_ptr, src_ptr, num_ints, ll_flag, BLOCK_SIZE: tl.constexpr):",
      "",
      "    iters = tl.cdiv(num_ints, BLOCK_SIZE)",
      "    src_ptr = tl.cast(src_ptr, dtype=tl.pi32_t)",
      "    dest_ptr = tl.cast(dest_ptr, dtype=tl.pi32_t)",
      "    for n in range(iters):",
      "        src_offsets = n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "        src_mask = src_offsets < num_ints",
      "        src = tl.load(src_ptr + src_offsets, mask=src_mask)",
      "        flags = tl.full((BLOCK_SIZE,), ll_flag, tl.int32)",
      "        dst = tl.interleave(src, flags)",
      "        dest_offset = n * BLOCK_SIZE * 2 + tl.arange(0, BLOCK_SIZE * 2)",
      "        dest_mask = dest_offset < num_ints * 2",
      "        tl.store(dest_ptr + dest_offset, dst, mask=dest_mask)"
    ],
    "file": "codes/46.py",
    "header": "def _pack_ll_block(dest_ptr, src_ptr, num_ints, ll_flag, BLOCK_SIZE: tl.constexpr):",
    "body": "iters = tl.cdiv(num_ints, BLOCK_SIZE)\nsrc_ptr = tl.cast(src_ptr, dtype=tl.pi32_t)\ndest_ptr = tl.cast(dest_ptr, dtype=tl.pi32_t)\nfor n in range(iters):\n    src_offsets = n * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n    src_mask = src_offsets < num_ints\n    src = tl.load(src_ptr + src_offsets, mask=src_mask)\n    flags = tl.full((BLOCK_SIZE,), ll_flag, tl.int32)\n    dst = tl.interleave(src, flags)\n    dest_offset = n * BLOCK_SIZE * 2 + tl.arange(0, BLOCK_SIZE * 2)\n    dest_mask = dest_offset < num_ints * 2\n    tl.store(dest_ptr + dest_offset, dst, mask=dest_mask)"
  },
  {
    "name": "_recv_ll_and_multimem_st_ll_block",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['ll_flag'])"
    ],
    "args": [
      {
        "name": "dest_ptr",
        "annotation": null
      },
      {
        "name": "src_ptr",
        "annotation": null
      },
      {
        "name": "num_ints",
        "annotation": null
      },
      {
        "name": "ll_flag",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _recv_ll_and_multimem_st_ll_block(dest_ptr, src_ptr, num_ints, ll_flag):",
      "",
      "    thread_idx = tid(0)",
      "    block_size = ntid(0)",
      "    src_ptr = tl.cast(src_ptr, tl.pointer_type(tl.int32))",
      "    dest_ptr = tl.cast(dest_ptr, tl.pointer_type(tl.int32))",
      "    dest_mc_ptr = libshmem_device.remote_mc_ptr(",
      "        libshmem_device.NVSHMEMX_TEAM_NODE, dest_ptr",
      "    )",
      "",
      "    for n in range(thread_idx, num_ints // 2, block_size):",
      "        data1, flag1, data2, flag2 = load_v4_u32(src_ptr + n * 4)",
      "        while flag1 != ll_flag or flag2 != ll_flag:",
      "            data1, flag1, data2, flag2 = load_v4_u32(src_ptr + n * 4)",
      "        multimem_st_v2_b32(dest_mc_ptr + n * 4, data1, flag1)",
      "        multimem_st_v2_b32(dest_mc_ptr + n * 4 + 2, data2, flag2)"
    ],
    "file": "codes/46.py",
    "header": "def _recv_ll_and_multimem_st_ll_block(dest_ptr, src_ptr, num_ints, ll_flag):",
    "body": "thread_idx = tid(0)\nblock_size = ntid(0)\nsrc_ptr = tl.cast(src_ptr, tl.pointer_type(tl.int32))\ndest_ptr = tl.cast(dest_ptr, tl.pointer_type(tl.int32))\ndest_mc_ptr = libshmem_device.remote_mc_ptr(libshmem_device.NVSHMEMX_TEAM_NODE, dest_ptr)\nfor n in range(thread_idx, num_ints // 2, block_size):\n    data1, flag1, data2, flag2 = load_v4_u32(src_ptr + n * 4)\n    while flag1 != ll_flag or flag2 != ll_flag:\n        data1, flag1, data2, flag2 = load_v4_u32(src_ptr + n * 4)\n    multimem_st_v2_b32(dest_mc_ptr + n * 4, data1, flag1)\n    multimem_st_v2_b32(dest_mc_ptr + n * 4 + 2, data2, flag2)"
  },
  {
    "name": "_forward_push_2d_ll_multimem_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_ll_buffer",
        "annotation": null
      },
      {
        "name": "nnodes",
        "annotation": "tl.constexpr"
      },
      {
        "name": "world_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_2d_ll_multimem_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_ll_buffer,",
      "    nnodes: tl.constexpr,",
      "    world_size: tl.constexpr,",
      "    rank,",
      "    signal_target,",
      "):",
      "",
      "    local_world_size = world_size // nnodes",
      "    local_rank = rank % local_world_size",
      "    nid = rank // local_world_size",
      "",
      "    pid = tl.program_id(0)",
      "    peer_nid = pid // local_world_size",
      "    peer_local_rank = pid % local_world_size",
      "    num_ints = bytes_per_rank // 4",
      "    thread_idx = tid(axis=0)",
      "",
      "    ll_buffer_int8 = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "",
      "    if peer_local_rank == local_rank:",
      "        if nid != peer_nid:",
      "            segment = peer_nid * local_world_size + local_rank",
      "            _recv_ll_and_multimem_st_ll_block(",
      "                ll_buffer_int8 + segment * bytes_per_rank * 2,",
      "                ll_buffer_int8 + segment * bytes_per_rank * 2,",
      "                num_ints,",
      "                signal_target,",
      "            )",
      "            _recv_ll_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                ll_buffer_int8 + segment * bytes_per_rank * 2,",
      "                num_ints,",
      "                signal_target,",
      "            )",
      "        else:",
      "            _pack_ll_block(",
      "                ll_buffer_int8 + rank * bytes_per_rank * 2,",
      "                symm_ptr + rank * bytes_per_rank,",
      "                num_ints,",
      "                signal_target,",
      "                2048,",
      "            )",
      "            __syncthreads()",
      "            wid = thread_idx // 32",
      "",
      "            if wid < nnodes and wid != nid:",
      "                peer_to = wid * local_world_size + local_rank",
      "                libshmem_device.putmem_nbi_warp(",
      "                    ll_buffer_int8 + rank * bytes_per_rank * 2,",
      "                    ll_buffer_int8 + rank * bytes_per_rank * 2,",
      "                    bytes_per_rank * 2,",
      "                    peer_to,",
      "                )",
      "",
      "            segment = peer_nid * local_world_size + local_rank",
      "            broadcast_naive_block(",
      "                ll_buffer_int8 + segment * bytes_per_rank * 2,",
      "                ll_buffer_int8 + segment * bytes_per_rank * 2,",
      "                bytes_per_rank * 2,",
      "            )",
      "    else:",
      "        segment_recv_local = peer_nid * local_world_size + peer_local_rank",
      "        _recv_ll_block(",
      "            symm_ptr + segment_recv_local * bytes_per_rank,",
      "            ll_buffer_int8 + segment_recv_local * bytes_per_rank * 2,",
      "            num_ints,",
      "            signal_target,",
      "        )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_2d_ll_multimem_kernel(symm_ptr, bytes_per_rank, symm_ll_buffer, nnodes: tl.constexpr, world_size: tl.constexpr, rank, signal_target):",
    "body": "local_world_size = world_size // nnodes\nlocal_rank = rank % local_world_size\nnid = rank // local_world_size\npid = tl.program_id(0)\npeer_nid = pid // local_world_size\npeer_local_rank = pid % local_world_size\nnum_ints = bytes_per_rank // 4\nthread_idx = tid(axis=0)\nll_buffer_int8 = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nif peer_local_rank == local_rank:\n    if nid != peer_nid:\n        segment = peer_nid * local_world_size + local_rank\n        _recv_ll_and_multimem_st_ll_block(ll_buffer_int8 + segment * bytes_per_rank * 2, ll_buffer_int8 + segment * bytes_per_rank * 2, num_ints, signal_target)\n        _recv_ll_block(symm_ptr + segment * bytes_per_rank, ll_buffer_int8 + segment * bytes_per_rank * 2, num_ints, signal_target)\n    else:\n        _pack_ll_block(ll_buffer_int8 + rank * bytes_per_rank * 2, symm_ptr + rank * bytes_per_rank, num_ints, signal_target, 2048)\n        __syncthreads()\n        wid = thread_idx // 32\n        if wid < nnodes and wid != nid:\n            peer_to = wid * local_world_size + local_rank\n            libshmem_device.putmem_nbi_warp(ll_buffer_int8 + rank * bytes_per_rank * 2, ll_buffer_int8 + rank * bytes_per_rank * 2, bytes_per_rank * 2, peer_to)\n        segment = peer_nid * local_world_size + local_rank\n        broadcast_naive_block(ll_buffer_int8 + segment * bytes_per_rank * 2, ll_buffer_int8 + segment * bytes_per_rank * 2, bytes_per_rank * 2)\nelse:\n    segment_recv_local = peer_nid * local_world_size + peer_local_rank\n    _recv_ll_block(symm_ptr + segment_recv_local * bytes_per_rank, ll_buffer_int8 + segment_recv_local * bytes_per_rank * 2, num_ints, signal_target)"
  },
  {
    "name": "_forward_push_2d_ll_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'signal_target'])"
    ],
    "args": [
      {
        "name": "symm_ptr",
        "annotation": null
      },
      {
        "name": "bytes_per_rank",
        "annotation": null
      },
      {
        "name": "symm_flag",
        "annotation": null
      },
      {
        "name": "symm_ll_buffer",
        "annotation": null
      },
      {
        "name": "nnodes",
        "annotation": "tl.constexpr"
      },
      {
        "name": "world_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def _forward_push_2d_ll_kernel(",
      "    symm_ptr,",
      "    bytes_per_rank,",
      "    symm_flag,",
      "    symm_ll_buffer,",
      "    nnodes: tl.constexpr,",
      "    world_size: tl.constexpr,",
      "    rank,",
      "    signal_target,",
      "):",
      "    local_world_size = world_size // nnodes",
      "    local_rank = rank % local_world_size",
      "    nid = rank // local_world_size",
      "",
      "    pid = tl.program_id(0)",
      "    peer_nid = pid // local_world_size",
      "    peer_local_rank = pid % local_world_size",
      "    thread_idx = tid(0)",
      "    num_ints = bytes_per_rank // 4",
      "",
      "    ll_buffer_int8 = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))",
      "    symm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))",
      "",
      "    if peer_local_rank == local_rank:",
      "        if peer_nid != nid:",
      "            segment = peer_nid * local_world_size + local_rank",
      "            _recv_ll_block(",
      "                symm_ptr + segment * bytes_per_rank,",
      "                ll_buffer_int8 + segment * bytes_per_rank * 2,",
      "                num_ints,",
      "                signal_target,",
      "            )",
      "            __syncthreads()",
      "            if thread_idx == 0:",
      "                st(symm_flag + segment, signal_target, scope=\"gpu\", semantic=\"release\")",
      "        else:",
      "            _pack_ll_block(",
      "                ll_buffer_int8 + rank * bytes_per_rank * 2,",
      "                symm_ptr + rank * bytes_per_rank,",
      "                num_ints,",
      "                signal_target,",
      "                2048,",
      "            )",
      "            __syncthreads()",
      "            wid = thread_idx // 32",
      "            if wid < nnodes and wid != nid:",
      "                peer_to = wid * local_world_size + local_rank",
      "                libshmem_device.putmem_nbi_warp(",
      "                    ll_buffer_int8 + rank * bytes_per_rank * 2,",
      "                    ll_buffer_int8 + rank * bytes_per_rank * 2,",
      "                    bytes_per_rank * 2,",
      "                    peer_to,",
      "                )",
      "",
      "            if thread_idx < world_size and thread_idx != rank:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + thread_idx,",
      "                    libshmem_device.NVSHMEM_CMP_EQ,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "",
      "    else:",
      "        peer = nid * local_world_size + peer_local_rank",
      "        segment = peer_nid * local_world_size + local_rank",
      "        if peer_nid != nid:",
      "            if thread_idx == 0:",
      "                libshmem_device.signal_wait_until(",
      "                    symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target",
      "                )",
      "            __syncthreads()",
      "        libshmem_device.putmem_signal_block(",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank,",
      "            bytes_per_rank,",
      "            symm_flag + segment,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/46.py",
    "header": "def _forward_push_2d_ll_kernel(symm_ptr, bytes_per_rank, symm_flag, symm_ll_buffer, nnodes: tl.constexpr, world_size: tl.constexpr, rank, signal_target):",
    "body": "local_world_size = world_size // nnodes\nlocal_rank = rank % local_world_size\nnid = rank // local_world_size\npid = tl.program_id(0)\npeer_nid = pid // local_world_size\npeer_local_rank = pid % local_world_size\nthread_idx = tid(0)\nnum_ints = bytes_per_rank // 4\nll_buffer_int8 = tl.cast(symm_ll_buffer, tl.pointer_type(tl.int8))\nsymm_ptr = tl.cast(symm_ptr, tl.pointer_type(tl.int8))\nif peer_local_rank == local_rank:\n    if peer_nid != nid:\n        segment = peer_nid * local_world_size + local_rank\n        _recv_ll_block(symm_ptr + segment * bytes_per_rank, ll_buffer_int8 + segment * bytes_per_rank * 2, num_ints, signal_target)\n        __syncthreads()\n        if thread_idx == 0:\n            st(symm_flag + segment, signal_target, scope='gpu', semantic='release')\n    else:\n        _pack_ll_block(ll_buffer_int8 + rank * bytes_per_rank * 2, symm_ptr + rank * bytes_per_rank, num_ints, signal_target, 2048)\n        __syncthreads()\n        wid = thread_idx // 32\n        if wid < nnodes and wid != nid:\n            peer_to = wid * local_world_size + local_rank\n            libshmem_device.putmem_nbi_warp(ll_buffer_int8 + rank * bytes_per_rank * 2, ll_buffer_int8 + rank * bytes_per_rank * 2, bytes_per_rank * 2, peer_to)\n        if thread_idx < world_size and thread_idx != rank:\n            libshmem_device.signal_wait_until(symm_flag + thread_idx, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\nelse:\n    peer = nid * local_world_size + peer_local_rank\n    segment = peer_nid * local_world_size + local_rank\n    if peer_nid != nid:\n        if thread_idx == 0:\n            libshmem_device.signal_wait_until(symm_flag + segment, libshmem_device.NVSHMEM_CMP_EQ, signal_target)\n        __syncthreads()\n    libshmem_device.putmem_signal_block(tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, tl.cast(symm_ptr, tl.pointer_type(tl.int8)) + segment * bytes_per_rank, bytes_per_rank, symm_flag + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "fused_chunk_ttt_linear_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "hbt",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_ttt_linear_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    eta,",
      "    w,",
      "    b,",
      "    o,",
      "    scale,",
      "    eps,",
      "    h0,",
      "    hb0,",
      "    ht,",
      "    hbt,",
      "    cu_seqlens,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    v_i = tl.arange(0, BV)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_e = tl.make_block_ptr(",
      "            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_e_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")",
      "        b_q = (b_q * scale).to(b_k.dtype)",
      "",
      "        b_A = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_A = tl.where(m_A, b_A, 0)",
      "        b_Ae = tl.where(m_A, b_e[:, None], 0.0)",
      "",
      "        b_o = -tl.dot(b_e[:, None] * b_A.to(b_v2.dtype), b_v2, allow_tf32=False)",
      "        b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v2.dtype), b_v2, allow_tf32=False)",
      "        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)",
      "        b_e_last = tl.load(p_e_last)",
      "        b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)",
      "        b_h = tl.where((v_i < V)[None, :], b_h, 0.0)",
      "        b_hb = tl.where((v_i < V), b_hb, 0.0)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/426.py",
    "header": "def fused_chunk_ttt_linear_fwd_kernel(q, k, v, eta, w, b, o, scale, eps, h0, hb0, ht, hbt, cu_seqlens, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_INITIAL_STATE_B: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_nh = tl.program_id(0)\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\no_i = tl.arange(0, BT)\nv_i = tl.arange(0, BV)\nm_A = o_i[:, None] >= o_i[None, :]\nb_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)\nb_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nb_hb = tl.zeros([BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option='zero').to(tl.float32)\nif USE_INITIAL_STATE_B:\n    p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n    b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option='zero').to(tl.float32)\nfor i_t in range(NT):\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_o = tl.make_block_ptr(o + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_e = tl.make_block_ptr(eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_e_last = eta + bos * H + i_h + (T - 1) * H if i_t == NT - 1 else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\n    b_kh = tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32) + b_hb[None, :]\n    b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)\n    mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n    xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n    rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n    b_kh_hat = (b_kh - mean) * rstd\n    b_v = b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype) + b_b[None, :].to(b_k.dtype) - b_v.to(b_k.dtype) + tl.trans(b_k)\n    b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n    b_v2 = rstd * (V * b_v - tl.sum(b_v, axis=1, keep_dims=True) - b_kh_hat.to(b_k.dtype) * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)) / V\n    b_q = tl.load(p_q, boundary_check=(0, 1), padding_option='zero')\n    b_e = tl.load(p_e, boundary_check=(0,), padding_option='zero')\n    b_q = (b_q * scale).to(b_k.dtype)\n    b_A = tl.dot(b_q, b_k, allow_tf32=False)\n    b_A = tl.where(m_A, b_A, 0)\n    b_Ae = tl.where(m_A, b_e[:, None], 0.0)\n    b_o = -tl.dot(b_e[:, None] * b_A.to(b_v2.dtype), b_v2, allow_tf32=False)\n    b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v2.dtype), b_v2, allow_tf32=False)\n    b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)\n    b_e_last = tl.load(p_e_last)\n    b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n    b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)\n    b_h = tl.where((v_i < V)[None, :], b_h, 0.0)\n    b_hb = tl.where(v_i < V, b_hb, 0.0)\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0))\n    p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "fused_chunk_ttt_linear_bwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v2",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_ttt_linear_bwd_kernel_h(",
      "    k,",
      "    v,",
      "    v2,",
      "    x,",
      "    y,",
      "    r,",
      "    w,",
      "    b,",
      "    eta,",
      "    h0,",
      "    hb0,",
      "    h,",
      "    do,",
      "    dq,",
      "    scale,",
      "    eps,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    bos, _ = i_n * T, i_n * T + T",
      "    NT = tl.cdiv(T, BT)",
      "    boh = i_n * NT",
      "",
      "    o_i = tl.arange(0, BT)",
      "    v_i = tl.arange(0, BV)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (0, 0),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_v2 = tl.make_block_ptr(",
      "            v2 + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_e = tl.make_block_ptr(",
      "            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_e_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_v2, b_v2.to(p_v2.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")",
      "        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_v2 = tl.where((v_i < V)[None, :], b_v2, 0.0)",
      "        b_ds = tl.dot(b_do, tl.trans(b_v2).to(b_do.dtype))",
      "        b_ds = tl.where(m_A, b_ds, 0)",
      "        b_ds = b_ds.to(b_k.dtype)",
      "        b_dq = tl.dot(b_do, tl.trans(b_h).to(b_do.dtype))",
      "        b_dq -= tl.dot(b_ds, tl.trans(b_k)) * b_e[:, None]",
      "        b_dq *= scale",
      "",
      "        b_e_last = tl.load(p_e_last)",
      "        b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)",
      "        b_h = tl.where((v_i < V)[None, :], b_h, 0.0)",
      "        b_hb = tl.where((v_i < V), b_hb, 0.0)",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/426.py",
    "header": "def fused_chunk_ttt_linear_bwd_kernel_h(k, v, v2, x, y, r, w, b, eta, h0, hb0, h, do, dq, scale, eps, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_INITIAL_STATE_B: tl.constexpr):",
    "body": "i_nh = tl.program_id(0)\ni_n, i_h = (i_nh // H, i_nh % H)\nbos, _ = (i_n * T, i_n * T + T)\nNT = tl.cdiv(T, BT)\nboh = i_n * NT\no_i = tl.arange(0, BT)\nv_i = tl.arange(0, BV)\nm_A = o_i[:, None] >= o_i[None, :]\nb_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)\nb_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nb_hb = tl.zeros([BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option='zero').to(tl.float32)\nif USE_INITIAL_STATE_B:\n    p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n    b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option='zero').to(tl.float32)\nfor i_t in range(NT):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_v2 = tl.make_block_ptr(v2 + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_x = tl.make_block_ptr(x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_y = tl.make_block_ptr(y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_r = tl.make_block_ptr(r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0))\n    p_e = tl.make_block_ptr(eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_dq = tl.make_block_ptr(dq + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_e_last = eta + bos * H + i_h + (T - 1) * H if i_t == NT - 1 else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\n    b_kh = tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32) + b_hb[None, :]\n    b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)\n    mean = tl.sum(b_kh, axis=1, keep_dims=True) / V\n    xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)\n    var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V\n    rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)\n    b_kh_hat = (b_kh - mean) * rstd\n    b_v = b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype) + b_b[None, :].to(b_k.dtype) - b_v.to(b_k.dtype) + tl.trans(b_k)\n    b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)\n    b_v2 = rstd * (V * b_v - tl.sum(b_v, axis=1, keep_dims=True) - b_kh_hat.to(b_k.dtype) * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)) / V\n    tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_v2, b_v2.to(p_v2.dtype.element_ty), boundary_check=(0, 1))\n    b_e = tl.load(p_e, boundary_check=(0,), padding_option='zero')\n    b_do = tl.load(p_do, boundary_check=(0, 1), padding_option='zero')\n    b_v2 = tl.where((v_i < V)[None, :], b_v2, 0.0)\n    b_ds = tl.dot(b_do, tl.trans(b_v2).to(b_do.dtype))\n    b_ds = tl.where(m_A, b_ds, 0)\n    b_ds = b_ds.to(b_k.dtype)\n    b_dq = tl.dot(b_do, tl.trans(b_h).to(b_do.dtype))\n    b_dq -= tl.dot(b_ds, tl.trans(b_k)) * b_e[:, None]\n    b_dq *= scale\n    b_e_last = tl.load(p_e_last)\n    b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)\n    b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)\n    b_h = tl.where((v_i < V)[None, :], b_h, 0.0)\n    b_hb = tl.where(v_i < V, b_hb, 0.0)\n    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "fused_chunk_ttt_linear_bwd_kernel_dh",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['dhb0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'USE_FINAL_STATE_GRADIENT_B': lambda args: args['dhbt'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in NUM_WARPS], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v2",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dhbt",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "dhb0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "de",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT_B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_ttt_linear_bwd_kernel_dh(",
      "    q,",
      "    k,",
      "    v,",
      "    v2,",
      "    x,",
      "    y,",
      "    r,",
      "    w,",
      "    b,",
      "    eta,",
      "    h,",
      "    dht,",
      "    dhbt,",
      "    dh0,",
      "    dhb0,",
      "    do,",
      "    dk,",
      "    dv,",
      "    de,",
      "    dw,",
      "    db,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT_B: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    bos, _ = i_n * T, i_n * T + T",
      "    NT = tl.cdiv(T, BT)",
      "    boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_dhb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option=\"zero\")",
      "    if USE_FINAL_STATE_GRADIENT_B:",
      "        p_dhbt = tl.make_block_ptr(dhbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    o_i = tl.arange(0, BT)",
      "    v_i = tl.arange(0, BV)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    m_A_t = o_i[:, None] <= o_i[None, :]",
      "    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_dw = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_w.dtype,",
      "    )",
      "    b_db = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_b.dtype,",
      "    )",
      "    p_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "    p_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (0, 0),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_v2 = tl.make_block_ptr(",
      "            v2 + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_e = tl.make_block_ptr(",
      "            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_de = tl.make_block_ptr(",
      "            de + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_e_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")",
      "        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_e_last = tl.load(p_e_last)",
      "        b_A = tl.dot(b_k, b_q)",
      "        b_A = -tl.where(m_A_t, b_A * scale * b_e[None, :], 0).to(do.dtype.element_ty)",
      "        b_Ae = -tl.where(m_A_t, b_e[None, :], 0).to(do.dtype.element_ty)",
      "        b_dv_new = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)",
      "        b_dv_new -= tl.dot(b_e_last * b_k, b_dh.to(b_k.dtype))",
      "        b_dv_new -= b_e_last * b_dhb.to(b_k.dtype)[None, :]",
      "",
      "        b_v2 = tl.load(p_v2, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_x = tl.load(p_x, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_y = tl.load(p_y, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option=\"zero\").to(",
      "            tl.float32",
      "        )",
      "        b_dy = (",
      "            b_rstd",
      "            * (",
      "                b_dv_new * V",
      "                - tl.sum(b_dv_new, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dx = (",
      "            -b_rstd",
      "            * (",
      "                b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True)",
      "                + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_drstd = tl.sum(",
      "            b_dv_new.to(b_rstd.dtype) * b_v2.to(b_rstd.dtype) / b_rstd,",
      "            axis=1,",
      "            keep_dims=True,",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_w = b_w.to(b_k.dtype)",
      "        b_b = b_b.to(b_k.dtype)",
      "        b_dv = -b_w * b_dy.to(b_k.dtype)",
      "        b_dk = b_w * b_dy.to(b_k.dtype)",
      "        b_dw += tl.sum(",
      "            2 * b_w * b_x * b_dy.to(b_k.dtype)",
      "            + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype),",
      "            axis=0,",
      "        ).to(b_dw.dtype)",
      "        b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)",
      "        b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_dkh = (",
      "            b_rstd",
      "            * (",
      "                V * b_dx",
      "                - tl.sum(b_dx, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V",
      "        b_dkh = tl.where((v_i < V)[None, :] * (o_i < T - i_t * BT)[:, None], b_dkh, 0.0)",
      "        b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)",
      "",
      "        b_ds = tl.dot(b_do, tl.trans(b_v2))",
      "        b_ds = tl.where(m_A, b_ds, 0)",
      "        b_ds = b_ds.to(b_k.dtype)",
      "        i_last = (BT - 1) if (i_t * BT + BT) <= T else (T % BT - 1)",
      "        mask = o_i == i_last",
      "        b_dk -= b_e_last * tl.dot(b_v2, tl.trans(b_dh).to(b_v2.dtype))",
      "        b_dk -= tl.dot(tl.trans(b_ds), tl.trans(b_q) * b_e[:, None])",
      "        b_de = mask * tl.sum(-b_dh * tl.trans(tl.dot(tl.trans(b_v2), b_k))).to(",
      "            b_k.dtype",
      "        )",
      "        b_de -= mask * tl.sum(b_dhb * tl.sum(b_v2, axis=0)).to(b_k.dtype)",
      "        b_de -= tl.sum(tl.dot(b_ds, b_k) * tl.trans(b_q).to(b_k.dtype), axis=1)",
      "        b_de -= tl.sum(b_ds, axis=1)",
      "        b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(",
      "            tl.trans(b_k).to(b_dkh.dtype), b_dkh",
      "        )",
      "        b_dhb += tl.sum(b_do + b_dkh, axis=0)",
      "        b_dh = tl.where((v_i < V)[None, :], b_dh, 0.0)",
      "        b_dhb = tl.where((v_i < V), b_dhb, 0.0)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_INITIAL_STATE_B:",
      "        p_dhb0 = tl.make_block_ptr(dhb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/426.py",
    "header": "def fused_chunk_ttt_linear_bwd_kernel_dh(q, k, v, v2, x, y, r, w, b, eta, h, dht, dhbt, dh0, dhb0, do, dk, dv, de, dw, db, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_INITIAL_STATE_B: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT_B: tl.constexpr):",
    "body": "i_nh = tl.program_id(0)\ni_n, i_h = (i_nh // H, i_nh % H)\nbos, _ = (i_n * T, i_n * T + T)\nNT = tl.cdiv(T, BT)\nboh = i_n * NT\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nb_dhb = tl.zeros([BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option='zero')\nif USE_FINAL_STATE_GRADIENT_B:\n    p_dhbt = tl.make_block_ptr(dhbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n    b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option='zero')\no_i = tl.arange(0, BT)\nv_i = tl.arange(0, BV)\nm_A = o_i[:, None] >= o_i[None, :]\nm_A_t = o_i[:, None] <= o_i[None, :]\nb_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)\nb_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)\nb_dw = tl.zeros([BV], dtype=b_w.dtype)\nb_db = tl.zeros([BV], dtype=b_b.dtype)\np_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\np_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\nfor i_t in range(NT - 1, -1, -1):\n    p_h = tl.make_block_ptr(h + ((boh + i_t) * H + i_h) * K * V, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_v2 = tl.make_block_ptr(v2 + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_x = tl.make_block_ptr(x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_y = tl.make_block_ptr(y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_r = tl.make_block_ptr(r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0))\n    p_e = tl.make_block_ptr(eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\n    p_de = tl.make_block_ptr(de + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_e_last = eta + bos * H + i_h + (T - 1) * H if i_t == NT - 1 else eta + bos * H + i_h + (i_t * BT + BT - 1) * H\n    b_q = tl.load(p_q, boundary_check=(0, 1), padding_option='zero')\n    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option='zero')\n    b_e = tl.load(p_e, boundary_check=(0,), padding_option='zero')\n    b_do = tl.load(p_do, boundary_check=(0, 1), padding_option='zero')\n    b_e_last = tl.load(p_e_last)\n    b_A = tl.dot(b_k, b_q)\n    b_A = -tl.where(m_A_t, b_A * scale * b_e[None, :], 0).to(do.dtype.element_ty)\n    b_Ae = -tl.where(m_A_t, b_e[None, :], 0).to(do.dtype.element_ty)\n    b_dv_new = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)\n    b_dv_new -= tl.dot(b_e_last * b_k, b_dh.to(b_k.dtype))\n    b_dv_new -= b_e_last * b_dhb.to(b_k.dtype)[None, :]\n    b_v2 = tl.load(p_v2, boundary_check=(0, 1), padding_option='zero').to(b_k.dtype)\n    b_x = tl.load(p_x, boundary_check=(0, 1), padding_option='zero').to(b_k.dtype)\n    b_y = tl.load(p_y, boundary_check=(0, 1), padding_option='zero').to(b_k.dtype)\n    b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option='zero').to(tl.float32)\n    b_dy = b_rstd * (b_dv_new * V - tl.sum(b_dv_new, axis=1, keep_dims=True) - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)) / V\n    b_dx = -b_rstd * (b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True) + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)) / V\n    b_drstd = tl.sum(b_dv_new.to(b_rstd.dtype) * b_v2.to(b_rstd.dtype) / b_rstd, axis=1, keep_dims=True)\n    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option='zero')\n    b_w = b_w.to(b_k.dtype)\n    b_b = b_b.to(b_k.dtype)\n    b_dv = -b_w * b_dy.to(b_k.dtype)\n    b_dk = b_w * b_dy.to(b_k.dtype)\n    b_dw += tl.sum(2 * b_w * b_x * b_dy.to(b_k.dtype) + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype), axis=0).to(b_dw.dtype)\n    b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)\n    b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)\n    b_h = tl.load(p_h, boundary_check=(0, 1), padding_option='zero')\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_dkh = b_rstd * (V * b_dx - tl.sum(b_dx, axis=1, keep_dims=True) - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)) / V\n    b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V\n    b_dkh = tl.where((v_i < V)[None, :] * (o_i < T - i_t * BT)[:, None], b_dkh, 0.0)\n    b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)\n    b_ds = tl.dot(b_do, tl.trans(b_v2))\n    b_ds = tl.where(m_A, b_ds, 0)\n    b_ds = b_ds.to(b_k.dtype)\n    i_last = BT - 1 if i_t * BT + BT <= T else T % BT - 1\n    mask = o_i == i_last\n    b_dk -= b_e_last * tl.dot(b_v2, tl.trans(b_dh).to(b_v2.dtype))\n    b_dk -= tl.dot(tl.trans(b_ds), tl.trans(b_q) * b_e[:, None])\n    b_de = mask * tl.sum(-b_dh * tl.trans(tl.dot(tl.trans(b_v2), b_k))).to(b_k.dtype)\n    b_de -= mask * tl.sum(b_dhb * tl.sum(b_v2, axis=0)).to(b_k.dtype)\n    b_de -= tl.sum(tl.dot(b_ds, b_k) * tl.trans(b_q).to(b_k.dtype), axis=1)\n    b_de -= tl.sum(b_ds, axis=1)\n    b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(tl.trans(b_k).to(b_dkh.dtype), b_dkh)\n    b_dhb += tl.sum(b_do + b_dkh, axis=0)\n    b_dh = tl.where((v_i < V)[None, :], b_dh, 0.0)\n    b_dhb = tl.where(v_i < V, b_dhb, 0.0)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))\ntl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))\ntl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))\nif USE_INITIAL_STATE:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\nif USE_INITIAL_STATE_B:\n    p_dhb0 = tl.make_block_ptr(dhb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))\n    tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_gated_delta_rule_fwd_kernel_h_blockdim64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'SAVE_NEW_VALUE': lambda args: args['v_new'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [2, 3, 4] for BV in [16, 32, 64]], key=['H', 'K', 'V', 'BT', 'USE_G'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "d",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SAVE_NEW_VALUE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gated_delta_rule_fwd_kernel_h_blockdim64(",
      "    k,",
      "    v,",
      "    d,",
      "    v_new,",
      "    g,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    SAVE_NEW_VALUE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h1 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 64:",
      "        b_h2 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 128:",
      "        b_h3 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 192:",
      "        b_h4 = tl.zeros([64, BV], dtype=tl.float32)",
      "",
      "    h += (boh * H + i_h) * K * V",
      "    v += (bos * H + i_h) * V",
      "    k += (bos * H + i_h) * K",
      "    d += (bos * H + i_h) * K",
      "    if SAVE_NEW_VALUE:",
      "        v_new += (bos * H + i_h) * V",
      "    stride_v = H * V",
      "    stride_h = H * K * V",
      "    stride_k = H * K",
      "    if USE_INITIAL_STATE:",
      "        h0 = h0 + i_nh * K * V",
      "    if STORE_FINAL_STATE:",
      "        ht = ht + i_nh * K * V",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0_1 = tl.make_block_ptr(h0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        b_h1 += tl.load(p_h0_1, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 64:",
      "            p_h0_2 = tl.make_block_ptr(",
      "                h0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_h2 += tl.load(p_h0_2, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 128:",
      "            p_h0_3 = tl.make_block_ptr(",
      "                h0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_h3 += tl.load(p_h0_3, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 192:",
      "            p_h0_4 = tl.make_block_ptr(",
      "                h0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_h4 += tl.load(p_h0_4, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h1 = tl.make_block_ptr(",
      "            h + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0)",
      "        )",
      "        tl.store(p_h1, b_h1.to(p_h1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_h2 = tl.make_block_ptr(",
      "                h + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_h2, b_h2.to(p_h2.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_h3 = tl.make_block_ptr(",
      "                h + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_h3, b_h3.to(p_h3.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_h4 = tl.make_block_ptr(",
      "                h + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_h4, b_h4.to(p_h4.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_v_new = (",
      "            tl.make_block_ptr(",
      "                v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            if SAVE_NEW_VALUE",
      "            else None",
      "        )",
      "        b_intermediate = tl.zeros([BT, BV], dtype=tl.float32)",
      "        p_d = tl.make_block_ptr(",
      "            d, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0)",
      "        )",
      "        b_d = tl.load(p_d, boundary_check=(0, 1))",
      "        b_intermediate += tl.dot(b_d, b_h1.to(b_d.dtype))",
      "        if K > 64:",
      "            p_d = tl.make_block_ptr(",
      "                d, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0)",
      "            )",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            b_intermediate += tl.dot(b_d, b_h2.to(b_d.dtype))",
      "        if K > 128:",
      "            p_d = tl.make_block_ptr(",
      "                d, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0)",
      "            )",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            b_intermediate += tl.dot(b_d, b_h3.to(b_d.dtype))",
      "        if K > 192:",
      "            p_d = tl.make_block_ptr(",
      "                d, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0)",
      "            )",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            b_intermediate += tl.dot(b_d, b_h4.to(b_d.dtype))",
      "        b_intermediate = -b_intermediate + tl.load(p_v, boundary_check=(0, 1))",
      "        b_intermediate = b_intermediate.to(k.dtype.element_ty)",
      "        if SAVE_NEW_VALUE:",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            tl.store(p_v_new, b_intermediate, boundary_check=(0, 1))",
      "",
      "        if USE_G:",
      "            last_idx = min((i_t + 1) * BT, T) - 1",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            b_g_last = exp(b_g_last)",
      "            b_h1 = b_h1 * b_g_last",
      "            if K > 64:",
      "                b_h2 = b_h2 * b_g_last",
      "            if K > 128:",
      "                b_h3 = b_h3 * b_g_last",
      "            if K > 192:",
      "                b_h4 = b_h4 * b_g_last",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_h1 += tl.dot(b_k, b_intermediate)",
      "        if K > 64:",
      "            p_k = tl.make_block_ptr(",
      "                k, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h2 += tl.dot(b_k, b_intermediate)",
      "        if K > 128:",
      "            p_k = tl.make_block_ptr(",
      "                k, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h3 += tl.dot(b_k, b_intermediate)",
      "        if K > 192:",
      "            p_k = tl.make_block_ptr(",
      "                k, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h4 += tl.dot(b_k, b_intermediate)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        tl.store(p_ht, b_h1.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_ht = tl.make_block_ptr(",
      "                ht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_ht, b_h2.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_ht = tl.make_block_ptr(",
      "                ht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_ht, b_h3.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_ht = tl.make_block_ptr(",
      "                ht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_ht, b_h4.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/368.py",
    "header": "def chunk_gated_delta_rule_fwd_kernel_h_blockdim64(k, v, d, v_new, g, h, h0, ht, cu_seqlens, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, SAVE_NEW_VALUE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_h1 = tl.zeros([64, BV], dtype=tl.float32)\nif K > 64:\n    b_h2 = tl.zeros([64, BV], dtype=tl.float32)\nif K > 128:\n    b_h3 = tl.zeros([64, BV], dtype=tl.float32)\nif K > 192:\n    b_h4 = tl.zeros([64, BV], dtype=tl.float32)\nh += (boh * H + i_h) * K * V\nv += (bos * H + i_h) * V\nk += (bos * H + i_h) * K\nd += (bos * H + i_h) * K\nif SAVE_NEW_VALUE:\n    v_new += (bos * H + i_h) * V\nstride_v = H * V\nstride_h = H * K * V\nstride_k = H * K\nif USE_INITIAL_STATE:\n    h0 = h0 + i_nh * K * V\nif STORE_FINAL_STATE:\n    ht = ht + i_nh * K * V\nif USE_INITIAL_STATE:\n    p_h0_1 = tl.make_block_ptr(h0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n    b_h1 += tl.load(p_h0_1, boundary_check=(0, 1)).to(tl.float32)\n    if K > 64:\n        p_h0_2 = tl.make_block_ptr(h0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0))\n        b_h2 += tl.load(p_h0_2, boundary_check=(0, 1)).to(tl.float32)\n    if K > 128:\n        p_h0_3 = tl.make_block_ptr(h0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0))\n        b_h3 += tl.load(p_h0_3, boundary_check=(0, 1)).to(tl.float32)\n    if K > 192:\n        p_h0_4 = tl.make_block_ptr(h0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0))\n        b_h4 += tl.load(p_h0_4, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(NT):\n    p_h1 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n    tl.store(p_h1, b_h1.to(p_h1.dtype.element_ty), boundary_check=(0, 1))\n    if K > 64:\n        p_h2 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_h2, b_h2.to(p_h2.dtype.element_ty), boundary_check=(0, 1))\n    if K > 128:\n        p_h3 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_h3, b_h3.to(p_h3.dtype.element_ty), boundary_check=(0, 1))\n    if K > 192:\n        p_h4 = tl.make_block_ptr(h + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_h4, b_h4.to(p_h4.dtype.element_ty), boundary_check=(0, 1))\n    p_v = tl.make_block_ptr(v, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_v_new = tl.make_block_ptr(v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)) if SAVE_NEW_VALUE else None\n    b_intermediate = tl.zeros([BT, BV], dtype=tl.float32)\n    p_d = tl.make_block_ptr(d, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0))\n    b_d = tl.load(p_d, boundary_check=(0, 1))\n    b_intermediate += tl.dot(b_d, b_h1.to(b_d.dtype))\n    if K > 64:\n        p_d = tl.make_block_ptr(d, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_intermediate += tl.dot(b_d, b_h2.to(b_d.dtype))\n    if K > 128:\n        p_d = tl.make_block_ptr(d, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_intermediate += tl.dot(b_d, b_h3.to(b_d.dtype))\n    if K > 192:\n        p_d = tl.make_block_ptr(d, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0))\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        b_intermediate += tl.dot(b_d, b_h4.to(b_d.dtype))\n    b_intermediate = -b_intermediate + tl.load(p_v, boundary_check=(0, 1))\n    b_intermediate = b_intermediate.to(k.dtype.element_ty)\n    if SAVE_NEW_VALUE:\n        p_v_new = tl.make_block_ptr(v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        tl.store(p_v_new, b_intermediate, boundary_check=(0, 1))\n    if USE_G:\n        last_idx = min((i_t + 1) * BT, T) - 1\n        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n        b_g_last = exp(b_g_last)\n        b_h1 = b_h1 * b_g_last\n        if K > 64:\n            b_h2 = b_h2 * b_g_last\n        if K > 128:\n            b_h3 = b_h3 * b_g_last\n        if K > 192:\n            b_h4 = b_h4 * b_g_last\n    p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_h1 += tl.dot(b_k, b_intermediate)\n    if K > 64:\n        p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h2 += tl.dot(b_k, b_intermediate)\n    if K > 128:\n        p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h3 += tl.dot(b_k, b_intermediate)\n    if K > 192:\n        p_k = tl.make_block_ptr(k, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_h4 += tl.dot(b_k, b_intermediate)\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n    tl.store(p_ht, b_h1.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n    if K > 64:\n        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_ht, b_h2.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n    if K > 128:\n        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_ht, b_h3.to(p_ht.dtype.element_ty), boundary_check=(0, 1))\n    if K > 192:\n        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_ht, b_h4.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_gated_delta_rule_bwd_kernel_dhu_blockdim64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [4, 3, 2, 1] for BV in [64, 32, 16]], key=['H', 'K', 'V', 'BT', 'BV', 'USE_G'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "d",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gated_delta_rule_bwd_kernel_dhu_blockdim64(",
      "    q,",
      "    k,",
      "    d,",
      "    g,",
      "    dht,",
      "    dh0,",
      "    do,",
      "    dh,",
      "    dv,",
      "    dv2,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh1 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 64:",
      "        b_dh2 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 128:",
      "        b_dh3 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 192:",
      "        b_dh4 = tl.zeros([64, BV], dtype=tl.float32)",
      "",
      "    dh += (boh * H + i_h) * K * V",
      "    dv += (bos * H + i_h) * V",
      "    dv2 += (bos * H + i_h) * V",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    d += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V",
      "    stride_v = H * V",
      "    stride_h = H * K * V",
      "    stride_k = H * K",
      "    if USE_INITIAL_STATE:",
      "        dh0 += i_nh * K * V",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        dht += i_nh * K * V",
      "",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht1 = tl.make_block_ptr(dht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        b_dh1 += tl.load(p_dht1, boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_dht2 = tl.make_block_ptr(",
      "                dht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_dh2 += tl.load(p_dht2, boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_dht3 = tl.make_block_ptr(",
      "                dht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_dh3 += tl.load(p_dht3, boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_dht4 = tl.make_block_ptr(",
      "                dht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_dh4 += tl.load(p_dht4, boundary_check=(0, 1))",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh1 = tl.make_block_ptr(",
      "            dh + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh1, b_dh1.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_dh2 = tl.make_block_ptr(",
      "                dh + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh2, b_dh2.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_dh3 = tl.make_block_ptr(",
      "                dh + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh3, b_dh3.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_dh4 = tl.make_block_ptr(",
      "                dh + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh4, b_dh4.to(p_dh4.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        if USE_G:",
      "            last_idx = min((i_t + 1) * BT, T) - 1",
      "            bg_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            bg_last = exp(bg_last)",
      "        else:",
      "            bg_last = None",
      "            last_idx = None",
      "",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dv2 = tl.make_block_ptr(",
      "            dv2, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "        b_dv += tl.load(p_dv, boundary_check=(0, 1))",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_k, b_dh1.to(b_k.dtype))",
      "",
      "        if K > 64:",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_dv += tl.dot(b_k, b_dh2.to(b_k.dtype))",
      "",
      "        if K > 128:",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_dv += tl.dot(b_k, b_dh3.to(b_k.dtype))",
      "",
      "        if K > 192:",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_dv += tl.dot(b_k, b_dh4.to(b_k.dtype))",
      "",
      "        tl.store(p_dv2, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        p_d = tl.make_block_ptr(",
      "            d, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)",
      "        )",
      "        b_d = tl.load(p_d, boundary_check=(0, 1))",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        if USE_G:",
      "            b_dh1 *= bg_last",
      "        b_dh1 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))",
      "        if K > 64:",
      "            p_q = tl.make_block_ptr(",
      "                q, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            p_d = tl.make_block_ptr(",
      "                d, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            if USE_G:",
      "                b_dh2 *= bg_last",
      "            b_dh2 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))",
      "        if K > 128:",
      "            p_q = tl.make_block_ptr(",
      "                q, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            p_d = tl.make_block_ptr(",
      "                d, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            if USE_G:",
      "                b_dh3 *= bg_last",
      "            b_dh3 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))",
      "        if K > 192:",
      "            p_q = tl.make_block_ptr(",
      "                q, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            p_d = tl.make_block_ptr(",
      "                d, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            if USE_G:",
      "                b_dh4 *= bg_last",
      "            b_dh4 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(dh0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        tl.store(p_dh0, b_dh1.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_dh1 = tl.make_block_ptr(",
      "                dh0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh1, b_dh2.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_dh2 = tl.make_block_ptr(",
      "                dh0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh2, b_dh3.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_dh3 = tl.make_block_ptr(",
      "                dh0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh3, b_dh4.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/368.py",
    "header": "def chunk_gated_delta_rule_bwd_kernel_dhu_blockdim64(q, k, d, g, dht, dh0, do, dh, dv, dv2, cu_seqlens, chunk_offsets, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_v, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_dh1 = tl.zeros([64, BV], dtype=tl.float32)\nif K > 64:\n    b_dh2 = tl.zeros([64, BV], dtype=tl.float32)\nif K > 128:\n    b_dh3 = tl.zeros([64, BV], dtype=tl.float32)\nif K > 192:\n    b_dh4 = tl.zeros([64, BV], dtype=tl.float32)\ndh += (boh * H + i_h) * K * V\ndv += (bos * H + i_h) * V\ndv2 += (bos * H + i_h) * V\nq += (bos * H + i_h) * K\nk += (bos * H + i_h) * K\nd += (bos * H + i_h) * K\ndo += (bos * H + i_h) * V\nstride_v = H * V\nstride_h = H * K * V\nstride_k = H * K\nif USE_INITIAL_STATE:\n    dh0 += i_nh * K * V\nif USE_FINAL_STATE_GRADIENT:\n    dht += i_nh * K * V\nif USE_FINAL_STATE_GRADIENT:\n    p_dht1 = tl.make_block_ptr(dht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n    b_dh1 += tl.load(p_dht1, boundary_check=(0, 1))\n    if K > 64:\n        p_dht2 = tl.make_block_ptr(dht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0))\n        b_dh2 += tl.load(p_dht2, boundary_check=(0, 1))\n    if K > 128:\n        p_dht3 = tl.make_block_ptr(dht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0))\n        b_dh3 += tl.load(p_dht3, boundary_check=(0, 1))\n    if K > 192:\n        p_dht4 = tl.make_block_ptr(dht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0))\n        b_dh4 += tl.load(p_dht4, boundary_check=(0, 1))\nfor i_t in range(NT - 1, -1, -1):\n    p_dh1 = tl.make_block_ptr(dh + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n    tl.store(p_dh1, b_dh1.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))\n    if K > 64:\n        p_dh2 = tl.make_block_ptr(dh + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh2, b_dh2.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))\n    if K > 128:\n        p_dh3 = tl.make_block_ptr(dh + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh3, b_dh3.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))\n    if K > 192:\n        p_dh4 = tl.make_block_ptr(dh + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh4, b_dh4.to(p_dh4.dtype.element_ty), boundary_check=(0, 1))\n    if USE_G:\n        last_idx = min((i_t + 1) * BT, T) - 1\n        bg_last = tl.load(g + (bos + last_idx) * H + i_h)\n        bg_last = exp(bg_last)\n    else:\n        bg_last = None\n        last_idx = None\n    p_dv = tl.make_block_ptr(dv, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv2 = tl.make_block_ptr(dv2, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_dv = tl.zeros([BT, BV], dtype=tl.float32)\n    b_dv += tl.load(p_dv, boundary_check=(0, 1))\n    p_k = tl.make_block_ptr(k, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dv += tl.dot(b_k, b_dh1.to(b_k.dtype))\n    if K > 64:\n        p_k = tl.make_block_ptr(k, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh2.to(b_k.dtype))\n    if K > 128:\n        p_k = tl.make_block_ptr(k, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh3.to(b_k.dtype))\n    if K > 192:\n        p_k = tl.make_block_ptr(k, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_dv += tl.dot(b_k, b_dh4.to(b_k.dtype))\n    tl.store(p_dv2, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\n    p_d = tl.make_block_ptr(d, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1))\n    p_q = tl.make_block_ptr(q, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1))\n    b_d = tl.load(p_d, boundary_check=(0, 1))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    if USE_G:\n        b_dh1 *= bg_last\n    b_dh1 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))\n    if K > 64:\n        p_q = tl.make_block_ptr(q, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1))\n        p_d = tl.make_block_ptr(d, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        if USE_G:\n            b_dh2 *= bg_last\n        b_dh2 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))\n    if K > 128:\n        p_q = tl.make_block_ptr(q, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1))\n        p_d = tl.make_block_ptr(d, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        if USE_G:\n            b_dh3 *= bg_last\n        b_dh3 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))\n    if K > 192:\n        p_q = tl.make_block_ptr(q, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1))\n        p_d = tl.make_block_ptr(d, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_q = (b_q * scale).to(b_q.dtype)\n        b_d = tl.load(p_d, boundary_check=(0, 1))\n        if USE_G:\n            b_dh4 *= bg_last\n        b_dh4 += tl.dot(b_q, b_do.to(b_q.dtype)) - tl.dot(b_d, b_dv.to(b_d.dtype))\nif USE_INITIAL_STATE:\n    p_dh0 = tl.make_block_ptr(dh0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))\n    tl.store(p_dh0, b_dh1.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))\n    if K > 64:\n        p_dh1 = tl.make_block_ptr(dh0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh1, b_dh2.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))\n    if K > 128:\n        p_dh2 = tl.make_block_ptr(dh0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh2, b_dh3.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))\n    if K > 192:\n        p_dh3 = tl.make_block_ptr(dh0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0))\n        tl.store(p_dh3, b_dh4.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "preprocess_qkw",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_Q': lambda args: args['q'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4] for BK in [16, 32, 64, 128]], key=['H', 'K', 'BT', 'BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "w_new",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_Q",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def preprocess_qkw(",
      "    q,",
      "    k,",
      "    w,",
      "    g,",
      "    q_new,",
      "    k_new,",
      "    w_new,",
      "    cu_seqlens,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    USE_Q: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_nh, i_t = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    k += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    w_new += (bos * H + i_h) * K",
      "    if USE_Q:",
      "        q += (bos * H + i_h) * K",
      "        q_new += (bos * H + i_h) * K",
      "    g += bos * H + i_h",
      "    stride_k = H * K",
      "    stride_g = H",
      "",
      "    last_idx = min((i_t + 1) * BT, T) - 1",
      "    b_g_last = tl.load(g + last_idx * stride_g).to(tl.float32)",
      "",
      "    p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "    p_w = tl.make_block_ptr(",
      "        w, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k_new = tl.make_block_ptr(",
      "        k_new, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_w_new = tl.make_block_ptr(",
      "        w_new, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1)).to(tl.float32)",
      "    b_w = tl.load(p_w, boundary_check=(0, 1)).to(tl.float32)",
      "    b_g = tl.load(p_g, boundary_check=(0,)).to(tl.float32)",
      "    b_d_last = exp(b_g_last - b_g)",
      "    b_d_begin = exp(b_g)",
      "",
      "    b_k = b_k * b_d_last[:, None]",
      "    b_w = b_w * b_d_begin[:, None]",
      "    tl.store(p_k_new, b_k.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_w_new, b_w.to(p_w_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if USE_Q:",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q_new = tl.make_block_ptr(",
      "            q_new, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1)).to(tl.float32)",
      "        b_q = b_q * b_d_begin[:, None]",
      "        tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/368.py",
    "header": "def preprocess_qkw(q, k, w, g, q_new, k_new, w_new, cu_seqlens, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, USE_Q: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_nh, i_t = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\nk += (bos * H + i_h) * K\nw += (bos * H + i_h) * K\nk_new += (bos * H + i_h) * K\nw_new += (bos * H + i_h) * K\nif USE_Q:\n    q += (bos * H + i_h) * K\n    q_new += (bos * H + i_h) * K\ng += bos * H + i_h\nstride_k = H * K\nstride_g = H\nlast_idx = min((i_t + 1) * BT, T) - 1\nb_g_last = tl.load(g + last_idx * stride_g).to(tl.float32)\np_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))\np_w = tl.make_block_ptr(w, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k_new = tl.make_block_ptr(k_new, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_w_new = tl.make_block_ptr(w_new, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1)).to(tl.float32)\nb_w = tl.load(p_w, boundary_check=(0, 1)).to(tl.float32)\nb_g = tl.load(p_g, boundary_check=(0,)).to(tl.float32)\nb_d_last = exp(b_g_last - b_g)\nb_d_begin = exp(b_g)\nb_k = b_k * b_d_last[:, None]\nb_w = b_w * b_d_begin[:, None]\ntl.store(p_k_new, b_k.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_w_new, b_w.to(p_w_new.dtype.element_ty), boundary_check=(0, 1))\nif USE_Q:\n    p_q = tl.make_block_ptr(q, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_q_new = tl.make_block_ptr(q_new, (T, K), (stride_k, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1)).to(tl.float32)\n    b_q = b_q * b_d_begin[:, None]\n    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "prepare_wy_repr_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "Aw",
        "annotation": null
      },
      {
        "name": "Au",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_bwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    g,",
      "    Aw,",
      "    Au,",
      "    dw,",
      "    du,",
      "    dk,",
      "    dv,",
      "    dbeta,",
      "    dg,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_dbeta = tl.zeros([BT], dtype=tl.float32)",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    p_beta = tl.make_block_ptr(",
      "        beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        Aw + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_dw = tl.load(p_dw, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)",
      "        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)",
      "        b_dk = b_dk_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)",
      "    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)",
      "    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(",
      "        k.dtype.element_ty",
      "    )",
      "",
      "    p_A = tl.make_block_ptr(",
      "        Au + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_dA2 = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_du = tl.make_block_ptr(",
      "            du + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_du = tl.load(p_du, boundary_check=(0, 1))",
      "        b_dA2 += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)",
      "        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)",
      "        b_dv = b_dv_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dv_beta * b_v, 1)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA2 = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA2, 0)",
      "    b_dA2 = tl.dot(b_dA2.to(b_A.dtype), b_A)",
      "    b_dA2 = tl.dot(b_A, b_dA2.to(b_A.dtype))",
      "    b_dA2 = tl.where(",
      "        tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA2, 0",
      "    ).to(k.dtype.element_ty)",
      "",
      "    p_g = tl.make_block_ptr(g + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_g = tl.load(p_g, boundary_check=(0,))",
      "    b_dA2 *= safe_exp(b_g[:, None] - b_g[None, :])",
      "    b_dA += b_dA2",
      "    b_dA = b_dA.to(k.dtype.element_ty)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_A += tl.dot(b_k_beta, tl.trans(b_k))",
      "        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)",
      "        b_dk += b_dk_beta * b_beta[:, None]",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    b_dA2 *= b_A",
      "    b_dg = tl.sum(b_dA2, axis=1) - tl.sum(b_dA2, axis=0)",
      "    p_dg = tl.make_block_ptr(dg + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    p_dbeta = tl.make_block_ptr(",
      "        dbeta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/379.py",
    "header": "def prepare_wy_repr_bwd_kernel(k, v, beta, g, Aw, Au, dw, du, dk, dv, dbeta, dg, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nb_dbeta = tl.zeros([BT], dtype=tl.float32)\nb_dA = tl.zeros([BT, BT], dtype=tl.float32)\np_beta = tl.make_block_ptr(beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\np_A = tl.make_block_ptr(Aw + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dw = tl.make_block_ptr(dw + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_dw = tl.load(p_dw, boundary_check=(0, 1))\n    b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)\n    b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)\n    b_dk = b_dk_beta * b_beta[:, None]\n    b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\nb_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)\nb_dA = tl.dot(b_dA.to(b_A.dtype), b_A)\nb_dA = tl.dot(b_A, b_dA.to(b_A.dtype))\nb_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(k.dtype.element_ty)\np_A = tl.make_block_ptr(Au + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1))\nb_A = tl.load(p_A, boundary_check=(0, 1))\nb_dA2 = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_dv = tl.make_block_ptr(dv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_du = tl.make_block_ptr(du + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)\n    b_du = tl.load(p_du, boundary_check=(0, 1))\n    b_dA2 += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)\n    b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)\n    b_dv = b_dv_beta * b_beta[:, None]\n    b_dbeta += tl.sum(b_dv_beta * b_v, 1)\n    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\nb_dA2 = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA2, 0)\nb_dA2 = tl.dot(b_dA2.to(b_A.dtype), b_A)\nb_dA2 = tl.dot(b_A, b_dA2.to(b_A.dtype))\nb_dA2 = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA2, 0).to(k.dtype.element_ty)\np_g = tl.make_block_ptr(g + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\nb_g = tl.load(p_g, boundary_check=(0,))\nb_dA2 *= safe_exp(b_g[:, None] - b_g[None, :])\nb_dA += b_dA2\nb_dA = b_dA.to(k.dtype.element_ty)\nb_A = tl.zeros([BT, BT], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_dk = tl.load(p_dk, boundary_check=(0, 1))\n    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_A += tl.dot(b_k_beta, tl.trans(b_k))\n    b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)\n    b_dbeta += tl.sum(b_dk_beta * b_k, 1)\n    b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)\n    b_dk += b_dk_beta * b_beta[:, None]\n    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\nb_dA2 *= b_A\nb_dg = tl.sum(b_dA2, axis=1) - tl.sum(b_dA2, axis=0)\np_dg = tl.make_block_ptr(dg + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\np_dbeta = tl.make_block_ptr(dbeta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))\ntl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))\ntl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "recompute_w_u_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "Aw",
        "annotation": null
      },
      {
        "name": "Au",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def recompute_w_u_fwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    w,",
      "    u,",
      "    Aw,",
      "    Au,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_Au = tl.make_block_ptr(",
      "        Au + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_Au = tl.load(p_Au, boundary_check=(0, 1))",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_u = tl.dot(b_Au, b_vb, allow_tf32=False)",
      "        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    b_Au = None",
      "    p_Aw = tl.make_block_ptr(",
      "        Aw + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_Aw = tl.load(p_Aw, boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_w = tl.dot(b_Aw, b_kb)",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/379.py",
    "header": "def recompute_w_u_fwd_kernel(k, v, beta, w, u, Aw, Au, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\np_beta = tl.make_block_ptr(beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\np_Au = tl.make_block_ptr(Au + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_Au = tl.load(p_Au, boundary_check=(0, 1))\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_u = tl.make_block_ptr(u + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)\n    b_u = tl.dot(b_Au, b_vb, allow_tf32=False)\n    tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))\ntl.debug_barrier()\nb_Au = None\np_Aw = tl.make_block_ptr(Aw + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_Aw = tl.load(p_Aw, boundary_check=(0, 1))\nfor i_k in range(tl.cdiv(K, BK)):\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    p_w = tl.make_block_ptr(w + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)\n    b_w = tl.dot(b_Aw, b_kb)\n    tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "triton_jagged_mean_kernel_simple_fused_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_simple_fused_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "codes/660.py",
    "header": "def triton_jagged_mean_kernel_simple_fused_sum_then_buffer(input_ptr_values, input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_b), tl.load(input_ptr_offsets + (pid_b + 1)))\nragged_len = ragged_end - ragged_start\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer += tl.sum(input, axis=0)\nbuffer_view = buffer.reshape((BLOCK_SIZE_M,))\nbuffer_view_mean = buffer_view * (1 / ragged_len)\noutput_offsets = offsets_m + pid_b * M\noutput_mask = output_offsets < M * (pid_b + 1)\ntl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
  },
  {
    "name": "triton_jagged_mean_kernel_simple_fused_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_simple_fused_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "codes/660.py",
    "header": "def triton_jagged_mean_kernel_simple_fused_buffer_then_sum(input_ptr_values, input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_b), tl.load(input_ptr_offsets + (pid_b + 1)))\nragged_len = ragged_end - ragged_start\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\nbuffer_sum = tl.sum(buffer, axis=0)\nbuffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\nbuffer_view_mean = buffer_view * (1 / ragged_len)\noutput_offsets = offsets_m + pid_b * M\noutput_mask = output_offsets < M * (pid_b + 1)\ntl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
  },
  {
    "name": "triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "codes/660.py",
    "header": "def triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer(input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_b), tl.load(input_ptr_offsets + (pid_b + 1)))\nragged_len = ragged_end - ragged_start\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer += tl.sum(input, axis=0)\nbuffer_view = buffer.reshape((BLOCK_SIZE_M,))\nbuffer_view_mean = buffer_view * (1 / ragged_len)\noutput_offsets = offsets_m + pid_b * M\noutput_mask = output_offsets < M * (pid_b + 1)\ntl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
  },
  {
    "name": "triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "codes/660.py",
    "header": "def triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum(input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_ragged), tl.load(input_ptr_offsets + (pid_ragged + 1)))\nragged_len = ragged_end - ragged_start\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\nbuffer_sum = tl.sum(buffer, axis=0)\nbuffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\nbuffer_view_mean = buffer_view * (1 / ragged_len)\noutput_offsets = offsets_m + pid_ragged * M\noutput_mask = output_offsets < M * (pid_ragged + 1)\ntl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
  },
  {
    "name": "_attn_fwd_ws",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsWS)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "desc_q",
        "annotation": null
      },
      {
        "name": "desc_k",
        "annotation": null
      },
      {
        "name": "desc_v",
        "annotation": null
      },
      {
        "name": "desc_o",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_ws(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    desc_q,",
      "    desc_k,",
      "    desc_v,",
      "    desc_o,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    _attn_fwd_compute_ws(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        desc_q,",
      "        desc_k,",
      "        desc_v,",
      "        desc_o,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        off_hz,",
      "        pid,",
      "        Z,",
      "        H,",
      "        N_CTX,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        HEAD_DIM,",
      "        STAGE,",
      "        ENABLE_TMA,",
      "        LOOP_SCHEDULE,",
      "    )"
    ],
    "file": "codes/631.py",
    "header": "def _attn_fwd_ws(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr, STAGE: tl.constexpr, ENABLE_TMA: tl.constexpr, LOOP_SCHEDULE: tl.constexpr, ENABLE_WS: tl.constexpr):",
    "body": "tl.static_assert(BLOCK_N <= HEAD_DIM)\npid = tl.program_id(0)\noff_hz = tl.program_id(1)\n_attn_fwd_compute_ws(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, off_hz, pid, Z, H, N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM, STAGE, ENABLE_TMA, LOOP_SCHEDULE)"
  },
  {
    "name": "_atn_fwd_base_opt",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsOrig + configsOpt)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "desc_q",
        "annotation": null
      },
      {
        "name": "desc_k",
        "annotation": null
      },
      {
        "name": "desc_v",
        "annotation": null
      },
      {
        "name": "desc_o",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _atn_fwd_base_opt(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    desc_q,",
      "    desc_k,",
      "    desc_v,",
      "    desc_o,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.assume(stride_qz >= 0)",
      "    tl.assume(stride_qh >= 0)",
      "    tl.assume(stride_qm >= 0)",
      "    tl.assume(stride_qk >= 0)",
      "    tl.assume(stride_kz >= 0)",
      "    tl.assume(stride_kh >= 0)",
      "    tl.assume(stride_kn >= 0)",
      "    tl.assume(stride_kk >= 0)",
      "    tl.assume(stride_vz >= 0)",
      "    tl.assume(stride_vh >= 0)",
      "    tl.assume(stride_vk >= 0)",
      "    tl.assume(stride_vn >= 0)",
      "    tl.assume(stride_oz >= 0)",
      "    tl.assume(stride_oh >= 0)",
      "    tl.assume(stride_om >= 0)",
      "    tl.assume(stride_on >= 0)",
      "    tl.assume(Z >= 0)",
      "    tl.assume(H >= 0)",
      "",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    _attn_fwd_compute(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        desc_q,",
      "        desc_k,",
      "        desc_v,",
      "        desc_o,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        off_hz,",
      "        pid,",
      "        Z,",
      "        H,",
      "        N_CTX,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        HEAD_DIM,",
      "        STAGE,",
      "        ENABLE_TMA,",
      "        LOOP_SCHEDULE,",
      "    )"
    ],
    "file": "codes/631.py",
    "header": "def _atn_fwd_base_opt(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr, STAGE: tl.constexpr, ENABLE_TMA: tl.constexpr, LOOP_SCHEDULE: tl.constexpr, ENABLE_WS: tl.constexpr):",
    "body": "tl.assume(stride_qz >= 0)\ntl.assume(stride_qh >= 0)\ntl.assume(stride_qm >= 0)\ntl.assume(stride_qk >= 0)\ntl.assume(stride_kz >= 0)\ntl.assume(stride_kh >= 0)\ntl.assume(stride_kn >= 0)\ntl.assume(stride_kk >= 0)\ntl.assume(stride_vz >= 0)\ntl.assume(stride_vh >= 0)\ntl.assume(stride_vk >= 0)\ntl.assume(stride_vn >= 0)\ntl.assume(stride_oz >= 0)\ntl.assume(stride_oh >= 0)\ntl.assume(stride_om >= 0)\ntl.assume(stride_on >= 0)\ntl.assume(Z >= 0)\ntl.assume(H >= 0)\ntl.static_assert(BLOCK_N <= HEAD_DIM)\npid = tl.program_id(0)\noff_hz = tl.program_id(1)\n_attn_fwd_compute(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, off_hz, pid, Z, H, N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM, STAGE, ENABLE_TMA, LOOP_SCHEDULE)"
  },
  {
    "name": "_attn_fwd_tma_unified",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsTma + configsTmaWS)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_tma_unified(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    desc_q = None",
      "    desc_k = None",
      "    desc_v = None",
      "    desc_o = None",
      "",
      "    if ENABLE_TMA:",
      "        desc_k = tl.make_tensor_descriptor(",
      "            K,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_N, HEAD_DIM],",
      "        )",
      "        if V.dtype == torch.float8_e5m2:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * HEAD_DIM, N_CTX],",
      "                strides=[N_CTX, 1],",
      "                block_shape=[HEAD_DIM, BLOCK_N],",
      "            )",
      "        else:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * N_CTX, HEAD_DIM],",
      "                strides=[HEAD_DIM, 1],",
      "                block_shape=[BLOCK_N, HEAD_DIM],",
      "            )",
      "",
      "        desc_q = tl.make_tensor_descriptor(",
      "            Q,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "        desc_o = tl.make_tensor_descriptor(",
      "            Out,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "",
      "    if ENABLE_WS:",
      "        _attn_fwd_compute_ws(",
      "            Q,",
      "            K,",
      "            V,",
      "            sm_scale,",
      "            M,",
      "            Out,",
      "            desc_q,",
      "            desc_k,",
      "            desc_v,",
      "            desc_o,",
      "            stride_qz,",
      "            stride_qh,",
      "            stride_qm,",
      "            stride_qk,",
      "            stride_kz,",
      "            stride_kh,",
      "            stride_kn,",
      "            stride_kk,",
      "            stride_vz,",
      "            stride_vh,",
      "            stride_vk,",
      "            stride_vn,",
      "            stride_oz,",
      "            stride_oh,",
      "            stride_om,",
      "            stride_on,",
      "            off_hz,",
      "            pid,",
      "            Z,",
      "            H,",
      "            N_CTX,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            HEAD_DIM,",
      "            STAGE,",
      "            ENABLE_TMA,",
      "            LOOP_SCHEDULE,",
      "        )",
      "    else:",
      "        _attn_fwd_compute(",
      "            Q,",
      "            K,",
      "            V,",
      "            sm_scale,",
      "            M,",
      "            Out,",
      "            desc_q,",
      "            desc_k,",
      "            desc_v,",
      "            desc_o,",
      "            stride_qz,",
      "            stride_qh,",
      "            stride_qm,",
      "            stride_qk,",
      "            stride_kz,",
      "            stride_kh,",
      "            stride_kn,",
      "            stride_kk,",
      "            stride_vz,",
      "            stride_vh,",
      "            stride_vk,",
      "            stride_vn,",
      "            stride_oz,",
      "            stride_oh,",
      "            stride_om,",
      "            stride_on,",
      "            off_hz,",
      "            pid,",
      "            Z,",
      "            H,",
      "            N_CTX,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            HEAD_DIM,",
      "            STAGE,",
      "            ENABLE_TMA,",
      "            LOOP_SCHEDULE,",
      "        )"
    ],
    "file": "codes/631.py",
    "header": "def _attn_fwd_tma_unified(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr, STAGE: tl.constexpr, ENABLE_TMA: tl.constexpr, LOOP_SCHEDULE: tl.constexpr, ENABLE_WS: tl.constexpr):",
    "body": "tl.static_assert(BLOCK_N <= HEAD_DIM)\npid = tl.program_id(0)\noff_hz = tl.program_id(1)\ndesc_q = None\ndesc_k = None\ndesc_v = None\ndesc_o = None\nif ENABLE_TMA:\n    desc_k = tl.make_tensor_descriptor(K, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_N, HEAD_DIM])\n    if V.dtype == torch.float8_e5m2:\n        desc_v = tl.make_tensor_descriptor(V, shape=[Z * H * HEAD_DIM, N_CTX], strides=[N_CTX, 1], block_shape=[HEAD_DIM, BLOCK_N])\n    else:\n        desc_v = tl.make_tensor_descriptor(V, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_N, HEAD_DIM])\n    desc_q = tl.make_tensor_descriptor(Q, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_M, HEAD_DIM])\n    desc_o = tl.make_tensor_descriptor(Out, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_M, HEAD_DIM])\nif ENABLE_WS:\n    _attn_fwd_compute_ws(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, off_hz, pid, Z, H, N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM, STAGE, ENABLE_TMA, LOOP_SCHEDULE)\nelse:\n    _attn_fwd_compute(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, off_hz, pid, Z, H, N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM, STAGE, ENABLE_TMA, LOOP_SCHEDULE)"
  },
  {
    "name": "_attn_fwd_tma_ws_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsTmaWSPersistent)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GRID_MULTIPLE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_tma_ws_persistent(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "    GRID_MULTIPLE: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "",
      "    n_tile_num = tl.cdiv(N_CTX, BLOCK_M)",
      "    prog_id = tl.program_id(0)",
      "    num_progs = tl.num_programs(0)",
      "    total_tiles = n_tile_num * Z * H",
      "",
      "    tiles_per_sm = total_tiles // num_progs",
      "    if prog_id < total_tiles % num_progs:",
      "        tiles_per_sm += 1",
      "",
      "    tile_idx = prog_id",
      "",
      "    if ENABLE_TMA:",
      "        desc_k = tl.make_tensor_descriptor(",
      "            K,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_N, HEAD_DIM],",
      "        )",
      "        if V.dtype == torch.float8_e5m2:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * HEAD_DIM, N_CTX],",
      "                strides=[N_CTX, 1],",
      "                block_shape=[HEAD_DIM, BLOCK_N],",
      "            )",
      "        else:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * N_CTX, HEAD_DIM],",
      "                strides=[HEAD_DIM, 1],",
      "                block_shape=[BLOCK_N, HEAD_DIM],",
      "            )",
      "",
      "        desc_q = tl.make_tensor_descriptor(",
      "            Q,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "        desc_o = tl.make_tensor_descriptor(",
      "            Out,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "",
      "    for _ in range(0, tiles_per_sm):",
      "",
      "        pid = tile_idx % n_tile_num",
      "        off_hz = tile_idx // n_tile_num",
      "        _attn_fwd_compute_ws(",
      "            Q,",
      "            K,",
      "            V,",
      "            sm_scale,",
      "            M,",
      "            Out,",
      "            desc_q,",
      "            desc_k,",
      "            desc_v,",
      "            desc_o,",
      "            stride_qz,",
      "            stride_qh,",
      "            stride_qm,",
      "            stride_qk,",
      "            stride_kz,",
      "            stride_kh,",
      "            stride_kn,",
      "            stride_kk,",
      "            stride_vz,",
      "            stride_vh,",
      "            stride_vk,",
      "            stride_vn,",
      "            stride_oz,",
      "            stride_oh,",
      "            stride_om,",
      "            stride_on,",
      "            off_hz,",
      "            pid,",
      "            Z,",
      "            H,",
      "            N_CTX,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            HEAD_DIM,",
      "            STAGE,",
      "            ENABLE_TMA,",
      "            LOOP_SCHEDULE,",
      "        )",
      "        tile_idx += num_progs"
    ],
    "file": "codes/631.py",
    "header": "def _attn_fwd_tma_ws_persistent(Q, K, V, sm_scale, M, Out, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, Z, H, N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr, STAGE: tl.constexpr, ENABLE_TMA: tl.constexpr, LOOP_SCHEDULE: tl.constexpr, ENABLE_WS: tl.constexpr, GRID_MULTIPLE: tl.constexpr):",
    "body": "tl.static_assert(BLOCK_N <= HEAD_DIM)\nn_tile_num = tl.cdiv(N_CTX, BLOCK_M)\nprog_id = tl.program_id(0)\nnum_progs = tl.num_programs(0)\ntotal_tiles = n_tile_num * Z * H\ntiles_per_sm = total_tiles // num_progs\nif prog_id < total_tiles % num_progs:\n    tiles_per_sm += 1\ntile_idx = prog_id\nif ENABLE_TMA:\n    desc_k = tl.make_tensor_descriptor(K, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_N, HEAD_DIM])\n    if V.dtype == torch.float8_e5m2:\n        desc_v = tl.make_tensor_descriptor(V, shape=[Z * H * HEAD_DIM, N_CTX], strides=[N_CTX, 1], block_shape=[HEAD_DIM, BLOCK_N])\n    else:\n        desc_v = tl.make_tensor_descriptor(V, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_N, HEAD_DIM])\n    desc_q = tl.make_tensor_descriptor(Q, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_M, HEAD_DIM])\n    desc_o = tl.make_tensor_descriptor(Out, shape=[Z * H * N_CTX, HEAD_DIM], strides=[HEAD_DIM, 1], block_shape=[BLOCK_M, HEAD_DIM])\nfor _ in range(0, tiles_per_sm):\n    pid = tile_idx % n_tile_num\n    off_hz = tile_idx // n_tile_num\n    _attn_fwd_compute_ws(Q, K, V, sm_scale, M, Out, desc_q, desc_k, desc_v, desc_o, stride_qz, stride_qh, stride_qm, stride_qk, stride_kz, stride_kh, stride_kn, stride_kk, stride_vz, stride_vh, stride_vk, stride_vn, stride_oz, stride_oh, stride_om, stride_on, off_hz, pid, Z, H, N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM, STAGE, ENABLE_TMA, LOOP_SCHEDULE)\n    tile_idx += num_progs"
  },
  {
    "name": "parallel_path_fwd_kernel_prepare_k_cache",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_fwd_kernel_prepare_k_cache(",
      "    k,",
      "    k_new,",
      "    h,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = triton.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    h += (boh * H + i_h) * K * K",
      "",
      "    stride_h = H * K * K",
      "    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_k = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_k += tl.load(p_k, boundary_check=(0, 1))",
      "    for k_block_idx in range(i_t + 1, tl.cdiv(T, BT)):",
      "        p_h = tl.make_block_ptr(",
      "            h + k_block_idx * stride_h, (K, K), (1, K), (0, 0), (BK, BK), (0, 1)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_k_minus = tl.dot(b_k.to(b_h.dtype), b_h)",
      "        b_k = b_k - b_k_minus",
      "    p_k_new = tl.make_block_ptr(",
      "        k_new, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_k_new, b_k.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/413.py",
    "header": "def parallel_path_fwd_kernel_prepare_k_cache(k, k_new, h, offsets, indices, chunk_offsets, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    i_n = i_b\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = triton.cdiv(T, BT)\n    boh = i_n * NT\nk += (bos * H + i_h) * K\nk_new += (bos * H + i_h) * K\nh += (boh * H + i_h) * K * K\nstride_h = H * K * K\np_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_k = tl.zeros([BT, BK], dtype=tl.float32)\nb_k += tl.load(p_k, boundary_check=(0, 1))\nfor k_block_idx in range(i_t + 1, tl.cdiv(T, BT)):\n    p_h = tl.make_block_ptr(h + k_block_idx * stride_h, (K, K), (1, K), (0, 0), (BK, BK), (0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_k_minus = tl.dot(b_k.to(b_h.dtype), b_h)\n    b_k = b_k - b_k_minus\np_k_new = tl.make_block_ptr(k_new, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\ntl.store(p_k_new, b_k.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_local_cumsum_scalar_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['B', 'H', 'BT', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_local_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if HEAD_FIRST:",
      "        p_s = tl.make_block_ptr(",
      "            s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "    else:",
      "        p_s = tl.make_block_ptr(s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        p_o = tl.make_block_ptr(o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)",
      "    b_o = tl.cumsum(b_s, axis=0)",
      "    if REVERSE:",
      "        b_z = tl.sum(b_s, axis=0)",
      "        b_o = -b_o + b_z[None] + b_s",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/427.py",
    "header": "def chunk_local_cumsum_scalar_kernel(s, o, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, BT: tl.constexpr, REVERSE: tl.constexpr, IS_VARLEN: tl.constexpr, HEAD_FIRST: tl.constexpr):",
    "body": "i_t, i_bh = (tl.program_id(0), tl.program_id(1))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nif HEAD_FIRST:\n    p_s = tl.make_block_ptr(s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    p_o = tl.make_block_ptr(o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\nelse:\n    p_s = tl.make_block_ptr(s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    p_o = tl.make_block_ptr(o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\nb_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\nb_o = tl.cumsum(b_s, axis=0)\nif REVERSE:\n    b_z = tl.sum(b_s, axis=0)\n    b_o = -b_o + b_z[None] + b_s\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_local_cumsum_vector_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BS': BS}, num_warps=num_warps) for BS in BS_LIST for num_warps in [2, 4, 8]], key=['B', 'H', 'S', 'BT', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_local_cumsum_vector_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    if REVERSE:",
      "        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "    else:",
      "        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "",
      "    if HEAD_FIRST:",
      "        p_s = tl.make_block_ptr(",
      "            s + (bos * H + i_h * T) * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (bos * H + i_h * T) * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "    else:",
      "        p_s = tl.make_block_ptr(",
      "            s + (bos * H + i_h) * S,",
      "            (T, S),",
      "            (H * S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (bos * H + i_h) * S,",
      "            (T, S),",
      "            (H * S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "    b_o = tl.dot(m_s, b_s, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/427.py",
    "header": "def chunk_local_cumsum_vector_kernel(s, o, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, REVERSE: tl.constexpr, IS_VARLEN: tl.constexpr, HEAD_FIRST: tl.constexpr):",
    "body": "i_s, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\no_i = tl.arange(0, BT)\nif REVERSE:\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\nelse:\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\nif HEAD_FIRST:\n    p_s = tl.make_block_ptr(s + (bos * H + i_h * T) * S, (T, S), (S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + (bos * H + i_h * T) * S, (T, S), (S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\nelse:\n    p_s = tl.make_block_ptr(s + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    p_o = tl.make_block_ptr(o + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\nb_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\nb_o = tl.dot(m_s, b_s, allow_tf32=False)\ntl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_global_cumsum_scalar_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [32, 64, 128, 256] for num_warps in [2, 4, 8] for num_stages in [1, 2, 3, 4]], key=['B', 'H', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    T = eos - bos",
      "",
      "    b_z = tl.zeros([], dtype=tl.float32)",
      "    NT = tl.cdiv(T, BT)",
      "    for i_c in range(NT):",
      "        i_t = NT - 1 - i_c if REVERSE else i_c",
      "        if HEAD_FIRST:",
      "            p_s = tl.make_block_ptr(",
      "                s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "            p_o = tl.make_block_ptr(",
      "                o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "        else:",
      "            p_s = tl.make_block_ptr(",
      "                s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "            p_o = tl.make_block_ptr(",
      "                o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)",
      "        b_o = tl.cumsum(b_s, axis=0)",
      "        b_ss = tl.sum(b_s, 0)",
      "        if REVERSE:",
      "            b_o = -b_o + b_ss + b_s",
      "        b_o += b_z",
      "        if i_c >= 0:",
      "            b_z += b_ss",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "codes/427.py",
    "header": "def chunk_global_cumsum_scalar_kernel(s, o, cu_seqlens, T, B: tl.constexpr, H: tl.constexpr, BT: tl.constexpr, REVERSE: tl.constexpr, IS_VARLEN: tl.constexpr, HEAD_FIRST: tl.constexpr):",
    "body": "i_nh = tl.program_id(0)\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\nT = eos - bos\nb_z = tl.zeros([], dtype=tl.float32)\nNT = tl.cdiv(T, BT)\nfor i_c in range(NT):\n    i_t = NT - 1 - i_c if REVERSE else i_c\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    else:\n        p_s = tl.make_block_ptr(s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\n        p_o = tl.make_block_ptr(o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))\n    b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)\n    b_o = tl.cumsum(b_s, axis=0)\n    b_ss = tl.sum(b_s, 0)\n    if REVERSE:\n        b_o = -b_o + b_ss + b_s\n    b_o += b_z\n    if i_c >= 0:\n        b_z += b_ss\n    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
  },
  {
    "name": "chunk_global_cumsum_vector_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [16, 32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [1, 2, 3, 4]], key=['B', 'H', 'S', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_cumsum_vector_kernel(",
      "    s,",
      "    z,",
      "    cu_seqlens,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_s, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    T = eos - bos",
      "",
      "    o_i = tl.arange(0, BT)",
      "    if REVERSE:",
      "        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "    else:",
      "        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "",
      "    b_z = tl.zeros([BS], dtype=tl.float32)",
      "    NT = tl.cdiv(T, BT)",
      "    for i_c in range(NT):",
      "        i_t = NT - 1 - i_c if REVERSE else i_c",
      "        if HEAD_FIRST:",
      "            p_s = tl.make_block_ptr(",
      "                s + (bos * H + i_h * T) * S,",
      "                (T, S),",
      "                (S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            p_z = tl.make_block_ptr(",
      "                z + (bos * H + i_h * T) * S,",
      "                (T, S),",
      "                (S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "        else:",
      "            p_s = tl.make_block_ptr(",
      "                s + (bos * H + i_h) * S,",
      "                (T, S),",
      "                (H * S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            p_z = tl.make_block_ptr(",
      "                z + (bos * H + i_h) * S,",
      "                (T, S),",
      "                (H * S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)",
      "        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_c >= 0:",
      "            b_z += tl.sum(b_s, 0)"
    ],
    "file": "codes/427.py",
    "header": "def chunk_global_cumsum_vector_kernel(s, z, cu_seqlens, T, B: tl.constexpr, H: tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, REVERSE: tl.constexpr, IS_VARLEN: tl.constexpr, HEAD_FIRST: tl.constexpr):",
    "body": "i_s, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\nT = eos - bos\no_i = tl.arange(0, BT)\nif REVERSE:\n    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)\nelse:\n    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)\nb_z = tl.zeros([BS], dtype=tl.float32)\nNT = tl.cdiv(T, BT)\nfor i_c in range(NT):\n    i_t = NT - 1 - i_c if REVERSE else i_c\n    if HEAD_FIRST:\n        p_s = tl.make_block_ptr(s + (bos * H + i_h * T) * S, (T, S), (S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + (bos * H + i_h * T) * S, (T, S), (S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    else:\n        p_s = tl.make_block_ptr(s + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n        p_z = tl.make_block_ptr(z + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\n    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\n    b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)\n    tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))\n    if i_c >= 0:\n        b_z += tl.sum(b_s, 0)"
  },
  {
    "name": "chunk_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    gk,",
      "    gv,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = i_n * NS",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        i_s = i_t // (BS // BT)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        o_h = ((boh + i_s) * H + i_h).to(tl.int64) * K * V",
      "        p_h = tl.make_block_ptr(",
      "            h + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        if i_t % (BS // BT) == 0:",
      "            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "",
      "        if USE_G:",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_h *= exp(b_g_last)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_h *= exp(b_gk_last)[:, None]",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_h *= exp(b_gv_last)[None, :]",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "        b_h += tl.dot(b_k, b_v)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/369.py",
    "header": "def chunk_fwd_kernel_h(k, v, h, g, gk, gv, h0, ht, cu_seqlens, split_offsets, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, USE_INITIAL_STATE: tl.constexpr, STORE_FINAL_STATE: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_h = (i_nh // H, i_nh % H)\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    NS = tl.cdiv(T, BS)\n    boh = tl.load(split_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    NS = tl.cdiv(T, BS)\n    boh = i_n * NS\nb_h = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_INITIAL_STATE:\n    p_h0 = tl.make_block_ptr(h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(NT):\n    i_s = i_t // (BS // BT)\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    o_h = ((boh + i_s) * H + i_h).to(tl.int64) * K * V\n    p_h = tl.make_block_ptr(h + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    if i_t % (BS // BT) == 0:\n        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    last_idx = min((i_t + 1) * BT, T) - 1\n    if USE_G:\n        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)\n        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_h *= exp(b_g_last)\n        b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n        b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)\n    if USE_GK:\n        p_gk = tl.make_block_ptr(gk + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n        b_h *= exp(b_gk_last)[:, None]\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)\n    if USE_GV:\n        p_gv = tl.make_block_ptr(gv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n        b_h *= exp(b_gv_last)[None, :]\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)\n    b_h += tl.dot(b_k, b_v)\nif STORE_FINAL_STATE:\n    p_ht = tl.make_block_ptr(ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_bwd_kernel_dh",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh(",
      "    q,",
      "    g,",
      "    gk,",
      "    gv,",
      "    do,",
      "    dh,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = i_n * NS",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        i_s = i_t // (BS // BT)",
      "        o_dh = ((boh + i_s) * H + i_h).to(tl.int64) * K * V",
      "        p_dh = tl.make_block_ptr(",
      "            dh + o_dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        if i_t % (BS // BT) == 0:",
      "            tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (K, T),",
      "            (1, HQ * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        if USE_G:",
      "            p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)",
      "",
      "            b_dh *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_q = (b_q * exp(b_gk)).to(b_q.dtype)",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_dh *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_do = (b_do * exp(b_gv)).to(b_do.dtype)",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_dh *= exp(b_gv_last)[None, :]",
      "",
      "        b_dh += tl.dot(b_q, b_do)",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/369.py",
    "header": "def chunk_bwd_kernel_dh(q, g, gk, gv, do, dh, dht, dh0, cu_seqlens, split_offsets, scale, T, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, USE_G: tl.constexpr, USE_GK: tl.constexpr, USE_GV: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    NS = tl.cdiv(T, BS)\n    boh = tl.load(split_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    NS = tl.cdiv(T, BS)\n    boh = i_n * NS\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(NT - 1, -1, -1):\n    i_s = i_t // (BS // BT)\n    o_dh = ((boh + i_s) * H + i_h).to(tl.int64) * K * V\n    p_dh = tl.make_block_ptr(dh + o_dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    if i_t % (BS // BT) == 0:\n        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    last_idx = min(i_t * BT + BT, T) - 1\n    p_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (K, T), (1, HQ * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_q = (b_q * scale).to(b_q.dtype)\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    if USE_G:\n        p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h\n        b_g_last = tl.load(g + (bos + last_idx) * H + i_h)\n        b_g = tl.load(p_g, mask=i_t * BT + tl.arange(0, BT) < T, other=0.0)\n        b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)\n        b_dh *= exp(b_g_last)\n    if USE_GK:\n        p_gk = tl.make_block_ptr(gk + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n        p_gk_last = gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_q = (b_q * exp(b_gk)).to(b_q.dtype)\n        b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n        b_dh *= exp(b_gk_last)[:, None]\n    if USE_GV:\n        p_gv = tl.make_block_ptr(gv + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n        p_gv_last = gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)\n        b_gv = tl.load(p_gv, boundary_check=(0, 1))\n        b_do = (b_do * exp(b_gv)).to(b_do.dtype)\n        b_gv_last = tl.load(p_gv_last, mask=i_v * BV + tl.arange(0, BV) < V, other=0.0)\n        b_dh *= exp(b_gv_last)[None, :]\n    b_dh += tl.dot(b_q, b_do)\nif STORE_INITIAL_STATE_GRADIENT:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_transform_qk_bwd_kernel_prepare",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_GATE': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "AT",
        "annotation": null
      },
      {
        "name": "dA_local",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_transform_qk_bwd_kernel_prepare(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    beta,",
      "    g_cumsum,",
      "    L,",
      "    D,",
      "    h,",
      "    q_new,",
      "    k_new,",
      "    AT,",
      "    dA_local,",
      "    dv,",
      "    do,",
      "    dg_cumsum,",
      "    scale,",
      "    indices,",
      "    offsets,",
      "    chunk_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    sm_scale = scale * 1.44269504",
      "",
      "    dA_local += (bos * HQ + i_hq) * BT",
      "    AT += (bos * H + i_h) * BT",
      "    q += (bos * HQ + i_hq) * K",
      "    q_new += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * HQ + i_hq) * V",
      "    dv += (bos * HQ + i_hq) * V",
      "    beta += bos * H + i_h",
      "    h += ((boh + i_t) * H + i_h) * K * K",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))",
      "    p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_kt = tl.load(p_k, boundary_check=(0, 1))",
      "    b_w = tl.load(p_w, boundary_check=(0, 1))",
      "    p_T = tl.make_block_ptr(AT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))",
      "    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_t = o_i[:, None] >= o_i[None, :]",
      "    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)",
      "",
      "    b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)",
      "",
      "    b_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)",
      "    b_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)",
      "    b_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(",
      "        b_w.dtype",
      "    )",
      "    b_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)",
      "",
      "    b_q = b_q - tl.dot(b_qwT, b_w_beta)",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0)",
      "    )",
      "    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if i_hq % G == 0:",
      "        b_h = tl.dot(tl.trans(b_w), b_Twb)",
      "        p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)",
      "        p_k_new = tl.make_block_ptr(",
      "            k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        tl.store(",
      "            p_k_new,",
      "            (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "    if USE_GATE:",
      "        p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))",
      "        b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])",
      "        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\"))",
      "",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_l, boundary_check=(0,))",
      "    p_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "    b_A_softmax = tl.exp2(",
      "        tl.where(",
      "            o_i[:, None] >= o_i[None, :], b_A * sm_scale - b_l[:, None], float(\"-inf\")",
      "        )",
      "    )",
      "    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "    p_dv = tl.make_block_ptr(dv, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_v = tl.make_block_ptr(v, (V, T), (1, H * V), (0, i_t * BT), (BV, BT), (0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dp = tl.dot(b_do, b_v)",
      "",
      "    b_dA = (b_dp - delta[:, None]) * b_A_softmax * scale",
      "    b_dgq = tl.sum(b_dA, axis=1) - tl.sum(b_dA, axis=0)",
      "    b_dA = b_dA.to(b_v.dtype)",
      "",
      "    if USE_GATE:",
      "        p_dg = tl.make_block_ptr(dg_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_dg, b_dgq.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "",
      "    p_dA = tl.make_block_ptr(",
      "        dA_local, (T, BT), (BT * HQ, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/407.py",
    "header": "def chunk_transform_qk_bwd_kernel_prepare(q, k, v, w, beta, g_cumsum, L, D, h, q_new, k_new, AT, dA_local, dv, do, dg_cumsum, scale, indices, offsets, chunk_offsets, T, G: tl.constexpr, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, BT: tl.constexpr, IS_VARLEN: tl.constexpr, USE_GATE: tl.constexpr):",
    "body": "i_t, i_nh = (tl.program_id(0), tl.program_id(1))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // G\nif IS_VARLEN:\n    i_n, i_t = (tl.load(indices + i_t * 2).to(tl.int32), tl.load(indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(tl.int32))\n    T = eos - bos\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nsm_scale = scale * 1.44269504\ndA_local += (bos * HQ + i_hq) * BT\nAT += (bos * H + i_h) * BT\nq += (bos * HQ + i_hq) * K\nq_new += (bos * HQ + i_hq) * K\nk += (bos * H + i_h) * K\nk_new += (bos * H + i_h) * K\nw += (bos * H + i_h) * K\nv += (bos * H + i_h) * V\ndo += (bos * HQ + i_hq) * V\ndv += (bos * HQ + i_hq) * V\nbeta += bos * H + i_h\nh += ((boh + i_t) * H + i_h) * K * K\nif USE_GATE:\n    g_cumsum += bos * HQ + i_hq\n    dg_cumsum += bos * HQ + i_hq\nL += bos * HQ + i_hq\nD += bos * HQ + i_hq\np_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))\np_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_kt = tl.load(p_k, boundary_check=(0, 1))\nb_w = tl.load(p_w, boundary_check=(0, 1))\np_T = tl.make_block_ptr(AT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))\nb_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)\no_i = tl.arange(0, BT)\nm_t = o_i[:, None] >= o_i[None, :]\np_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))\nb_beta = tl.load(p_beta, boundary_check=(0,))\nb_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)\nb_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)\nb_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)\nb_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)\nb_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(b_w.dtype)\nb_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)\nb_q = b_q - tl.dot(b_qwT, b_w_beta)\np_q_new = tl.make_block_ptr(q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0))\ntl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))\nif i_hq % G == 0:\n    b_h = tl.dot(tl.trans(b_w), b_Twb)\n    p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))\n    tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))\n    b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)\n    p_k_new = tl.make_block_ptr(k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1))\n    tl.store(p_k_new, (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty), boundary_check=(0, 1))\nif USE_GATE:\n    p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))\n    b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])\n    b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float('-inf'))\np_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\nb_l = tl.load(p_l, boundary_check=(0,))\np_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\ndelta = tl.load(p_delta, boundary_check=(0,))\nb_A_softmax = tl.exp2(tl.where(o_i[:, None] >= o_i[None, :], b_A * sm_scale - b_l[:, None], float('-inf')))\np_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\nb_do = tl.load(p_do, boundary_check=(0, 1))\nb_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)\np_dv = tl.make_block_ptr(dv, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))\ntl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))\np_v = tl.make_block_ptr(v, (V, T), (1, H * V), (0, i_t * BT), (BV, BT), (0, 1))\nb_v = tl.load(p_v, boundary_check=(0, 1))\nb_dp = tl.dot(b_do, b_v)\nb_dA = (b_dp - delta[:, None]) * b_A_softmax * scale\nb_dgq = tl.sum(b_dA, axis=1) - tl.sum(b_dA, axis=0)\nb_dA = b_dA.to(b_v.dtype)\nif USE_GATE:\n    p_dg = tl.make_block_ptr(dg_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_dg, b_dgq.to(p_dg.dtype.element_ty), boundary_check=(0,))\np_dA = tl.make_block_ptr(dA_local, (T, BT), (BT * HQ, 1), (i_t * BT, 0), (BT, BT), (1, 0))\ntl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "nvshmem_device_producer_all_gather_2d_put_block_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['rank', 'local_world_size', 'world_size'])"
    ],
    "args": [
      {
        "name": "ag_buffer_ptr",
        "annotation": null
      },
      {
        "name": "signal_buffer_ptr",
        "annotation": null
      },
      {
        "name": "elem_per_rank",
        "annotation": null
      },
      {
        "name": "size_per_elem",
        "annotation": null
      },
      {
        "name": "signal_target",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "local_world_size",
        "annotation": null
      },
      {
        "name": "world_size",
        "annotation": null
      },
      {
        "name": "DISPATCH_BLOCK_NUM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEND_BLOCK_NUM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def nvshmem_device_producer_all_gather_2d_put_block_kernel(",
      "    ag_buffer_ptr,",
      "    signal_buffer_ptr,",
      "    elem_per_rank,",
      "    size_per_elem,",
      "    signal_target,",
      "    rank,",
      "    local_world_size,",
      "    world_size,",
      "    DISPATCH_BLOCK_NUM: tl.constexpr,",
      "    SEND_BLOCK_NUM: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    thread_idx = tid(axis=0)",
      "",
      "    n_nodes = world_size // local_world_size",
      "    n_nodes = world_size // local_world_size",
      "    local_rank = rank % local_world_size",
      "    node_rank = rank // local_world_size",
      "",
      "    if pid < DISPATCH_BLOCK_NUM:",
      "        peer = (local_rank + pid + 1) % local_world_size + node_rank * local_world_size",
      "        for i in range(n_nodes):",
      "            segment = local_rank + ((node_rank + i) % n_nodes) * local_world_size",
      "            if thread_idx == 0:",
      "                libshmem_device.signal_wait_until(",
      "                    signal_buffer_ptr + segment,",
      "                    libshmem_device.NVSHMEM_CMP_GE,",
      "                    signal_target,",
      "                )",
      "            __syncthreads()",
      "            libshmem_device.putmem_signal_block(",
      "                ag_buffer_ptr + segment * elem_per_rank,",
      "                ag_buffer_ptr + segment * elem_per_rank,",
      "                elem_per_rank * size_per_elem,",
      "                signal_buffer_ptr + segment,",
      "                signal_target,",
      "                libshmem_device.NVSHMEM_SIGNAL_SET,",
      "                peer,",
      "            )",
      "    else:",
      "        if thread_idx == 0:",
      "            libshmem_device.signal_wait_until(",
      "                signal_buffer_ptr + rank,",
      "                libshmem_device.NVSHMEM_CMP_GE,",
      "                signal_target,",
      "            )",
      "        __syncthreads()",
      "        global_send_pid = pid % SEND_BLOCK_NUM + 1",
      "        peer = local_rank + (node_rank + global_send_pid) % n_nodes * local_world_size",
      "        libshmem_device.putmem_signal_block(",
      "            ag_buffer_ptr + rank * elem_per_rank,",
      "            ag_buffer_ptr + rank * elem_per_rank,",
      "            elem_per_rank * size_per_elem,",
      "            signal_buffer_ptr + rank,",
      "            signal_target,",
      "            libshmem_device.NVSHMEM_SIGNAL_SET,",
      "            peer,",
      "        )"
    ],
    "file": "codes/36.py",
    "header": "def nvshmem_device_producer_all_gather_2d_put_block_kernel(ag_buffer_ptr, signal_buffer_ptr, elem_per_rank, size_per_elem, signal_target, rank, local_world_size, world_size, DISPATCH_BLOCK_NUM: tl.constexpr, SEND_BLOCK_NUM: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nthread_idx = tid(axis=0)\nn_nodes = world_size // local_world_size\nn_nodes = world_size // local_world_size\nlocal_rank = rank % local_world_size\nnode_rank = rank // local_world_size\nif pid < DISPATCH_BLOCK_NUM:\n    peer = (local_rank + pid + 1) % local_world_size + node_rank * local_world_size\n    for i in range(n_nodes):\n        segment = local_rank + (node_rank + i) % n_nodes * local_world_size\n        if thread_idx == 0:\n            libshmem_device.signal_wait_until(signal_buffer_ptr + segment, libshmem_device.NVSHMEM_CMP_GE, signal_target)\n        __syncthreads()\n        libshmem_device.putmem_signal_block(ag_buffer_ptr + segment * elem_per_rank, ag_buffer_ptr + segment * elem_per_rank, elem_per_rank * size_per_elem, signal_buffer_ptr + segment, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)\nelse:\n    if thread_idx == 0:\n        libshmem_device.signal_wait_until(signal_buffer_ptr + rank, libshmem_device.NVSHMEM_CMP_GE, signal_target)\n    __syncthreads()\n    global_send_pid = pid % SEND_BLOCK_NUM + 1\n    peer = local_rank + (node_rank + global_send_pid) % n_nodes * local_world_size\n    libshmem_device.putmem_signal_block(ag_buffer_ptr + rank * elem_per_rank, ag_buffer_ptr + rank * elem_per_rank, elem_per_rank * size_per_elem, signal_buffer_ptr + rank, signal_target, libshmem_device.NVSHMEM_SIGNAL_SET, peer)"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'] == 0, 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size'] == 0, 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size'] == 0})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "state_gate_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_n_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    state_gate_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    x_size: tl.int32,",
      "    input_batch_stride: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    weight_n_stride: tl.int32,",
      "    weight_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_m_blocks = tl.cdiv(m_size, m_block_size)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    num_blocks = num_m_blocks * num_x_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    m_block = block // num_x_blocks",
      "    x_block = block % num_x_blocks",
      "    m_offset = m_block * m_block_size",
      "    x_offset = x_block * x_block_size",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr + batch * m_size * x_size,",
      "        shape=(m_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(m_offset, x_offset),",
      "        block_shape=(m_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    state_block_ptr = tl.make_block_ptr(",
      "        state_gate_ptr + batch * m_size * n_size,",
      "        shape=(m_size, n_size),",
      "        strides=(n_size, 1),",
      "        offsets=(m_offset, x_offset),",
      "        block_shape=(m_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    gate_block_ptr = tl.make_block_ptr(",
      "        state_gate_ptr + batch * m_size * n_size,",
      "        shape=(m_size, n_size),",
      "        strides=(n_size, 1),",
      "        offsets=(m_offset, x_offset + x_size),",
      "        block_shape=(m_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    state = language.Linear.forward(",
      "        input_ptr + batch * input_batch_stride,",
      "        weight_ptr,",
      "        bias_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        x_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        x_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_x_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "    gate = language.Linear.forward(",
      "        input_ptr + batch * input_batch_stride,",
      "        weight_ptr,",
      "        bias_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        x_offset + x_size,",
      "        use_accelerator,",
      "        m_block_size,",
      "        x_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_x_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "    output = state * language.math.GELU.forward(gate)",
      "",
      "    if require_m_boundary_check & require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype))",
      "        tl.store(state_block_ptr, state.to(dtype))",
      "        tl.store(gate_block_ptr, gate.to(dtype))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))",
      "        tl.store(state_block_ptr, state.to(dtype), boundary_check=(0, 1))",
      "        tl.store(gate_block_ptr, gate.to(dtype), boundary_check=(0, 1))"
    ],
    "file": "codes/516.py",
    "header": "def forward(output_ptr: tl.tensor, state_gate_ptr: tl.tensor, input_ptr: tl.tensor, weight_ptr: tl.tensor, bias_ptr: tl.tensor, m_size: tl.int32, n_size: tl.int32, k_size: tl.int32, x_size: tl.int32, input_batch_stride: tl.int32, input_m_stride: tl.int32, input_k_stride: tl.int32, weight_n_stride: tl.int32, weight_k_stride: tl.int32, use_accelerator: tl.constexpr, dtype: tl.constexpr, m_block_size: tl.constexpr, k_block_size: tl.constexpr, x_block_size: tl.constexpr, require_m_boundary_check: tl.constexpr, require_k_boundary_check: tl.constexpr, require_x_boundary_check: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nnum_m_blocks = tl.cdiv(m_size, m_block_size)\nnum_x_blocks = tl.cdiv(x_size, x_block_size)\nnum_blocks = num_m_blocks * num_x_blocks\nbatch = pid // num_blocks\nblock = pid % num_blocks\nm_block = block // num_x_blocks\nx_block = block % num_x_blocks\nm_offset = m_block * m_block_size\nx_offset = x_block * x_block_size\noutput_block_ptr = tl.make_block_ptr(output_ptr + batch * m_size * x_size, shape=(m_size, x_size), strides=(x_size, 1), offsets=(m_offset, x_offset), block_shape=(m_block_size, x_block_size), order=(1, 0))\nstate_block_ptr = tl.make_block_ptr(state_gate_ptr + batch * m_size * n_size, shape=(m_size, n_size), strides=(n_size, 1), offsets=(m_offset, x_offset), block_shape=(m_block_size, x_block_size), order=(1, 0))\ngate_block_ptr = tl.make_block_ptr(state_gate_ptr + batch * m_size * n_size, shape=(m_size, n_size), strides=(n_size, 1), offsets=(m_offset, x_offset + x_size), block_shape=(m_block_size, x_block_size), order=(1, 0))\nstate = language.Linear.forward(input_ptr + batch * input_batch_stride, weight_ptr, bias_ptr, m_size, n_size, k_size, input_m_stride, input_k_stride, weight_n_stride, weight_k_stride, m_offset, x_offset, use_accelerator, m_block_size, x_block_size, k_block_size, require_m_boundary_check, require_x_boundary_check, require_k_boundary_check, dtype)\ngate = language.Linear.forward(input_ptr + batch * input_batch_stride, weight_ptr, bias_ptr, m_size, n_size, k_size, input_m_stride, input_k_stride, weight_n_stride, weight_k_stride, m_offset, x_offset + x_size, use_accelerator, m_block_size, x_block_size, k_block_size, require_m_boundary_check, require_x_boundary_check, require_k_boundary_check, dtype)\noutput = state * language.math.GELU.forward(gate)\nif require_m_boundary_check & require_x_boundary_check:\n    tl.store(output_block_ptr, output.to(dtype))\n    tl.store(state_block_ptr, state.to(dtype))\n    tl.store(gate_block_ptr, gate.to(dtype))\nelse:\n    tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))\n    tl.store(state_block_ptr, state.to(dtype), boundary_check=(0, 1))\n    tl.store(gate_block_ptr, gate.to(dtype), boundary_check=(0, 1))"
  },
  {
    "name": "packunpack_sequence_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [4, 8, 16, 32]], key=['D', 'PADDING_SIDE', 'PACK'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDING_SIDE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PACK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def packunpack_sequence_kernel(",
      "    x,",
      "    y,",
      "    cu_seqlens,",
      "    S,",
      "    D,",
      "    BD: tl.constexpr,",
      "    PADDING_SIDE: tl.constexpr,",
      "    PACK: tl.constexpr,",
      "):",
      "    i_d, i_s, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    bos, eos = tl.load(cu_seqlens + i_b), tl.load(cu_seqlens + i_b + 1)",
      "",
      "    T = eos - bos",
      "    if PADDING_SIDE == \"left\":",
      "        NP = S - T",
      "        if i_s < NP:",
      "            return",
      "        i_t = bos + (i_s - NP)",
      "    else:",
      "        if i_s >= T:",
      "            return",
      "        i_t = bos + i_s",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    if PACK:",
      "        b_x = tl.load(x + (i_b * S + i_s) * D + o_d, mask=mask)",
      "        tl.store(y + i_t * D + o_d, b_x, mask=mask)",
      "    else:",
      "        b_x = tl.load(x + i_t * D + o_d, mask=mask)",
      "        tl.store(y + (i_b * S + i_s) * D + o_d, b_x, mask=mask)"
    ],
    "file": "codes/433.py",
    "header": "def packunpack_sequence_kernel(x, y, cu_seqlens, S, D, BD: tl.constexpr, PADDING_SIDE: tl.constexpr, PACK: tl.constexpr):",
    "body": "i_d, i_s, i_b = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\nbos, eos = (tl.load(cu_seqlens + i_b), tl.load(cu_seqlens + i_b + 1))\nT = eos - bos\nif PADDING_SIDE == 'left':\n    NP = S - T\n    if i_s < NP:\n        return\n    i_t = bos + (i_s - NP)\nelse:\n    if i_s >= T:\n        return\n    i_t = bos + i_s\no_d = i_d * BD + tl.arange(0, BD)\nmask = o_d < D\nif PACK:\n    b_x = tl.load(x + (i_b * S + i_s) * D + o_d, mask=mask)\n    tl.store(y + i_t * D + o_d, b_x, mask=mask)\nelse:\n    b_x = tl.load(x + i_t * D + o_d, mask=mask)\n    tl.store(y + (i_b * S + i_s) * D + o_d, b_x, mask=mask)"
  },
  {
    "name": "matmul_kernel_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            start_m = pid_m * BLOCK_SIZE_M",
      "            start_n = pid_n * BLOCK_SIZE_N",
      "            offs_am = tl.arange(0, BLOCK_SIZE_M)",
      "            offs_bn = tl.arange(0, BLOCK_SIZE_N)",
      "            offs_am = tl.where(offs_am < M - start_m, offs_am, 0)",
      "            offs_bn = tl.where(offs_bn < N - start_n, offs_bn, 0)",
      "            offs_am = tl.max_contiguous(",
      "                tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M",
      "            )",
      "            offs_bn = tl.max_contiguous(",
      "                tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N",
      "            )",
      "        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "        a = tl.load(",
      "            a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0",
      "        )",
      "        b = tl.load(",
      "            b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0",
      "        )",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "            if c_ptr.dtype == tl.float8e4nv:",
      "                c = accumulator.to(tl.float8e4nv)",
      "            else:",
      "                c = accumulator.to(tl.float16)",
      "            tl.store(c_ptrs, c, mask=c_mask)",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/640.py",
    "header": "def matmul_kernel_persistent(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, NUM_SMS: tl.constexpr):",
    "body": "start_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\noffs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\npid_m = 0\npid_n = 0\noffs_am = tl.arange(0, BLOCK_SIZE_M)\noffs_bn = tl.arange(0, BLOCK_SIZE_N)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        start_m = pid_m * BLOCK_SIZE_M\n        start_n = pid_n * BLOCK_SIZE_N\n        offs_am = tl.arange(0, BLOCK_SIZE_M)\n        offs_bn = tl.arange(0, BLOCK_SIZE_N)\n        offs_am = tl.where(offs_am < M - start_m, offs_am, 0)\n        offs_bn = tl.where(offs_bn < N - start_n, offs_bn, 0)\n        offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)\n        offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n    a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    if ki == k_tiles - 1:\n        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n        if c_ptr.dtype == tl.float8e4nv:\n            c = accumulator.to(tl.float8e4nv)\n        else:\n            c = accumulator.to(tl.float16)\n        tl.store(c_ptrs, c, mask=c_mask)\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "matmul_kernel_tma_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc_ptr",
        "annotation": null
      },
      {
        "name": "b_desc_ptr",
        "annotation": null
      },
      {
        "name": "c_desc_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_tma_persistent(",
      "    a_desc_ptr,",
      "    b_desc_ptr,",
      "    c_desc_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    FP8_OUTPUT: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = tl._experimental_descriptor_load(",
      "            a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype",
      "        )",
      "        b = tl._experimental_descriptor_load(",
      "            b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype",
      "        )",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            c = accumulator.to(dtype)",
      "",
      "            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/640.py",
    "header": "def matmul_kernel_tma_persistent(a_desc_ptr, b_desc_ptr, c_desc_ptr, M, N, K, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, FP8_OUTPUT: tl.constexpr, NUM_SMS: tl.constexpr):",
    "body": "dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\npid_m = 0\npid_n = 0\noffs_am = 0\noffs_bn = 0\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        group_id = tile_id // num_pid_in_group\n        first_pid_m = group_id * GROUP_SIZE_M\n        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n        pid_m = first_pid_m + tile_id % group_size_m\n        pid_n = tile_id % num_pid_in_group // group_size_m\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n    offs_k = ki * BLOCK_SIZE_K\n    a = tl._experimental_descriptor_load(a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype)\n    b = tl._experimental_descriptor_load(b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype)\n    accumulator = tl.dot(a, b.T, accumulator)\n    if ki == k_tiles - 1:\n        c = accumulator.to(dtype)\n        tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "streamk_gemm",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=tuning_configs, key=['M', 'N', 'K'])",
      "@triton.heuristics(values={'EVEN_M': lambda args: args['M'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['N'] % args['BLOCK_N'] == 0, 'EVEN_K': lambda args: args['K'] % args['BLOCK_K'] == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "bias_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_bias_m",
        "annotation": null
      },
      {
        "name": "stride_bias_n",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_XCDS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STREAMK_TILES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_BUFFER_OPS_ASSUMES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def streamk_gemm(",
      "    A,",
      "    B,",
      "    C,",
      "    bias_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_bias_m,",
      "    stride_bias_n,",
      "    stride_cm,",
      "    stride_cn,",
      "    HAS_BIAS: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    NUM_XCDS: tl.constexpr,",
      "    STREAMK_TILES: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,",
      "):",
      "    if ENABLE_BUFFER_OPS_ASSUMES:",
      "        tl.assume(M >= 0)",
      "        tl.assume(N >= 0)",
      "        tl.assume(K >= 0)",
      "        tl.assume(stride_am >= 0)",
      "        tl.assume(stride_ak >= 0)",
      "        tl.assume(stride_bn >= 0)",
      "        tl.assume(stride_bk >= 0)",
      "        tl.assume(stride_cm >= 0)",
      "        tl.assume(stride_cn >= 0)",
      "    if stride_bias_m:",
      "        tl.assume(stride_bias_m >= 0)",
      "    if stride_bias_n:",
      "        tl.assume(stride_bias_n >= 0)",
      "",
      "    pid = tl.program_id(0)",
      "    if NUM_XCDS != 1:",
      "        pid = (pid % NUM_XCDS) * (NUM_SMS // NUM_XCDS) + (pid // NUM_XCDS)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    iters_per_tile = tl.cdiv(K, BLOCK_K)",
      "    total_tiles = num_pid_m * num_pid_n",
      "    total_full_tiles = total_tiles - STREAMK_TILES",
      "",
      "    acc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32",
      "    for tile_id in range(pid, total_full_tiles, NUM_SMS):",
      "        num_pid_in_group = GROUP_M * num_pid_n",
      "        group_id = tile_id // num_pid_in_group",
      "        first_pid_m = group_id * GROUP_M",
      "        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)",
      "        pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "        rk = tl.arange(0, BLOCK_K)",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "",
      "        loop_k = tl.cdiv(K, BLOCK_K)",
      "        if not EVEN_K:",
      "            loop_k -= 1",
      "",
      "        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)",
      "",
      "        if HAS_BIAS:",
      "            mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "            bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m",
      "            bias = tl.load(",
      "                bias_ptr + (tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N))),",
      "                mask=mask,",
      "            ).to(acc.dtype)",
      "",
      "        for k in range(0, loop_k):",
      "            a = tl.load(tl.multiple_of(A_BASE, (1, 16)))",
      "            b = tl.load(tl.multiple_of(B_BASE, (16, 1)))",
      "            acc += tl.dot(a, b)",
      "            A_BASE += BLOCK_K * stride_ak",
      "            B_BASE += BLOCK_K * stride_bk",
      "",
      "        if not EVEN_K:",
      "            k = loop_k",
      "            rk = k * BLOCK_K + tl.arange(0, BLOCK_K)",
      "            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "            B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "            A_BASE = tl.multiple_of(A_BASE, (1, 16))",
      "            B_BASE = tl.multiple_of(B_BASE, (16, 1))",
      "            a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)",
      "            b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)",
      "            acc += tl.dot(a, b)",
      "",
      "        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn",
      "        mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "",
      "        if HAS_BIAS:",
      "            acc += bias",
      "",
      "        c = acc.to(C.type.element_ty)",
      "        tl.store(C_, c, mask=mask)",
      "",
      "    tl.assume(pid >= 0)",
      "    total_streamk_iters = STREAMK_TILES * iters_per_tile",
      "    streamk_iters_pcu = total_streamk_iters // NUM_SMS",
      "    streamk_remainder_iters = total_streamk_iters % NUM_SMS",
      "",
      "    start_iter = (",
      "        total_full_tiles * iters_per_tile",
      "        + pid * streamk_iters_pcu",
      "        + tl.minimum(pid, streamk_remainder_iters)",
      "    )",
      "",
      "    last_iter = (",
      "        total_full_tiles * iters_per_tile",
      "        + (pid + 1) * streamk_iters_pcu",
      "        + tl.minimum(pid + 1, streamk_remainder_iters)",
      "    )",
      "    while start_iter < last_iter:",
      "        remainder = start_iter % iters_per_tile",
      "        tile_iter_end = start_iter + (iters_per_tile - remainder)",
      "        tile_id = start_iter // iters_per_tile",
      "        end_iter = tl.minimum(tile_iter_end, last_iter)",
      "",
      "        num_pid_in_group = GROUP_M * num_pid_n",
      "        group_id = tile_id // num_pid_in_group",
      "        first_pid_m = group_id * GROUP_M",
      "        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)",
      "        pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) % M",
      "        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N) % N",
      "        rk = tl.arange(0, BLOCK_K)",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        A_BASE = (",
      "            A",
      "            + rm[:, None] * stride_am",
      "            + rk[None, :] * stride_ak",
      "            + BLOCK_K * stride_ak * remainder",
      "        )",
      "        B_BASE = (",
      "            B",
      "            + rk[:, None] * stride_bk",
      "            + rn[None, :] * stride_bn",
      "            + BLOCK_K * stride_bk * remainder",
      "        )",
      "        A_BASE = tl.multiple_of(A_BASE, (1, 16))",
      "        B_BASE = tl.multiple_of(B_BASE, (16, 1))",
      "",
      "        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)",
      "        if HAS_BIAS:",
      "            mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "            bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m",
      "            bias = tl.load(",
      "                bias_ptr + (tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N))),",
      "                mask=mask,",
      "            ).to(acc.dtype)",
      "        for current_iter in range(start_iter, end_iter):",
      "            if EVEN_K:",
      "                if EVEN_M:",
      "                    a = tl.load(A_BASE)",
      "                else:",
      "                    mask_a = (rm < M)[:, None]",
      "                    a = tl.load(A_BASE, mask=mask_a, other=0.0)",
      "                if EVEN_N:",
      "                    b = tl.load(B_BASE)",
      "                else:",
      "                    mask_b = (rn < N)[None, :]",
      "                    b = tl.load(B_BASE, mask=mask_b, other=0.0)",
      "            else:",
      "                global_k_offset = (current_iter % iters_per_tile) * BLOCK_K",
      "                k_mask = global_k_offset + rk < K",
      "                if EVEN_M:",
      "                    a = tl.load(A_BASE, mask=k_mask[None, :], other=0.0)",
      "                else:",
      "                    mask_a = (rm < M)[:, None]",
      "                    a = tl.load(A_BASE, mask=k_mask[None, :] & mask_a, other=0.0)",
      "                if EVEN_N:",
      "                    b = tl.load(B_BASE, mask=k_mask[:, None], other=0.0)",
      "                else:",
      "                    mask_b = (rn < N)[None, :]",
      "                    b = tl.load(B_BASE, mask=k_mask[:, None] & mask_b, other=0.0)",
      "            acc += tl.dot(a, b)",
      "            A_BASE += BLOCK_K * stride_ak",
      "            B_BASE += BLOCK_K * stride_bk",
      "",
      "        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn",
      "        mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "",
      "        if HAS_BIAS:",
      "            acc += bias",
      "",
      "        c = acc.to(C.type.element_ty)",
      "        tl.atomic_add(C_, c, mask=mask, sem=\"relaxed\")",
      "",
      "        start_iter = end_iter"
    ],
    "file": "codes/650.py",
    "header": "def streamk_gemm(A, B, C, bias_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_bias_m, stride_bias_n, stride_cm, stride_cn, HAS_BIAS: tl.constexpr, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr, GROUP_M: tl.constexpr, NUM_SMS: tl.constexpr, NUM_XCDS: tl.constexpr, STREAMK_TILES: tl.constexpr, EVEN_M: tl.constexpr, EVEN_N: tl.constexpr, EVEN_K: tl.constexpr, ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr):",
    "body": "if ENABLE_BUFFER_OPS_ASSUMES:\n    tl.assume(M >= 0)\n    tl.assume(N >= 0)\n    tl.assume(K >= 0)\n    tl.assume(stride_am >= 0)\n    tl.assume(stride_ak >= 0)\n    tl.assume(stride_bn >= 0)\n    tl.assume(stride_bk >= 0)\n    tl.assume(stride_cm >= 0)\n    tl.assume(stride_cn >= 0)\nif stride_bias_m:\n    tl.assume(stride_bias_m >= 0)\nif stride_bias_n:\n    tl.assume(stride_bias_n >= 0)\npid = tl.program_id(0)\nif NUM_XCDS != 1:\n    pid = pid % NUM_XCDS * (NUM_SMS // NUM_XCDS) + pid // NUM_XCDS\nnum_pid_m = tl.cdiv(M, BLOCK_M)\nnum_pid_n = tl.cdiv(N, BLOCK_N)\niters_per_tile = tl.cdiv(K, BLOCK_K)\ntotal_tiles = num_pid_m * num_pid_n\ntotal_full_tiles = total_tiles - STREAMK_TILES\nacc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32\nfor tile_id in range(pid, total_full_tiles, NUM_SMS):\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = tile_id // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + tile_id % num_pid_in_group % group_size_m\n    pid_n = tile_id % num_pid_in_group // group_size_m\n    rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    rk = tl.arange(0, BLOCK_K)\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n    A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n    B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n    loop_k = tl.cdiv(K, BLOCK_K)\n    if not EVEN_K:\n        loop_k -= 1\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    if HAS_BIAS:\n        mask = (rm < M)[:, None] & (rn < N)[None, :]\n        bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m\n        bias = tl.load(bias_ptr + tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N)), mask=mask).to(acc.dtype)\n    for k in range(0, loop_k):\n        a = tl.load(tl.multiple_of(A_BASE, (1, 16)))\n        b = tl.load(tl.multiple_of(B_BASE, (16, 1)))\n        acc += tl.dot(a, b)\n        A_BASE += BLOCK_K * stride_ak\n        B_BASE += BLOCK_K * stride_bk\n    if not EVEN_K:\n        k = loop_k\n        rk = k * BLOCK_K + tl.arange(0, BLOCK_K)\n        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n        A_BASE = tl.multiple_of(A_BASE, (1, 16))\n        B_BASE = tl.multiple_of(B_BASE, (16, 1))\n        a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)\n        b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)\n        acc += tl.dot(a, b)\n    rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n    C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if HAS_BIAS:\n        acc += bias\n    c = acc.to(C.type.element_ty)\n    tl.store(C_, c, mask=mask)\ntl.assume(pid >= 0)\ntotal_streamk_iters = STREAMK_TILES * iters_per_tile\nstreamk_iters_pcu = total_streamk_iters // NUM_SMS\nstreamk_remainder_iters = total_streamk_iters % NUM_SMS\nstart_iter = total_full_tiles * iters_per_tile + pid * streamk_iters_pcu + tl.minimum(pid, streamk_remainder_iters)\nlast_iter = total_full_tiles * iters_per_tile + (pid + 1) * streamk_iters_pcu + tl.minimum(pid + 1, streamk_remainder_iters)\nwhile start_iter < last_iter:\n    remainder = start_iter % iters_per_tile\n    tile_iter_end = start_iter + (iters_per_tile - remainder)\n    tile_id = start_iter // iters_per_tile\n    end_iter = tl.minimum(tile_iter_end, last_iter)\n    num_pid_in_group = GROUP_M * num_pid_n\n    group_id = tile_id // num_pid_in_group\n    first_pid_m = group_id * GROUP_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)\n    pid_m = first_pid_m + tile_id % num_pid_in_group % group_size_m\n    pid_n = tile_id % num_pid_in_group // group_size_m\n    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) % M\n    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N) % N\n    rk = tl.arange(0, BLOCK_K)\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n    A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak + BLOCK_K * stride_ak * remainder\n    B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn + BLOCK_K * stride_bk * remainder\n    A_BASE = tl.multiple_of(A_BASE, (1, 16))\n    B_BASE = tl.multiple_of(B_BASE, (16, 1))\n    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)\n    if HAS_BIAS:\n        mask = (rm < M)[:, None] & (rn < N)[None, :]\n        bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m\n        bias = tl.load(bias_ptr + tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N)), mask=mask).to(acc.dtype)\n    for current_iter in range(start_iter, end_iter):\n        if EVEN_K:\n            if EVEN_M:\n                a = tl.load(A_BASE)\n            else:\n                mask_a = (rm < M)[:, None]\n                a = tl.load(A_BASE, mask=mask_a, other=0.0)\n            if EVEN_N:\n                b = tl.load(B_BASE)\n            else:\n                mask_b = (rn < N)[None, :]\n                b = tl.load(B_BASE, mask=mask_b, other=0.0)\n        else:\n            global_k_offset = current_iter % iters_per_tile * BLOCK_K\n            k_mask = global_k_offset + rk < K\n            if EVEN_M:\n                a = tl.load(A_BASE, mask=k_mask[None, :], other=0.0)\n            else:\n                mask_a = (rm < M)[:, None]\n                a = tl.load(A_BASE, mask=k_mask[None, :] & mask_a, other=0.0)\n            if EVEN_N:\n                b = tl.load(B_BASE, mask=k_mask[:, None], other=0.0)\n            else:\n                mask_b = (rn < N)[None, :]\n                b = tl.load(B_BASE, mask=k_mask[:, None] & mask_b, other=0.0)\n        acc += tl.dot(a, b)\n        A_BASE += BLOCK_K * stride_ak\n        B_BASE += BLOCK_K * stride_bk\n    rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M\n    rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)\n    C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    mask = (rm < M)[:, None] & (rn < N)[None, :]\n    if HAS_BIAS:\n        acc += bias\n    c = acc.to(C.type.element_ty)\n    tl.atomic_add(C_, c, mask=mask, sem='relaxed')\n    start_iter = end_iter"
  },
  {
    "name": "kernel_gemm_rs_producer_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata, repr=_gemm_rs_persistent_repr)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "barrier_ptr",
        "annotation": null
      },
      {
        "name": "counter_ptr",
        "annotation": null
      },
      {
        "name": "LOCAL_WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EPILOGUE_SUBTILE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_gemm_rs_producer_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    barrier_ptr,",
      "    counter_ptr,",
      "    LOCAL_WORLD_SIZE: tl.constexpr,",
      "    WORLD_SIZE: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    EPILOGUE_SUBTILE: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "",
      "    rank = dl.rank()",
      "    dtype = c_ptr.dtype.element_ty",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "    NNODES = WORLD_SIZE // LOCAL_WORLD_SIZE",
      "",
      "    a_desc = tl.make_tensor_descriptor(",
      "        a_ptr,",
      "        shape=[M, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K],",
      "    )",
      "    b_desc = tl.make_tensor_descriptor(",
      "        b_ptr,",
      "        shape=[N, K],",
      "        strides=[K, 1],",
      "        block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K],",
      "    )",
      "    c_desc = tl.make_tensor_descriptor(",
      "        c_ptr,",
      "        shape=[M, N],",
      "        strides=[N, 1],",
      "        block_shape=[",
      "            BLOCK_SIZE_M,",
      "            BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2,",
      "        ],",
      "    )",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    M_per_rank = M // WORLD_SIZE",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    pid_m_offset = (rank + 1) * M_per_rank // BLOCK_SIZE_M",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            pid_m, pid_n = swizzle_2d(tile_id, num_pid_m, num_pid_n, GROUP_SIZE_M)",
      "            if NNODES != 1:",
      "                pid_m = threadblock_swizzle_gemm_reduce_scatter_kernel(",
      "                    pid_m, M, rank, WORLD_SIZE, NNODES, BLOCK_SIZE_M",
      "                )",
      "            else:",
      "                pid_m = (pid_m + pid_m_offset) % num_pid_m",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = a_desc.load([offs_am, offs_k])",
      "        b = b_desc.load([offs_bn, offs_k])",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            if EPILOGUE_SUBTILE:",
      "                acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))",
      "                acc = tl.permute(acc, (0, 2, 1))",
      "                acc0, acc1 = tl.split(acc)",
      "                c0 = acc0.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c0)",
      "                c1 = acc1.to(dtype)",
      "                c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)",
      "            else:",
      "                c = accumulator.to(dtype)",
      "                c_desc.store([offs_am, offs_bn], c)",
      "",
      "            counter_start = offs_am // M_per_rank",
      "            counter_end = (offs_am + BLOCK_SIZE_M - 1) // M_per_rank",
      "            counter_end = min(counter_end, WORLD_SIZE - 1)",
      "            for counter_id in range(counter_start, counter_end + 1):",
      "                m_start = M_per_rank * counter_id",
      "                m_end = M_per_rank * (counter_id + 1) - 1",
      "                tiled_m_start = m_start // BLOCK_SIZE_M",
      "                tiled_m_end = m_end // BLOCK_SIZE_M",
      "                tiled_m_size = tiled_m_end - tiled_m_start + 1",
      "                val = tl.atomic_add(",
      "                    counter_ptr + counter_id, 1, sem=\"release\", scope=\"gpu\"",
      "                )",
      "                if val == tiled_m_size * num_pid_n - 1:",
      "                    dl.notify(",
      "                        barrier_ptr + counter_id, rank, signal=1, comm_scope=\"gpu\"",
      "                    )",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "codes/43.py",
    "header": "def kernel_gemm_rs_producer_persistent(a_ptr, b_ptr, c_ptr, M, N, K, barrier_ptr, counter_ptr, LOCAL_WORLD_SIZE: tl.constexpr, WORLD_SIZE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, EPILOGUE_SUBTILE: tl.constexpr, NUM_SMS: tl.constexpr):",
    "body": "rank = dl.rank()\ndtype = c_ptr.dtype.element_ty\nstart_pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nk_tiles = tl.cdiv(K, BLOCK_SIZE_K)\nnum_tiles = num_pid_m * num_pid_n\nNNODES = WORLD_SIZE // LOCAL_WORLD_SIZE\na_desc = tl.make_tensor_descriptor(a_ptr, shape=[M, K], strides=[K, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_K])\nb_desc = tl.make_tensor_descriptor(b_ptr, shape=[N, K], strides=[K, 1], block_shape=[BLOCK_SIZE_N, BLOCK_SIZE_K])\nc_desc = tl.make_tensor_descriptor(c_ptr, shape=[M, N], strides=[N, 1], block_shape=[BLOCK_SIZE_M, BLOCK_SIZE_N if not EPILOGUE_SUBTILE else BLOCK_SIZE_N // 2])\ntiles_per_SM = num_tiles // NUM_SMS\nif start_pid < num_tiles % NUM_SMS:\n    tiles_per_SM += 1\ntile_id = start_pid - NUM_SMS\nki = -1\npid_m = 0\npid_n = 0\noffs_am = 0\noffs_bn = 0\nM_per_rank = M // WORLD_SIZE\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\npid_m_offset = (rank + 1) * M_per_rank // BLOCK_SIZE_M\nfor _ in range(0, k_tiles * tiles_per_SM):\n    ki = tl.where(ki == k_tiles - 1, 0, ki + 1)\n    if ki == 0:\n        tile_id += NUM_SMS\n        pid_m, pid_n = swizzle_2d(tile_id, num_pid_m, num_pid_n, GROUP_SIZE_M)\n        if NNODES != 1:\n            pid_m = threadblock_swizzle_gemm_reduce_scatter_kernel(pid_m, M, rank, WORLD_SIZE, NNODES, BLOCK_SIZE_M)\n        else:\n            pid_m = (pid_m + pid_m_offset) % num_pid_m\n        offs_am = pid_m * BLOCK_SIZE_M\n        offs_bn = pid_n * BLOCK_SIZE_N\n    offs_k = ki * BLOCK_SIZE_K\n    a = a_desc.load([offs_am, offs_k])\n    b = b_desc.load([offs_bn, offs_k])\n    accumulator = tl.dot(a, b.T, accumulator)\n    if ki == k_tiles - 1:\n        if EPILOGUE_SUBTILE:\n            acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))\n            acc = tl.permute(acc, (0, 2, 1))\n            acc0, acc1 = tl.split(acc)\n            c0 = acc0.to(dtype)\n            c_desc.store([offs_am, offs_bn], c0)\n            c1 = acc1.to(dtype)\n            c_desc.store([offs_am, offs_bn + BLOCK_SIZE_N // 2], c1)\n        else:\n            c = accumulator.to(dtype)\n            c_desc.store([offs_am, offs_bn], c)\n        counter_start = offs_am // M_per_rank\n        counter_end = (offs_am + BLOCK_SIZE_M - 1) // M_per_rank\n        counter_end = min(counter_end, WORLD_SIZE - 1)\n        for counter_id in range(counter_start, counter_end + 1):\n            m_start = M_per_rank * counter_id\n            m_end = M_per_rank * (counter_id + 1) - 1\n            tiled_m_start = m_start // BLOCK_SIZE_M\n            tiled_m_end = m_end // BLOCK_SIZE_M\n            tiled_m_size = tiled_m_end - tiled_m_start + 1\n            val = tl.atomic_add(counter_ptr + counter_id, 1, sem='release', scope='gpu')\n            if val == tiled_m_size * num_pid_n - 1:\n                dl.notify(barrier_ptr + counter_id, rank, signal=1, comm_scope='gpu')\n        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
  },
  {
    "name": "kernel_gemm_rs_producer_non_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata, repr=_gemm_rs_non_persistent_repr)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "barrier_ptr",
        "annotation": null
      },
      {
        "name": "counter_ptr",
        "annotation": null
      },
      {
        "name": "LOCAL_WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "WORLD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_gemm_rs_producer_non_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    barrier_ptr,",
      "    counter_ptr,",
      "    LOCAL_WORLD_SIZE: tl.constexpr,",
      "    WORLD_SIZE: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    tl.static_assert(a_ptr.dtype.is_ptr(), \"A should be a pointer\")",
      "    tl.static_assert(b_ptr.dtype.is_ptr(), \"B should be a pointer\")",
      "    tl.static_assert(c_ptr.dtype.is_ptr(), \"C should be a pointer\")",
      "    a_dtype = a_ptr.dtype.element_ty",
      "    b_dtype = b_ptr.dtype.element_ty",
      "    tl.static_assert(a_dtype == b_dtype, \"A and B should have the same dtype\")",
      "",
      "    rank = dl.rank()",
      "    NNODES = WORLD_SIZE // LOCAL_WORLD_SIZE",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    M_per_rank = M // WORLD_SIZE",
      "",
      "    pid_m, pid_n = swizzle_2d(pid, num_pid_m, num_pid_n, GROUP_SIZE_M)",
      "",
      "    if NNODES != 1:",
      "        pid_m = threadblock_swizzle_gemm_reduce_scatter_kernel(",
      "            pid_m, M, rank, WORLD_SIZE, NNODES, BLOCK_SIZE_M",
      "        )",
      "    else:",
      "        pid_m_offset = (rank + 1) * M_per_rank // BLOCK_SIZE_M",
      "        pid_m = (pid_m + pid_m_offset) % num_pid_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    if a_ptr.dtype.element_ty == tl.int8:",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)",
      "    else:",
      "        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    out_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "",
      "    tl.store(c_ptrs, accumulator, mask=out_mask)",
      "",
      "    segment_start = pid_m * BLOCK_SIZE_M // M_per_rank",
      "    segment_end = (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) // M_per_rank",
      "    __syncthreads()",
      "    segment = segment_start + tid(axis=0)",
      "    if segment <= segment_end:",
      "        m_start = M_per_rank * segment",
      "        m_end = M_per_rank * (segment + 1) - 1",
      "        tiled_m_start = m_start // BLOCK_SIZE_M",
      "        tiled_m_end = m_end // BLOCK_SIZE_M",
      "        tiled_m_size = tiled_m_end - tiled_m_start + 1",
      "        val = atomic_add(counter_ptr + segment, 1, semantic=\"release\", scope=\"gpu\")",
      "        if val == num_pid_n * tiled_m_size - 1:",
      "            atomic_add(barrier_ptr + segment, 1, semantic=\"release\", scope=\"gpu\")"
    ],
    "file": "codes/43.py",
    "header": "def kernel_gemm_rs_producer_non_persistent(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, barrier_ptr, counter_ptr, LOCAL_WORLD_SIZE: tl.constexpr, WORLD_SIZE: tl.constexpr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "tl.static_assert(a_ptr.dtype.is_ptr(), 'A should be a pointer')\ntl.static_assert(b_ptr.dtype.is_ptr(), 'B should be a pointer')\ntl.static_assert(c_ptr.dtype.is_ptr(), 'C should be a pointer')\na_dtype = a_ptr.dtype.element_ty\nb_dtype = b_ptr.dtype.element_ty\ntl.static_assert(a_dtype == b_dtype, 'A and B should have the same dtype')\nrank = dl.rank()\nNNODES = WORLD_SIZE // LOCAL_WORLD_SIZE\npid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nM_per_rank = M // WORLD_SIZE\npid_m, pid_n = swizzle_2d(pid, num_pid_m, num_pid_n, GROUP_SIZE_M)\nif NNODES != 1:\n    pid_m = threadblock_swizzle_gemm_reduce_scatter_kernel(pid_m, M, rank, WORLD_SIZE, NNODES, BLOCK_SIZE_M)\nelse:\n    pid_m_offset = (rank + 1) * M_per_rank // BLOCK_SIZE_M\n    pid_m = (pid_m + pid_m_offset) % num_pid_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\nif a_ptr.dtype.element_ty == tl.int8:\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)\nelse:\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator += tl.dot(a, b)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nout_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, accumulator, mask=out_mask)\nsegment_start = pid_m * BLOCK_SIZE_M // M_per_rank\nsegment_end = (min((pid_m + 1) * BLOCK_SIZE_M, M) - 1) // M_per_rank\n__syncthreads()\nsegment = segment_start + tid(axis=0)\nif segment <= segment_end:\n    m_start = M_per_rank * segment\n    m_end = M_per_rank * (segment + 1) - 1\n    tiled_m_start = m_start // BLOCK_SIZE_M\n    tiled_m_end = m_end // BLOCK_SIZE_M\n    tiled_m_size = tiled_m_end - tiled_m_start + 1\n    val = atomic_add(counter_ptr + segment, 1, semantic='release', scope='gpu')\n    if val == num_pid_n * tiled_m_size - 1:\n        atomic_add(barrier_ptr + segment, 1, semantic='release', scope='gpu')"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, K, BLOCK_SIZE_K):",
      "",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/563.py",
    "header": "def matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, K, BLOCK_SIZE_K):\n    a = tl.load(a_ptrs)\n    b = tl.load(b_ptrs)\n    accumulator += tl.dot(a, b)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nc = accumulator.to(tl.float16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "layer_norm_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "bias_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "scale_by_weight",
        "annotation": "tl.constexpr"
      },
      {
        "name": "add_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    bias_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    eps,",
      "    scale_by_weight: tl.constexpr,",
      "    add_bias: tl.constexpr,",
      "    save_stats: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "    mean = tl.sum(input, axis=1) / feat_dim",
      "    diff = tl.where(feat_mask[None, :], input - mean[:, None], 0)",
      "    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / feat_dim + eps)",
      "",
      "    if save_stats:",
      "        tl.store(mean_pointer + batch_offset, mean, mask=batch_mask)",
      "        tl.store(inv_std_pointer + batch_offset, inv_std, mask=batch_mask)",
      "",
      "    output = diff * inv_std[:, None]",
      "    if scale_by_weight:",
      "        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)",
      "        output *= weight",
      "        if add_bias:",
      "            bias = tl.load(bias_pointer + feat_offset, mask=feat_mask)",
      "            output += bias",
      "",
      "    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])"
    ],
    "file": "codes/12.py",
    "header": "def layer_norm_forward_kernel(input_pointer, weight_pointer, bias_pointer, mean_pointer, inv_std_pointer, output_pointer, batch_dim, feat_dim, input_batch_stride, input_feat_stride, output_batch_stride, output_feat_stride, eps, scale_by_weight: tl.constexpr, add_bias: tl.constexpr, save_stats: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr):",
    "body": "batch_pid = tl.program_id(axis=0)\nbatch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\nfeat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\nbatch_mask = batch_offset < batch_dim\nfeat_mask = feat_offset < feat_dim\ninput_pointer += input_batch_stride * batch_offset[:, None] + input_feat_stride * feat_offset[None, :]\noutput_pointer += output_batch_stride * batch_offset[:, None] + output_feat_stride * feat_offset[None, :]\ninput = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(tl.float32)\nmean = tl.sum(input, axis=1) / feat_dim\ndiff = tl.where(feat_mask[None, :], input - mean[:, None], 0)\ninv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / feat_dim + eps)\nif save_stats:\n    tl.store(mean_pointer + batch_offset, mean, mask=batch_mask)\n    tl.store(inv_std_pointer + batch_offset, inv_std, mask=batch_mask)\noutput = diff * inv_std[:, None]\nif scale_by_weight:\n    weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n    output *= weight\n    if add_bias:\n        bias = tl.load(bias_pointer + feat_offset, mask=feat_mask)\n        output += bias\ntl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])"
  },
  {
    "name": "layer_norm_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "weight_grad_pointer",
        "annotation": null
      },
      {
        "name": "bias_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "weight_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "bias_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "bias_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "scale_by_weight",
        "annotation": "tl.constexpr"
      },
      {
        "name": "add_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    weight_pointer,",
      "    input_grad_pointer,",
      "    weight_grad_pointer,",
      "    bias_grad_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    weight_grad_batch_stride,",
      "    weight_grad_feat_stride,",
      "    bias_grad_batch_stride,",
      "    bias_grad_feat_stride,",
      "    scale_by_weight: tl.constexpr,",
      "    add_bias: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    output_grad_pointer += (",
      "        output_grad_batch_stride * batch_offset[:, None]",
      "        + output_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_grad_pointer += (",
      "        input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    output_grad = tl.load(",
      "        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    ).to(tl.float32)",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "    mean = tl.load(mean_pointer + batch_offset, mask=batch_mask)",
      "    inv_std = tl.load(inv_std_pointer + batch_offset, mask=batch_mask)",
      "    pre_lin = (input - mean[:, None]) * inv_std[:, None]",
      "",
      "    if scale_by_weight:",
      "        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)",
      "        weight_output_grad_prod = weight * output_grad",
      "",
      "    else:",
      "        weight_output_grad_prod = output_grad",
      "",
      "    term1 = tl.sum(pre_lin * weight_output_grad_prod, axis=1) / feat_dim",
      "    term1 = pre_lin * term1[:, None]",
      "    term2 = tl.sum(weight_output_grad_prod, axis=1) / feat_dim",
      "    input_grad = inv_std[:, None] * (weight_output_grad_prod - (term1 + term2[:, None]))",
      "",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    )",
      "",
      "    if scale_by_weight:",
      "        weight_grad_pointer += (",
      "            weight_grad_batch_stride * batch_pid + weight_grad_feat_stride * feat_offset",
      "        )",
      "        tl.store(",
      "            weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0), mask=feat_mask",
      "        )",
      "",
      "        if add_bias:",
      "            bias_grad_pointer += (",
      "                bias_grad_batch_stride * batch_pid + bias_grad_feat_stride * feat_offset",
      "            )",
      "            tl.store(bias_grad_pointer, tl.sum(output_grad, axis=0), mask=feat_mask)"
    ],
    "file": "codes/12.py",
    "header": "def layer_norm_backward_kernel(output_grad_pointer, input_pointer, mean_pointer, inv_std_pointer, weight_pointer, input_grad_pointer, weight_grad_pointer, bias_grad_pointer, batch_dim, feat_dim, output_grad_batch_stride, output_grad_feat_stride, input_batch_stride, input_feat_stride, input_grad_batch_stride, input_grad_feat_stride, weight_grad_batch_stride, weight_grad_feat_stride, bias_grad_batch_stride, bias_grad_feat_stride, scale_by_weight: tl.constexpr, add_bias: tl.constexpr, BLOCK_SIZE_BATCH: tl.constexpr, BLOCK_SIZE_FEAT: tl.constexpr):",
    "body": "batch_pid = tl.program_id(axis=0)\nbatch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)\nfeat_offset = tl.arange(0, BLOCK_SIZE_FEAT)\nbatch_mask = batch_offset < batch_dim\nfeat_mask = feat_offset < feat_dim\noutput_grad_pointer += output_grad_batch_stride * batch_offset[:, None] + output_grad_feat_stride * feat_offset[None, :]\ninput_pointer += input_batch_stride * batch_offset[:, None] + input_feat_stride * feat_offset[None, :]\ninput_grad_pointer += input_grad_batch_stride * batch_offset[:, None] + input_grad_feat_stride * feat_offset[None, :]\noutput_grad = tl.load(output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(tl.float32)\ninput = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(tl.float32)\nmean = tl.load(mean_pointer + batch_offset, mask=batch_mask)\ninv_std = tl.load(inv_std_pointer + batch_offset, mask=batch_mask)\npre_lin = (input - mean[:, None]) * inv_std[:, None]\nif scale_by_weight:\n    weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)\n    weight_output_grad_prod = weight * output_grad\nelse:\n    weight_output_grad_prod = output_grad\nterm1 = tl.sum(pre_lin * weight_output_grad_prod, axis=1) / feat_dim\nterm1 = pre_lin * term1[:, None]\nterm2 = tl.sum(weight_output_grad_prod, axis=1) / feat_dim\ninput_grad = inv_std[:, None] * (weight_output_grad_prod - (term1 + term2[:, None]))\ntl.store(input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :])\nif scale_by_weight:\n    weight_grad_pointer += weight_grad_batch_stride * batch_pid + weight_grad_feat_stride * feat_offset\n    tl.store(weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0), mask=feat_mask)\n    if add_bias:\n        bias_grad_pointer += bias_grad_batch_stride * batch_pid + bias_grad_feat_stride * feat_offset\n        tl.store(bias_grad_pointer, tl.sum(output_grad, axis=0), mask=feat_mask)"
  },
  {
    "name": "bf16xbf16_matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bf16xbf16_matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/635.py",
    "header": "def bf16xbf16_matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n    accumulator = tl.dot(a, b, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nc = accumulator.to(tl.bfloat16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "bf16xint16_matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TRANSPOSE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bf16xint16_matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    TRANSPOSE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)",
      "        if TRANSPOSE:",
      "            tl.static_assert(a.dtype == tl.int16)",
      "            tl.static_assert(b.dtype == tl.bfloat16)",
      "            a_bf16 = a.to(tl.bfloat16)",
      "            b_bf16 = b",
      "        else:",
      "            tl.static_assert(a.dtype == tl.bfloat16)",
      "            tl.static_assert(b.dtype == tl.int16)",
      "            a_bf16 = a",
      "            b_bf16 = b.to(tl.bfloat16)",
      "",
      "        accumulator = tl.dot(a_bf16, b_bf16, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "codes/635.py",
    "header": "def bf16xint16_matmul_kernel(a_ptr, b_ptr, c_ptr, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, TRANSPOSE: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\nnum_pid_in_group = GROUP_SIZE_M * num_pid_n\ngroup_id = pid // num_pid_in_group\nfirst_pid_m = group_id * GROUP_SIZE_M\ngroup_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\npid_m = first_pid_m + pid % num_pid_in_group % group_size_m\npid_n = pid % num_pid_in_group // group_size_m\noffs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\noffs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\noffs_k = tl.arange(0, BLOCK_SIZE_K)\na_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\nb_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\naccumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\nfor k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n    a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n    b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)\n    if TRANSPOSE:\n        tl.static_assert(a.dtype == tl.int16)\n        tl.static_assert(b.dtype == tl.bfloat16)\n        a_bf16 = a.to(tl.bfloat16)\n        b_bf16 = b\n    else:\n        tl.static_assert(a.dtype == tl.bfloat16)\n        tl.static_assert(b.dtype == tl.int16)\n        a_bf16 = a\n        b_bf16 = b.to(tl.bfloat16)\n    accumulator = tl.dot(a_bf16, b_bf16, accumulator)\n    a_ptrs += BLOCK_SIZE_K * stride_ak\n    b_ptrs += BLOCK_SIZE_K * stride_bk\nc = accumulator.to(tl.bfloat16)\noffs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\noffs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\nc_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\nc_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\ntl.store(c_ptrs, c, mask=c_mask)"
  },
  {
    "name": "triton_jagged_sum_kernel_simple_fused_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_simple_fused_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "codes/664.py",
    "header": "def triton_jagged_sum_kernel_simple_fused_sum_then_buffer(input_ptr_values, input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_ragged), tl.load(input_ptr_offsets + (pid_ragged + 1)))\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer += tl.sum(input, axis=0)\nbuffer_view = buffer.reshape((BLOCK_SIZE_M,))\noutput_offsets = offsets_m + pid_ragged * M\noutput_mask = output_offsets < M * (pid_ragged + 1)\ntl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
  },
  {
    "name": "triton_jagged_sum_kernel_simple_fused_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_simple_fused_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "codes/664.py",
    "header": "def triton_jagged_sum_kernel_simple_fused_buffer_then_sum(input_ptr_values, input_ptr_offsets, output_ptr, M, MAX_SEQLEN, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_ragged), tl.load(input_ptr_offsets + (pid_ragged + 1)))\nfor block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):\n    block_start_ragged = ragged_start + block_pos\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\nbuffer_sum = tl.sum(buffer, axis=0)\nbuffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\noutput_offsets = offsets_m + pid_ragged * M\noutput_mask = output_offsets < M * (pid_ragged + 1)\ntl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
  },
  {
    "name": "triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "codes/664.py",
    "header": "def triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer(input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_b), tl.load(input_ptr_offsets + (pid_b + 1)))\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    input = tl.load(input_ptr_values + idxs, mask=mask, other=0)\n    buffer += tl.sum(input, axis=0)\nbuffer_view = buffer.reshape((BLOCK_SIZE_M,))\noutput_offsets = offsets_m + pid_b * M\noutput_mask = output_offsets < M * (pid_b + 1)\ntl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
  },
  {
    "name": "triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "codes/664.py",
    "header": "def triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum(input_ptr_values, input_ptr_offsets, output_ptr, M, BLOCK_SIZE_RAGGED: tl.constexpr, BLOCK_SIZE_M: tl.constexpr):",
    "body": "pid = tl.program_id(axis=0)\npid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)\npid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)\nbuffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)\nblock_start_m = pid_m * BLOCK_SIZE_M\noffsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)\nmask_m = offsets_m < M\nragged_start, ragged_end = (tl.load(input_ptr_offsets + pid_ragged), tl.load(input_ptr_offsets + (pid_ragged + 1)))\nfor block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):\n    offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)\n    mask_ragged = offsets_ragged < ragged_end\n    idxs = offsets_ragged[:, None] * M + offsets_m\n    mask = mask_ragged[:, None] & mask_m\n    buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)\nbuffer_sum = tl.sum(buffer, axis=0)\nbuffer_view = buffer_sum.reshape((BLOCK_SIZE_M,))\noutput_offsets = offsets_m + pid_ragged * M\noutput_mask = output_offsets < M * (pid_ragged + 1)\ntl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
  },
  {
    "name": "chunk_rwkv6_fwd_cumsum_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BS': BS}, num_warps=num_warps, num_stages=num_stages) for BS in [16, 32, 64] for num_warps in [4, 8, 16] for num_stages in [2, 3, 4]], key=['S', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "oi",
        "annotation": null
      },
      {
        "name": "oe",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_cumsum_kernel(",
      "    s,",
      "    oi,",
      "    oe,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_i = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0).to(tl.float32)",
      "    m_e = tl.where(o_i[:, None] > o_i[None, :], 1.0, 0.0).to(tl.float32)",
      "",
      "    p_s = tl.make_block_ptr(",
      "        s + (bos * H + i_h) * S,",
      "        (T, S),",
      "        (H * S, 1),",
      "        (i_t * BT, i_s * BS),",
      "        (BT, BS),",
      "        (1, 0),",
      "    )",
      "    p_oi = tl.make_block_ptr(",
      "        oi + (bos * H + i_h) * S,",
      "        (T, S),",
      "        (H * S, 1),",
      "        (i_t * BT, i_s * BS),",
      "        (BT, BS),",
      "        (1, 0),",
      "    )",
      "    p_oe = tl.make_block_ptr(",
      "        oe + (bos * H + i_h) * S,",
      "        (T, S),",
      "        (H * S, 1),",
      "        (i_t * BT, i_s * BS),",
      "        (BT, BS),",
      "        (1, 0),",
      "    )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "    b_oi = tl.dot(m_i, b_s)",
      "    b_oe = tl.dot(m_e, b_s)",
      "    tl.store(",
      "        p_oi,",
      "        b_oi.to(p_oi.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_oe,",
      "        b_oe.to(p_oe.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_fwd_cumsum_kernel(s, oi, oe, cu_seqlens, chunk_indices, T, H: tl.constexpr, S: tl.constexpr, BT: tl.constexpr, BS: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_s, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\no_i = tl.arange(0, BT)\nm_i = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0).to(tl.float32)\nm_e = tl.where(o_i[:, None] > o_i[None, :], 1.0, 0.0).to(tl.float32)\np_s = tl.make_block_ptr(s + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\np_oi = tl.make_block_ptr(oi + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\np_oe = tl.make_block_ptr(oe + (bos * H + i_h) * S, (T, S), (H * S, 1), (i_t * BT, i_s * BS), (BT, BS), (1, 0))\nb_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)\nb_oi = tl.dot(m_i, b_s)\nb_oe = tl.dot(m_e, b_s)\ntl.store(p_oi, b_oi.to(p_oi.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))\ntl.store(p_oe, b_oe.to(p_oe.dtype.element_ty, fp_downcast_rounding='rtne'), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BC'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_inter(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_i, i_j = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "    if i_i <= i_j:",
      "        return",
      "",
      "    m_i = i_t * BT + i_i * BC + tl.arange(0, BC) < T",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_gq = tl.make_block_ptr(",
      "            ge + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            gi + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_gq = tl.where(",
      "            m_i[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float(\"-inf\")",
      "        )",
      "        b_qg = b_q * exp(b_gq - b_gn[None, :]) * scale",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_kg = b_k * exp(b_gn[:, None] - b_gk)",
      "",
      "        b_A += tl.dot(b_qg, b_kg)",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_fwd_A_kernel_intra_sub_inter(q, k, gi, ge, A, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_i, i_j = (i_c // NC, i_c % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nif i_t * BT + i_i * BC >= T:\n    return\nif i_i <= i_j:\n    return\nm_i = i_t * BT + i_i * BC + tl.arange(0, BC) < T\nb_A = tl.zeros([BC, BC], dtype=tl.float32)\nfor i_k in range(tl.cdiv(K, BK)):\n    o_k = i_k * BK + tl.arange(0, BK)\n    m_k = o_k < K\n    p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_gq = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\n    p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n    p_gk = tl.make_block_ptr(gi + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT + i_j * BC), (BK, BC), (0, 1))\n    p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_gq = tl.where(m_i[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float('-inf'))\n    b_qg = b_q * exp(b_gq - b_gn[None, :]) * scale\n    b_k = tl.load(p_k, boundary_check=(0, 1))\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_kg = b_k * exp(b_gn[:, None] - b_gk)\n    b_A += tl.dot(b_qg, b_kg)\np_A = tl.make_block_ptr(A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\ntl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BK', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    u,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "",
      "    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (0,), (BK,), (0,))",
      "    b_u = tl.load(p_u, boundary_check=(0,))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i > j, b_A * scale, 0.0)",
      "        b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_qj += H * K",
      "        p_kj += H * K",
      "        p_gk += H * K"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra(q, k, gi, ge, u, A, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_i, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_j = i_i\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nif i_t * BT + i_i * BC >= T:\n    return\no_i = tl.arange(0, BC)\no_k = tl.arange(0, BK)\nm_k = o_k < K\nm_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\no_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\np_g = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0))\np_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\np_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\np_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_g = tl.load(p_g, boundary_check=(0, 1))\np_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (0,), (BK,), (0,))\nb_u = tl.load(p_u, boundary_check=(0,))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n    b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n    b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n    b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)\n    b_A = tl.where(o_i > j, b_A * scale, 0.0)\n    b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))\n    tl.store(A + o_A + j, b_A, mask=m_A)\n    p_qj += H * K\n    p_kj += H * K\n    p_gk += H * K"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC', 'BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    u,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_tc // NC, i_tc % NC",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "",
      "    o_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "",
      "    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK), (BK,), (0,))",
      "    b_u = tl.load(p_u, boundary_check=(0,))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i > j, b_A * scale, 0.0)",
      "        b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_qj += H * K",
      "        p_kj += H * K",
      "        p_gk += H * K"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(q, k, gi, ge, u, A, cu_seqlens, chunk_indices, scale, B: tl.constexpr, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_tc, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_t, i_i = (i_tc // NC, i_tc % NC)\ni_j = i_i\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    all = B * T\nif i_t * BT + i_i * BC >= T:\n    return\no_i = tl.arange(0, BC)\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\nm_A = i_t * BT + i_i * BC + tl.arange(0, BC) < T\no_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_g = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\np_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\np_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_g = tl.load(p_g, boundary_check=(0, 1))\np_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), i_k * BK, (BK,), (0,))\nb_u = tl.load(p_u, boundary_check=(0,))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n    b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n    b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)\n    b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)\n    b_A = tl.where(o_i > j, b_A * scale, 0.0)\n    b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))\n    tl.store(A + o_A + j, b_A, mask=m_A)\n    p_qj += H * K\n    p_kj += H * K\n    p_gk += H * K"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(",
      "    A,",
      "    A2,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    NK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_c * BC >= T:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(0, NK):",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_k * all + bos) * H * BC + i_h * BC,",
      "            (T, BC),",
      "            (H * BC, 1),",
      "            (i_t * BT + i_c * BC, 0),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        b_A += tl.load(p_A, boundary_check=(0, 1))",
      "    p_A2 = tl.make_block_ptr(",
      "        A2 + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_c * BC, i_c * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(A, A2, cu_seqlens, chunk_indices, T, B: tl.constexpr, H: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, NK: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_t, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    all = T\n    T = eos - bos\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\n    all = B * T\nif i_t * BT + i_c * BC >= T:\n    return\nb_A = tl.zeros([BC, BC], dtype=tl.float32)\nfor i_k in range(0, NK):\n    p_A = tl.make_block_ptr(A + (i_k * all + bos) * H * BC + i_h * BC, (T, BC), (H * BC, 1), (i_t * BT + i_c * BC, 0), (BC, BC), (1, 0))\n    b_A += tl.load(p_A, boundary_check=(0, 1))\np_A2 = tl.make_block_ptr(A2 + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + i_c * BC, i_c * BC), (BC, BC), (1, 0))\ntl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_rwkv6_bwd_kernel_dh",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BK_LIST for BV in BV_LIST for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_bwd_kernel_dh(",
      "    q,",
      "    gi,",
      "    ge,",
      "    do,",
      "    dh,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (K, T),",
      "            (1, HQ * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        p_gk = tl.make_block_ptr(",
      "            ge + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_gk_last = (",
      "            gi + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "        )",
      "",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_q = (b_q * exp(b_gk) * scale).to(b_q.dtype)",
      "        b_gk_last = tl.load(",
      "            p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "        )",
      "        b_dh *= exp(b_gk_last)[:, None]",
      "        b_dh += tl.dot(b_q, b_do)",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_bwd_kernel_dh(q, gi, ge, do, dh, dht, dh0, cu_seqlens, chunk_offsets, scale, T, HQ: tl.constexpr, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, NG: tl.constexpr, STORE_INITIAL_STATE_GRADIENT: tl.constexpr, USE_FINAL_STATE_GRADIENT: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_v, i_nh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_n, i_hq = (i_nh // HQ, i_nh % HQ)\ni_h = i_hq // NG\nif IS_VARLEN:\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\n    boh = tl.load(chunk_offsets + i_n).to(tl.int32)\nelse:\n    bos, eos = (i_n * T, i_n * T + T)\n    NT = tl.cdiv(T, BT)\n    boh = i_n * NT\nb_dh = tl.zeros([BK, BV], dtype=tl.float32)\nif USE_FINAL_STATE_GRADIENT:\n    p_dht = tl.make_block_ptr(dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)\nfor i_t in range(NT - 1, -1, -1):\n    p_dh = tl.make_block_ptr(dh + ((boh + i_t) * H + i_h) * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))\n    last_idx = min(i_t * BT + BT, T) - 1\n    p_q = tl.make_block_ptr(q + (bos * HQ + i_hq) * K, (K, T), (1, HQ * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_do = tl.make_block_ptr(do + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    b_q = tl.load(p_q, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    p_gk = tl.make_block_ptr(ge + (bos * H + i_h) * K, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1))\n    p_gk_last = gi + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)\n    b_gk = tl.load(p_gk, boundary_check=(0, 1))\n    b_q = (b_q * exp(b_gk) * scale).to(b_q.dtype)\n    b_gk_last = tl.load(p_gk_last, mask=i_k * BK + tl.arange(0, BK) < K, other=0.0)\n    b_dh *= exp(b_gk_last)[:, None]\n    b_dh += tl.dot(b_q, b_do)\nif STORE_INITIAL_STATE_GRADIENT:\n    p_dh0 = tl.make_block_ptr(dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0))\n    tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_rwkv6_bwd_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BK', 'NC', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_bwd_kernel_intra(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_ge = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_ge = tl.load(p_ge, boundary_check=(0, 1))",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    if i_i > 0:",
      "        p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(0, i_i):",
      "            p_k = tl.make_block_ptr(",
      "                k + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gk = tl.make_block_ptr(",
      "                gi + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (T, BT),",
      "                (H * BT, 1),",
      "                (i_t * BT + i_i * BC, i_j * BC),",
      "                (BC, BC),",
      "                (1, 0),",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_kg = b_k * exp(b_gn[None, :] - b_gk)",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dq += tl.dot(b_dA, b_kg)",
      "        b_dq *= exp(b_ge - b_gn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "    )",
      "    p_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gkj = gi + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)",
      "",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] > j",
      "",
      "        b_dq += tl.where(",
      "            m_i, b_dA[:, None] * b_kj[None, :] * exp(b_ge - b_gkj[None, :]), 0.0",
      "        )",
      "        p_kj += H * K",
      "        p_gkj += H * K",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_gk = tl.make_block_ptr(",
      "        gi + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    NC = min(NC, tl.cdiv(T - i_t * BT, BC))",
      "    if i_i < NC - 1:",
      "        p_gn = gi + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(i_i + 1, NC):",
      "            m_j = (i_t * BT + i_j * BC + tl.arange(0, BC)) < T",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gq = tl.make_block_ptr(",
      "                ge + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (BT, T),",
      "                (1, H * BT),",
      "                (i_i * BC, i_t * BT + i_j * BC),",
      "                (BC, BC),",
      "                (0, 1),",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_gq = tl.where(",
      "                m_j[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float(\"-inf\")",
      "            )",
      "            b_qg = b_q * exp(b_gq - b_gn[None, :])",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dk += tl.dot(b_dA, b_qg)",
      "        b_dk *= exp(b_gn[None, :] - b_gk)",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "        + tl.arange(0, BC)",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gqj = ge + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j * H * BT)",
      "",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] < j",
      "        b_dk += tl.where(",
      "            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0",
      "        )",
      "        p_qj += H * K",
      "        p_gqj += H * K",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_bwd_kernel_intra(q, k, gi, ge, dA, dq, dk, cu_seqlens, chunk_indices, T, H: tl.constexpr, K: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, BK: tl.constexpr, NC: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_c, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\ni_t, i_i = (i_c // NC, i_c % NC)\nif IS_VARLEN:\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\nelse:\n    bos, eos = (i_b * T, i_b * T + T)\nT = eos - bos\nif i_t * BT + i_i * BC >= T:\n    return\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\np_ge = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nb_ge = tl.load(p_ge, boundary_check=(0, 1))\nb_dq = tl.zeros([BC, BK], dtype=tl.float32)\nif i_i > 0:\n    p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    for i_j in range(0, i_i):\n        p_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gk = tl.make_block_ptr(gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT + i_i * BC, i_j * BC), (BC, BC), (1, 0))\n        b_k = tl.load(p_k, boundary_check=(0, 1))\n        b_gk = tl.load(p_gk, boundary_check=(0, 1))\n        b_kg = b_k * exp(b_gn[None, :] - b_gk)\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dq += tl.dot(b_dA, b_kg)\n    b_dq *= exp(b_ge - b_gn[None, :])\no_i = tl.arange(0, BC)\nm_dA = i_t * BT + i_i * BC + tl.arange(0, BC) < T\no_dA = bos * H * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_i * BC\np_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_gkj = gi + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_dq = tl.make_block_ptr(dq + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)\n    b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)\n    b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)\n    m_i = o_i[:, None] > j\n    b_dq += tl.where(m_i, b_dA[:, None] * b_kj[None, :] * exp(b_ge - b_gkj[None, :]), 0.0)\n    p_kj += H * K\n    p_gkj += H * K\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.debug_barrier()\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\np_gk = tl.make_block_ptr(gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_gk = tl.load(p_gk, boundary_check=(0, 1))\nb_dk = tl.zeros([BC, BK], dtype=tl.float32)\nNC = min(NC, tl.cdiv(T - i_t * BT, BC))\nif i_i < NC - 1:\n    p_gn = gi + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k\n    b_gn = tl.load(p_gn, mask=m_k, other=0)\n    for i_j in range(i_i + 1, NC):\n        m_j = i_t * BT + i_j * BC + tl.arange(0, BC) < T\n        p_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_gq = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_j * BC, i_k * BK), (BC, BK), (1, 0))\n        p_dA = tl.make_block_ptr(dA + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (i_i * BC, i_t * BT + i_j * BC), (BC, BC), (0, 1))\n        b_q = tl.load(p_q, boundary_check=(0, 1))\n        b_gq = tl.where(m_j[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float('-inf'))\n        b_qg = b_q * exp(b_gq - b_gn[None, :])\n        b_dA = tl.load(p_dA, boundary_check=(0, 1))\n        b_dk += tl.dot(b_dA, b_qg)\n    b_dk *= exp(b_gn[None, :] - b_gk)\no_dA = bos * H * BT + (i_t * BT + i_i * BC) * H * BT + i_h * BT + i_i * BC + tl.arange(0, BC)\np_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_gqj = ge + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k\np_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT + i_i * BC, i_k * BK), (BC, BK), (1, 0))\nfor j in range(0, min(BC, T - i_t * BT - i_i * BC)):\n    b_dA = tl.load(dA + o_dA + j * H * BT)\n    b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)\n    b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)\n    m_i = o_i[:, None] < j\n    b_dk += tl.where(m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0)\n    p_qj += H * K\n    p_gqj += H * K\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "chunk_rwkv6_bwd_kernel_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in BK_LIST for BV in BV_LIST for num_warps in [2, 4, 8]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dq2",
        "annotation": null
      },
      {
        "name": "dk2",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_bwd_kernel_inter(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    gi,",
      "    ge,",
      "    u,",
      "    do,",
      "    dh,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    dq2,",
      "    dk2,",
      "    dg,",
      "    du,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_gk = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_gi = tl.make_block_ptr(",
      "        gi + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_gn = gi + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k",
      "    b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dgk = tl.zeros(",
      "        [",
      "            BK,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dgk += tl.sum(b_h * b_dh, axis=0)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "    b_dgk *= exp(b_gn)",
      "    b_dq *= scale",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_gi = tl.load(p_gi, boundary_check=(0, 1))",
      "    b_dq = b_dq * exp(b_gk)",
      "    b_dk = b_dk * exp(b_gn[None, :] - b_gi)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dA_dig = dA + ((bos + i_t * BT + o_i) * H + i_h) * BT + o_i",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dgk += tl.sum(b_dk * b_k, axis=0)",
      "",
      "    b_dq += tl.load(p_dq, boundary_check=(0, 1))",
      "    b_dk += tl.load(p_dk, boundary_check=(0, 1))",
      "    b_dg = b_q * b_dq - b_k * b_dk",
      "    b_dg = (",
      "        b_dg",
      "        - tl.cumsum(b_dg, axis=0)",
      "        + tl.sum(b_dg, axis=0)[None, :]",
      "        + b_dgk[None, :]",
      "        - b_q * b_dq",
      "    )",
      "",
      "    b_dA_dig = tl.load(p_dA_dig, mask=(i_t * BT + o_i) < T, other=0)",
      "",
      "    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))",
      "    b_u = tl.load(p_u, boundary_check=(0,))",
      "",
      "    b_dq += b_dA_dig[:, None] * b_u[None, :] * b_k",
      "    b_dk += b_dA_dig[:, None] * b_u[None, :] * b_q",
      "    b_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)",
      "    p_du = tl.make_block_ptr(",
      "        du + (i_tg * H + i_h) * K, (K,), (1,), (i_k * BK,), (BK,), (0,)",
      "    )",
      "    tl.store(p_du, b_du, boundary_check=(0,))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/417.py",
    "header": "def chunk_rwkv6_bwd_kernel_inter(q, k, v, h, gi, ge, u, do, dh, dA, dq, dk, dq2, dk2, dg, du, cu_seqlens, chunk_indices, scale, T, H: tl.constexpr, K: tl.constexpr, V: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr, BV: tl.constexpr, IS_VARLEN: tl.constexpr):",
    "body": "i_k, i_t, i_bh = (tl.program_id(0), tl.program_id(1), tl.program_id(2))\ni_b, i_h = (i_bh // H, i_bh % H)\nif IS_VARLEN:\n    i_tg = i_t\n    i_n, i_t = (tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(chunk_indices + i_t * 2 + 1).to(tl.int32))\n    bos, eos = (tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(tl.int32))\n    T = eos - bos\n    NT = tl.cdiv(T, BT)\nelse:\n    NT = tl.cdiv(T, BT)\n    i_tg = i_b * NT + i_t\n    bos, eos = (i_b * T, i_b * T + T)\no_k = i_k * BK + tl.arange(0, BK)\nm_k = o_k < K\np_gk = tl.make_block_ptr(ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_gi = tl.make_block_ptr(gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_gn = gi + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k\nb_gn = tl.load(p_gn, mask=m_k, other=0)\nb_dq = tl.zeros([BT, BK], dtype=tl.float32)\nb_dk = tl.zeros([BT, BK], dtype=tl.float32)\nb_dgk = tl.zeros([BK], dtype=tl.float32)\nfor i_v in range(tl.cdiv(V, BV)):\n    p_v = tl.make_block_ptr(v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_do = tl.make_block_ptr(do + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0))\n    p_h = tl.make_block_ptr(h + (i_tg * H + i_h) * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    p_dh = tl.make_block_ptr(dh + (i_tg * H + i_h) * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1))\n    b_v = tl.load(p_v, boundary_check=(0, 1))\n    b_do = tl.load(p_do, boundary_check=(0, 1))\n    b_h = tl.load(p_h, boundary_check=(0, 1))\n    b_dh = tl.load(p_dh, boundary_check=(0, 1))\n    b_dgk += tl.sum(b_h * b_dh, axis=0)\n    b_dq += tl.dot(b_do, b_h.to(b_do.dtype))\n    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))\nb_dgk *= exp(b_gn)\nb_dq *= scale\nb_gk = tl.load(p_gk, boundary_check=(0, 1))\nb_gi = tl.load(p_gi, boundary_check=(0, 1))\nb_dq = b_dq * exp(b_gk)\nb_dk = b_dk * exp(b_gn[None, :] - b_gi)\no_i = tl.arange(0, BT)\np_q = tl.make_block_ptr(q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_k = tl.make_block_ptr(k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dq = tl.make_block_ptr(dq + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dA_dig = dA + ((bos + i_t * BT + o_i) * H + i_h) * BT + o_i\nb_q = tl.load(p_q, boundary_check=(0, 1))\nb_k = tl.load(p_k, boundary_check=(0, 1))\nb_dgk += tl.sum(b_dk * b_k, axis=0)\nb_dq += tl.load(p_dq, boundary_check=(0, 1))\nb_dk += tl.load(p_dk, boundary_check=(0, 1))\nb_dg = b_q * b_dq - b_k * b_dk\nb_dg = b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :] + b_dgk[None, :] - b_q * b_dq\nb_dA_dig = tl.load(p_dA_dig, mask=i_t * BT + o_i < T, other=0)\np_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))\nb_u = tl.load(p_u, boundary_check=(0,))\nb_dq += b_dA_dig[:, None] * b_u[None, :] * b_k\nb_dk += b_dA_dig[:, None] * b_u[None, :] * b_q\nb_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)\np_du = tl.make_block_ptr(du + (i_tg * H + i_h) * K, (K,), (1,), (i_k * BK,), (BK,), (0,))\ntl.store(p_du, b_du, boundary_check=(0,))\np_dq = tl.make_block_ptr(dq2 + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dk = tl.make_block_ptr(dk2 + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\np_dg = tl.make_block_ptr(dg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0))\ntl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))\ntl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "consumer_gemm_persistent_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "use_cuda_graph": true
        }
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 2, 'kpack': 1, 'matrix_instr_nonkdim': 16}, num_warps=8, num_stages=2), triton.Config({'BLOCK_SIZE_M': 256, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 1, 'waves_per_eu': 0}, num_warps=8, num_stages=2)], key=['M', 'N', 'K'], use_cuda_graph=True)",
      "@triton.heuristics({'EVEN_K': lambda args: args['K'] % args['BLOCK_SIZE_K'] == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "localA",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "rank",
        "annotation": null
      },
      {
        "name": "world_size",
        "annotation": null
      },
      {
        "name": "barrier_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M_PER_CHUNK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_XCDS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def consumer_gemm_persistent_kernel(",
      "    A,",
      "    localA,",
      "    B,",
      "    C,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    rank,",
      "    world_size,",
      "    barrier_ptr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    M_PER_CHUNK: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    NUM_XCDS: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    if NUM_XCDS != 1:",
      "        pid = (pid % NUM_XCDS) * (NUM_SMS // NUM_XCDS) + (pid // NUM_XCDS)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    total_tiles = num_pid_m * num_pid_n",
      "",
      "    tl.assume(stride_am > 0)",
      "    tl.assume(stride_ak > 0)",
      "    tl.assume(stride_bn > 0)",
      "    tl.assume(stride_bk > 0)",
      "    tl.assume(stride_cm > 0)",
      "    tl.assume(stride_cn > 0)",
      "",
      "    M_per_rank = M // world_size",
      "    pid_m_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)",
      "",
      "    acc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32",
      "    for tile_id in range(pid, total_tiles, NUM_SMS):",
      "        num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "        group_id = tile_id // num_pid_in_group",
      "        first_pid_m = group_id * GROUP_SIZE_M",
      "        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)",
      "        pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "        num_pid_m_per_copy_chunk = M_PER_CHUNK // BLOCK_SIZE_M",
      "        chunk_offset = pid_m // (num_pid_m_per_copy_chunk * world_size)",
      "        rank_offset = (",
      "            pid_m % (num_pid_m_per_copy_chunk * world_size) // num_pid_m_per_copy_chunk",
      "        )",
      "        block_offset = pid_m % num_pid_m_per_copy_chunk",
      "",
      "        rank_offset = (rank_offset + rank) % world_size",
      "        pid_m = (",
      "            rank_offset * M_per_rank",
      "            + chunk_offset * M_PER_CHUNK",
      "            + block_offset * BLOCK_SIZE_M",
      "        ) // BLOCK_SIZE_M",
      "",
      "        offs_am = pid_m * BLOCK_SIZE_M",
      "        offs_sig = offs_am // M_PER_CHUNK",
      "        offs_rank = pid_m // pid_m_per_rank",
      "",
      "        if offs_rank != rank:",
      "            wait_eq_sys(barrier_ptr + offs_sig, 1)",
      "",
      "        rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "        rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "        rk = tl.arange(0, BLOCK_SIZE_K)",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "",
      "        if offs_rank == rank:",
      "            rm = rm % M_per_rank",
      "            A_BASE = localA + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "        else:",
      "            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "",
      "        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "        tl.assume(pid_m > 0)",
      "        tl.assume(pid_n > 0)",
      "",
      "        loop_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "        if not EVEN_K:",
      "            loop_k -= 1",
      "",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)",
      "        for k in range(0, loop_k):",
      "            a = tl.load(tl.multiple_of(A_BASE, (1, 16)))",
      "            b = tl.load(tl.multiple_of(B_BASE, (16, 1)))",
      "            acc += tl.dot(a, b)",
      "            A_BASE += BLOCK_SIZE_K * stride_ak",
      "            B_BASE += BLOCK_SIZE_K * stride_bk",
      "",
      "        if not EVEN_K:",
      "            k = loop_k",
      "            rk = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "            B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "            A_BASE = tl.multiple_of(A_BASE, (1, 16))",
      "            B_BASE = tl.multiple_of(B_BASE, (16, 1))",
      "            a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)",
      "            b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)",
      "            acc += tl.dot(a, b)",
      "",
      "        c = acc.to(C.type.element_ty)",
      "        rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "        rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "        c_mask = (rm[:, None] < M) & (rn[None, :] < N)",
      "        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn",
      "        tl.store(C_, c, c_mask)"
    ],
    "file": "codes/77.py",
    "header": "def consumer_gemm_persistent_kernel(A, localA, B, C, M, N, K, stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn, rank, world_size, barrier_ptr, BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr, GROUP_SIZE_M: tl.constexpr, M_PER_CHUNK: tl.constexpr, NUM_SMS: tl.constexpr, NUM_XCDS: tl.constexpr, EVEN_K: tl.constexpr):",
    "body": "pid = tl.program_id(0)\nif NUM_XCDS != 1:\n    pid = pid % NUM_XCDS * (NUM_SMS // NUM_XCDS) + pid // NUM_XCDS\nnum_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\nnum_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\ntotal_tiles = num_pid_m * num_pid_n\ntl.assume(stride_am > 0)\ntl.assume(stride_ak > 0)\ntl.assume(stride_bn > 0)\ntl.assume(stride_bk > 0)\ntl.assume(stride_cm > 0)\ntl.assume(stride_cn > 0)\nM_per_rank = M // world_size\npid_m_per_rank = tl.cdiv(M_per_rank, BLOCK_SIZE_M)\nacc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32\nfor tile_id in range(pid, total_tiles, NUM_SMS):\n    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n    group_id = tile_id // num_pid_in_group\n    first_pid_m = group_id * GROUP_SIZE_M\n    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n    pid_m = first_pid_m + tile_id % num_pid_in_group % group_size_m\n    pid_n = tile_id % num_pid_in_group // group_size_m\n    num_pid_m_per_copy_chunk = M_PER_CHUNK // BLOCK_SIZE_M\n    chunk_offset = pid_m // (num_pid_m_per_copy_chunk * world_size)\n    rank_offset = pid_m % (num_pid_m_per_copy_chunk * world_size) // num_pid_m_per_copy_chunk\n    block_offset = pid_m % num_pid_m_per_copy_chunk\n    rank_offset = (rank_offset + rank) % world_size\n    pid_m = (rank_offset * M_per_rank + chunk_offset * M_PER_CHUNK + block_offset * BLOCK_SIZE_M) // BLOCK_SIZE_M\n    offs_am = pid_m * BLOCK_SIZE_M\n    offs_sig = offs_am // M_PER_CHUNK\n    offs_rank = pid_m // pid_m_per_rank\n    if offs_rank != rank:\n        wait_eq_sys(barrier_ptr + offs_sig, 1)\n    rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    rk = tl.arange(0, BLOCK_SIZE_K)\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    if offs_rank == rank:\n        rm = rm % M_per_rank\n        A_BASE = localA + rm[:, None] * stride_am + rk[None, :] * stride_ak\n    else:\n        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n    B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n    tl.assume(pid_m > 0)\n    tl.assume(pid_n > 0)\n    loop_k = tl.cdiv(K, BLOCK_SIZE_K)\n    if not EVEN_K:\n        loop_k -= 1\n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=acc_dtype)\n    for k in range(0, loop_k):\n        a = tl.load(tl.multiple_of(A_BASE, (1, 16)))\n        b = tl.load(tl.multiple_of(B_BASE, (16, 1)))\n        acc += tl.dot(a, b)\n        A_BASE += BLOCK_SIZE_K * stride_ak\n        B_BASE += BLOCK_SIZE_K * stride_bk\n    if not EVEN_K:\n        k = loop_k\n        rk = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak\n        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn\n        A_BASE = tl.multiple_of(A_BASE, (1, 16))\n        B_BASE = tl.multiple_of(B_BASE, (16, 1))\n        a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)\n        b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)\n        acc += tl.dot(a, b)\n    c = acc.to(C.type.element_ty)\n    rm = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n    rn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n    rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_SIZE_M), BLOCK_SIZE_M)\n    rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_SIZE_N), BLOCK_SIZE_N)\n    c_mask = (rm[:, None] < M) & (rn[None, :] < N)\n    C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    tl.store(C_, c, c_mask)"
  },
  {
    "name": "layer_norm_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for BT in [32, 64, 128] for num_warps in [2, 4, 8]], key=['D', 'NB', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "res",
        "annotation": null
      },
      {
        "name": "res_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel(",
      "    x,",
      "    y,",
      "    w,",
      "    b,",
      "    res,",
      "    res_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    T,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "",
      "    o_t = i_t * BT + tl.arange(0, BT)",
      "    o_g = o_t % G",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        p_res = tl.make_block_ptr(res, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        p_res_out = tl.make_block_ptr(",
      "            res_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)",
      "        )",
      "        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=1) / D",
      "        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))",
      "        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    else:",
      "        b_xbar = tl.where(m_d[None, :], b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "",
      "    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(",
      "            tl.float32",
      "        )",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(",
      "            tl.float32",
      "        )",
      "    b_x_hat = (",
      "        (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "        if not IS_RMS_NORM",
      "        else b_x * b_rstd[:, None]",
      "    )",
      "    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b",
      "",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "codes/359.py",
    "header": "def layer_norm_fwd_kernel(x, y, w, b, res, res_out, mean, rstd, eps, T, G: tl.constexpr, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, NB: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\no_t = i_t * BT + tl.arange(0, BT)\no_g = o_t % G\no_d = tl.arange(0, BD)\nm_d = o_d < D\np_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\nb_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\nif HAS_RESIDUAL:\n    p_res = tl.make_block_ptr(res, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)\nif STORE_RESIDUAL_OUT:\n    p_res_out = tl.make_block_ptr(res_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\n    tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))\nif not IS_RMS_NORM:\n    b_mean = tl.sum(b_x, axis=1) / D\n    p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))\n    tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))\n    b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\nelse:\n    b_xbar = tl.where(m_d[None, :], b_x, 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=1) / D\nb_rstd = 1 / tl.sqrt(b_var + eps)\np_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))\ntl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))\nif HAS_WEIGHT:\n    b_w = tl.load(w + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(tl.float32)\nb_x_hat = (b_x - b_mean[:, None]) * b_rstd[:, None] if not IS_RMS_NORM else b_x * b_rstd[:, None]\nb_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat\nif HAS_BIAS:\n    b_y = b_y + b_b\np_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))\ntl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
  },
  {
    "name": "layer_norm_fwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8, 16]], key=['D', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "res",
        "annotation": null
      },
      {
        "name": "res_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel1(",
      "    x,",
      "    y,",
      "    w,",
      "    b,",
      "    res,",
      "    res_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    i_g = i_t % G",
      "",
      "    x += i_t * D",
      "    y += i_t * D",
      "    if HAS_RESIDUAL:",
      "        res += i_t * D",
      "    if STORE_RESIDUAL_OUT:",
      "        res_out += i_t * D",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    b_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        b_x += tl.load(res + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(res_out + o_d, b_x, mask=m_d)",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=0) / D",
      "        tl.store(mean + i_t, b_mean)",
      "        b_xbar = tl.where(m_d, b_x - b_mean, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    else:",
      "        b_xbar = tl.where(m_d, b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    tl.store(rstd + i_t, b_rstd)",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + i_g * D + o_d, mask=m_d).to(tl.float32)",
      "    b_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b",
      "",
      "    tl.store(y + o_d, b_y, mask=m_d)"
    ],
    "file": "codes/359.py",
    "header": "def layer_norm_fwd_kernel1(x, y, w, b, res, res_out, mean, rstd, eps, G: tl.constexpr, D: tl.constexpr, BD: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_RESIDUAL: tl.constexpr, STORE_RESIDUAL_OUT: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr):",
    "body": "i_t = tl.program_id(0)\ni_g = i_t % G\nx += i_t * D\ny += i_t * D\nif HAS_RESIDUAL:\n    res += i_t * D\nif STORE_RESIDUAL_OUT:\n    res_out += i_t * D\no_d = tl.arange(0, BD)\nm_d = o_d < D\nb_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)\nif HAS_RESIDUAL:\n    b_x += tl.load(res + o_d, mask=m_d, other=0.0).to(tl.float32)\nif STORE_RESIDUAL_OUT:\n    tl.store(res_out + o_d, b_x, mask=m_d)\nif not IS_RMS_NORM:\n    b_mean = tl.sum(b_x, axis=0) / D\n    tl.store(mean + i_t, b_mean)\n    b_xbar = tl.where(m_d, b_x - b_mean, 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\nelse:\n    b_xbar = tl.where(m_d, b_x, 0.0)\n    b_var = tl.sum(b_xbar * b_xbar, axis=0) / D\nb_rstd = 1 / tl.sqrt(b_var + eps)\ntl.store(rstd + i_t, b_rstd)\nif HAS_WEIGHT:\n    b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + i_g * D + o_d, mask=m_d).to(tl.float32)\nb_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\nb_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat\nif HAS_BIAS:\n    b_y = b_y + b_b\ntl.store(y + o_d, b_y, mask=m_d)"
  },
  {
    "name": "layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for BT in [32, 64] for num_warps in [2, 4, 8]], key=['D', 'NB', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dres",
        "annotation": null
      },
      {
        "name": "dres_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel(",
      "    x,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dw,",
      "    db,",
      "    dres,",
      "    dres_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    GS: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    i_g, i_sg = i_s // GS, i_s % GS",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)",
      "        b_dw = tl.zeros((BT, BD), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + i_g * D + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "        b_db = tl.zeros((BT, BD), dtype=tl.float32)",
      "",
      "    T = min(i_sg * BS + BS, T // G)",
      "    for i_t in range(i_sg * BS, T, BT):",
      "        p_x = tl.make_block_ptr(",
      "            x + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "        )",
      "        p_dy = tl.make_block_ptr(",
      "            dy + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "        )",
      "        p_dx = tl.make_block_ptr(",
      "            dx + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            p_mean = tl.make_block_ptr(mean + i_g, (T,), (G,), (i_t,), (BT,), (0,))",
      "            b_mean = tl.load(p_mean, boundary_check=(0,))",
      "        p_rstd = tl.make_block_ptr(rstd + i_g, (T,), (G,), (i_t,), (BT,), (0,))",
      "        b_rstd = tl.load(p_rstd, boundary_check=(0,))",
      "",
      "        b_xhat = (",
      "            (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "            if not IS_RMS_NORM",
      "            else b_x * b_rstd[:, None]",
      "        )",
      "        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)",
      "",
      "        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat",
      "        if HAS_BIAS:",
      "            b_y = b_y + b_b[None, :]",
      "        if RECOMPUTE_OUTPUT:",
      "            p_y = tl.make_block_ptr(",
      "                y + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_wdy = b_dy",
      "",
      "        if HAS_WEIGHT or HAS_BIAS:",
      "            m_t = (i_t + tl.arange(0, BT)) < T",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)",
      "        if HAS_BIAS:",
      "            b_db += tl.where(m_t[:, None], b_dy, 0.0)",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_c2 = tl.sum(b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]",
      "        if HAS_DRESIDUAL:",
      "            p_dres = tl.make_block_ptr(",
      "                dres + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            p_dres_in = tl.make_block_ptr(",
      "                dres_in + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            tl.store(",
      "                p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)"
    ],
    "file": "codes/359.py",
    "header": "def layer_norm_bwd_kernel(x, w, b, y, dy, dx, dw, db, dres, dres_in, mean, rstd, T, G: tl.constexpr, D: tl.constexpr, BS: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr, NB: tl.constexpr, GS: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "i_s = tl.program_id(0)\ni_g, i_sg = (i_s // GS, i_s % GS)\no_d = tl.arange(0, BD)\nm_d = o_d < D\nif HAS_WEIGHT:\n    b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)\n    b_dw = tl.zeros((BT, BD), dtype=tl.float32)\nif HAS_BIAS:\n    b_b = tl.load(b + i_g * D + o_d, mask=m_d, other=0.0).to(tl.float32)\n    b_db = tl.zeros((BT, BD), dtype=tl.float32)\nT = min(i_sg * BS + BS, T // G)\nfor i_t in range(i_sg * BS, T, BT):\n    p_x = tl.make_block_ptr(x + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0))\n    p_dy = tl.make_block_ptr(dy + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0))\n    p_dx = tl.make_block_ptr(dx + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0))\n    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)\n    b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)\n    if not IS_RMS_NORM:\n        p_mean = tl.make_block_ptr(mean + i_g, (T,), (G,), (i_t,), (BT,), (0,))\n        b_mean = tl.load(p_mean, boundary_check=(0,))\n    p_rstd = tl.make_block_ptr(rstd + i_g, (T,), (G,), (i_t,), (BT,), (0,))\n    b_rstd = tl.load(p_rstd, boundary_check=(0,))\n    b_xhat = (b_x - b_mean[:, None]) * b_rstd[:, None] if not IS_RMS_NORM else b_x * b_rstd[:, None]\n    b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)\n    b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat\n    if HAS_BIAS:\n        b_y = b_y + b_b[None, :]\n    if RECOMPUTE_OUTPUT:\n        p_y = tl.make_block_ptr(y + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0))\n        tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))\n    b_wdy = b_dy\n    if HAS_WEIGHT or HAS_BIAS:\n        m_t = i_t + tl.arange(0, BT) < T\n    if HAS_WEIGHT:\n        b_wdy = b_dy * b_w\n        b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)\n    if HAS_BIAS:\n        b_db += tl.where(m_t[:, None], b_dy, 0.0)\n    if not IS_RMS_NORM:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n        b_c2 = tl.sum(b_wdy, axis=1) / D\n        b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]\n    else:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D\n        b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]\n    if HAS_DRESIDUAL:\n        p_dres = tl.make_block_ptr(dres + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0))\n        b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)\n        b_dx += b_dres\n    if STORE_DRESIDUAL:\n        p_dres_in = tl.make_block_ptr(dres_in + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0))\n        tl.store(p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1))\n    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))\nif HAS_WEIGHT:\n    tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)\nif HAS_BIAS:\n    tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)"
  },
  {
    "name": "layer_norm_bwd_kernel1",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8]], key=['D', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dres",
        "annotation": null
      },
      {
        "name": "dres_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel1(",
      "    x,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dw,",
      "    db,",
      "    dres,",
      "    dres_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    GS: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    i_g, i_sg = i_s // GS, i_s % GS",
      "",
      "    o_d = tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + i_g * D + o_d, mask=mask).to(tl.float32)",
      "        b_dw = tl.zeros((BD,), dtype=tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b_b = tl.load(b + i_g * D + o_d, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_db = tl.zeros((BD,), dtype=tl.float32)",
      "",
      "    for i_t in range(i_sg * BS * G + i_g, min((i_sg * BS + BS) * G + i_g, T), G):",
      "        b_x = tl.load(x + i_t * D + o_d, mask=mask, other=0).to(tl.float32)",
      "        b_dy = tl.load(dy + i_t * D + o_d, mask=mask, other=0).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            b_mean = tl.load(mean + i_t)",
      "        b_rstd = tl.load(rstd + i_t)",
      "",
      "        b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "        b_xhat = tl.where(mask, b_xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat",
      "            if HAS_BIAS:",
      "                b_y = b_y + b_b",
      "            tl.store(y + i_t * D + o_d, b_y, mask=mask)",
      "        b_wdy = b_dy",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += b_dy * b_xhat",
      "        if HAS_BIAS:",
      "            b_db += b_dy",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_c2 = tl.sum(b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1) * b_rstd",
      "        if HAS_DRESIDUAL:",
      "            b_dres = tl.load(dres + i_t * D + o_d, mask=mask, other=0).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        b_dx = tl.cast(b_dx, dtype=dx.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "        if STORE_DRESIDUAL:",
      "            tl.store(dres_in + i_t * D + o_d, b_dx, mask=mask)",
      "        tl.store(dx + i_t * D + o_d, b_dx, mask=mask)",
      "",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, b_dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, b_db, mask=mask)"
    ],
    "file": "codes/359.py",
    "header": "def layer_norm_bwd_kernel1(x, w, b, y, dy, dx, dw, db, dres, dres_in, mean, rstd, T, G: tl.constexpr, D: tl.constexpr, BS: tl.constexpr, BD: tl.constexpr, GS: tl.constexpr, IS_RMS_NORM: tl.constexpr, HAS_DRESIDUAL: tl.constexpr, STORE_DRESIDUAL: tl.constexpr, HAS_WEIGHT: tl.constexpr, HAS_BIAS: tl.constexpr, RECOMPUTE_OUTPUT: tl.constexpr):",
    "body": "i_s = tl.program_id(0)\ni_g, i_sg = (i_s // GS, i_s % GS)\no_d = tl.arange(0, BD)\nmask = o_d < D\nif HAS_WEIGHT:\n    b_w = tl.load(w + i_g * D + o_d, mask=mask).to(tl.float32)\n    b_dw = tl.zeros((BD,), dtype=tl.float32)\nif RECOMPUTE_OUTPUT and HAS_BIAS:\n    b_b = tl.load(b + i_g * D + o_d, mask=mask, other=0.0).to(tl.float32)\nif HAS_BIAS:\n    b_db = tl.zeros((BD,), dtype=tl.float32)\nfor i_t in range(i_sg * BS * G + i_g, min((i_sg * BS + BS) * G + i_g, T), G):\n    b_x = tl.load(x + i_t * D + o_d, mask=mask, other=0).to(tl.float32)\n    b_dy = tl.load(dy + i_t * D + o_d, mask=mask, other=0).to(tl.float32)\n    if not IS_RMS_NORM:\n        b_mean = tl.load(mean + i_t)\n    b_rstd = tl.load(rstd + i_t)\n    b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd\n    b_xhat = tl.where(mask, b_xhat, 0.0)\n    if RECOMPUTE_OUTPUT:\n        b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat\n        if HAS_BIAS:\n            b_y = b_y + b_b\n        tl.store(y + i_t * D + o_d, b_y, mask=mask)\n    b_wdy = b_dy\n    if HAS_WEIGHT:\n        b_wdy = b_dy * b_w\n        b_dw += b_dy * b_xhat\n    if HAS_BIAS:\n        b_db += b_dy\n    if not IS_RMS_NORM:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n        b_c2 = tl.sum(b_wdy, axis=0) / D\n        b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd\n    else:\n        b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D\n        b_dx = (b_wdy - b_xhat * b_c1) * b_rstd\n    if HAS_DRESIDUAL:\n        b_dres = tl.load(dres + i_t * D + o_d, mask=mask, other=0).to(tl.float32)\n        b_dx += b_dres\n    b_dx = tl.cast(b_dx, dtype=dx.dtype.element_ty, fp_downcast_rounding='rtne')\n    if STORE_DRESIDUAL:\n        tl.store(dres_in + i_t * D + o_d, b_dx, mask=mask)\n    tl.store(dx + i_t * D + o_d, b_dx, mask=mask)\nif HAS_WEIGHT:\n    tl.store(dw + i_s * D + o_d, b_dw, mask=mask)\nif HAS_BIAS:\n    tl.store(db + i_s * D + o_d, b_db, mask=mask)"
  },
  {
    "name": "min_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 512, 'BLOCK_N': 512}), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 512}), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32}), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}), triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 16}), triton.Config({'BLOCK_M': 16, 'BLOCK_N': 256})], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "inp",
        "annotation": null
      },
      {
        "name": "out_value",
        "annotation": null
      },
      {
        "name": "out_index",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def min_kernel(",
      "    inp,",
      "    out_value,",
      "    out_index,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    min_values = tl.full([BLOCK_M], dtype=tl.float32, value=float(\"inf\"))",
      "    argmin_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)",
      "    for start_n in range(0, N, BLOCK_N):",
      "",
      "        n_offset = start_n + tl.arange(0, BLOCK_N)",
      "",
      "        offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "",
      "        mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "        inp_ptrs = inp + offset",
      "",
      "        inp_vals = tl.load(inp_ptrs, mask=mask, other=float(\"inf\"))",
      "        local_min, local_argmin = tl.min(inp_vals, 1, return_indices=True)",
      "",
      "        update = local_min < min_values",
      "        min_values = tl.where(update, local_min, min_values)",
      "        argmin_values = tl.where(update, start_n + local_argmin, argmin_values)",
      "",
      "    offset_index = m_offset * K + pid_k",
      "    out_value_ptrs = out_value + offset_index",
      "    out_index_ptrs = out_index + offset_index",
      "    mask1 = m_offset < M",
      "",
      "    tl.store(out_value_ptrs, min_values, mask=mask1)",
      "    tl.store(out_index_ptrs, argmin_values, mask=mask1)"
    ],
    "file": "codes/1258.py",
    "header": "def min_kernel(inp, out_value, out_index, M, N, K, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr):",
    "body": "pid_m = tl.program_id(0)\npid_k = tl.program_id(1)\nm_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\nmin_values = tl.full([BLOCK_M], dtype=tl.float32, value=float('inf'))\nargmin_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)\nfor start_n in range(0, N, BLOCK_N):\n    n_offset = start_n + tl.arange(0, BLOCK_N)\n    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k\n    mask = m_offset[:, None] < M and n_offset[None, :] < N\n    inp_ptrs = inp + offset\n    inp_vals = tl.load(inp_ptrs, mask=mask, other=float('inf'))\n    local_min, local_argmin = tl.min(inp_vals, 1, return_indices=True)\n    update = local_min < min_values\n    min_values = tl.where(update, local_min, min_values)\n    argmin_values = tl.where(update, start_n + local_argmin, argmin_values)\noffset_index = m_offset * K + pid_k\nout_value_ptrs = out_value + offset_index\nout_index_ptrs = out_index + offset_index\nmask1 = m_offset < M\ntl.store(out_value_ptrs, min_values, mask=mask1)\ntl.store(out_index_ptrs, argmin_values, mask=mask1)"
  }
]