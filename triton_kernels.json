[
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K: tl.constexpr,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    tl.static_assert(",
      "        K % (4 * BLOCK_SIZE_K) == 0,",
      "        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",",
      "    )",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)",
      "",
      "    for i in range(4):",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):",
      "            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j",
      "",
      "            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0)",
      "",
      "            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)",
      "",
      "            mask = 3 << (2 * i)",
      "",
      "            b = (b_uint8 & mask) >> (2 * i)",
      "",
      "            tensor_full = tl.full((1,), 1, dtype=tl.int8)",
      "",
      "            accumulator += tl.dot(a, (b.to(tl.int8) - tensor_full), out_dtype=tl.int32)",
      "",
      "            a_ptrs += BLOCK_SIZE_K * stride_ak",
      "            b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/linkedin_Liger-Kernel/src/liger_kernel/ops/experimental/411.py"
  },
  {
    "name": "causal_conv1d_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_WEIGHT': lambda args: args['weight'] is not None, 'HAS_BIAS': lambda args: args['bias'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [4, 8, 16, 32]], key=['B', 'D', 'W', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "weight",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "W",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def causal_conv1d_fwd_kernel(",
      "    x,",
      "    y,",
      "    weight,",
      "    bias,",
      "    residual,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    D: tl.constexpr,",
      "    W: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    o_w = tl.arange(0, W)",
      "    m_d = o_d < D",
      "",
      "    if HAS_WEIGHT:",
      "",
      "        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "    b_y = tl.zeros((BT, BD), dtype=tl.float32)",
      "    for i_w in tl.static_range(-W + 1, 1):",
      "        p_yi = tl.make_block_ptr(",
      "            x + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_yi = tl.load(p_yi, boundary_check=(0, 1))",
      "        if HAS_WEIGHT:",
      "            b_yi *= tl.sum(b_w * (o_w == (i_w + W - 1)), 1)",
      "        b_y += b_yi",
      "    if HAS_BIAS:",
      "        b_y += tl.load(bias + o_d, mask=m_d).to(tl.float32)",
      "",
      "    if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":",
      "        b_y = b_y * tl.sigmoid(b_y)",
      "",
      "    if HAS_RESIDUAL:",
      "        p_residual = tl.make_block_ptr(",
      "            residual + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        b_residual = tl.load(p_residual, boundary_check=(0, 1))",
      "        b_y += b_residual",
      "",
      "    p_y = tl.make_block_ptr(",
      "        y + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_y,",
      "        tl.cast(b_y, dtype=p_y.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/211.py"
  },
  {
    "name": "causal_conv1d_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_WEIGHT': lambda args: args['dw'] is not None, 'HAS_BIAS': lambda args: args['db'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [4, 8, 16, 32]], key=['B', 'D', 'W', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "weight",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "W",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def causal_conv1d_bwd_kernel(",
      "    x,",
      "    y,",
      "    weight,",
      "    bias,",
      "    residual,",
      "    dy,",
      "    dx,",
      "    dw,",
      "    db,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    D: tl.constexpr,",
      "    W: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)",
      "        T = eos - bos",
      "    else:",
      "        i_tg = i_b * tl.num_programs(1) + i_t",
      "        i_n = i_b",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    o_w = tl.arange(0, W)",
      "    m_d = o_d < D",
      "",
      "    if HAS_WEIGHT:",
      "        p_x = tl.make_block_ptr(",
      "            x + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        b_x = tl.load(p_x, boundary_check=(0, 1))",
      "",
      "        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)",
      "",
      "    b_dx = tl.zeros((BT, BD), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_db = tl.zeros((BD,), dtype=tl.float32)",
      "    for i_w in tl.static_range(0, W):",
      "        p_dy = tl.make_block_ptr(",
      "            dy + bos * D, (T, D), (D, 1), (i_t * BT + i_w, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_dy = tl.load(p_dy, boundary_check=(0, 1))",
      "",
      "        if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":",
      "            p_y = tl.make_block_ptr(",
      "                y + bos * D,",
      "                (T, D),",
      "                (D, 1),",
      "                (i_t * BT + i_w, i_d * BD),",
      "                (BT, BD),",
      "                (1, 0),",
      "            )",
      "",
      "            b_y = tl.load(p_y, boundary_check=(0, 1)).to(tl.float32)",
      "            b_ys = tl.sigmoid(b_y)",
      "            b_dy = b_dy * b_ys * (1 + b_y * (1 - b_ys))",
      "",
      "        b_wdy = b_dy",
      "        if HAS_WEIGHT:",
      "",
      "            b_wdy = b_wdy * tl.sum(b_w * (o_w == (W - i_w - 1)), 1)",
      "",
      "            b_dw = tl.sum(b_dy * b_x, 0)",
      "            tl.store(",
      "                dw + i_tg * D * W + o_d * W + W - i_w - 1,",
      "                b_dw.to(dw.dtype.element_ty),",
      "                mask=m_d,",
      "            )",
      "        if HAS_BIAS and i_w == 0:",
      "            b_db += tl.sum(b_dy, 0)",
      "        b_dx += b_wdy",
      "    if HAS_BIAS:",
      "        b_db = tl.cast(b_db, dtype=db.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "        tl.store(db + i_tg * D + o_d, b_db, mask=m_d)",
      "",
      "    p_dx = tl.make_block_ptr(",
      "        dx + bos * D, (T, D), (D, 1), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_dx,",
      "        tl.cast(b_dx, dtype=p_dx.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/211.py"
  },
  {
    "name": "causal_conv1d_update_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_WEIGHT': lambda args: args['weight'] is not None, 'HAS_BIAS': lambda args: args['bias'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None})"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "cache",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "weight",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "W",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def causal_conv1d_update_kernel(",
      "    x,",
      "    cache,",
      "    residual,",
      "    y,",
      "    weight,",
      "    bias,",
      "    D: tl.constexpr,",
      "    W: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "):",
      "    i_d, i_n = tl.program_id(0), tl.program_id(1)",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    o_w = tl.arange(0, W)",
      "    m_d = o_d < D",
      "    m_c = o_w < W - 1",
      "",
      "    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=0).to(tl.float32)",
      "",
      "    p_cache = tl.make_block_ptr(",
      "        cache + i_n * D * W, (D, W), (W, 1), (i_d * BD, 1), (BD, W), (1, 0)",
      "    )",
      "",
      "    b_cache = tl.load(p_cache, boundary_check=(0, 1)).to(tl.float32)",
      "    b_cache = tl.where(m_c[None, :], b_cache, b_x[:, None])",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(weight + o_d[:, None] * W + o_w, mask=m_d[:, None], other=0)",
      "        b_y = tl.sum(b_cache * b_w, 1)",
      "    else:",
      "        b_y = tl.sum(b_cache, 1)",
      "    if HAS_BIAS:",
      "        b_y += tl.load(bias + o_d, mask=m_d)",
      "",
      "    if ACTIVATION == \"swish\" or ACTIVATION == \"silu\":",
      "        b_y = b_y * tl.sigmoid(b_y)",
      "",
      "    if HAS_RESIDUAL:",
      "        b_y += tl.load(residual + i_n * D + o_d, mask=m_d, other=0)",
      "",
      "    tl.store(",
      "        y + i_n * D + o_d,",
      "        tl.cast(b_y, dtype=y.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        mask=m_d,",
      "    )",
      "",
      "    b_cache = tl.cast(",
      "        b_cache, dtype=cache.dtype.element_ty, fp_downcast_rounding=\"rtne\"",
      "    )",
      "",
      "    p_cache = tl.make_block_ptr(",
      "        cache + i_n * D * W, (D, W), (W, 1), (i_d * BD, 0), (BD, W), (1, 0)",
      "    )",
      "    tl.store(p_cache, b_cache, boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/211.py"
  },
  {
    "name": "layer_norm_fwd_kernel_quant",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel_quant(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    RESIDUAL_OUT,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    N,",
      "    eps,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    if HAS_WEIGHT:",
      "        w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "",
      "    y = x_hat * w if HAS_WEIGHT else x_hat",
      "    if HAS_BIAS:",
      "        y = y + b",
      "",
      "    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-5)",
      "",
      "    y = tl.extra.cuda.libdevice.round(y * scale)",
      "    y = tl.maximum(tl.minimum(y, 127), -128) / scale",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/212.py"
  },
  {
    "name": "layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    DRESIDUAL_IN,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    if HAS_WEIGHT:",
      "        w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "        dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w if HAS_WEIGHT else xhat",
      "            if HAS_BIAS:",
      "                y = y + b",
      "",
      "            scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), 1e-5)",
      "",
      "            y = tl.extra.cuda.libdevice.round(y * scale)",
      "            y = tl.maximum(tl.minimum(y, 127), -128) / scale",
      "",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = dy",
      "        if HAS_WEIGHT:",
      "            wdy = dy * w",
      "            dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    if HAS_WEIGHT:",
      "        tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/212.py"
  },
  {
    "name": "cross_entropy_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "z_loss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "label_smoothing",
        "annotation": null
      },
      {
        "name": "logit_scale",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "n_rows",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_fwd_kernel(",
      "    loss_ptr,",
      "    lse_ptr,",
      "    z_loss_ptr,",
      "    logits_ptr,",
      "    labels_ptr,",
      "    label_smoothing,",
      "    logit_scale,",
      "    lse_square_scale,",
      "    ignore_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    n_rows,",
      "    logits_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "    SPLIT: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    logits = tl.load(",
      "        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "    )",
      "    logits = logits.to(tl.float32) * logit_scale",
      "    max_logits = tl.max(logits, 0)",
      "    if HAS_SMOOTHING:",
      "        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)",
      "    lse = log(tl.sum(exp(logits - max_logits), 0)) + max_logits",
      "    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)",
      "    if label_idx == ignore_index:",
      "        loss = 0.0",
      "        z_loss = 0.0",
      "    else:",
      "        label_idx -= class_start_idx",
      "        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(",
      "            n_cols, (col_block_idx + 1) * BLOCK_SIZE",
      "        ):",
      "            logits_label = tl.load(logits_ptr + label_idx) * logit_scale",
      "            if HAS_SMOOTHING:",
      "                loss = (",
      "                    (lse if not SPLIT else 0.0)",
      "                    - label_smoothing * sum_logits / total_classes",
      "                    - (1 - label_smoothing) * logits_label",
      "                )",
      "            else:",
      "                loss = (lse if not SPLIT else 0.0) - logits_label",
      "        else:",
      "",
      "            if HAS_SMOOTHING:",
      "                loss = label_smoothing * (",
      "                    (lse if not SPLIT else 0.0) - sum_logits / total_classes",
      "                )",
      "            else:",
      "                loss = 0.0",
      "        if not SPLIT:",
      "            z_loss = lse_square_scale * lse * lse",
      "            loss += z_loss",
      "        else:",
      "            z_loss = 0.0",
      "    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)",
      "    if not SPLIT:",
      "        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/213.py"
  },
  {
    "name": "cross_entropy_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['label_smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "dlogits_ptr",
        "annotation": null
      },
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "label_smoothing",
        "annotation": null
      },
      {
        "name": "logit_scale",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "dlogits_row_stride",
        "annotation": null
      },
      {
        "name": "dloss_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_bwd_kernel(",
      "    dlogits_ptr,",
      "    dloss_ptr,",
      "    logits_ptr,",
      "    lse_ptr,",
      "    labels_ptr,",
      "    label_smoothing,",
      "    logit_scale,",
      "    lse_square_scale,",
      "    ignore_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    logits_row_stride,",
      "    dlogits_row_stride,",
      "    dloss_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    if label_idx != ignore_index:",
      "        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)",
      "    else:",
      "        dloss = 0.0",
      "    logits = (",
      "        tl.load(",
      "            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "        ).to(tl.float32)",
      "        * logit_scale",
      "    )",
      "    lse = tl.load(lse_ptr + row_idx)",
      "    probs = exp(logits - lse)",
      "    probs += 2.0 * lse_square_scale * lse * probs",
      "    label_idx -= class_start_idx",
      "    if HAS_SMOOTHING:",
      "        smooth_negative = label_smoothing / total_classes",
      "        probs = (",
      "            tl.where(col_offsets == label_idx, probs - (1 - label_smoothing), probs)",
      "            - smooth_negative",
      "        )",
      "    else:",
      "        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)",
      "    tl.store(",
      "        dlogits_ptr + col_offsets,",
      "        (dloss * logit_scale) * probs,",
      "        mask=col_offsets < n_cols,",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/213.py"
  },
  {
    "name": "layer_norm_gated_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_RESIDUAL_OUT': lambda args: args['residual_out'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [8, 16, 32, 64] for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'NB', 'IS_RMS_NORM', 'STORE_RESIDUAL_OUT', 'HAS_RESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "residual_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_fwd_kernel(",
      "    x,",
      "    g,",
      "    y,",
      "    w,",
      "    b,",
      "    residual,",
      "    residual_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        p_res = tl.make_block_ptr(",
      "            residual, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)",
      "        )",
      "        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        p_res_out = tl.make_block_ptr(",
      "            residual_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)",
      "        )",
      "        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=1) / D",
      "        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))",
      "        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    else:",
      "        b_xbar = tl.where(m_d[None, :], b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "",
      "    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)",
      "    b_x_hat = (",
      "        (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "        if not IS_RMS_NORM",
      "        else b_x * b_rstd[:, None]",
      "    )",
      "    b_y = b_x_hat * b_w[None, :] if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b[None, :]",
      "",
      "    p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)",
      "    if ACTIVATION == \"swish\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"silu\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"sigmoid\":",
      "        b_y = b_y * tl.sigmoid(b_g)",
      "",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/216.py"
  },
  {
    "name": "layer_norm_gated_fwd_kernel1",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_RESIDUAL_OUT': lambda args: args['residual_out'] is not None, 'HAS_RESIDUAL': lambda args: args['residual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'IS_RMS_NORM', 'STORE_RESIDUAL_OUT', 'HAS_RESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "residual",
        "annotation": null
      },
      {
        "name": "residual_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_fwd_kernel1(",
      "    x,",
      "    g,",
      "    y,",
      "    w,",
      "    b,",
      "    residual,",
      "    residual_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    y += i_t * D",
      "    g += i_t * D",
      "    if HAS_RESIDUAL:",
      "        residual += i_t * D",
      "    if STORE_RESIDUAL_OUT:",
      "        residual_out += i_t * D",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    b_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        b_x += tl.load(residual + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(residual_out + o_d, b_x, mask=m_d)",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=0) / D",
      "        tl.store(mean + i_t, b_mean)",
      "        b_xbar = tl.where(m_d, b_x - b_mean, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    else:",
      "        b_xbar = tl.where(m_d, b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    tl.store(rstd + i_t, b_rstd)",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=m_d).to(tl.float32)",
      "    b_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b",
      "",
      "    b_g = tl.load(g + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if ACTIVATION == \"swish\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"silu\":",
      "        b_y = b_y * b_g * tl.sigmoid(b_g)",
      "    elif ACTIVATION == \"sigmoid\":",
      "        b_y = b_y * tl.sigmoid(b_g)",
      "",
      "    tl.store(y + o_d, b_y, mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/216.py"
  },
  {
    "name": "layer_norm_gated_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DRESIDUAL': lambda args: args['dresidual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None, 'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [8, 16, 32, 64] for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'NB', 'IS_RMS_NORM', 'HAS_DRESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dresidual",
        "annotation": null
      },
      {
        "name": "dresidual_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_bwd_kernel(",
      "    x,",
      "    g,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dg,",
      "    dw,",
      "    db,",
      "    dresidual,",
      "    dresidual_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=m_d).to(tl.float32)",
      "        b_dw = tl.zeros((BT, BD), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "        b_db = tl.zeros((BT, BD), dtype=tl.float32)",
      "",
      "    T = min(i_s * BS + BS, T)",
      "    for i_t in range(i_s * BS, T, BT):",
      "        p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_g = tl.make_block_ptr(g, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "        p_dg = tl.make_block_ptr(dg, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "",
      "        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t,), (BT,), (0,))",
      "            b_mean = tl.load(p_mean, boundary_check=(0,))",
      "        p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t,), (BT,), (0,))",
      "        b_rstd = tl.load(p_rstd, boundary_check=(0,))",
      "",
      "        b_xhat = (",
      "            (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "            if not IS_RMS_NORM",
      "            else b_x * b_rstd[:, None]",
      "        )",
      "        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)",
      "",
      "        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat",
      "        if HAS_BIAS:",
      "            b_y = b_y + b_b[None, :]",
      "        if RECOMPUTE_OUTPUT:",
      "            p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0))",
      "            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_sigmoid_g = tl.sigmoid(b_g)",
      "        if ACTIVATION == \"swish\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"silu\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"sigmoid\":",
      "            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)",
      "            b_dy = b_dy * b_sigmoid_g",
      "        b_wdy = b_dy",
      "",
      "        if HAS_WEIGHT or HAS_BIAS:",
      "            m_t = (i_t + tl.arange(0, BT)) < T",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)",
      "        if HAS_BIAS:",
      "            b_db += tl.where(m_t[:, None], b_dy, 0.0)",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_c2 = tl.sum(b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]",
      "        if HAS_DRESIDUAL:",
      "            p_dres = tl.make_block_ptr(",
      "                dresidual, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            p_dres_in = tl.make_block_ptr(",
      "                dresidual_in, (T, D), (D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            tl.store(",
      "                p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/216.py"
  },
  {
    "name": "layer_norm_gated_bwd_kernel1",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DRESIDUAL': lambda args: args['dresidual'] is not None, 'HAS_WEIGHT': lambda args: args['w'] is not None, 'HAS_BIAS': lambda args: args['b'] is not None, 'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['D', 'IS_RMS_NORM', 'STORE_DRESIDUAL', 'HAS_DRESIDUAL', 'HAS_WEIGHT'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dresidual",
        "annotation": null
      },
      {
        "name": "dresidual_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_gated_bwd_kernel1(",
      "    x,",
      "    g,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dg,",
      "    dw,",
      "    db,",
      "    dresidual,",
      "    dresidual_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    o_d = tl.arange(0, BD)",
      "    mask = o_d < D",
      "    x += i_s * BS * D",
      "    g += i_s * BS * D",
      "    if HAS_DRESIDUAL:",
      "        dresidual += i_s * BS * D",
      "    if STORE_DRESIDUAL:",
      "        dresidual_in += i_s * BS * D",
      "    dy += i_s * BS * D",
      "    dx += i_s * BS * D",
      "    dg += i_s * BS * D",
      "    if RECOMPUTE_OUTPUT:",
      "        y += i_s * BS * D",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_d, mask=mask).to(tl.float32)",
      "        b_dw = tl.zeros((BD,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_d, mask=mask, other=0.0).to(tl.float32)",
      "        b_db = tl.zeros((BD,), dtype=tl.float32)",
      "",
      "    for i_t in range(i_s * BS, min(i_s * BS + BS, T)):",
      "",
      "        b_x = tl.load(x + o_d, mask=mask, other=0).to(tl.float32)",
      "        b_g = tl.load(g + o_d, mask=mask, other=0).to(tl.float32)",
      "        b_dy = tl.load(dy + o_d, mask=mask, other=0).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            b_mean = tl.load(mean + i_t)",
      "        b_rstd = tl.load(rstd + i_t)",
      "",
      "        b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "        b_xhat = tl.where(mask, b_xhat, 0.0)",
      "",
      "        b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat",
      "        if HAS_BIAS:",
      "            b_y = b_y + b_b",
      "        if RECOMPUTE_OUTPUT:",
      "            tl.store(y + o_d, b_y, mask=mask)",
      "",
      "        b_sigmoid_g = tl.sigmoid(b_g)",
      "        if ACTIVATION == \"swish\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"silu\":",
      "            b_dg = b_dy * b_y * (b_sigmoid_g + b_g * b_sigmoid_g * (1 - b_sigmoid_g))",
      "            b_dy = b_dy * b_g * b_sigmoid_g",
      "        elif ACTIVATION == \"sigmoid\":",
      "            b_dg = b_dy * b_y * b_sigmoid_g * (1 - b_sigmoid_g)",
      "            b_dy = b_dy * b_sigmoid_g",
      "        b_wdy = b_dy",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += b_dy * b_xhat",
      "        if HAS_BIAS:",
      "            b_db += b_dy",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_c2 = tl.sum(b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1) * b_rstd",
      "        if HAS_DRESIDUAL:",
      "            b_dres = tl.load(dresidual + o_d, mask=mask, other=0).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(dresidual_in + o_d, b_dx, mask=mask)",
      "        tl.store(dx + o_d, b_dx, mask=mask)",
      "        tl.store(dg + o_d, b_dg, mask=mask)",
      "",
      "        x += D",
      "        g += D",
      "        if HAS_DRESIDUAL:",
      "            dresidual += D",
      "        if STORE_DRESIDUAL:",
      "            dresidual_in += D",
      "        if RECOMPUTE_OUTPUT:",
      "            y += D",
      "        dy += D",
      "        dx += D",
      "        dg += D",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, b_dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, b_db, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/216.py"
  },
  {
    "name": "grpo_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': BLOCK_SIZE}, num_warps=NUM_WARPS, num_stages=NUM_STAGES) for BLOCK_SIZE in [1024, 2048, 4096, 8192] for NUM_WARPS in [8, 16, 32] for NUM_STAGES in [1, 2, 4]], key=['B', 'N'])"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "ref_logp_ptr",
        "annotation": null
      },
      {
        "name": "input_ids_ptr",
        "annotation": null
      },
      {
        "name": "advantages_ptr",
        "annotation": null
      },
      {
        "name": "completion_mask_ptr",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "save_kl",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "start_idx",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def grpo_fwd_kernel(",
      "    logits_ptr,",
      "    ref_logp_ptr,",
      "    input_ids_ptr,",
      "    advantages_ptr,",
      "    completion_mask_ptr,",
      "    loss_ptr,",
      "    lse_ptr,",
      "    beta,",
      "    save_kl: tl.constexpr,",
      "    B,",
      "    M,",
      "    N,",
      "    L,",
      "    start_idx,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "",
      "    off_b = row_idx // L",
      "    N = tl.cast(N, tl.int64)",
      "",
      "    loss_ptr += row_idx",
      "",
      "    completion_mask_ptr += row_idx",
      "    not_skip = tl.load(completion_mask_ptr).to(tl.int1)",
      "    if not_skip == 1:",
      "        ref_logp_ptr += row_idx",
      "        lse_ptr += row_idx",
      "        advantages_ptr += off_b",
      "        logits_ptr += N * (row_idx + off_b)",
      "        input_ids_ptr += row_idx + (off_b + 1) * start_idx",
      "        base_cols = tl.arange(0, BLOCK_SIZE)",
      "",
      "        m_i = -float(\"inf\")",
      "        l_i = 0.0",
      "        for start_n in tl.range(0, N, BLOCK_SIZE):",
      "            cols = start_n + base_cols",
      "            mask = cols < N",
      "            logits = tl.load(logits_ptr + cols, mask=mask, other=-float(\"inf\")).to(",
      "                tl.float32",
      "            )",
      "            m_ij = tl.max(logits)",
      "            new_m_i = tl.maximum(m_i, m_ij)",
      "            l_i = l_i * exp(m_i - new_m_i) + tl.sum(exp(logits - new_m_i))",
      "            m_i = new_m_i",
      "        lse = log(l_i) + m_i",
      "",
      "        idx = tl.load(input_ids_ptr)",
      "        x = tl.load(logits_ptr + idx).to(tl.float32)",
      "        advantage = tl.load(advantages_ptr).to(tl.float32)",
      "        ref_logp = tl.load(ref_logp_ptr)",
      "        logp = x - lse",
      "        diff = ref_logp - logp",
      "        kl = exp(diff) - diff - 1",
      "        loss = kl * beta - advantage",
      "",
      "        tl.store(loss_ptr, loss.to(loss_ptr.dtype.element_ty))",
      "        tl.store(lse_ptr, lse.to(lse_ptr.dtype.element_ty))",
      "        if save_kl:",
      "            tl.store(loss_ptr + M, kl.to(loss_ptr.dtype.element_ty))",
      "    else:",
      "",
      "        tl.store(loss_ptr, 0.0)",
      "        if save_kl:",
      "            tl.store(loss_ptr + M, 0.0)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/217.py"
  },
  {
    "name": "grpo_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=NUM_WARPS, num_stages=NUM_STAGES) for NUM_WARPS in [32] for NUM_STAGES in [4]], key=['B', 'N'])"
    ],
    "args": [
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "dlogits_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "ref_logp_ptr",
        "annotation": null
      },
      {
        "name": "input_ids_ptr",
        "annotation": null
      },
      {
        "name": "advantages_ptr",
        "annotation": null
      },
      {
        "name": "completion_mask_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "start_idx",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def grpo_bwd_kernel(",
      "    dloss_ptr,",
      "    dlogits_ptr,",
      "    logits_ptr,",
      "    ref_logp_ptr,",
      "    input_ids_ptr,",
      "    advantages_ptr,",
      "    completion_mask_ptr,",
      "    lse_ptr,",
      "    beta,",
      "    B,",
      "    N,",
      "    L,",
      "    start_idx,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    row_idx = tl.program_id(0)",
      "    off_b = row_idx // L",
      "",
      "    N = tl.cast(N, tl.int64)",
      "",
      "    dlogits_ptr += N * (row_idx + off_b)",
      "    base_cols = tl.arange(0, BLOCK_SIZE)",
      "    completion_mask_ptr += row_idx",
      "    not_skip = tl.load(completion_mask_ptr).to(tl.int1)",
      "",
      "    if not_skip == 1:",
      "        lse_ptr += row_idx",
      "        dloss_ptr += row_idx",
      "        advantages_ptr += off_b",
      "        ref_logp_ptr += row_idx",
      "        logits_ptr += N * (row_idx + off_b)",
      "        input_ids_ptr += row_idx + (off_b + 1) * start_idx",
      "        dloss = tl.load(dloss_ptr).to(tl.float32)",
      "        lse = tl.load(lse_ptr).to(tl.float32)",
      "        idx = tl.load(input_ids_ptr)",
      "        x = tl.load(logits_ptr + idx).to(tl.float32)",
      "        advantage = tl.load(advantages_ptr).to(tl.float32)",
      "        ref_logp = tl.load(ref_logp_ptr)",
      "",
      "        tl.debug_barrier()",
      "        logp = x - lse",
      "",
      "        dlogp = (beta * (-1.0 * exp(ref_logp - logp) + 1) - advantage) * dloss",
      "",
      "        for start_n in tl.range(0, N, BLOCK_SIZE):",
      "            cols = start_n + base_cols",
      "            mask = cols < N",
      "            logits = tl.load(logits_ptr + cols, mask=mask, other=-float(\"inf\")).to(",
      "                tl.float32",
      "            )",
      "            probs = exp(logits - lse)",
      "            dlogits = tl.where(cols == idx, 1 - probs, -probs) * dlogp",
      "",
      "            tl.store(",
      "                dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask",
      "            )",
      "    else:",
      "        dlogits = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "        for start_n in tl.range(0, N, BLOCK_SIZE):",
      "            cols = start_n + base_cols",
      "            mask = cols < N",
      "",
      "            tl.store(",
      "                dlogits_ptr + cols, dlogits.to(dlogits_ptr.dtype.element_ty), mask=mask",
      "            )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/217.py"
  },
  {
    "name": "l2norm_fwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "eps",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_fwd_kernel1(",
      "    x,",
      "    y,",
      "    D,",
      "    BD: tl.constexpr,",
      "    eps,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    y += i_t * D",
      "",
      "    cols = tl.arange(0, BD)",
      "    mask = cols < D",
      "    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=0)",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "",
      "    b_y = b_x * b_rstd",
      "    tl.store(y + cols, b_y, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/218.py"
  },
  {
    "name": "l2norm_bwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_bwd_kernel1(",
      "    x,",
      "    dy,",
      "    dx,",
      "    eps,",
      "    D,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    x += i_t * D",
      "    dx += i_t * D",
      "    dy += i_t * D",
      "",
      "    cols = tl.arange(0, BD)",
      "    mask = cols < D",
      "    b_x = tl.load(x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x)",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    b_dy = tl.load(dy + cols, mask=mask, other=0.0).to(tl.float32)",
      "    b_dx = b_dy * b_rstd - tl.sum(b_dy * b_x) * (1 / (b_var + eps)) * b_rstd * b_x",
      "    tl.store(dx + cols, b_dx, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/218.py"
  },
  {
    "name": "l2norm_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16] for BT in BT_LIST], key=['D', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_fwd_kernel(",
      "    x,",
      "    y,",
      "    eps,",
      "    NB: tl.constexpr,",
      "    T: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=1)",
      "    b_y = b_x / tl.sqrt(b_var + eps)[:, None]",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/218.py"
  },
  {
    "name": "l2norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16] for BT in BT_LIST], key=['D', 'NB'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def l2norm_bwd_kernel(",
      "    x,",
      "    dy,",
      "    dx,",
      "    eps,",
      "    NB: tl.constexpr,",
      "    T: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    p_dy = tl.make_block_ptr(dy, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    p_dx = tl.make_block_ptr(dx, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)",
      "    b_var = tl.sum(b_x * b_x, axis=1)[:, None]",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    b_dx = (",
      "        b_dy * b_rstd",
      "        - tl.sum(b_dy * b_x, axis=1)[:, None] / (b_var + eps) * b_rstd * b_x",
      "    )",
      "    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/218.py"
  },
  {
    "name": "layer_norm_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for BT in [32, 64, 128] for num_warps in [2, 4, 8]], key=['D', 'NB', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "res",
        "annotation": null
      },
      {
        "name": "res_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel(",
      "    x,",
      "    y,",
      "    w,",
      "    b,",
      "    res,",
      "    res_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    T,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "",
      "    o_t = i_t * BT + tl.arange(0, BT)",
      "    o_g = o_t % G",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    p_x = tl.make_block_ptr(x, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        p_res = tl.make_block_ptr(res, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "        b_x += tl.load(p_res, boundary_check=(0, 1)).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        p_res_out = tl.make_block_ptr(",
      "            res_out, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0)",
      "        )",
      "        tl.store(p_res_out, b_x.to(p_res_out.dtype.element_ty), boundary_check=(0, 1))",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=1) / D",
      "        p_mean = tl.make_block_ptr(mean, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_mean, b_mean.to(p_mean.dtype.element_ty), boundary_check=(0,))",
      "        b_xbar = tl.where(m_d[None, :], b_x - b_mean[:, None], 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    else:",
      "        b_xbar = tl.where(m_d[None, :], b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=1) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "",
      "    p_rstd = tl.make_block_ptr(rstd, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_rstd, b_rstd.to(p_rstd.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(",
      "            tl.float32",
      "        )",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + o_g[:, None] * D + o_d[None, :], mask=m_d[None, :]).to(",
      "            tl.float32",
      "        )",
      "    b_x_hat = (",
      "        (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "        if not IS_RMS_NORM",
      "        else b_x * b_rstd[:, None]",
      "    )",
      "    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b",
      "",
      "    p_y = tl.make_block_ptr(y, (T, D), (D, 1), (i_t * BT, 0), (BT, BD), (1, 0))",
      "    tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/219.py"
  },
  {
    "name": "layer_norm_fwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8, 16]], key=['D', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "res",
        "annotation": null
      },
      {
        "name": "res_out",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel1(",
      "    x,",
      "    y,",
      "    w,",
      "    b,",
      "    res,",
      "    res_out,",
      "    mean,",
      "    rstd,",
      "    eps,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    i_t = tl.program_id(0)",
      "    i_g = i_t % G",
      "",
      "    x += i_t * D",
      "    y += i_t * D",
      "    if HAS_RESIDUAL:",
      "        res += i_t * D",
      "    if STORE_RESIDUAL_OUT:",
      "        res_out += i_t * D",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    b_x = tl.load(x + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        b_x += tl.load(res + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(res_out + o_d, b_x, mask=m_d)",
      "    if not IS_RMS_NORM:",
      "        b_mean = tl.sum(b_x, axis=0) / D",
      "        tl.store(mean + i_t, b_mean)",
      "        b_xbar = tl.where(m_d, b_x - b_mean, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    else:",
      "        b_xbar = tl.where(m_d, b_x, 0.0)",
      "        b_var = tl.sum(b_xbar * b_xbar, axis=0) / D",
      "    b_rstd = 1 / tl.sqrt(b_var + eps)",
      "    tl.store(rstd + i_t, b_rstd)",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + i_g * D + o_d, mask=m_d).to(tl.float32)",
      "    b_x_hat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "    b_y = b_x_hat * b_w if HAS_WEIGHT else b_x_hat",
      "    if HAS_BIAS:",
      "        b_y = b_y + b_b",
      "",
      "    tl.store(y + o_d, b_y, mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/219.py"
  },
  {
    "name": "layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for BT in [32, 64] for num_warps in [2, 4, 8]], key=['D', 'NB', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dres",
        "annotation": null
      },
      {
        "name": "dres_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NB",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel(",
      "    x,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dw,",
      "    db,",
      "    dres,",
      "    dres_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    NB: tl.constexpr,",
      "    GS: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    i_g, i_sg = i_s // GS, i_s % GS",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + i_g * D + o_d, mask=m_d).to(tl.float32)",
      "        b_dw = tl.zeros((BT, BD), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        b_b = tl.load(b + i_g * D + o_d, mask=m_d, other=0.0).to(tl.float32)",
      "        b_db = tl.zeros((BT, BD), dtype=tl.float32)",
      "",
      "    T = min(i_sg * BS + BS, T // G)",
      "    for i_t in range(i_sg * BS, T, BT):",
      "        p_x = tl.make_block_ptr(",
      "            x + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "        )",
      "        p_dy = tl.make_block_ptr(",
      "            dy + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "        )",
      "        p_dx = tl.make_block_ptr(",
      "            dx + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dy = tl.load(p_dy, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            p_mean = tl.make_block_ptr(mean + i_g, (T,), (G,), (i_t,), (BT,), (0,))",
      "            b_mean = tl.load(p_mean, boundary_check=(0,))",
      "        p_rstd = tl.make_block_ptr(rstd + i_g, (T,), (G,), (i_t,), (BT,), (0,))",
      "        b_rstd = tl.load(p_rstd, boundary_check=(0,))",
      "",
      "        b_xhat = (",
      "            (b_x - b_mean[:, None]) * b_rstd[:, None]",
      "            if not IS_RMS_NORM",
      "            else b_x * b_rstd[:, None]",
      "        )",
      "        b_xhat = tl.where(m_d[None, :], b_xhat, 0.0)",
      "",
      "        b_y = b_xhat * b_w[None, :] if HAS_WEIGHT else b_xhat",
      "        if HAS_BIAS:",
      "            b_y = b_y + b_b[None, :]",
      "        if RECOMPUTE_OUTPUT:",
      "            p_y = tl.make_block_ptr(",
      "                y + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            tl.store(p_y, b_y.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_wdy = b_dy",
      "",
      "        if HAS_WEIGHT or HAS_BIAS:",
      "            m_t = (i_t + tl.arange(0, BT)) < T",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += tl.where(m_t[:, None], b_dy * b_xhat, 0.0)",
      "        if HAS_BIAS:",
      "            b_db += tl.where(m_t[:, None], b_dy, 0.0)",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_c2 = tl.sum(b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1[:, None] + b_c2[:, None])) * b_rstd[:, None]",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=1) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1[:, None]) * b_rstd[:, None]",
      "        if HAS_DRESIDUAL:",
      "            p_dres = tl.make_block_ptr(",
      "                dres + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            b_dres = tl.load(p_dres, boundary_check=(0, 1)).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            p_dres_in = tl.make_block_ptr(",
      "                dres_in + i_g * D, (T, D), (G * D, 1), (i_t, 0), (BT, BD), (1, 0)",
      "            )",
      "            tl.store(",
      "                p_dres_in, b_dx.to(p_dres_in.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, tl.sum(b_dw, axis=0), mask=m_d)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, tl.sum(b_db, axis=0), mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/219.py"
  },
  {
    "name": "layer_norm_bwd_kernel1",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['y'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8]], key=['D', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dres",
        "annotation": null
      },
      {
        "name": "dres_in",
        "annotation": null
      },
      {
        "name": "mean",
        "annotation": null
      },
      {
        "name": "rstd",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_WEIGHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel1(",
      "    x,",
      "    w,",
      "    b,",
      "    y,",
      "    dy,",
      "    dx,",
      "    dw,",
      "    db,",
      "    dres,",
      "    dres_in,",
      "    mean,",
      "    rstd,",
      "    T,",
      "    G: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    GS: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_WEIGHT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    i_s = tl.program_id(0)",
      "    i_g, i_sg = i_s // GS, i_s % GS",
      "",
      "    o_d = tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    if HAS_WEIGHT:",
      "        b_w = tl.load(w + i_g * D + o_d, mask=mask).to(tl.float32)",
      "        b_dw = tl.zeros((BD,), dtype=tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b_b = tl.load(b + i_g * D + o_d, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b_db = tl.zeros((BD,), dtype=tl.float32)",
      "",
      "    for i_t in range(i_sg * BS * G + i_g, min((i_sg * BS + BS) * G + i_g, T), G):",
      "        b_x = tl.load(x + i_t * D + o_d, mask=mask, other=0).to(tl.float32)",
      "        b_dy = tl.load(dy + i_t * D + o_d, mask=mask, other=0).to(tl.float32)",
      "",
      "        if not IS_RMS_NORM:",
      "            b_mean = tl.load(mean + i_t)",
      "        b_rstd = tl.load(rstd + i_t)",
      "",
      "        b_xhat = (b_x - b_mean) * b_rstd if not IS_RMS_NORM else b_x * b_rstd",
      "        b_xhat = tl.where(mask, b_xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            b_y = b_xhat * b_w if HAS_WEIGHT else b_xhat",
      "            if HAS_BIAS:",
      "                b_y = b_y + b_b",
      "            tl.store(y + i_t * D + o_d, b_y, mask=mask)",
      "        b_wdy = b_dy",
      "        if HAS_WEIGHT:",
      "            b_wdy = b_dy * b_w",
      "            b_dw += b_dy * b_xhat",
      "        if HAS_BIAS:",
      "            b_db += b_dy",
      "        if not IS_RMS_NORM:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_c2 = tl.sum(b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - (b_xhat * b_c1 + b_c2)) * b_rstd",
      "        else:",
      "            b_c1 = tl.sum(b_xhat * b_wdy, axis=0) / D",
      "            b_dx = (b_wdy - b_xhat * b_c1) * b_rstd",
      "        if HAS_DRESIDUAL:",
      "            b_dres = tl.load(dres + i_t * D + o_d, mask=mask, other=0).to(tl.float32)",
      "            b_dx += b_dres",
      "",
      "        b_dx = tl.cast(b_dx, dtype=dx.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "        if STORE_DRESIDUAL:",
      "            tl.store(dres_in + i_t * D + o_d, b_dx, mask=mask)",
      "        tl.store(dx + i_t * D + o_d, b_dx, mask=mask)",
      "",
      "    if HAS_WEIGHT:",
      "        tl.store(dw + i_s * D + o_d, b_dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(db + i_s * D + o_d, b_db, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/219.py"
  },
  {
    "name": "layer_norm_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None, 'HAS_Z': lambda args: args['Z'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_fwd_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    Z,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/220.py"
  },
  {
    "name": "layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None, 'HAS_Z': lambda args: args['Z'] is not None, 'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DZ",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dz_row",
        "annotation": null
      },
      {
        "name": "stride_dw_row",
        "annotation": null
      },
      {
        "name": "stride_db_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Z,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DZ,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_z_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dz_row,",
      "    stride_dw_row,",
      "    stride_db_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row + group * N",
      "    if HAS_Z:",
      "        Z += row_start * stride_z_row + group * N",
      "        DZ += row_start * stride_dz_row + group * N",
      "    DY += row_start * stride_dy_row + group * N",
      "    DX += row_start * stride_dx_row + group * N",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:",
      "        B += group * N",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            x_og = x",
      "            x = x_og * z * tl.sigmoid(z)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if HAS_Z and NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            z_sigmoid = tl.sigmoid(z)",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            if RECOMPUTE_OUTPUT:",
      "                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)",
      "            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dy *= z * z_sigmoid",
      "        else:",
      "            if RECOMPUTE_OUTPUT:",
      "                y = xhat * w + b if HAS_BIAS else xhat * w",
      "                tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        c1 = tl.sum(xhat * wdy, axis=0) / N",
      "        if not IS_RMS_NORM:",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            dx = (wdy - xhat * c1) * rstd",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z_sigmoid = tl.sigmoid(z)",
      "            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dx *= z * z_sigmoid",
      "",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_Z:",
      "            Z += stride_z_row",
      "            DZ += stride_dz_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/220.py"
  },
  {
    "name": "rotary_embedding_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['B', 'H', 'D', 'INTERLEAVED'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "cos",
        "annotation": null
      },
      {
        "name": "sin",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "seq_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "R",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TR",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_SEQLEN_OFFSETS_TENSOR",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "INTERLEAVED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CONJUGATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rotary_embedding_kernel(",
      "    x,",
      "    cos,",
      "    sin,",
      "    y,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    seq_offsets,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    D: tl.constexpr,",
      "    R: tl.constexpr,",
      "    TR: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_SEQLEN_OFFSETS_TENSOR: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    INTERLEAVED: tl.constexpr,",
      "    CONJUGATE: tl.constexpr,",
      "):",
      "    i_t, i_b, i_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n), tl.load(cu_seqlens + i_n + 1)",
      "        T = eos - bos",
      "        x = x + bos * H * D + i_h * D",
      "        y = y + bos * H * D + i_h * D",
      "    else:",
      "        i_n = i_b",
      "        x = x + i_n * T * H * D + i_h * D",
      "        y = y + i_n * T * H * D + i_h * D",
      "",
      "    if i_t * BT >= T:",
      "        return",
      "",
      "    o_t = i_t * BT + tl.arange(0, BT)",
      "    if not IS_SEQLEN_OFFSETS_TENSOR:",
      "        o_cs = o_t + seq_offsets",
      "    else:",
      "        o_cs = o_t + tl.load(seq_offsets + i_n)",
      "    m_t = (o_t >= 0) & (o_t < T) & (o_cs >= 0) & (o_cs < TR)",
      "",
      "    if not INTERLEAVED:",
      "",
      "        o_r = tl.arange(0, BD // 2)",
      "        p_x = x + o_t[:, None] * H * D + o_r[None, :]",
      "        p_cos = cos + (o_cs[:, None] * R + o_r[None, :])",
      "        p_sin = sin + (o_cs[:, None] * R + o_r[None, :])",
      "        mask = m_t[:, None] & (o_r < R)[None, :]",
      "",
      "        b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)",
      "        b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)",
      "        b_x0 = tl.load(p_x, mask=mask, other=0.0).to(tl.float32)",
      "        b_x1 = tl.load(p_x + R, mask=mask, other=0.0).to(tl.float32)",
      "        if CONJUGATE:",
      "            b_sin = -b_sin",
      "        b_o0 = b_x0 * b_cos - b_x1 * b_sin",
      "        b_o1 = b_x0 * b_sin + b_x1 * b_cos",
      "",
      "        p_y = y + (o_t[:, None] * H * D + o_r[None, :])",
      "        tl.store(p_y, b_o0, mask=mask)",
      "        tl.store(p_y + R, b_o1, mask=mask)",
      "    else:",
      "",
      "        o_d = tl.arange(0, BD)",
      "        o_d_swap = o_d + ((o_d + 1) % 2) * 2 - 1",
      "        o_d_repeat = tl.arange(0, BD) // 2",
      "        p_x0 = x + o_t[:, None] * H * D + o_d[None, :]",
      "        p_x1 = x + o_t[:, None] * H * D + o_d_swap[None, :]",
      "        p_cos = cos + (o_cs[:, None] * R + o_d_repeat[None, :])",
      "        p_sin = sin + (o_cs[:, None] * R + o_d_repeat[None, :])",
      "        mask = m_t[:, None] & (o_d_repeat < R)[None, :]",
      "",
      "        b_cos = tl.load(p_cos, mask=mask, other=1.0).to(tl.float32)",
      "        b_sin = tl.load(p_sin, mask=mask, other=0.0).to(tl.float32)",
      "        b_x0 = tl.load(p_x0, mask=mask, other=0.0).to(tl.float32)",
      "        b_x1 = tl.load(p_x1, mask=mask, other=0.0).to(tl.float32)",
      "        if CONJUGATE:",
      "            b_sin = -b_sin",
      "        b_o0 = b_x0 * b_cos",
      "        b_o1 = b_x1 * b_sin",
      "        b_y = tl.where(o_d[None, :] % 2 == 0, b_o0 - b_o1, b_o0 + b_o1)",
      "        p_y = y + (o_t[:, None] * H * D + o_d[None, :])",
      "        tl.store(p_y, b_y, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/221.py"
  },
  {
    "name": "token_shift_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [1, 2, 3, 4]], key=['BD'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def token_shift_fwd_kernel(",
      "    x,",
      "    y,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "",
      "    if IS_VARLEN:",
      "        i_n = i_b",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        g_t = i_t + bos",
      "",
      "        if g_t >= eos:",
      "            return",
      "",
      "        is_first_pos = i_t == 0",
      "    else:",
      "        g_t = i_t",
      "        is_first_pos = g_t == 0",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    if IS_VARLEN:",
      "        base_offset = g_t * D + o_d",
      "    else:",
      "        base_offset = i_b * T * D + g_t * D + o_d",
      "",
      "    b_x = tl.load(x + base_offset, mask=m_d)",
      "",
      "    if is_first_pos:",
      "",
      "        tl.store(y + base_offset, -b_x, mask=m_d)",
      "    else:",
      "",
      "        if IS_VARLEN:",
      "            prev_offset = (g_t - 1) * D + o_d",
      "        else:",
      "            prev_offset = i_b * T * D + (g_t - 1) * D + o_d",
      "",
      "        prev_values = tl.load(x + prev_offset, mask=m_d)",
      "        delta = prev_values - b_x",
      "        tl.store(y + base_offset, delta, mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/222.py"
  },
  {
    "name": "token_shift_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [1, 2, 3, 4]], key=['D'])"
    ],
    "args": [
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dy",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def token_shift_bwd_kernel(",
      "    dx,",
      "    dy,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "    if IS_VARLEN:",
      "        i_n = i_b",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        g_t = i_t + bos",
      "        if g_t >= eos:",
      "            return",
      "",
      "        is_last_pos = g_t == eos - 1",
      "    else:",
      "        g_t = i_t",
      "        is_last_pos = g_t == T - 1",
      "",
      "    o_d = tl.arange(0, BD)",
      "    m_d = o_d < D",
      "",
      "    if IS_VARLEN:",
      "        base_offset = g_t * D + o_d",
      "    else:",
      "        base_offset = i_b * T * D + g_t * D + o_d",
      "",
      "    b_dy = tl.load(dy + base_offset, mask=m_d)",
      "",
      "    if is_last_pos:",
      "",
      "        b_dx = -b_dy",
      "    else:",
      "",
      "        if IS_VARLEN:",
      "            next_offset = (g_t + 1) * D + o_d",
      "        else:",
      "            next_offset = i_b * T * D + (g_t + 1) * D + o_d",
      "",
      "        b_dx = -b_dy + tl.load(dy + next_offset, mask=m_d)",
      "",
      "    tl.store(dx + base_offset, b_dx, mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/modules/222.py"
  },
  {
    "name": "chunk_abc_fwd_kernel_h",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORMK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    z,",
      "    h,",
      "    h0,",
      "    ht,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    NORMK: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "    if NORMK:",
      "        p_z0 = tl.make_block_ptr(",
      "            z + i_bh * T * K, (T * K,), (1,), (i_k * BK,), (BK,), (0,)",
      "        )",
      "    else:",
      "        p_z0 = tl.make_block_ptr(",
      "            z + i_bh * T * V, (T * V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "    b_zp = tl.load(p_z0).to(tl.float32)",
      "    for i_t in range(NT):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        if NORMK:",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * K,",
      "                (T * K,),",
      "                (1,),",
      "                ((i_t * BT + BT - 1) * K + i_k * BK,),",
      "                (BK,),",
      "                (0,),",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zp - b_zc), b_zc",
      "",
      "            b_h = b_h * b_r[:, None]",
      "            b_k = exp(b_k - b_zc[:, None]).to(b_k.dtype)",
      "        else:",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * V,",
      "                (T * V,),",
      "                (1,),",
      "                ((i_t * BT + BT - 1) * V + i_v * BV,),",
      "                (BV,),",
      "                (0,),",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zp - b_zc), b_zc",
      "",
      "            b_h = b_h * b_r[None, :]",
      "            b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)",
      "",
      "        b_h += tl.dot(b_k, b_v, allow_tf32=False)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_fwd_kernel_intra_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_intra_K(",
      "    v,",
      "    z,",
      "    o,",
      "    A,",
      "    T,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * V,",
      "        (T * V,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC) * V + i_v * BV,),",
      "        (BV,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_o = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(0, i_i):",
      "        p_A = tl.make_block_ptr(",
      "            A + i_bh * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_A, exp(b_v - b_zn[None, :]).to(b_v.dtype), allow_tf32=False)",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "    b_o *= exp(b_zn[None, :] - b_z)",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    for j in range(0, BC):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "",
      "        b_A = tl.load(A + o_A + j, mask=m_A, other=0)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] >= j",
      "        b_o += tl.where(m_i, b_A[:, None] * exp(b_v[None, :] - b_z), 0)",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_fwd_kernel_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_K(",
      "    q,",
      "    k,",
      "    z,",
      "    h,",
      "    o,",
      "    A,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h, allow_tf32=False)",
      "",
      "        b_A += tl.dot(b_q, b_k, allow_tf32=False)",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "",
      "    p_zp = tl.make_block_ptr(",
      "        z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)",
      "    )",
      "    b_zp = tl.load(p_zp, boundary_check=(0,))",
      "    b_o = b_o * exp(b_zp[None, :] - b_z)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    if i_v == 0:",
      "        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_fwd_kernel_intra_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_intra_V(",
      "    q,",
      "    k,",
      "    z,",
      "    A,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC",
      "    n_bh = tl.num_programs(2)",
      "",
      "    if i_i > i_j:",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (K, T),",
      "            (1, K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_k * n_bh + i_bh) * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        p_zn = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "",
      "        b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_q = (b_q * exp(b_zn[None, :] - b_z) * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k = exp(b_k - b_zn[:, None]).to(b_k.dtype)",
      "",
      "        b_A = tl.dot(b_q, b_k, allow_tf32=False)",
      "        tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))",
      "    elif i_i == i_j:",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_j * BC) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "",
      "        o_i = tl.arange(0, BC)",
      "        o_A = (",
      "            (i_bh + i_k * n_bh) * T * BT",
      "            + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT",
      "            + i_j * BC",
      "        )",
      "        m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "        for j in range(0, BC):",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0,)).to(tl.float32)",
      "",
      "            b_A = tl.sum(b_q * exp(b_k[None, :] - b_z) * scale, 1)",
      "            b_A = tl.where(o_i >= j, b_A, 0.0)",
      "            tl.store(A + o_A + j, b_A.to(b_q.dtype), mask=m_A)",
      "",
      "            p_k = tl.advance(p_k, (K,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_fwd_kernel_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_fwd_kernel_V(",
      "    q,",
      "    v,",
      "    z,",
      "    h,",
      "    o,",
      "    A,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_zp = tl.make_block_ptr(",
      "            z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "",
      "        b_zp = tl.load(p_zp, boundary_check=(0,))",
      "        b_q = (b_q * exp(b_zp[None, :] - b_z)).to(b_q.dtype)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        if i_k >= 0:",
      "            b_o += tl.dot(b_q, b_h, allow_tf32=False)",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_o += tl.dot(b_A.to(b_v.dtype), b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_dh",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORMK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_dh(",
      "    q,",
      "    z,",
      "    do,",
      "    dh,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    NORMK: tl.constexpr,",
      "):",
      "    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    b_zp = tl.full([BK if NORMK else BV], float(\"inf\"), dtype=tl.float32)",
      "    for i_t in range(NT - 1, -1, -1):",
      "        i_p = tl.maximum(i_t * BT - 1, 0)",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        if NORMK:",
      "            p_z = tl.make_block_ptr(",
      "                z + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "            )",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zc - b_zp), b_zc",
      "",
      "            b_z = tl.load(p_z, boundary_check=(0, 1))",
      "            b_q = (b_q * exp(b_zc[:, None] - b_z)).to(b_q.dtype)",
      "",
      "            b_dh = b_dh * b_r[:, None]",
      "        else:",
      "            p_z = tl.make_block_ptr(",
      "                z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            p_zc = tl.make_block_ptr(",
      "                z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)",
      "            )",
      "",
      "            b_zc = tl.load(p_zc, boundary_check=(0,))",
      "            b_r, b_zp = exp(b_zc - b_zp), b_zc",
      "",
      "            b_z = tl.load(p_z, boundary_check=(0,))",
      "            b_do = (b_do * exp(b_zc[None, :] - b_z)).to(b_do.dtype)",
      "",
      "            b_dh = b_dh * b_r[None, :]",
      "",
      "        b_dh += tl.dot(b_q, b_do, allow_tf32=False)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_V(",
      "    k,",
      "    v,",
      "    z,",
      "    h,",
      "    A,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dA,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "    n_bh = tl.num_programs(2)",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_zc = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T * K,),",
      "        (1,),",
      "        ((i_t * BT + BT - 1) * K + i_k * BK,),",
      "        (BK,),",
      "        (0,),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (BT, T), (1, BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "",
      "    b_zc = tl.load(p_zc, boundary_check=(0,))",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_k = exp(b_k - b_zc[None, :]).to(b_k.dtype)",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * V * K,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * n_bh + i_bh) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dv = tl.dot(b_k, b_dh, allow_tf32=False)",
      "        if i_k == 0:",
      "            b_dv += tl.dot(b_A.to(b_do.dtype), b_do, allow_tf32=False)",
      "        b_do = (b_do * scale).to(b_do.dtype)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_dA += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)",
      "",
      "        b_dq += tl.dot(b_do, b_h, allow_tf32=False)",
      "",
      "        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_zp = tl.make_block_ptr(",
      "        z + i_bh * T * K, (T * K,), (1,), (i_p * K + i_k * BK,), (BK,), (0,)",
      "    )",
      "",
      "    b_zp = tl.load(p_zp, boundary_check=(0,))",
      "",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "    b_z = exp(b_zp[None, :] - b_z)",
      "",
      "    b_dq = b_dq * b_z",
      "    b_dk = b_dk * b_k",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dA = tl.make_block_ptr(",
      "        dA + i_bh * T * BT,",
      "        (",
      "            T,",
      "            BT,",
      "        ),",
      "        (BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_dA = tl.where(m_s, b_dA, 0.0).to(b_k.dtype)",
      "    if i_k == 0:",
      "        tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_intra_V",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_intra_V(",
      "    q,",
      "    k,",
      "    z,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    T,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    p_z = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T * K,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC) * K + i_k * BK,),",
      "        (BK,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_z = tl.load(p_z, boundary_check=(0, 1))",
      "    b_zq = exp(b_zn[None, :] - b_z)",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    for i_j in range(0, i_i):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_j * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_dA = tl.make_block_ptr(",
      "            dA + i_bh * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kz = exp(b_k - b_zn[None, :]).to(b_k.dtype)",
      "",
      "        b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "        b_dq += tl.dot(b_dA, b_kz, allow_tf32=False)",
      "    b_dq *= b_zq",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_i * BC",
      "    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    for j in range(0, BC):",
      "        p_kj = tl.make_block_ptr(",
      "            k + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "",
      "        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)",
      "",
      "        b_kj = tl.load(p_kj, boundary_check=(0,)).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] >= j",
      "",
      "        b_dq += tl.where(m_i, b_dA[:, None] * exp(b_kj[None, :] - b_z), 0.0)",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * K,",
      "        (T * K,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC + BC - 1) * K + i_k * BK,),",
      "        (BK,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_kz = exp(b_k - b_zn[None, :])",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_j * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i_t * BT + i_j * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_dA = tl.make_block_ptr(",
      "            dA + i_bh * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_j * BC, i_i * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_qz = (b_q * exp(b_zn[None, :] - b_z)).to(b_q.dtype)",
      "",
      "        b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "        b_dk += tl.dot(tl.trans(b_dA), b_qz, allow_tf32=False)",
      "    b_dk *= b_kz",
      "",
      "    o_dA = i_bh * T * BT + (i_t * BT + i_i * BC) * BT + i_i * BC + tl.arange(0, BC)",
      "    for j in range(0, BC):",
      "        p_qj = tl.make_block_ptr(",
      "            q + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "        p_zj = tl.make_block_ptr(",
      "            z + i_bh * T * K,",
      "            (T * K,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * K + i_k * BK,),",
      "            (BK,),",
      "            (0,),",
      "        )",
      "",
      "        b_dA = tl.load(dA + o_dA + j * BT, mask=(i_t * BT + i_i * BC + j < T), other=0)",
      "",
      "        b_qj = tl.load(p_qj, boundary_check=(0,)).to(tl.float32)",
      "        b_zj = tl.load(p_zj, boundary_check=(0,)).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dk += tl.where(",
      "            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_k - b_zj[None, :]), 0.0",
      "        )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_intra_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_intra_K(",
      "    v,",
      "    z,",
      "    do,",
      "    dA,",
      "    scale,",
      "    T,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC",
      "    n_bh = tl.num_programs(2)",
      "",
      "    if i_i > i_j:",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (V, T),",
      "            (1, V),",
      "            (i_v * BV, i_t * BT + i_j * BC),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_zn = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_dA = tl.make_block_ptr(",
      "            dA + (i_bh + i_v * n_bh) * T * BT,",
      "            (T, BT),",
      "            (BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "",
      "        b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_zn[None, :] - b_z) * scale).to(b_do.dtype)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v = exp(b_v - b_zn[:, None]).to(b_v.dtype)",
      "",
      "        b_dA = tl.dot(b_do, b_v, allow_tf32=False)",
      "        tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))",
      "    elif i_i == i_j:",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_j * BC) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale",
      "",
      "        o_i = tl.arange(0, BC)",
      "        o_A = (",
      "            (i_bh + i_v * n_bh) * T * BT",
      "            + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT",
      "            + i_j * BC",
      "        )",
      "        m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "        for j in range(0, BC):",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0,)).to(tl.float32)",
      "",
      "            b_dA = tl.sum(b_do * exp(b_v[None, :] - b_z), 1)",
      "            b_dA = tl.where(o_i >= j, b_dA, 0)",
      "            tl.store(dA + o_A + j, b_dA.to(b_do.dtype), mask=m_A)",
      "",
      "            p_v = tl.advance(p_v, (V,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_K",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_K(",
      "    q,",
      "    k,",
      "    v,",
      "    z,",
      "    h,",
      "    A,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dA,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_p = tl.maximum(i_t * BT - 1, 0)",
      "    n_bh = tl.num_programs(2)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (i_k * n_bh + i_bh) * T * BT,",
      "        (",
      "            T,",
      "            BT,",
      "        ),",
      "        (BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k), allow_tf32=False)",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_zp = tl.make_block_ptr(",
      "            z + i_bh * T * V, (T * V,), (1,), (i_p * V + i_v * BV,), (BV,), (0,)",
      "        )",
      "        p_zc = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + BT - 1) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * NT * K * V + i_t * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * NT * K * V + i_t * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * n_bh + i_bh) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_zp = tl.load(p_zp, boundary_check=(0,))",
      "        b_zc = tl.load(p_zc, boundary_check=(0,))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v = exp(b_v - b_zc[None, :]).to(b_v.dtype)",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_z = exp(b_zp[None, :] - b_z)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * b_z * scale).to(b_do.dtype)",
      "",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dq += tl.dot(b_do, b_h, allow_tf32=False)",
      "        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)",
      "",
      "        b_dv = b_v * tl.dot(b_k, b_dh, allow_tf32=False)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dA = tl.make_block_ptr(",
      "        dA + i_bh * T * BT,",
      "        (",
      "            T,",
      "            BT,",
      "        ),",
      "        (BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "    b_dq += tl.dot(b_dA, b_k, allow_tf32=False)",
      "    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q, allow_tf32=False)",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * T * K, (T, K), (K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_intra_KV",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_intra_KV(",
      "    v,",
      "    z,",
      "    A,",
      "    do,",
      "    dv,",
      "    T,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * V,",
      "        (T * V,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC + BC - 1) * V + i_v * BV,),",
      "        (BV,),",
      "        (0,),",
      "    )",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + i_bh * T * BT,",
      "            (BT, T),",
      "            (1, BT),",
      "            (i_i * BC, i_t * BT + i_j * BC),",
      "            (BC, BC),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_zn[None, :] - b_z)).to(b_do.dtype)",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_A, b_do, allow_tf32=False)",
      "    b_dv *= exp(b_v - b_zn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    for j in range(0, BC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + i_bh * T * BT,",
      "            (T * BT,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * BT + i_i * BC,),",
      "            (BC,),",
      "            (0,),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V,",
      "            (T * V,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * V + i_v * BV,),",
      "            (BV,),",
      "            (0,),",
      "        )",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0,))",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0,))",
      "        b_do = tl.load(p_do, boundary_check=(0,))",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dv += tl.where(",
      "            m_i, exp(b_v - b_z[None, :]) * b_A[:, None] * b_do[None, :], 0.0",
      "        )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + i_bh * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_rcum_inter",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "ss",
        "annotation": null
      },
      {
        "name": "doo",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_rcum_inter(",
      "    s,",
      "    z,",
      "    ss,",
      "    doo,",
      "    T,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "    i_m, i_bh = tl.program_id(0), tl.program_id(1)",
      "",
      "    b_sp = tl.zeros(",
      "        [",
      "            BS,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    b_zp = tl.full(",
      "        [",
      "            BS,",
      "        ],",
      "        float(\"inf\"),",
      "        dtype=tl.float32,",
      "    )",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_s = tl.make_block_ptr(",
      "            s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "        p_zc = tl.make_block_ptr(",
      "            z + i_bh * T * S, (T * S,), (1,), ((i_t * BT) * S + i_m * BS,), (BS,), (0,)",
      "        )",
      "        p_ss = tl.make_block_ptr(",
      "            ss + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "        p_doo = tl.make_block_ptr(",
      "            doo + i_bh * T * S, (T, S), (S, 1), (i_t * BT, i_m * BS), (BT, BS), (1, 0)",
      "        )",
      "",
      "        b_zc = tl.load(p_zc, boundary_check=(0,))",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1))",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_ss = tl.load(p_ss, boundary_check=(0, 1))",
      "",
      "        b_doo = exp(b_s - b_zp[None, :]) * b_sp[None, :]",
      "        tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_sp = b_sp * exp(b_zc - b_zp) + tl.sum(b_ss * exp(b_zc[None, :] - b_z), 0)",
      "        b_zp = b_zc"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "chunk_abc_bwd_kernel_rcum_intra",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "ss",
        "annotation": null
      },
      {
        "name": "doo",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_abc_bwd_kernel_rcum_intra(",
      "    s,",
      "    z,",
      "    ss,",
      "    doo,",
      "    T,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_s, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "",
      "    o_i = tl.arange(0, BC)",
      "    m_o = tl.full([BC, BC], 1.0, dtype=tl.float32)",
      "",
      "    p_s = tl.make_block_ptr(",
      "        s + i_bh * T * S,",
      "        (T, S),",
      "        (S, 1),",
      "        (i_t * BT + i_i * BC, i_s * BS),",
      "        (BC, BS),",
      "        (1, 0),",
      "    )",
      "    p_zn = tl.make_block_ptr(",
      "        z + i_bh * T * S,",
      "        (T * S,),",
      "        (1,),",
      "        ((i_t * BT + i_i * BC + BC - 1) * S + i_s * BS,),",
      "        (BS,),",
      "        (0,),",
      "    )",
      "    p_doo = tl.make_block_ptr(",
      "        doo + i_bh * T * S,",
      "        (T, S),",
      "        (S, 1),",
      "        (i_t * BT + i_i * BC, i_s * BS),",
      "        (BC, BS),",
      "        (1, 0),",
      "    )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1))",
      "",
      "    b_zn = tl.load(p_zn, boundary_check=(0,))",
      "",
      "    b_doo = tl.zeros([BC, BS], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT + i_j * BC, i_s * BS),",
      "            (BC, BS),",
      "            (1, 0),",
      "        )",
      "        p_ss = tl.make_block_ptr(",
      "            ss + i_bh * T * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT + i_j * BC, i_s * BS),",
      "            (BC, BS),",
      "            (1, 0),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0, 1))",
      "        b_ss = tl.load(p_ss, boundary_check=(0, 1))",
      "",
      "        b_doo += b_ss * exp(b_zn[None, :] - b_z)",
      "    b_doo = exp(b_s - b_zn[None, :]) * tl.dot(",
      "        m_o.to(b_s.dtype), b_doo.to(b_s.dtype), allow_tf32=False",
      "    )",
      "",
      "    for j in range(0, BC):",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S,",
      "            (T * S,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * S + i_s * BS,),",
      "            (BS,),",
      "            (0,),",
      "        )",
      "        p_ss = tl.make_block_ptr(",
      "            ss + i_bh * T * S,",
      "            (T * S,),",
      "            (1,),",
      "            ((i_t * BT + i_i * BC + j) * S + i_s * BS,),",
      "            (BS,),",
      "            (0,),",
      "        )",
      "",
      "        b_z = tl.load(p_z, boundary_check=(0,))",
      "        b_ss = tl.load(p_ss, boundary_check=(0,))",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_doo += tl.where(m_i, exp(b_s - b_z[None, :]) * b_ss[None, :], 0.0)",
      "    b_doo += tl.load(p_doo, boundary_check=(0, 1))",
      "    tl.store(p_doo, b_doo.to(p_doo.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/abc/223.py"
  },
  {
    "name": "naive_attn_decoding_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([] if check_shared_mem('hopper') else [8]) for num_stages in [2, 3, 4, 5]], key=['H', 'G', 'K', 'V', 'BK', 'BV', 'USE_G'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "gate_scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def naive_attn_decoding_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    g_cumsum,",
      "    scale,",
      "    gate_scale,",
      "    cu_seqlens,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_v, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    bos, eos = tl.load(cu_seqlens + i_b).to(tl.int32), tl.load(cu_seqlens + i_b + 1).to(",
      "        tl.int32",
      "    )",
      "    T = eos - bos",
      "",
      "    p_q = tl.make_block_ptr(q + i_bh * K, (K,), (1,), (0,), (BK,), (0,))",
      "    p_o = tl.make_block_ptr(o + i_bh * V, (V,), (1,), (0,), (BV,), (0,))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0,))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_o = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    b_m = tl.full(",
      "        [",
      "            1,",
      "        ],",
      "        float(\"-inf\"),",
      "        dtype=tl.float32,",
      "    )",
      "    b_acc = tl.zeros(",
      "        [",
      "            1,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (T - 1,), (1,), (0,)",
      "        )",
      "        b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)",
      "    else:",
      "        b_gq = None",
      "",
      "    for i_s in range(0, T, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.sum(b_q[None, :] * b_k, 1)",
      "",
      "        mask = i_s + tl.arange(0, BS) < T",
      "        b_s = tl.where(mask, b_s, float(\"-inf\"))",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += (b_gq - b_gk) * gate_scale",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = exp(b_s - b_m)",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 0)",
      "",
      "        b_o = b_o * b_r + tl.sum(b_p[:, None] * b_v, 0)",
      "        b_mp = b_m",
      "    b_o = b_o / b_acc",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/attn/224.py"
  },
  {
    "name": "parallel_attn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([8] if check_shared_mem('hopper') else []) for num_stages in [2, 3, 4, 5]], key=['B', 'H', 'HQ', 'G', 'K', 'V', 'BK', 'BV', 'USE_G', 'IS_VARLEN'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_attn_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    g_cumsum,",
      "    lse,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_lse = tl.make_block_ptr(",
      "        lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([BT], float(\"-inf\"), dtype=tl.float32)",
      "    b_acc = tl.zeros([BT], dtype=tl.float32)",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_gq = tl.load(p_g, boundary_check=(0,)).to(tl.float32)",
      "    else:",
      "        b_gq = None",
      "",
      "    for i_s in range(0, i_t * BT, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = safe_exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "        b_mp = b_m",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        o_k = i_s + tl.arange(0, BS)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, float(\"-inf\"))",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = safe_exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "        b_mp = b_m",
      "",
      "    b_o = b_o / b_acc[:, None]",
      "    b_m += log(b_acc)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/attn/225.py"
  },
  {
    "name": "parallel_attn_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([8] if check_shared_mem('hopper') else []) for num_stages in [2, 3, 4, 5]], key=['B', 'H', 'HQ', 'G', 'K', 'V', 'BK', 'BV', 'USE_G', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_attn_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    dg_cumsum,",
      "    g_cumsum,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_lse = tl.make_block_ptr(",
      "        lse + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_delta = tl.make_block_ptr(",
      "        delta + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse, boundary_check=(0,))",
      "    b_delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    if USE_G:",
      "        b_dg = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        p_gq = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "    else:",
      "        b_gq = None",
      "        b_dg = None",
      "",
      "    for i_s in range(0, i_t * BT, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_s),",
      "            (BV, BS),",
      "            (0, 1),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "",
      "        b_p = safe_exp(b_s - b_lse[:, None])",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "        if USE_G:",
      "            b_dg += tl.sum(b_ds, 1)",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_s),",
      "            (BV, BS),",
      "            (0, 1),",
      "        )",
      "",
      "        o_k = i_s + tl.arange(0, BS)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[:, None] - b_gk[None, :]",
      "            b_s = tl.where(o_q[:, None] >= o_k[None, :], b_s, -float(\"inf\"))",
      "",
      "        b_p = safe_exp(b_s - b_lse[:, None])",
      "        b_p = tl.where(o_q[:, None] >= o_k[None, :], b_p, 0)",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "        if USE_G:",
      "            b_dg += tl.sum(b_ds, 1)",
      "",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_dg = tl.make_block_ptr(",
      "            dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/attn/225.py"
  },
  {
    "name": "parallel_attn_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4] + ([8] if check_shared_mem('hopper') else []) for num_stages in [2, 3, 4, 5]], key=['B', 'H', 'HQ', 'G', 'K', 'V', 'BK', 'BV', 'USE_G', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_attn_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    dg_cumsum,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    o_k = i_t * BT + tl.arange(0, BT)",
      "",
      "    if USE_G:",
      "        p_gk = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_gk = tl.load(p_gk, boundary_check=(0,)).to(tl.float32)",
      "        b_dg = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_gk = None",
      "        b_dg = None",
      "",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = tl.make_block_ptr(",
      "            lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "        p_delta = tl.make_block_ptr(",
      "            delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "",
      "        o_q = i_s + tl.arange(0, BS)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse, boundary_check=(0,))",
      "        b_delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[None, :] - b_gk[:, None]",
      "            b_s = tl.where(o_k[:, None] <= o_q[None, :], b_s, -float(\"inf\"))",
      "        b_p = safe_exp(b_s - b_lse[None, :])",
      "        b_p = tl.where(o_k[:, None] <= o_q[None, :], b_p, 0)",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "        if USE_G:",
      "            b_dg -= tl.sum(b_ds, 1)",
      "",
      "    for i_s in range((i_t + 1) * BT, tl.cdiv(T, BS) * BS, BS):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_s, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_s, i_v * BV),",
      "            (BS, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = tl.make_block_ptr(",
      "            lse + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "        p_delta = tl.make_block_ptr(",
      "            delta + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "        )",
      "",
      "        o_q = i_s + tl.arange(0, BS)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse, boundary_check=(0,))",
      "        b_delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(",
      "                g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_s,), (BS,), (0,)",
      "            )",
      "            b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "            b_s += b_gq[None, :] - b_gk[:, None]",
      "        b_p = safe_exp(b_s - b_lse[None, :])",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "        if USE_G:",
      "            b_dg -= tl.sum(b_ds, 1)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_dg = tl.make_block_ptr(",
      "            dg_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/attn/225.py"
  },
  {
    "name": "fused_chunk_based_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_based_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    z,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    o_i = tl.arange(0, BT)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_h_0o = tl.zeros([BV], dtype=tl.float32)",
      "",
      "    b_h_1o = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_h_2o = tl.zeros([BK * BK, BV], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + i_k * B * H) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (0, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    p_z = z + (i_bh + i_k * B * H) * T + tl.arange(0, BT)",
      "    k_2o = tl.zeros([1, BK * BK], dtype=tl.float32)",
      "    k_1o = tl.zeros([1, BK], dtype=tl.float32)",
      "    k_0o = 0",
      "",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_k_2o = b_k[:, None, :] * b_k[None, :, :]",
      "        b_k_2o = tl.reshape(b_k_2o, [BK * BK, BT]).to(b_k.dtype)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(b_k.dtype)",
      "        b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "        b_z = tl.zeros([BT], dtype=tl.float32)",
      "",
      "        b_o += b_h_0o",
      "        b_z += k_0o",
      "",
      "        b_o += tl.dot(b_q, b_h_1o.to(b_q.dtype), allow_tf32=False)",
      "        b_z += tl.sum(b_q * k_1o, axis=1)",
      "",
      "        b_q_2o = b_q[:, :, None] * b_q[:, None, :]",
      "        b_q_2o = tl.reshape(b_q_2o, [BT, BK * BK]).to(b_k.dtype)",
      "        b_o += tl.dot(b_q_2o, b_h_2o.to(b_q_2o.dtype), allow_tf32=False) * 0.5",
      "        b_z += tl.sum(b_q_2o * k_2o, axis=1) * 0.5",
      "",
      "        k_1o += tl.sum(b_k, axis=1)[None, :]",
      "        k_2o += tl.sum(b_k_2o, axis=1)[None, :]",
      "        k_0o += BT",
      "",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = 1 + b_s + 0.5 * b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_z += tl.sum(b_s, axis=1)",
      "        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(",
      "            p_z, b_z.to(p_z.dtype.element_ty), mask=(i * BT + tl.arange(0, BT)) < T",
      "        )",
      "",
      "        b_h_2o = b_h_2o + tl.dot(b_k_2o.to(b_v.dtype), b_v, allow_tf32=False)",
      "        b_h_1o = b_h_1o + tl.dot(b_k, b_v, allow_tf32=False)",
      "        b_h_0o = b_h_0o + tl.sum(b_v, axis=0)",
      "",
      "        p_q = tl.advance(p_q, (BT, 0))",
      "        p_k = tl.advance(p_k, (0, BT))",
      "        p_v = tl.advance(p_v, (BT, 0))",
      "        p_o = tl.advance(p_o, (BT, 0))",
      "        p_z += BT"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/based/226.py"
  },
  {
    "name": "parallel_based_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_based_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    z,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % (NV)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_o = tl.zeros([BTL, BV], dtype=tl.float32)",
      "    b_z = tl.zeros([BTL], dtype=tl.float32)",
      "",
      "    for _ in range(0, i_c * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, (b_k), allow_tf32=False)",
      "        b_s = 1 + b_s + 0.5 * b_s * b_s",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "",
      "    tl.debug_barrier()",
      "    o_q = tl.arange(0, BTL)",
      "",
      "    o_k = tl.arange(0, BTS)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = 1 + b_s + 0.5 * b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "        o_k += BTS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + B * H * i_k) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_c * BTL, i_v * BV),",
      "        (BTL, BV),",
      "        (1, 0),",
      "    )",
      "    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(",
      "        p_z, b_z.to(p_z.dtype.element_ty), mask=((i_c * BTL + tl.arange(0, BTL)) < T)",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/based/227.py"
  },
  {
    "name": "parallel_based_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_based_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % NV",
      "    _parallel_based_bwd_dq(",
      "        i_bh, i_c, i_k, i_v, q, k, v, do, dz, dq, scale, T, B, H, BTL, BTS, BK, BV, K, V",
      "    )",
      "    tl.debug_barrier()",
      "    _parallel_based_bwd_dkv(",
      "        i_bh,",
      "        i_c,",
      "        i_k,",
      "        i_v,",
      "        q,",
      "        k,",
      "        v,",
      "        do,",
      "        dz,",
      "        dk,",
      "        dv,",
      "        scale,",
      "        T,",
      "        B,",
      "        H,",
      "        BTL,",
      "        BTS,",
      "        BK,",
      "        BV,",
      "        K,",
      "        V,",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/based/227.py"
  },
  {
    "name": "chunk_gated_delta_rule_fwd_kernel_h_blockdim64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'SAVE_NEW_VALUE': lambda args: args['v_new'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [2, 3, 4] for BV in [32, 64]], key=['H', 'K', 'V', 'BT', 'USE_G'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SAVE_NEW_VALUE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gated_delta_rule_fwd_kernel_h_blockdim64(",
      "    k,",
      "    v,",
      "    w,",
      "    v_new,",
      "    g,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    SAVE_NEW_VALUE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h1 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 64:",
      "        b_h2 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 128:",
      "        b_h3 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 192:",
      "        b_h4 = tl.zeros([64, BV], dtype=tl.float32)",
      "",
      "    h += (boh * H + i_h) * K * V",
      "    v += (bos * H + i_h) * V",
      "    k += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    if SAVE_NEW_VALUE:",
      "        v_new += (bos * H + i_h) * V",
      "    stride_v = H * V",
      "    stride_h = H * K * V",
      "    stride_k = H * K",
      "    if USE_INITIAL_STATE:",
      "        h0 = h0 + i_nh * K * V",
      "    if STORE_FINAL_STATE:",
      "        ht = ht + i_nh * K * V",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0_1 = tl.make_block_ptr(h0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        b_h1 += tl.load(p_h0_1, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 64:",
      "            p_h0_2 = tl.make_block_ptr(",
      "                h0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_h2 += tl.load(p_h0_2, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 128:",
      "            p_h0_3 = tl.make_block_ptr(",
      "                h0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_h3 += tl.load(p_h0_3, boundary_check=(0, 1)).to(tl.float32)",
      "        if K > 192:",
      "            p_h0_4 = tl.make_block_ptr(",
      "                h0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_h4 += tl.load(p_h0_4, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h1 = tl.make_block_ptr(",
      "            h + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0)",
      "        )",
      "        tl.store(p_h1, b_h1.to(p_h1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_h2 = tl.make_block_ptr(",
      "                h + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_h2, b_h2.to(p_h2.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_h3 = tl.make_block_ptr(",
      "                h + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_h3, b_h3.to(p_h3.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_h4 = tl.make_block_ptr(",
      "                h + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_h4, b_h4.to(p_h4.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_v_new = (",
      "            tl.make_block_ptr(",
      "                v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            if SAVE_NEW_VALUE",
      "            else None",
      "        )",
      "        b_v_new = tl.zeros([BT, BV], dtype=tl.float32)",
      "        p_w = tl.make_block_ptr(",
      "            w, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0)",
      "        )",
      "        b_w = tl.load(p_w, boundary_check=(0, 1))",
      "        b_v_new += tl.dot(b_w, b_h1.to(b_w.dtype))",
      "        if K > 64:",
      "            p_w = tl.make_block_ptr(",
      "                w, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0)",
      "            )",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_v_new += tl.dot(b_w, b_h2.to(b_w.dtype))",
      "        if K > 128:",
      "            p_w = tl.make_block_ptr(",
      "                w, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0)",
      "            )",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_v_new += tl.dot(b_w, b_h3.to(b_w.dtype))",
      "        if K > 192:",
      "            p_w = tl.make_block_ptr(",
      "                w, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0)",
      "            )",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_v_new += tl.dot(b_w, b_h4.to(b_w.dtype))",
      "        b_v_new = -b_v_new + tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        if SAVE_NEW_VALUE:",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            tl.store(",
      "                p_v_new, b_v_new.to(p_v_new.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "",
      "        if USE_G:",
      "            last_idx = min((i_t + 1) * BT, T) - 1",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            p_g = tl.make_block_ptr(",
      "                g + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "            b_g = tl.load(p_g, boundary_check=(0,))",
      "            b_v_new = b_v_new * safe_exp(b_g_last - b_g)[:, None]",
      "            b_g_last = exp(b_g_last)",
      "            b_h1 = b_h1 * b_g_last",
      "            if K > 64:",
      "                b_h2 = b_h2 * b_g_last",
      "            if K > 128:",
      "                b_h3 = b_h3 * b_g_last",
      "            if K > 192:",
      "                b_h4 = b_h4 * b_g_last",
      "        b_v_new = b_v_new.to(k.dtype.element_ty)",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_h1 += tl.dot(b_k, b_v_new)",
      "        if K > 64:",
      "            p_k = tl.make_block_ptr(",
      "                k, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h2 += tl.dot(b_k, b_v_new)",
      "        if K > 128:",
      "            p_k = tl.make_block_ptr(",
      "                k, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h3 += tl.dot(b_k, b_v_new)",
      "        if K > 192:",
      "            p_k = tl.make_block_ptr(",
      "                k, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_h4 += tl.dot(b_k, b_v_new)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(ht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        tl.store(p_ht, b_h1.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_ht = tl.make_block_ptr(",
      "                ht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_ht, b_h2.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_ht = tl.make_block_ptr(",
      "                ht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_ht, b_h3.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_ht = tl.make_block_ptr(",
      "                ht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_ht, b_h4.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/228.py"
  },
  {
    "name": "chunk_gated_delta_rule_bwd_kernel_dhu_blockdim64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [4, 3, 2] for BV in [64, 32]], key=['H', 'K', 'V', 'BT', 'BV', 'USE_G'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gated_delta_rule_bwd_kernel_dhu_blockdim64(",
      "    q,",
      "    k,",
      "    w,",
      "    g,",
      "    dht,",
      "    dh0,",
      "    do,",
      "    dh,",
      "    dv,",
      "    dv2,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh1 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 64:",
      "        b_dh2 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 128:",
      "        b_dh3 = tl.zeros([64, BV], dtype=tl.float32)",
      "    if K > 192:",
      "        b_dh4 = tl.zeros([64, BV], dtype=tl.float32)",
      "",
      "    dh += (boh * H + i_h) * K * V",
      "    dv += (bos * H + i_h) * V",
      "    dv2 += (bos * H + i_h) * V",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V",
      "    stride_v = H * V",
      "    stride_h = H * K * V",
      "    stride_k = H * K",
      "    if USE_INITIAL_STATE:",
      "        dh0 += i_nh * K * V",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        dht += i_nh * K * V",
      "",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht1 = tl.make_block_ptr(dht, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        b_dh1 += tl.load(p_dht1, boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_dht2 = tl.make_block_ptr(",
      "                dht, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_dh2 += tl.load(p_dht2, boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_dht3 = tl.make_block_ptr(",
      "                dht, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_dh3 += tl.load(p_dht3, boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_dht4 = tl.make_block_ptr(",
      "                dht, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            b_dh4 += tl.load(p_dht4, boundary_check=(0, 1))",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh1 = tl.make_block_ptr(",
      "            dh + i_t * stride_h, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh1, b_dh1.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_dh2 = tl.make_block_ptr(",
      "                dh + i_t * stride_h, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh2, b_dh2.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_dh3 = tl.make_block_ptr(",
      "                dh + i_t * stride_h, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh3, b_dh3.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_dh4 = tl.make_block_ptr(",
      "                dh + i_t * stride_h, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh4, b_dh4.to(p_dh4.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        if USE_G:",
      "            last_idx = min((i_t + 1) * BT, T) - 1",
      "            bg_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            bg_last_exp = exp(bg_last)",
      "            p_g = tl.make_block_ptr(",
      "                g + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "            b_g = tl.load(p_g, boundary_check=(0,))",
      "            b_g_exp = exp(b_g)",
      "        else:",
      "            bg_last = None",
      "            last_idx = None",
      "            b_g = None",
      "            b_g_exp = None",
      "",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_wo = tl.make_block_ptr(",
      "            do, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dv2 = tl.make_block_ptr(",
      "            dv2, (T, V), (stride_v, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "",
      "        b_wo = tl.load(p_wo, boundary_check=(0, 1))",
      "        b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_k, 1), (i_t * BT, 0), (BT, 64), (1, 0)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_k, b_dh1.to(b_k.dtype))",
      "",
      "        if K > 64:",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (stride_k, 1), (i_t * BT, 64), (BT, 64), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_dv += tl.dot(b_k, b_dh2.to(b_k.dtype))",
      "",
      "        if K > 128:",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (stride_k, 1), (i_t * BT, 128), (BT, 64), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_dv += tl.dot(b_k, b_dh3.to(b_k.dtype))",
      "",
      "        if K > 192:",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (stride_k, 1), (i_t * BT, 192), (BT, 64), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_dv += tl.dot(b_k, b_dh4.to(b_k.dtype))",
      "",
      "        if USE_G:",
      "            b_dv *= safe_exp(bg_last - b_g)[:, None]",
      "        b_dv += tl.load(p_dv, boundary_check=(0, 1))",
      "",
      "        tl.store(p_dv2, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        p_w = tl.make_block_ptr(",
      "            w, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, stride_k), (0, i_t * BT), (64, BT), (0, 1)",
      "        )",
      "        b_w = tl.load(p_w, boundary_check=(0, 1))",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        if USE_G:",
      "            b_dh1 *= bg_last_exp",
      "            b_q = b_q * b_g_exp[None, :]",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_dh1 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))",
      "        if K > 64:",
      "            p_q = tl.make_block_ptr(",
      "                q, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w, (K, T), (1, stride_k), (64, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            if USE_G:",
      "                b_dh2 *= bg_last_exp",
      "                b_q = b_q * b_g_exp[None, :]",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "            b_dh2 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))",
      "        if K > 128:",
      "            p_q = tl.make_block_ptr(",
      "                q, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w, (K, T), (1, stride_k), (128, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            if USE_G:",
      "                b_dh3 *= bg_last_exp",
      "                b_q = b_q * b_g_exp[None, :]",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "            b_dh3 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))",
      "        if K > 192:",
      "            p_q = tl.make_block_ptr(",
      "                q, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w, (K, T), (1, stride_k), (192, i_t * BT), (64, BT), (0, 1)",
      "            )",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            if USE_G:",
      "                b_dh4 *= bg_last_exp",
      "                b_q = b_q * b_g_exp[None, :]",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "            b_dh4 += tl.dot(b_q, b_wo.to(b_q.dtype)) - tl.dot(b_w, b_dv.to(b_w.dtype))",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(dh0, (K, V), (V, 1), (0, i_v * BV), (64, BV), (1, 0))",
      "        tl.store(p_dh0, b_dh1.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 64:",
      "            p_dh1 = tl.make_block_ptr(",
      "                dh0, (K, V), (V, 1), (64, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh1, b_dh2.to(p_dh1.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 128:",
      "            p_dh2 = tl.make_block_ptr(",
      "                dh0, (K, V), (V, 1), (128, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh2, b_dh3.to(p_dh2.dtype.element_ty), boundary_check=(0, 1))",
      "        if K > 192:",
      "            p_dh3 = tl.make_block_ptr(",
      "                dh0, (K, V), (V, 1), (192, i_v * BV), (64, BV), (1, 0)",
      "            )",
      "            tl.store(p_dh3, b_dh4.to(p_dh3.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/228.py"
  },
  {
    "name": "chunk_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    gk,",
      "    gv,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = i_n * NS",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        i_s = i_t // (BS // BT)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        o_h = ((boh + i_s) * H + i_h).to(tl.int64) * K * V",
      "        p_h = tl.make_block_ptr(",
      "            h + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        if i_t % (BS // BT) == 0:",
      "            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "",
      "        if USE_G:",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_h *= exp(b_g_last)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_h *= exp(b_gk_last)[:, None]",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_h *= exp(b_gv_last)[None, :]",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "        b_h += tl.dot(b_k, b_v)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/229.py"
  },
  {
    "name": "chunk_bwd_kernel_dh",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh(",
      "    q,",
      "    g,",
      "    gk,",
      "    gv,",
      "    do,",
      "    dh,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = i_n * NS",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        i_s = i_t // (BS // BT)",
      "        o_dh = ((boh + i_s) * H + i_h).to(tl.int64) * K * V",
      "        p_dh = tl.make_block_ptr(",
      "            dh + o_dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        if i_t % (BS // BT) == 0:",
      "            tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (K, T),",
      "            (1, HQ * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        if USE_G:",
      "            p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)",
      "",
      "            b_dh *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_q = (b_q * exp(b_gk)).to(b_q.dtype)",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_dh *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_do = (b_do * exp(b_gv)).to(b_do.dtype)",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_dh *= exp(b_gv_last)[None, :]",
      "",
      "        b_dh += tl.dot(b_q, b_do)",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/229.py"
  },
  {
    "name": "chunk_fwd_kernel_h_parallel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_parallel(",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    gk,",
      "    gv,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    NV = tl.cdiv(V, BV)",
      "",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        i_n, i_tg = i_b, i_b * NT + i_t",
      "    i_nh = i_n * H + i_h",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (K, T),",
      "        (1, H * K),",
      "        (i_k * BK, i_t * BT),",
      "        (BK, BT),",
      "        (0, 1),",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_h = tl.make_block_ptr(",
      "        h + (i_tg * H + i_h) * K * V,",
      "        (K, V),",
      "        (V, 1),",
      "        (i_k * BK, i_v * BV),",
      "        (BK, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if i_t == 0:",
      "        if USE_INITIAL_STATE:",
      "            p_h0 = tl.make_block_ptr(",
      "                h0 + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "        else:",
      "            b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "    if USE_G:",
      "        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "        b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)",
      "",
      "    if USE_GK:",
      "        p_gk = tl.make_block_ptr(",
      "            gk + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_gk_last = (",
      "            gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "        )",
      "        b_gk_last = tl.load(",
      "            p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "        )",
      "",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)",
      "",
      "    if USE_GV:",
      "        p_gv = tl.make_block_ptr(",
      "            gv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_gv_last = (",
      "            gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "        )",
      "",
      "        b_gv_last = tl.load(",
      "            p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "        )",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "    b_h = tl.dot(b_k, b_v)",
      "    if i_t < NT - 1:",
      "        p_h = tl.make_block_ptr(",
      "            h + ((i_tg + 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/230.py"
  },
  {
    "name": "chunk_fwd_kernel_h_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "kvt",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_reduction(",
      "    h,",
      "    g,",
      "    gk,",
      "    gv,",
      "    kvt,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "        if i_t > 0:",
      "            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        if USE_G:",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            b_h *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_h *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_h *= exp(b_gv_last)[None, :]",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_kvt = tl.make_block_ptr(",
      "            kvt + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h += tl.load(p_kvt, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/230.py"
  },
  {
    "name": "chunk_bwd_kernel_dh_parallel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_parallel(",
      "    q,",
      "    g,",
      "    gk,",
      "    gv,",
      "    do,",
      "    dh,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    NV = tl.cdiv(V, BV)",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        i_n, i_tg = i_b, i_b * NT + i_t",
      "    i_nh = i_n * HQ + i_hq",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K,",
      "        (K, T),",
      "        (1, HQ * K),",
      "        (i_k * BK, i_t * BT),",
      "        (BK, BT),",
      "        (0, 1),",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_dh = tl.make_block_ptr(",
      "        dh + (i_tg * H + i_h) * K * V,",
      "        (K, V),",
      "        (V, 1),",
      "        (i_k * BK, i_v * BV),",
      "        (BK, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if i_t == NT - 1:",
      "        if USE_FINAL_STATE_GRADIENT:",
      "            p_dht = tl.make_block_ptr(",
      "                dht + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_dh = tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "        else:",
      "            b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    if USE_G:",
      "        p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h",
      "        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "        b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)",
      "",
      "    if USE_GK:",
      "        p_gk = tl.make_block_ptr(",
      "            gk + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_q = (b_q * exp(b_gk)).to(b_q.dtype)",
      "",
      "    if USE_GV:",
      "        p_gv = tl.make_block_ptr(",
      "            gv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_gv)).to(b_do.dtype)",
      "",
      "    b_dh = tl.dot(b_q, b_do)",
      "    if i_t > 0:",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((i_tg - 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/230.py"
  },
  {
    "name": "chunk_bwd_kernel_dh_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for BV in [32, 64, 128] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "doq0",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_reduction(",
      "    g,",
      "    gk,",
      "    gv,",
      "    dh,",
      "    doq0,",
      "    dh0,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_dh += tl.load(p_dh, boundary_check=(0, 1)).to(tl.float32)",
      "        if i_t < NT - 1:",
      "            tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "        if USE_G:",
      "            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            b_dh *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_dh *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_dh *= exp(b_gv_last)[None, :]",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_doq0 = tl.make_block_ptr(",
      "            doq0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_doq0, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/230.py"
  },
  {
    "name": "chunk_fwd_kernel_h_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "hs",
        "annotation": null
      },
      {
        "name": "hr",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_split(",
      "    k,",
      "    v,",
      "    g,",
      "    gk,",
      "    gv,",
      "    hs,",
      "    hr,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    split_indices,",
      "    T,",
      "    S: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "",
      "    i_k, i_v, i_sh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_ss, i_h = i_sh // H, i_sh % H",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "    i_nh = i_n * H + i_h",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if i_s == 0:",
      "        if USE_INITIAL_STATE:",
      "            p_h0 = tl.make_block_ptr(",
      "                h0 + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_h += tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "        p_hr = tl.make_block_ptr(",
      "            hr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))",
      "    for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        if USE_G:",
      "            b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "            p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_h *= exp(b_g_last)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_v = (b_v * exp(b_g_last - b_g)[:, None]).to(b_v.dtype)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_h *= exp(b_gk_last)[:, None]",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_k = (b_k * exp(b_gk_last[:, None] - b_gk)).to(b_k.dtype)",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_h *= exp(b_gv_last)[None, :]",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_v = (b_v * exp(b_gv_last[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "        b_h += tl.dot(b_k, b_v)",
      "",
      "    if NS > 1:",
      "        p_hs = tl.make_block_ptr(",
      "            hs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_hs, b_h.to(p_hs.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/231.py"
  },
  {
    "name": "chunk_fwd_kernel_h_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "hs",
        "annotation": null
      },
      {
        "name": "hr",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_h_reduction(",
      "    g,",
      "    gk,",
      "    gv,",
      "    hs,",
      "    hr,",
      "    ht,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    S: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NS = tl.cdiv(T, S)",
      "        boh = i_n * NS",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for i_s in range(1, NS):",
      "        p_hs = tl.make_block_ptr(",
      "            hs + ((boh + i_s - 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_hr = tl.make_block_ptr(",
      "            hr + ((boh + i_s) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_hr, b_h.to(p_hr.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        for i_t in range(tl.cdiv(i_s * S, BT), tl.cdiv(min(i_s * S + S, T), BT)):",
      "            last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "            if USE_G:",
      "                b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "                b_h *= exp(b_g_last)",
      "",
      "            if USE_GK:",
      "                p_gk_last = (",
      "                    gk",
      "                    + (bos + last_idx) * H * K",
      "                    + i_h * K",
      "                    + i_k * BK",
      "                    + tl.arange(0, BK)",
      "                )",
      "                b_gk_last = tl.load(",
      "                    p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "                )",
      "                b_h *= exp(b_gk_last)[:, None]",
      "",
      "            if USE_GV:",
      "                p_gv_last = (",
      "                    gv",
      "                    + (bos + last_idx) * H * V",
      "                    + i_h * V",
      "                    + i_v * BV",
      "                    + tl.arange(0, BV)",
      "                )",
      "                b_gv_last = tl.load(",
      "                    p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "                )",
      "                b_h *= exp(b_gv_last)[None, :]",
      "",
      "    if NS > 1:",
      "        if STORE_FINAL_STATE:",
      "            p_hs = tl.make_block_ptr(",
      "                hs + ((boh + NS - 1) * H + i_h) * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            p_ht = tl.make_block_ptr(",
      "                ht + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_h += tl.load(p_hs, boundary_check=(0, 1)).to(tl.float32)",
      "            tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/231.py"
  },
  {
    "name": "chunk_bwd_kernel_dh_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dhs",
        "annotation": null
      },
      {
        "name": "dhr",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_split(",
      "    q,",
      "    g,",
      "    gk,",
      "    gv,",
      "    do,",
      "    dht,",
      "    dhs,",
      "    dhr,",
      "    dh0,",
      "    cu_seqlens,",
      "    split_indices,",
      "    scale,",
      "    T,",
      "    S: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "",
      "    i_k, i_v, i_sh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_ss, i_hq = i_sh // HQ, i_sh % HQ",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "    i_nh = i_n * HQ + i_hq",
      "    i_h = i_hq // NG",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if i_s == NS - 1:",
      "        if USE_FINAL_STATE_GRADIENT:",
      "            p_dht = tl.make_block_ptr(",
      "                dht + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "        p_dhr = tl.make_block_ptr(",
      "            dhr + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_t in range(",
      "        tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1",
      "    ):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (K, T),",
      "            (1, HQ * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "        if USE_G:",
      "            p_g = g + (bos + i_t * BT + tl.arange(0, BT)) * H + i_h",
      "            b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "            b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "            b_q = (b_q * exp(b_g)[None, :]).to(b_q.dtype)",
      "            b_dh *= exp(b_g_last)",
      "",
      "        if USE_GK:",
      "            p_gk = tl.make_block_ptr(",
      "                gk + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT),",
      "                (BK, BT),",
      "                (0, 1),",
      "            )",
      "            p_gk_last = (",
      "                gk + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "            )",
      "",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_q = (b_q * exp(b_gk)).to(b_q.dtype)",
      "            b_gk_last = tl.load(",
      "                p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "            )",
      "            b_dh *= exp(b_gk_last)[:, None]",
      "",
      "        if USE_GV:",
      "            p_gv = tl.make_block_ptr(",
      "                gv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT, i_v * BV),",
      "                (BT, BV),",
      "                (1, 0),",
      "            )",
      "            p_gv_last = (",
      "                gv + (bos + last_idx) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "            )",
      "",
      "            b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "            b_do = (b_do * exp(b_gv)).to(b_do.dtype)",
      "",
      "            b_gv_last = tl.load(",
      "                p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "            )",
      "            b_dh *= exp(b_gv_last)[None, :]",
      "",
      "        b_dh += tl.dot(b_q, b_do)",
      "",
      "    if NS > 1:",
      "        p_dhs = tl.make_block_ptr(",
      "            dhs + i_sh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dhs, b_dh.to(p_dhs.dtype.element_ty), boundary_check=(0, 1))",
      "    elif STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/231.py"
  },
  {
    "name": "chunk_bwd_kernel_dh_reduction",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT', 'USE_G', 'USE_GK', 'USE_GV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "dhs",
        "annotation": null
      },
      {
        "name": "dhr",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dh_reduction(",
      "    g,",
      "    gk,",
      "    gv,",
      "    dhs,",
      "    dhr,",
      "    dh0,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    S: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NS = tl.cdiv(T, S)",
      "        boh = i_n * NS",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_s in range(NS - 2, -1, -1):",
      "        p_dhs = tl.make_block_ptr(",
      "            dhs + ((boh + i_s + 1) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dhr = tl.make_block_ptr(",
      "            dhr + ((boh + i_s) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)",
      "        tl.store(p_dhr, b_dh.to(p_dhr.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        for i_t in range(",
      "            tl.cdiv(min(i_s * S + S, T), BT) - 1, tl.cdiv(i_s * S, BT) - 1, -1",
      "        ):",
      "            last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "            if USE_G:",
      "                b_g_last = tl.load(g + (bos + last_idx) * H + i_h)",
      "                b_dh *= exp(b_g_last)",
      "",
      "            if USE_GK:",
      "                p_gk_last = (",
      "                    gk",
      "                    + (bos + last_idx) * H * K",
      "                    + i_h * K",
      "                    + i_k * BK",
      "                    + tl.arange(0, BK)",
      "                )",
      "                b_gk_last = tl.load(",
      "                    p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "                )",
      "                b_dh *= exp(b_gk_last)[:, None]",
      "",
      "            if USE_GV:",
      "                p_gv_last = (",
      "                    gv",
      "                    + (bos + last_idx) * H * V",
      "                    + i_h * V",
      "                    + i_v * BV",
      "                    + tl.arange(0, BV)",
      "                )",
      "                b_gv_last = tl.load(",
      "                    p_gv_last, mask=(i_v * BV + tl.arange(0, BV) < V), other=0.0",
      "                )",
      "                b_dh *= exp(b_gv_last)[None, :]",
      "",
      "    if NS > 1:",
      "        if STORE_INITIAL_STATE_GRADIENT:",
      "            p_dhs = tl.make_block_ptr(",
      "                dhs + (boh * H + i_h) * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            p_dh0 = tl.make_block_ptr(",
      "                dh0 + i_nh * K * V,",
      "                (K, V),",
      "                (V, 1),",
      "                (i_k * BK, i_v * BV),",
      "                (BK, BV),",
      "                (1, 0),",
      "            )",
      "            b_dh += tl.load(p_dhs, boundary_check=(0, 1)).to(tl.float32)",
      "            tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/231.py"
  },
  {
    "name": "chunk_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h).to(tl.int64) * K * V",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h)",
      "",
      "        b_A += tl.dot(b_q, b_k)",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_o = b_o * exp(b_g)[:, None]",
      "        b_A = b_A * safe_exp(b_g[:, None] - b_g[None, :])",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_A = tl.where(m_A, b_A, 0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    b_o = b_o * scale + tl.dot(b_A.to(b_v.dtype), b_v) * scale",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/232.py"
  },
  {
    "name": "chunk_bwd_kernel_dqkwg",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_G': lambda args: args['g'] is not None, 'USE_DW': lambda args: args['dw'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'USE_G', 'USE_DW'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dqkwg(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dg,",
      "    w,",
      "    dv,",
      "    dw,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_DW: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if USE_G:",
      "        dg += i_k * B * H * T",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    dh += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "",
      "    if USE_DW:",
      "        dw += (bos * H + i_h) * K",
      "        dv += (bos * H + i_h) * V",
      "        w += (bos * H + i_h) * K",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dg_last = (",
      "        tl.zeros(",
      "            [",
      "                1,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        if USE_G",
      "        else None",
      "    )",
      "    b_dw = tl.zeros([BT, BK], dtype=tl.float32) if USE_DW else None",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        if USE_G:",
      "            b_dg_last += tl.sum(b_h * b_dh)",
      "",
      "        b_ds += tl.dot(b_do, tl.trans(b_v))",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "        if USE_DW:",
      "            p_dv = tl.make_block_ptr(",
      "                dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "            )",
      "            b_dv = tl.load(p_dv, boundary_check=(0, 1))",
      "            b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))",
      "",
      "    if USE_DW:",
      "        p_dw = tl.make_block_ptr(",
      "            dw, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        tl.store(p_dw, -b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    o_i = tl.arange(0, BT)",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "",
      "    if USE_G:",
      "        b_dg = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        g += bos * H + i_h",
      "        dg += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)",
      "        b_dg_last *= exp(b_g_last)",
      "",
      "        b_dq = b_dq * exp(b_g)[:, None] * scale",
      "        b_dg += tl.sum(b_dq * b_q, axis=1)",
      "",
      "        b_dk = b_dk * safe_exp(-b_g + b_g_last)[:, None]",
      "        b_dg -= tl.sum(b_k * b_dk, axis=1)",
      "        b_dg_last += tl.sum(b_dk * b_k)",
      "",
      "        b_ds = (",
      "            tl.where(",
      "                o_i[:, None] >= o_i[None, :],",
      "                b_ds * safe_exp(b_g[:, None] - b_g[None, :]),",
      "                0,",
      "            )",
      "            * scale",
      "        )",
      "        b_ds2 = b_ds * tl.dot(b_q, tl.trans(b_k))",
      "        b_dg += tl.sum(b_ds2, axis=1)",
      "        b_dg -= tl.sum(b_ds2, axis=0)",
      "",
      "        b_ds = b_ds.to(b_k.dtype)",
      "",
      "        b_dq += tl.dot(b_ds, b_k)",
      "        b_dk += tl.dot(tl.trans(b_ds), b_q)",
      "        p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "",
      "        b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "    else:",
      "        b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)",
      "        b_ds = b_ds.to(b_k.dtype)",
      "        b_dq += tl.dot(b_ds, b_k)",
      "        b_dk += tl.dot(tl.trans(b_ds), b_q) * scale",
      "        b_dq *= scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/232.py"
  },
  {
    "name": "chunk_bwd_kernel_dv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_G': lambda args: args['g'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dv(",
      "    q,",
      "    k,",
      "    g,",
      "    do,",
      "    dv,",
      "    dh,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "    dh += (i_tg * H + i_h).to(tl.int64) * K * V",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_k, b_q)",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)",
      "        b_dv *= safe_exp(-b_g + b_g_last)[:, None]",
      "",
      "    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]",
      "    if USE_G:",
      "        b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(",
      "            do.dtype.element_ty",
      "        )",
      "    else:",
      "        b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)",
      "    p_do = tl.make_block_ptr(",
      "        do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_dv += tl.dot(b_A.to(b_do.dtype), b_do)",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/232.py"
  },
  {
    "name": "chunk_bwd_kernel_dv_local",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dv_local(",
      "    q,",
      "    k,",
      "    g,",
      "    do,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, H * K), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_k, b_q)",
      "",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "        p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "",
      "    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]",
      "    if USE_G:",
      "        b_A = tl.where(mask, b_A * safe_exp(b_g[None, :] - b_g[:, None]) * scale, 0).to(",
      "            do.dtype.element_ty",
      "        )",
      "    else:",
      "        b_A = tl.where(mask, b_A * scale, 0).to(do.dtype.element_ty)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (H * V, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dv = tl.dot(b_A.to(b_do.dtype), b_do)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/232.py"
  },
  {
    "name": "chunk_scaled_dot_kkt_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_G': lambda args: args['g_cumsum'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['H', 'K', 'BT', 'IS_VARLEN', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_scaled_dot_kkt_fwd_kernel(",
      "    k,",
      "    beta,",
      "    g_cumsum,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_t = tl.arange(0, BT)",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = b_k * b_beta[:, None]",
      "        b_A += tl.dot(b_kb.to(b_k.dtype), tl.trans(b_k))",
      "",
      "    if USE_G:",
      "        p_g = tl.make_block_ptr(",
      "            g_cumsum + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g = tl.load(p_g, boundary_check=(0,))",
      "        b_g_diff = b_g[:, None] - b_g[None, :]",
      "        b_A = b_A * safe_exp(b_g_diff)",
      "",
      "    b_A = tl.where(o_t[:, None] > o_t[None, :], b_A, 0)",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/233.py"
  },
  {
    "name": "fused_recurrent_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BK', 'BV', 'USE_GK', 'USE_GV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    gk,",
      "    gv,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    p_q = (",
      "        q",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_k = (",
      "        k",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_v = (",
      "        v",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_o = (",
      "        o",
      "        + ((i_k * all + bos) + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    if USE_G:",
      "        p_g = g + (bos + ((T - 1) if REVERSE else 0)) * H + i_h",
      "    if USE_GK:",
      "        p_gk = (",
      "            gk",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "            + i_h * K",
      "            + i_k * BK",
      "            + tl.arange(0, BK)",
      "        )",
      "    if USE_GV:",
      "        p_gv = (",
      "            gv",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "            + i_h * V",
      "            + i_v * BV",
      "            + tl.arange(0, BV)",
      "        )",
      "",
      "    mask_k = (i_k * BK + tl.arange(0, BK)) < K",
      "    mask_v = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        if USE_GK:",
      "            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gk[None, :])",
      "        if USE_GV:",
      "            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gv[:, None])",
      "        if USE_G:",
      "            b_g = tl.load(p_g).to(tl.float32)",
      "            b_h = b_h * exp(b_g)",
      "        b_h += b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        p_q += (-1 if REVERSE else 1) * H * K",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_o += (-1 if REVERSE else 1) * H * V",
      "        if USE_GK:",
      "            p_gk += (-1 if REVERSE else 1) * H * K",
      "        if USE_GV:",
      "            p_gv += (-1 if REVERSE else 1) * H * V",
      "        if USE_G:",
      "            p_g += (-1 if REVERSE else 1) * H",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/234.py"
  },
  {
    "name": "fused_recurrent_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BK', 'BV', 'USE_GK', 'USE_GV', 'USE_G'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "gv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    gk,",
      "    gv,",
      "    h0,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "    USE_GK: tl.constexpr,",
      "    USE_GV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    p_k = (",
      "        k",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_v = (",
      "        v",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_do = (",
      "        do",
      "        + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_dq = (",
      "        dq",
      "        + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    if USE_G:",
      "        p_g = g + (bos + ((T - 1) if REVERSE else 0)) * H + i_h",
      "    if USE_GK:",
      "        p_gk = (",
      "            gk",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * K",
      "            + i_h * K",
      "            + i_k * BK",
      "            + tl.arange(0, BK)",
      "        )",
      "    if USE_GV:",
      "        p_gv = (",
      "            gv",
      "            + (bos + ((T - 1) if REVERSE else 0)) * H * V",
      "            + i_h * V",
      "            + i_v * BV",
      "            + tl.arange(0, BV)",
      "        )",
      "",
      "    mask_k = i_k * BK + tl.arange(0, BK) < K",
      "    mask_v = i_v * BV + tl.arange(0, BV) < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        if USE_G:",
      "            b_g = tl.load(p_g).to(tl.float32)",
      "            b_h = b_h * exp(b_g)",
      "        if USE_GK:",
      "            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gk[:, None])",
      "        if USE_GV:",
      "            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)",
      "            b_h = b_h * exp(b_gv[None, :])",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        b_dq = b_h * b_do[None, :]",
      "        b_dq = tl.sum(b_dq, axis=1) * scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_do += (-1 if REVERSE else 1) * H * V",
      "        p_dq += (-1 if REVERSE else 1) * H * K",
      "        if USE_G:",
      "            p_g += (-1 if REVERSE else 1) * H",
      "        if USE_GK:",
      "            p_gk += (-1 if REVERSE else 1) * H * K",
      "        if USE_GV:",
      "            p_gv += (-1 if REVERSE else 1) * H * V",
      "",
      "    tl.debug_barrier()",
      "",
      "    p_q = (",
      "        q",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_k = (",
      "        k",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_v = (",
      "        v",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_do = (",
      "        do",
      "        + (bos + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    p_dk = (",
      "        dk",
      "        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "    )",
      "    p_dv = (",
      "        dv",
      "        + ((i_k * all + bos) + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "    )",
      "    if USE_G:",
      "        p_g = g + (bos + ((T - 1) if not REVERSE else 0)) * H + i_h",
      "    if USE_GK:",
      "        p_gk = (",
      "            gk",
      "            + (bos + ((T - 1) if not REVERSE else 0)) * H * K",
      "            + i_h * K",
      "            + i_k * BK",
      "            + tl.arange(0, BK)",
      "        )",
      "    if USE_GV:",
      "        p_gv = (",
      "            gv",
      "            + (bos + ((T - 1) if not REVERSE else 0)) * H * V",
      "            + i_h * V",
      "            + i_v * BV",
      "            + tl.arange(0, BV)",
      "        )",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = (",
      "            dht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_dh += tl.load(p_dht, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_dh += b_q[:, None] * b_do[None, :]",
      "        b_dk = tl.sum(b_dh * b_v[None, :], axis=1)",
      "        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)",
      "        if USE_G:",
      "            b_g = tl.load(p_g).to(tl.float32)",
      "            b_dh *= exp(b_g)",
      "        if USE_GK:",
      "            b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "            b_dh *= exp(b_gk)[:, None]",
      "        if USE_GV:",
      "            b_gv = tl.load(p_gv, mask=mask_v, other=0).to(tl.float32)",
      "            b_dh *= exp(b_gv)[None, :]",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)",
      "",
      "        p_q += (1 if REVERSE else -1) * H * K",
      "        p_k += (1 if REVERSE else -1) * H * K",
      "        p_v += (1 if REVERSE else -1) * H * V",
      "        p_do += (1 if REVERSE else -1) * H * V",
      "        p_dk += (1 if REVERSE else -1) * H * K",
      "        p_dv += (1 if REVERSE else -1) * H * V",
      "        if USE_G:",
      "            p_g += (1 if REVERSE else -1) * H",
      "        if USE_GK:",
      "            p_gk += (1 if REVERSE else -1) * H * K",
      "        if USE_GV:",
      "            p_gv += (1 if REVERSE else -1) * H * V",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = (",
      "            dh0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/common/234.py"
  },
  {
    "name": "fused_recurrent_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_BETA_HEADWISE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    u,",
      "    beta,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_BETA_HEADWISE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_u = u + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    else:",
      "        p_beta = beta + bos * H + i_h",
      "    p_o = o + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    mask_k = (i_k * BK + tl.arange(0, BK)) < K",
      "    mask_v = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_v_minus = tl.sum(b_h * b_k[None, :], axis=1)",
      "        b_v -= b_v_minus",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        tl.store(p_u, b_v.to(p_v.dtype.element_ty), mask=mask_v)",
      "        b_v *= b_beta",
      "        b_h += b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "",
      "        p_q += H * K",
      "        p_k += H * K",
      "        p_o += H * V",
      "        p_v += H * V",
      "        p_u += H * V",
      "        p_beta += H * (V if IS_BETA_HEADWISE else 1)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/235.py"
  },
  {
    "name": "fused_recurrent_delta_rule_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_BETA_HEADWISE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_delta_rule_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    beta,",
      "    h0,",
      "    dh0,",
      "    dht,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    db,",
      "    cu_seqlens,",
      "    scale,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NK: tl.constexpr,",
      "    IS_BETA_HEADWISE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    mask_k = i_k * BK + tl.arange(0, BK) < K",
      "    mask_v = i_v * BV + tl.arange(0, BV) < V",
      "",
      "    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_dk = (",
      "        dk",
      "        + ((i_v * all + bos) * H + i_h) * K",
      "        + i_k * BK",
      "        + tl.arange(0, BK)",
      "        + (T - 1) * H * K",
      "    )",
      "    p_dv = (",
      "        dv",
      "        + ((i_k * all + bos) * H + i_h) * V",
      "        + i_v * BV",
      "        + tl.arange(0, BV)",
      "        + (T - 1) * H * V",
      "    )",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos + T - 1) * H * V + i_h * V + i_v * BV + tl.arange(0, BV)",
      "        p_dbeta = (",
      "            db",
      "            + ((i_v * NK + i_k) * all + bos + T - 1) * H * V",
      "            + i_h * V",
      "            + tl.arange(0, BV)",
      "        )",
      "    else:",
      "        p_beta = beta + (bos + T - 1) * H + i_h",
      "        p_dbeta = db + (i_v * all + bos + T - 1) * H + i_h",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_ht = (",
      "            dht",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "    for _ in range(T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_dh += b_q[:, None] * b_do[None, :]",
      "        b_dk = tl.sum(b_dh * (b_v * b_beta)[None, :], axis=1)",
      "        b_dv = tl.sum(b_dh * b_k[:, None], axis=0)",
      "",
      "        b_db = b_dv * b_v if IS_BETA_HEADWISE else tl.sum(b_dv * b_v)",
      "        b_dv = b_dv * b_beta",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)",
      "        if IS_BETA_HEADWISE:",
      "            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty), mask=mask_v)",
      "        else:",
      "            tl.store(p_dbeta, b_db.to(p_dbeta.dtype.element_ty))",
      "",
      "        b_dh -= b_k[:, None] * b_dv[None, :]",
      "",
      "        p_q -= H * K",
      "        p_k -= H * K",
      "        p_v -= H * V",
      "        p_do -= H * V",
      "        p_dk -= H * K",
      "        p_dv -= H * V",
      "        p_dbeta -= H * (V if IS_BETA_HEADWISE else 1)",
      "        p_beta -= H * (V if IS_BETA_HEADWISE else 1)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = (",
      "            dh0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        tl.store(",
      "            p_dh0,",
      "            b_dh.to(p_dh0.dtype.element_ty),",
      "            mask=mask_k[:, None] & mask_v[None, :],",
      "        )",
      "",
      "    tl.debug_barrier()",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    p_q = q + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + (bos * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    else:",
      "        p_beta = beta + bos * H + i_h",
      "    p_do = do + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_dq = dq + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_dk = dk + ((i_v * all + bos) * H + i_h) * K + i_k * BK + tl.arange(0, BK)",
      "    p_dv = dv + ((i_k * all + bos) * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    if USE_INITIAL_STATE:",
      "        mask_h = mask_k[:, None] & mask_v[None, :]",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_dk = tl.load(p_dk, mask=mask_k, other=0).to(tl.float32)",
      "        b_dv = tl.load(p_dv, mask=mask_v, other=0).to(tl.float32)",
      "        b_dk -= tl.sum(b_dv[None, :] * b_h, axis=1)",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_v *= b_beta",
      "",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        b_dq = b_h * b_do[None, :]",
      "        d_q = tl.sum(b_dq, axis=1) * scale",
      "        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += H * K",
      "        p_v += H * V",
      "        p_do += H * V",
      "        p_dq += H * K",
      "        p_dk += H * K",
      "        p_dv += H * V",
      "        p_beta += H * (V if IS_BETA_HEADWISE else 1)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/235.py"
  },
  {
    "name": "chunk_transform_qk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BT', 'K', 'V'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "A_local",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "OUTPUT_ATTENTIONS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_transform_qk_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    beta,",
      "    o,",
      "    A,",
      "    q_new,",
      "    k_new,",
      "    A_local,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    OUTPUT_ATTENTIONS: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_q = (tl.load(p_q, boundary_check=(0, 1)) * scale).to(p_q.dtype.element_ty)",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    p_T = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_T = tl.load(p_T, boundary_check=(0, 1))",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_t = o_i[:, None] >= o_i[None, :]",
      "    b_qk = tl.where(m_t, tl.dot(b_q, tl.trans(b_k), allow_tf32=False), 0).to(b_q.dtype)",
      "    m_t = o_i[:, None] > o_i[None, :]",
      "    b_kk = tl.where(m_t, tl.dot(b_k, tl.trans(b_k), allow_tf32=False), 0).to(b_k.dtype)",
      "",
      "    p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "",
      "    b_qkT = tl.dot(b_qk, b_T, allow_tf32=False).to(b_k.dtype)",
      "",
      "    if OUTPUT_ATTENTIONS:",
      "        p_a = tl.make_block_ptr(",
      "            A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "        )",
      "        tl.store(p_a, b_qkT.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_kkT = tl.dot(b_kk, b_T, allow_tf32=False).to(b_k.dtype)",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o, tl.dot(b_qkT, b_v).to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_q_new,",
      "        (b_q - tl.dot(b_qkT, b_k_beta, allow_tf32=False)).to(p_q_new.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    p_k_new = tl.make_block_ptr(",
      "        k_new + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_k_new = b_k - tl.dot(tl.trans(b_kkT), b_k_beta, allow_tf32=False)",
      "    tl.store(p_k_new, b_k_new.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/236.py"
  },
  {
    "name": "save_intra_chunk_attn",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2)], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A_local",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def save_intra_chunk_attn(",
      "    A,",
      "    A_local,",
      "    T,",
      "    BT: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * T, (T, T), (T, 1), (i_t * BT, i_t * BT), (BT, BT), (1, 0)",
      "    )",
      "    p_A_local = tl.make_block_ptr(",
      "        A_local + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_A_local = tl.load(p_A_local, boundary_check=(0, 1))",
      "    tl.store(p_A, b_A_local.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/236.py"
  },
  {
    "name": "parallel_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "k2",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "o_new",
        "annotation": null
      },
      {
        "name": "attn",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "OUTPUT_ATTENTIONS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    k2,",
      "    v,",
      "    beta,",
      "    o,",
      "    o_new,",
      "    attn,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    OUTPUT_ATTENTIONS: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_q += tl.load(p_q, boundary_check=(0, 1))",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_o += tl.load(p_o, boundary_check=(0, 1))",
      "",
      "    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_k2 = tl.make_block_ptr(",
      "            k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "        m_s = tl.arange(0, BT) >= (offset - i_t * BT + BS)",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)",
      "        b_s = tl.where(m_s[:, None], b_s, 0)",
      "",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)",
      "        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False)",
      "",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn + i_bh * T * T,",
      "                (T, T),",
      "                (T, 1),",
      "                (i_t * BT, offset),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for offset in range(i_t * BT - BS, -BS, -BS):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (K, T), (1, K), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_beta = tl.make_block_ptr(beta + i_bh * T, (T,), (1,), (offset,), (BS,), (0,))",
      "        p_k2 = tl.make_block_ptr(",
      "            k2 + i_bh * T * K, (T, K), (K, 1), (offset, 0), (BS, BK), (1, 0)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k, allow_tf32=False)",
      "",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        b_k2 = (tl.load(p_k2, boundary_check=(0, 1)) * b_beta[:, None]).to(b_v.dtype)",
      "        b_q -= tl.dot(b_s.to(b_v.dtype), b_k2, allow_tf32=False).to(b_q.dtype)",
      "",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn + i_bh * T * T,",
      "                (T, T),",
      "                (T, 1),",
      "                (i_t * BT, offset),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_o_new = tl.make_block_ptr(",
      "        o_new + i_bh * T * V, (T, V), (V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o_new, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/236.py"
  },
  {
    "name": "recompute_w_u_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def recompute_w_u_fwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    w,",
      "    u,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_u = tl.dot(b_A.to(b_vb.dtype), b_vb, allow_tf32=False)",
      "        tl.store(p_u, (b_u).to(p_u.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_w = tl.dot(b_A.to(b_kb.dtype), b_kb, allow_tf32=False)",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/237.py"
  },
  {
    "name": "prepare_wy_repr_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_bwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    A,",
      "    dw,",
      "    du,",
      "    dk,",
      "    dv,",
      "    dbeta,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "    b_dbeta = tl.zeros([BT], dtype=tl.float32)",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_du = tl.make_block_ptr(",
      "            du + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_du = tl.load(p_du, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_du, tl.trans(b_v_beta), allow_tf32=False)",
      "        b_dv_beta = tl.dot(b_A, b_du, allow_tf32=False)",
      "        b_dv = b_dv_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dv_beta * b_v, 1)",
      "",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_dw = tl.load(p_dw, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_dw, tl.trans(b_k_beta), allow_tf32=False)",
      "        b_dk_beta = tl.dot(b_A, b_dw, allow_tf32=False)",
      "        b_dk = b_dk_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)",
      "    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)",
      "    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(",
      "        k.dtype.element_ty",
      "    )",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "",
      "        b_dk_beta = tl.dot(b_dA, b_k, allow_tf32=False)",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "        b_dk += tl.dot(tl.trans(b_dA), b_k_beta, allow_tf32=False)",
      "        b_dk += b_dk_beta * b_beta[:, None]",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_dbeta = tl.make_block_ptr(",
      "        dbeta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/delta_rule/237.py"
  },
  {
    "name": "fused_recurrent_gated_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_BETA_HEADWISE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_QK_L2NORM_IN_KERNEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_gated_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    beta,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HV: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_BETA_HEADWISE: tl.constexpr,",
      "    USE_QK_L2NORM_IN_KERNEL: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hv = i_nh // HV, i_nh % HV",
      "    i_h = i_hv // (HV // H)",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "",
      "    p_q = q + (bos * H + i_h) * K + o_k",
      "    p_k = k + (bos * H + i_h) * K + o_k",
      "    p_v = v + (bos * HV + i_hv) * V + o_v",
      "    if IS_BETA_HEADWISE:",
      "        p_beta = beta + (bos * HV + i_hv) * V + o_v",
      "    else:",
      "        p_beta = beta + bos * HV + i_hv",
      "    p_g = g + bos * HV + i_hv",
      "    p_o = o + ((i_k * all + bos) * HV + i_hv) * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g).to(tl.float32)",
      "",
      "        if USE_QK_L2NORM_IN_KERNEL:",
      "            b_q = b_q / (tl.sqrt(tl.sum(b_q * b_q)) + 1e-6)",
      "            b_k = b_k / (tl.sqrt(tl.sum(b_k * b_k)) + 1e-6)",
      "        b_q = b_q * scale",
      "",
      "        b_h *= exp(b_g)",
      "",
      "        b_v -= tl.sum(b_h * b_k[:, None], 0)",
      "        if IS_BETA_HEADWISE:",
      "            b_beta = tl.load(p_beta, mask=mask_v, other=0).to(tl.float32)",
      "        else:",
      "            b_beta = tl.load(p_beta).to(tl.float32)",
      "        b_v *= b_beta",
      "",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "",
      "        b_o = tl.sum(b_h * b_q[:, None], 0)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "",
      "        p_q += H * K",
      "        p_k += H * K",
      "        p_o += HV * V",
      "        p_v += HV * V",
      "        p_g += HV",
      "        p_beta += HV * (V if IS_BETA_HEADWISE else 1)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gated_delta_rule/238.py"
  },
  {
    "name": "prepare_wy_repr_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_bwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    g,",
      "    A,",
      "    dw,",
      "    du,",
      "    dk,",
      "    dv,",
      "    dbeta,",
      "    dg,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_beta = tl.make_block_ptr(",
      "        beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_g = tl.make_block_ptr(g + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_g = tl.load(p_g, boundary_check=(0,))",
      "    b_g_exp = tl.exp(b_g)",
      "",
      "    b_dbeta = tl.zeros([BT], dtype=tl.float32)",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dg = tl.zeros([BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k_beta_g = (b_k * b_beta[:, None] * b_g_exp[:, None]).to(b_k.dtype)",
      "        b_dw = tl.load(p_dw, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_dw, tl.trans(b_k_beta_g))",
      "        b_dk_beta_g = tl.dot(b_A, b_dw)",
      "        b_dk = b_dk_beta_g * b_beta[:, None] * b_g_exp[:, None]",
      "        b_dbeta += tl.sum(b_dk_beta_g * b_k * b_g_exp[:, None], 1)",
      "        b_dg += tl.sum(b_dk_beta_g * b_k * b_g_exp[:, None] * b_beta[:, None], 1)",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_du = tl.make_block_ptr(",
      "            du + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v_beta = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_du = tl.load(p_du, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_du, tl.trans(b_v_beta))",
      "        b_dv_beta = tl.dot(b_A, b_du)",
      "        b_dv = b_dv_beta * b_beta[:, None]",
      "        b_dbeta += tl.sum(b_dv_beta * b_v, 1)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dA, 0)",
      "    b_dA = tl.dot(b_dA.to(b_A.dtype), b_A)",
      "    b_dA = tl.dot(b_A, b_dA.to(b_A.dtype))",
      "    b_dA = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dA, 0).to(",
      "        k.dtype.element_ty",
      "    )",
      "",
      "    b_dA *= safe_exp(b_g[:, None] - b_g[None, :])",
      "    b_dA = b_dA.to(k.dtype.element_ty)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "        b_k_beta = (b_k * b_beta[:, None]).to(b_k.dtype)",
      "        b_A += tl.dot(b_k_beta, tl.trans(b_k))",
      "        b_dk_beta = tl.dot(b_dA, b_k)",
      "        b_dbeta += tl.sum(b_dk_beta * b_k, 1)",
      "        b_dk += tl.dot(tl.trans(b_dA), b_k_beta)",
      "        b_dk += b_dk_beta * b_beta[:, None]",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA_A = b_dA * b_A",
      "    b_dg += tl.sum(b_dA_A, axis=1) - tl.sum(b_dA_A, axis=0)",
      "    p_dg = tl.make_block_ptr(dg + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    p_dbeta = tl.make_block_ptr(",
      "        dbeta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_dbeta, b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gated_delta_rule/239.py"
  },
  {
    "name": "recompute_w_u_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def recompute_w_u_fwd_kernel(",
      "    k,",
      "    v,",
      "    beta,",
      "    w,",
      "    u,",
      "    A,",
      "    g,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    p_beta = tl.make_block_ptr(",
      "        beta + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_g = tl.make_block_ptr(g + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_g = tl.exp(tl.load(p_g, boundary_check=(0,)))",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_vb = (b_v * b_beta[:, None]).to(b_v.dtype)",
      "        b_u = tl.dot(b_A, b_vb, allow_tf32=False)",
      "        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_kb = (b_k * b_beta[:, None] * b_g[:, None]).to(b_k.dtype)",
      "        b_w = tl.dot(b_A, b_kb)",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gated_delta_rule/239.py"
  },
  {
    "name": "chunk_dplr_bwd_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BK', 'BT', 'K'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "dAqk",
        "annotation": null
      },
      {
        "name": "dAqb",
        "annotation": null
      },
      {
        "name": "dAak",
        "annotation": null
      },
      {
        "name": "dAab",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "da",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dqg",
        "annotation": null
      },
      {
        "name": "dkg",
        "annotation": null
      },
      {
        "name": "dag",
        "annotation": null
      },
      {
        "name": "dbg",
        "annotation": null
      },
      {
        "name": "dgk",
        "annotation": null
      },
      {
        "name": "dgk_offset",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GATHER_SUPPORTED",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_intra(",
      "    q,",
      "    k,",
      "    a,",
      "    b,",
      "    gi,",
      "    ge,",
      "    dAqk,",
      "    dAqb,",
      "    dAak,",
      "    dAab,",
      "    dq,",
      "    dk,",
      "    da,",
      "    db,",
      "    dqg,",
      "    dkg,",
      "    dag,",
      "    dbg,",
      "    dgk,",
      "    dgk_offset,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    GATHER_SUPPORTED: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = (i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32)",
      "",
      "    if i_t * BT >= T:",
      "        return",
      "",
      "    ge += (bos * H + i_h) * K",
      "    gi += (bos * H + i_h) * K",
      "    q += (bos * H + i_h) * K",
      "    a += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    da += (bos * H + i_h) * K",
      "    db += (bos * H + i_h) * K",
      "    dqg += (bos * H + i_h) * K",
      "    dag += (bos * H + i_h) * K",
      "    dkg += (bos * H + i_h) * K",
      "    dbg += (bos * H + i_h) * K",
      "    dgk += (bos * H + i_h) * K",
      "    dgk_offset += (bos * H + i_h) * K",
      "    dAqk += (bos * H + i_h) * BT",
      "    dAqb += (bos * H + i_h) * BT",
      "    dAak += (bos * H + i_h) * BT",
      "    dAab += (bos * H + i_h) * BT",
      "",
      "    stride_qk = H * K",
      "    stride_A = H * BT",
      "",
      "    p_ge = tl.make_block_ptr(",
      "        ge, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_gi = tl.make_block_ptr(",
      "        gi, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "",
      "    b_ge = tl.load(p_ge, boundary_check=(0, 1))",
      "    b_gi = tl.load(p_gi, boundary_check=(0, 1))",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    b_da = tl.zeros([BC, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "    b_db = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    p_dAqk = tl.make_block_ptr(",
      "        dAqk, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_dAab = tl.make_block_ptr(",
      "        dAab, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_dAqb = tl.make_block_ptr(",
      "        dAqb, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_dAak = tl.make_block_ptr(",
      "        dAak, (T, BT), (stride_A, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    o_i = tl.arange(0, BC)",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_b = tl.make_block_ptr(",
      "        b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_a = tl.make_block_ptr(",
      "        a, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_b = tl.load(p_b, boundary_check=(0, 1))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_a = tl.load(p_a, boundary_check=(0, 1))",
      "    b_dAqk = tl.load(p_dAqk, boundary_check=(0, 1))",
      "    b_dAab = tl.load(p_dAab, boundary_check=(0, 1))",
      "    b_dAqb = tl.load(p_dAqb, boundary_check=(0, 1))",
      "    b_dAak = tl.load(p_dAak, boundary_check=(0, 1))",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    for j in range(0, min(BC, T - i_t * BT)):",
      "",
      "        if GATHER_SUPPORTED:",
      "            row_idx = tl.full([1, BK], j, dtype=tl.int16)",
      "            col_idx = tl.full([BC, 1], j, dtype=tl.int16)",
      "            row_idx_bc = tl.full([1, BC], j, dtype=tl.int16)",
      "",
      "            b_kj = gather(b_k, row_idx, axis=0)",
      "            b_bj = gather(b_b, row_idx, axis=0)",
      "            b_gij = gather(b_gi, row_idx, axis=0)",
      "            b_gej = gather(b_ge, row_idx, axis=0)",
      "            b_qj = gather(b_q, row_idx, axis=0)",
      "            b_aj = gather(b_a, row_idx, axis=0)",
      "",
      "            b_dAqk_j = gather(b_dAqk, col_idx, axis=1)",
      "            b_dAab_j = gather(b_dAab, col_idx, axis=1)",
      "            b_dAqb_j = gather(b_dAqb, col_idx, axis=1)",
      "            b_dAak_j = gather(b_dAak, col_idx, axis=1)",
      "",
      "            b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_qk_j = tl.sum(gather(b_dAqk, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_ab_j = tl.sum(gather(b_dAab, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_qb_j = tl.sum(gather(b_dAqb, row_idx_bc, axis=0), 0)[:, None]",
      "            b_dA_ak_j = tl.sum(gather(b_dAak, row_idx_bc, axis=0), 0)[:, None]",
      "        else:",
      "            mask_idx = tl.arange(0, BC) == j",
      "            b_kj = tl.sum(tl.where(mask_idx[:, None], b_k, 0), 0)[None, :]",
      "            b_bj = tl.sum(tl.where(mask_idx[:, None], b_b, 0), 0)[None, :]",
      "            b_gij = tl.sum(tl.where(mask_idx[:, None], b_gi, 0), 0)[None, :]",
      "            b_gej = tl.sum(tl.where(mask_idx[:, None], b_ge, 0), 0)[None, :]",
      "            b_dAqk_j = tl.sum(tl.where(mask_idx[None, :], b_dAqk, 0), 1)[:, None]",
      "            b_dAab_j = tl.sum(tl.where(mask_idx[None, :], b_dAab, 0), 1)[:, None]",
      "            b_dAqb_j = tl.sum(tl.where(mask_idx[None, :], b_dAqb, 0), 1)[:, None]",
      "            b_dAak_j = tl.sum(tl.where(mask_idx[None, :], b_dAak, 0), 1)[:, None]",
      "            b_dA_qk_j = tl.sum(tl.where(mask_idx[:, None], b_dAqk, 0), 0)[:, None]",
      "            b_dA_ab_j = tl.sum(tl.where(mask_idx[:, None], b_dAab, 0), 0)[:, None]",
      "            b_dA_qb_j = tl.sum(tl.where(mask_idx[:, None], b_dAqb, 0), 0)[:, None]",
      "            b_dA_ak_j = tl.sum(tl.where(mask_idx[:, None], b_dAak, 0), 0)[:, None]",
      "",
      "            b_qj = tl.sum(tl.where(mask_idx[:, None], b_q, 0), 0)[None, :]",
      "            b_aj = tl.sum(tl.where(mask_idx[:, None], b_a, 0), 0)[None, :]",
      "",
      "        m_e = o_i[:, None] > j",
      "        m_i = o_i[:, None] >= j",
      "        tmp1 = exp(b_gi - b_gij)",
      "        tmp2 = exp(b_ge - b_gij)",
      "        b_dq += tl.where(m_i, b_dAqk_j * b_kj * tmp1, 0.0)",
      "        b_dq += tl.where(m_i, b_dAqb_j * b_bj * tmp1, 0.0)",
      "        b_da += tl.where(m_e, b_dAab_j * b_bj * tmp2, 0.0)",
      "        b_da += tl.where(m_e, b_dAak_j * b_kj * tmp2, 0.0)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        m_e = o_i[:, None] < j",
      "        tmp1 = exp(b_gij - b_gi)",
      "        tmp2 = exp(b_gej - b_gi)",
      "        b_dk += tl.where(m_i, b_dA_qk_j * b_qj * tmp1, 0.0)",
      "        b_dk += tl.where(m_e, b_dA_ak_j * b_aj * tmp2, 0.0)",
      "        b_db += tl.where(m_i, b_dA_qb_j * b_qj * tmp1, 0.0)",
      "        b_db += tl.where(m_e, b_dA_ab_j * b_aj * tmp2, 0.0)",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_da = tl.make_block_ptr(",
      "        da, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_db = tl.make_block_ptr(",
      "        db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dgk = tl.make_block_ptr(",
      "        dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dgk_offset = tl.make_block_ptr(",
      "        dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dqg = tl.make_block_ptr(",
      "        dqg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dkg = tl.make_block_ptr(",
      "        dkg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dag = tl.make_block_ptr(",
      "        dag, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_dbg = tl.make_block_ptr(",
      "        dbg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BC, BK), (1, 0)",
      "    )",
      "    p_gn = gi + (min(i_t * BT + BT, T) - 1) * stride_qk + o_k",
      "    p_gn = tl.max_contiguous(tl.multiple_of(p_gn, BK), BK)",
      "    b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "    b_da += tl.load(p_dag, boundary_check=(0, 1)) * exp(b_ge)",
      "    b_dq += tl.load(p_dqg, boundary_check=(0, 1)) * exp(b_gi) * scale",
      "    tmp = exp(b_gn[None, :] - b_gi)",
      "    b_dk += tl.load(p_dkg, boundary_check=(0, 1)).to(tl.float32) * tmp",
      "    b_db += tl.load(p_dbg, boundary_check=(0, 1)).to(tl.float32) * tmp",
      "    tl.store(p_dq, (b_dq).to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_da, b_da.to(p_da.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))",
      "    b_dgk = (b_dq * b_q + b_da * b_a - b_dk * b_k - b_db * b_b).to(tl.float32)",
      "    b_dgk_offset = b_da * b_a",
      "    tl.store(p_dgk, b_dgk.to(p_dgk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(",
      "        p_dgk_offset,",
      "        b_dgk_offset.to(p_dgk_offset.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/240.py"
  },
  {
    "name": "chunk_dplr_bwd_dgk_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4] for BK in [32, 64]], key=['BK', 'BT', 'K'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "dgk",
        "annotation": null
      },
      {
        "name": "dgk_offset",
        "annotation": null
      },
      {
        "name": "dgk_last",
        "annotation": null
      },
      {
        "name": "dgk_output",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_dgk_kernel(",
      "    dgk,",
      "    dgk_offset,",
      "    dgk_last,",
      "    dgk_output,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = (i_b * NT + i_t).to(tl.int32)",
      "        bos, eos = (i_b * T).to(tl.int32), (i_b * T + T).to(tl.int32)",
      "",
      "    stride_qk = H * K",
      "    dgk += (bos * H + i_h) * K",
      "    dgk_offset += (bos * H + i_h) * K",
      "    dgk_last += (i_tg * H + i_h) * K",
      "    dgk_output += (bos * H + i_h) * K",
      "    p_dgk_last = dgk_last + tl.arange(0, BK) + i_k * BK",
      "    m_k = tl.arange(0, BK) + i_k * BK < K",
      "    b_dgk_last = tl.load(p_dgk_last, mask=m_k, other=0)",
      "    p_dgk_offset = tl.make_block_ptr(",
      "        dgk_offset, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dgk = tl.make_block_ptr(",
      "        dgk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_dgk = tl.load(p_dgk, boundary_check=(0, 1))",
      "    b_dgk_offset = tl.load(p_dgk_offset, boundary_check=(0, 1))",
      "",
      "    b_dgk_cumsum = tl.cumsum(b_dgk, 0, reverse=True)",
      "    b_dgk_cumsum += b_dgk_last[None, :]",
      "    b_dgk_cumsum -= b_dgk_offset",
      "    p_dgk_output = tl.make_block_ptr(",
      "        dgk_output, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_dgk_output,",
      "        b_dgk_cumsum.to(p_dgk_output.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/240.py"
  },
  {
    "name": "chunk_dplr_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BK', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "ag",
        "annotation": null
      },
      {
        "name": "bg",
        "annotation": null
      },
      {
        "name": "Aqk",
        "annotation": null
      },
      {
        "name": "Aqb",
        "annotation": null
      },
      {
        "name": "Aab",
        "annotation": null
      },
      {
        "name": "Aak",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GATHER_SUPPORTED",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    a,",
      "    b,",
      "    gi,",
      "    ge,",
      "    qg,",
      "    kg,",
      "    ag,",
      "    bg,",
      "    Aqk,",
      "    Aqb,",
      "    Aab,",
      "    Aak,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    GATHER_SUPPORTED: tl.constexpr,",
      "):",
      "    i_t, i_b, i_h = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + tl.arange(0, BC)) < T",
      "    last_idx = min((i_t + 1) * BT, T) - 1",
      "    o_A = (bos + i_t * BT + tl.arange(0, BC)) * H * BT + i_h * BT",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_a = tl.make_block_ptr(",
      "        a + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_b = tl.make_block_ptr(",
      "        b + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_gi = tl.make_block_ptr(",
      "        gi + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_ge = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_g_last = gi + (bos * H + i_h) * K + last_idx * H * K + tl.arange(0, BK)",
      "    b_g_last = tl.load(p_g_last, mask=m_k, other=0)",
      "    p_qg = tl.make_block_ptr(",
      "        qg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_kg = tl.make_block_ptr(",
      "        kg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_ag = tl.make_block_ptr(",
      "        ag + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_bg = tl.make_block_ptr(",
      "        bg + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BC, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = b_q * scale",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_a = tl.load(p_a, boundary_check=(0, 1))",
      "    b_b = tl.load(p_b, boundary_check=(0, 1))",
      "    b_gi = tl.load(p_gi, boundary_check=(0, 1)).to(tl.float32)",
      "    b_ge = tl.load(p_ge, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    g_exp = exp(b_gi)",
      "    g_exp_inv = exp(-b_gi + b_g_last[None, :])",
      "    b_qg = b_q * g_exp",
      "    b_kg = b_k * g_exp_inv",
      "    b_bg = b_b * g_exp_inv",
      "    b_ag = b_a * exp(b_ge)",
      "    tl.store(",
      "        p_qg,",
      "        b_qg.to(p_qg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_bg,",
      "        b_bg.to(p_bg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_ag,",
      "        b_ag.to(p_ag.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_kg,",
      "        b_kg.to(p_kg.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    b_q = b_q.to(b_k.dtype)",
      "",
      "    for j in range(0, min(BC, T - i_t * BT)):",
      "",
      "        if GATHER_SUPPORTED:",
      "            row_idx = tl.full([1, BK], j, dtype=tl.int16)",
      "",
      "            b_k_j = gather(b_k, row_idx, axis=0)",
      "            b_gk_j = gather(b_gi, row_idx, axis=0)",
      "            b_b_j = gather(b_b, row_idx, axis=0)",
      "        else:",
      "            mask = tl.arange(0, BC) == j",
      "            b_k_j = tl.sum(tl.where(mask[:, None], b_k, 0), 0)[None, :]",
      "            b_gk_j = tl.sum(tl.where(mask[:, None], b_gi, 0), 0)[None, :]",
      "            b_b_j = tl.sum(tl.where(mask[:, None], b_b, 0), 0)[None, :]",
      "        tmp = exp(b_gi - b_gk_j)",
      "        b_A_qk = tl.sum(b_q * b_k_j * tmp, 1)",
      "        m_i = (o_i >= j).to(tl.float32)",
      "        b_A_qk = b_A_qk * m_i",
      "        b_A_qb = tl.sum(b_q * b_b_j * tmp, 1)",
      "        b_A_qb = b_A_qb * m_i",
      "        tmp2 = exp(b_ge - b_gk_j)",
      "        b_A_ak = tl.sum(b_a * b_k_j * tmp2, 1)",
      "        m_i2 = (o_i > j).to(tl.float32)",
      "        b_A_ak = b_A_ak * m_i2",
      "        b_A_ab = tl.sum(b_a * b_b_j * tmp2, 1)",
      "        b_A_ab = b_A_ab * m_i2",
      "",
      "        tl.store(",
      "            Aqk + o_A + j,",
      "            b_A_qk.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )",
      "        tl.store(",
      "            Aqb + o_A + j,",
      "            b_A_qb.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )",
      "        tl.store(",
      "            Aab + o_A + j,",
      "            b_A_ab.to(dtype=Aqb.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )",
      "        tl.store(",
      "            Aak + o_A + j,",
      "            b_A_ak.to(dtype=Aqk.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            mask=m_A,",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/241.py"
  },
  {
    "name": "chunk_dplr_bwd_kernel_dhu",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV', 'V'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "bg",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_dhu(",
      "    qg,",
      "    bg,",
      "    w,",
      "    gk,",
      "    dht,",
      "    dh0,",
      "    do,",
      "    dh,",
      "    dv,",
      "    dv2,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1))",
      "",
      "    mask_k = tl.arange(0, BK) < K",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        b_dh_tmp = tl.zeros([BK, BV], dtype=tl.float32)",
      "        for i_c in range(tl.cdiv(BT, BC) - 1, -1, -1):",
      "            p_qg = tl.make_block_ptr(",
      "                qg + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_bg = tl.make_block_ptr(",
      "                bg + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_dv = tl.make_block_ptr(",
      "                dv + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_do = tl.make_block_ptr(",
      "                do + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_dv2 = tl.make_block_ptr(",
      "                dv2 + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "",
      "            b_qg = tl.load(p_qg, boundary_check=(0, 1))",
      "",
      "            b_bg = tl.load(p_bg, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "",
      "            b_do = tl.load(p_do, boundary_check=(0, 1))",
      "            b_dv = tl.load(p_dv, boundary_check=(0, 1))",
      "            b_dv2 = b_dv + tl.dot(b_bg, b_dh.to(b_bg.dtype))",
      "            tl.store(p_dv2, b_dv2.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "            b_dh_tmp += tl.dot(b_qg, b_do.to(b_qg.dtype))",
      "            b_dh_tmp += tl.dot(b_w, b_dv2.to(b_qg.dtype))",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "        bg_last = tl.load(",
      "            gk + ((bos + last_idx) * H + i_h) * K + tl.arange(0, BK), mask=mask_k",
      "        )",
      "        b_dh *= exp(bg_last)[:, None]",
      "        b_dh += b_dh_tmp",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/242.py"
  },
  {
    "name": "chunk_dplr_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "bg",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_fwd_kernel_h(",
      "    kg,",
      "    v,",
      "    w,",
      "    bg,",
      "    u,",
      "    v_new,",
      "    gk,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_hc = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):",
      "            p_kg = tl.make_block_ptr(",
      "                kg + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_bg = tl.make_block_ptr(",
      "                bg + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_w = tl.make_block_ptr(",
      "                w + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_v = tl.make_block_ptr(",
      "                v + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_u = tl.make_block_ptr(",
      "                u + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "",
      "            b_kg = tl.load(p_kg, boundary_check=(0, 1))",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_w = tl.load(p_w, boundary_check=(0, 1))",
      "            b_bg = tl.load(p_bg, boundary_check=(0, 1))",
      "            b_v2 = tl.dot(b_w, b_h.to(b_w.dtype)) + tl.load(p_u, boundary_check=(0, 1))",
      "            b_hc += tl.dot(b_kg, b_v)",
      "            b_hc += tl.dot(b_bg.to(b_hc.dtype), b_v2)",
      "            tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "        b_g_last = tl.load(",
      "            gk + (bos + last_idx) * H * K + i_h * K + o_k, mask=o_k < K",
      "        ).to(tl.float32)",
      "        b_h *= exp(b_g_last[:, None])",
      "        b_h += b_hc",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(",
      "            p_ht,",
      "            b_h.to(p_ht.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            boundary_check=(0, 1),",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/243.py"
  },
  {
    "name": "chunk_dplr_bwd_kernel_dAu",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BV', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "A_qb",
        "annotation": null
      },
      {
        "name": "dA_qk",
        "annotation": null
      },
      {
        "name": "dA_qb",
        "annotation": null
      },
      {
        "name": "dv_new",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_dAu(",
      "    v,",
      "    do,",
      "    v_new,",
      "    A_qb,",
      "    dA_qk,",
      "    dA_qb,",
      "    dv_new,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "",
      "    b_dA_qk = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dA_qb = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    p_A_qb = tl.make_block_ptr(",
      "        A_qb + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A_qb = tl.load(p_A_qb, boundary_check=(0, 1))",
      "",
      "    b_A_qb = tl.where(",
      "        tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_A_qb, 0.0",
      "    ).to(b_A_qb.dtype)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        p_dv_new = tl.make_block_ptr(",
      "            dv_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_v_new = tl.load(p_v_new, boundary_check=(0, 1))",
      "        b_dA_qk += tl.dot(b_do, b_v)",
      "        b_dA_qb += tl.dot(b_do, b_v_new)",
      "        b_dv_new = tl.dot(tl.trans(b_A_qb), b_do)",
      "",
      "        tl.store(",
      "            p_dv_new, b_dv_new.to(p_dv_new.dtype.element_ty), boundary_check=(0, 1)",
      "        )",
      "",
      "    p_dA_qk = tl.make_block_ptr(",
      "        dA_qk + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_dA_qb = tl.make_block_ptr(",
      "        dA_qb + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "    b_dA_qk = tl.where(m_s, b_dA_qk * scale, 0.0)",
      "    tl.store(p_dA_qk, b_dA_qk.to(p_dA_qk.dtype.element_ty), boundary_check=(0, 1))",
      "    b_dA_qb = tl.where(m_s, b_dA_qb * scale, 0.0)",
      "    tl.store(p_dA_qb, b_dA_qb.to(p_dA_qb.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/244.py"
  },
  {
    "name": "chunk_dplr_bwd_o_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "dgk_last",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_o_kernel(",
      "    v,",
      "    v_new,",
      "    h,",
      "    do,",
      "    dh,",
      "    dk,",
      "    db,",
      "    w,",
      "    dq,",
      "    dv,",
      "    dw,",
      "    gk,",
      "    dgk_last,",
      "    k,",
      "    b,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    v_new += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    dh += (i_tg * H + i_h) * K * V",
      "    dk += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    db += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    dw += (bos * H + i_h) * K",
      "    dv += (bos * H + i_h) * V",
      "    dq += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "",
      "    dgk_last += (i_tg * H + i_h) * K",
      "    gk += (bos * H + i_h) * K",
      "",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dw = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_db = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dgk_last = tl.zeros([BK], dtype=tl.float32)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v_new = tl.load(p_v_new, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_dgk_last += tl.sum((b_h * b_dh).to(tl.float32), axis=0)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "        b_db += tl.dot(b_v_new, b_dh.to(b_v_new.dtype))",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        b_dv = tl.load(p_dv, boundary_check=(0, 1))",
      "        b_dw += tl.dot(b_dv.to(b_v.dtype), b_h.to(b_v.dtype))",
      "",
      "    m_k = (i_k * BK + tl.arange(0, BK)) < K",
      "    last_idx = min(i_t * BT + BT, T) - 1",
      "    b_gk_last = tl.load(",
      "        gk + last_idx * stride_qk + i_k * BK + tl.arange(0, BK),",
      "        mask=m_k,",
      "        other=float(\"-inf\"),",
      "    )",
      "    b_dgk_last *= exp(b_gk_last)",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_b = tl.make_block_ptr(",
      "        b, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_b = tl.load(p_b, boundary_check=(0, 1))",
      "    b_dgk_last += tl.sum(b_k * b_dk, axis=0)",
      "    b_dgk_last += tl.sum(b_b * b_db, axis=0)",
      "    tl.store(dgk_last + tl.arange(0, BK) + i_k * BK, b_dgk_last, mask=m_k)",
      "",
      "    p_dw = tl.make_block_ptr(",
      "        dw, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_db = tl.make_block_ptr(",
      "        db, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/244.py"
  },
  {
    "name": "chunk_dplr_bwd_kernel_dv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4] for BK in BK_LIST for BV in BK_LIST], key=['BT'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "A_qk",
        "annotation": null
      },
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_bwd_kernel_dv(",
      "    A_qk,",
      "    kg,",
      "    do,",
      "    dv,",
      "    dh,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    A_qk += (bos * H + i_h) * BT",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "    kg += (bos * H + i_h) * K",
      "    dh += (i_tg * H + i_h) * K * V",
      "",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_A = H * BT",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_kg = tl.make_block_ptr(",
      "            kg, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_kg = tl.load(p_kg, boundary_check=(0, 1))",
      "        b_dv += tl.dot(b_kg, b_dh.to(b_kg.dtype))",
      "",
      "    p_Aqk = tl.make_block_ptr(",
      "        A_qk, (BT, T), (1, stride_A), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    b_A = tl.where(",
      "        tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :],",
      "        tl.load(p_Aqk, boundary_check=(0, 1)),",
      "        0,",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_dv += tl.dot(b_A.to(b_do.dtype), b_do)",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/244.py"
  },
  {
    "name": "chunk_dplr_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BK_LIST for BV in BK_LIST for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "A_qk",
        "annotation": null
      },
      {
        "name": "A_qb",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_dplr_fwd_kernel_o(",
      "    qg,",
      "    v,",
      "    v_new,",
      "    A_qk,",
      "    A_qb,",
      "    h,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_qg = tl.make_block_ptr(",
      "            qg + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_qg = tl.load(p_qg, boundary_check=(0, 1))",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_qg, b_h)",
      "",
      "    p_Aqk = tl.make_block_ptr(",
      "        A_qk + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_Aqb = tl.make_block_ptr(",
      "        A_qb + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_v_new = tl.make_block_ptr(",
      "        v_new + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "    b_Aqk = tl.load(p_Aqk, boundary_check=(0, 1))",
      "    b_Aqb = tl.load(p_Aqb, boundary_check=(0, 1))",
      "    b_Aqk = tl.where(m_s, b_Aqk, 0)",
      "    b_Aqb = tl.where(m_s, b_Aqb, 0)",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_v_new = tl.load(p_v_new, boundary_check=(0, 1))",
      "    b_o = (",
      "        b_o",
      "        + tl.dot(b_Aqk.to(b_v.dtype), b_v)",
      "        + tl.dot(b_Aqb.to(b_v_new.dtype), b_v_new)",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/245.py"
  },
  {
    "name": "fused_recurrent_dplr_delta_rule_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BV in [16, 32, 64] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "gk",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_dplr_delta_rule_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    a,",
      "    b,",
      "    gk,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_k = tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_q = q + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_a = a + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_b = b + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_gk = gk + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_o = o + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "",
      "        tmp = tl.sum(b_h * b_a[None, :], axis=1)",
      "        b_h = exp(b_gk)[None, :] * b_h + (",
      "            tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "        )",
      "        b_o = tl.sum(b_h * b_q[None, :], axis=1)",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        p_q += (-1 if REVERSE else 1) * H * K",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_a += (-1 if REVERSE else 1) * H * K",
      "        p_b += (-1 if REVERSE else 1) * H * K",
      "        p_gk += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_o += (-1 if REVERSE else 1) * H * V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/246.py"
  },
  {
    "name": "prepare_wy_repr_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config(triton_config, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "A_ak",
        "annotation": null
      },
      {
        "name": "ag",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv0",
        "annotation": null
      },
      {
        "name": "dag",
        "annotation": null
      },
      {
        "name": "dAak",
        "annotation": null
      },
      {
        "name": "dAab",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_bwd_kernel(",
      "    A_ab_inv,",
      "    A_ak,",
      "    ag,",
      "    v,",
      "    dw,",
      "    du,",
      "    dv,",
      "    dv0,",
      "    dag,",
      "    dAak,",
      "    dAab,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_Aak_t = tl.make_block_ptr(",
      "        A_ak + (bos * H + i_h) * BT,",
      "        (BT, T),",
      "        (1, H * BT),",
      "        (0, i_t * BT),",
      "        (BT, BT),",
      "        (0, 1),",
      "    )",
      "    p_Aab_inv_t = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (BT, T),",
      "        (1, H * BT),",
      "        (0, i_t * BT),",
      "        (BT, BT),",
      "        (0, 1),",
      "    )",
      "    p_dAak = tl.make_block_ptr(",
      "        dAak + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_dAab = tl.make_block_ptr(",
      "        dAab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A_ab_inv_t = tl.load(p_Aab_inv_t, boundary_check=(0, 1))",
      "    b_A_ak_t = tl.load(p_Aak_t, boundary_check=(0, 1))",
      "    b_A_ak_t = tl.where(",
      "        tl.arange(0, BT)[:, None] < tl.arange(0, BT)[None, :], b_A_ak_t, 0",
      "    )",
      "    b_A_ab_inv_t = tl.where(",
      "        tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A_ab_inv_t, 0",
      "    )",
      "    b_A_tmp_t = tl.dot(b_A_ak_t, b_A_ab_inv_t).to(v.dtype.element_ty)",
      "    b_dA_tmp = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv0 = tl.make_block_ptr(",
      "            dv0 + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_du = tl.make_block_ptr(",
      "            du + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_du = tl.load(p_du, boundary_check=(0, 1))",
      "        b_dA_tmp += tl.dot(b_du.to(b_v.dtype), tl.trans(b_v))",
      "        b_dv0 = tl.load(p_dv0, boundary_check=(0, 1))",
      "        b_dv = b_dv0 + tl.dot(b_A_tmp_t, b_du)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    m_i = tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :]",
      "    b_dA_tmp = tl.where(m_i, b_dA_tmp, 0)",
      "    b_dA_ak = tl.dot(b_A_ab_inv_t, b_dA_tmp)",
      "    b_dA_ak = tl.where(m_i, b_dA_ak, 0)",
      "    tl.store(p_dAak, b_dA_ak, boundary_check=(0, 1))",
      "    b_dA_ab_inv = tl.dot(b_dA_tmp, b_A_ak_t)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_ag = tl.make_block_ptr(",
      "            ag + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dag = tl.make_block_ptr(",
      "            dag + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_ag = tl.load(p_ag, boundary_check=(0, 1))",
      "        b_dw = tl.load(p_dw, boundary_check=(0, 1))",
      "        b_dA_ab_inv += tl.dot(b_dw, tl.trans(b_ag))",
      "        b_dag = tl.dot(b_A_ab_inv_t.to(b_dw.dtype), b_dw)",
      "        tl.store(p_dag, b_dag.to(p_dag.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dA_ab_inv = tl.where(",
      "        tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dA_ab_inv, 0",
      "    )",
      "    b_dA_ab_inv = tl.dot(b_A_ab_inv_t, b_dA_ab_inv)",
      "    b_dA_ab_inv = tl.dot(b_dA_ab_inv, b_A_ab_inv_t)",
      "    b_dA_ab_inv = tl.where(m_i, b_dA_ab_inv, 0)",
      "    tl.store(p_dAab, b_dA_ab_inv, boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/247.py"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk32",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A_ab",
        "annotation": null
      },
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk32(",
      "    A_ab,",
      "    A_ab_inv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    p_Aab = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_Aab_inv = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    b_A_ab = tl.load(p_Aab, boundary_check=(0, 1))",
      "    b_A_ab = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A_ab, 0)",
      "    for i in range(1, BT):",
      "        mask = tl.arange(0, BT) == i",
      "        b_a = tl.sum(tl.where(mask[:, None], b_A_ab, 0), 0)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A_ab, 0) * (tl.arange(0, BT) < i)",
      "        b_A_ab = tl.where(mask[:, None], b_a, b_A_ab)",
      "    b_A_ab += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]",
      "    tl.store(p_Aab_inv, b_A_ab.to(p_Aab_inv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/248.py"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BC'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A_ab",
        "annotation": null
      },
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GATHER_SUPPORTED",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk64(",
      "    A_ab,",
      "    A_ab_inv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    GATHER_SUPPORTED: tl.constexpr",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_A1 = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A2 = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A3 = tl.make_block_ptr(",
      "        A_ab + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv1 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv2 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv3 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A_inv4 = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A = tl.load(p_A1, boundary_check=(0, 1))",
      "    b_A2 = tl.load(p_A2, boundary_check=(0, 1))",
      "    b_A3 = tl.load(p_A3, boundary_check=(0, 1))",
      "    b_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)",
      "    b_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)",
      "",
      "    for i in range(1, BC):",
      "        if GATHER_SUPPORTED:",
      "            row_idx = tl.full([1, BC], i, dtype=tl.int16)",
      "",
      "            b_a = tl.sum(gather(b_A, row_idx, axis=0), 0)",
      "            b_a2 = tl.sum(gather(b_A2, row_idx, axis=0), 0)",
      "        else:",
      "            mask = tl.arange(0, BC) == i",
      "            b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)",
      "            b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)",
      "        mask = tl.arange(0, BC) == i",
      "",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)",
      "        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "        b_A2 = tl.where(mask[:, None], b_a2, b_A2)",
      "",
      "    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A3 = tl.dot(tl.dot(b_A2, b_A3), b_A)",
      "",
      "    tl.store(",
      "        p_A_inv1,",
      "        b_A.to(p_A_inv1.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_A_inv2,",
      "        b_A2.to(p_A_inv2.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_A_inv3,",
      "        b_A3.to(p_A_inv3.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    tl.store(",
      "        p_A_inv4,",
      "        tl.zeros([BC, BC], dtype=tl.float32).to(p_A_inv4.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/248.py"
  },
  {
    "name": "wu_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV', 'IS_VARLEN'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "ag",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "A_ab_inv",
        "annotation": null
      },
      {
        "name": "A_ak",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def wu_fwd_kernel(",
      "    w,",
      "    u,",
      "    ag,",
      "    v,",
      "    A_ab_inv,",
      "    A_ak,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_s = tl.arange(0, BT)",
      "",
      "    p_A_ab_inv = tl.make_block_ptr(",
      "        A_ab_inv + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_A_ak = tl.make_block_ptr(",
      "        A_ak + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_Aab_inv = tl.load(p_A_ab_inv, boundary_check=(0, 1))",
      "    b_Aak = tl.load(p_A_ak, boundary_check=(0, 1))",
      "    b_Aab_inv = tl.where(o_s[:, None] >= o_s[None, :], b_Aab_inv, 0)",
      "    b_Aak = tl.where(o_s[:, None] > o_s[None, :], b_Aak, 0)",
      "",
      "    b_Aak = tl.dot(b_Aab_inv, b_Aak)",
      "",
      "    b_Aak = b_Aak.to(v.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "    b_Aab_inv = b_Aab_inv.to(ag.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_ag = tl.make_block_ptr(",
      "            ag + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_ag = tl.load(p_ag, boundary_check=(0, 1))",
      "        b_w = tl.dot(b_Aab_inv, b_ag)",
      "        tl.store(",
      "            p_w,",
      "            b_w.to(p_w.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_u = tl.dot(b_Aak, b_v)",
      "        tl.store(",
      "            p_u,",
      "            b_u.to(p_u.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "            boundary_check=(0, 1),",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/dplr/248.py"
  },
  {
    "name": "chunk_generalized_iplr_delta_rule_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8, 16]], key=['BT', 'BK', 'BV'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "d",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_generalized_iplr_delta_rule_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    d,",
      "    b,",
      "    u,",
      "    v_new,",
      "    h,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_hc = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "        for i_c in range(tl.cdiv(min(BT, T - i_t * BT), BC)):",
      "            p_k = tl.make_block_ptr(",
      "                k + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_b = tl.make_block_ptr(",
      "                b + (bos * H + i_h) * K,",
      "                (K, T),",
      "                (1, H * K),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_d = tl.make_block_ptr(",
      "                d + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_v = tl.make_block_ptr(",
      "                v + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_u = tl.make_block_ptr(",
      "                u + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new + (bos * H + i_h) * V,",
      "                (T, V),",
      "                (H * V, 1),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            b_b = tl.load(p_b, boundary_check=(0, 1))",
      "            b_v2 = tl.dot(b_d, b_h.to(b_d.dtype)) + tl.load(p_u, boundary_check=(0, 1))",
      "            b_hc += tl.dot(b_k, b_v)",
      "            b_hc += tl.dot(b_b, b_v2.to(b_k.dtype))",
      "            tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "        b_h += b_hc",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/249.py"
  },
  {
    "name": "chunk_generalized_iplr_delta_rule_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BKV_LIST for BV in BKV_LIST for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_generalized_iplr_delta_rule_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    u,",
      "    b,",
      "    h,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    u += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_Aqk = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_Aqb = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_b = tl.make_block_ptr(",
      "            b, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_b = tl.load(p_b, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h)",
      "",
      "        b_Aqk += tl.dot(b_q, b_k)",
      "",
      "        b_Aqb += tl.dot(b_q, b_b)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_Aqk = tl.where(m_A, b_Aqk, 0)",
      "    b_Aqb = tl.where(m_A, b_Aqb, 0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_u = tl.make_block_ptr(",
      "        u, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_u = tl.load(p_u, boundary_check=(0, 1))",
      "    b_o = (",
      "        b_o + tl.dot(b_Aqk.to(b_v.dtype), b_v) + tl.dot(b_Aqb.to(b_u.dtype), b_u)",
      "    ) * scale",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/249.py"
  },
  {
    "name": "fused_recurrent_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BV in [32, 64] for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "ha",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    a,",
      "    b,",
      "    o,",
      "    ha,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    H,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    p_q = q + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_k = k + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_a = a + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_b = b + (bos * H + i_h) * K + tl.arange(0, BK)",
      "    p_ha = ha + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_v = v + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "    p_o = o + (bos * H + i_h) * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    mask_k = tl.arange(0, BK) < K",
      "    mask_v = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[None, :]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "",
      "        tmp = tl.sum(b_h * b_a[None, :], axis=1)",
      "        b_h += tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        tl.store(p_ha, tmp.to(p_ha.dtype.element_ty), mask=mask_v)",
      "        p_q += K * H",
      "        p_k += K * H",
      "        p_o += V * H",
      "        p_v += V * H",
      "        p_ha += V * H",
      "        p_a += K * H",
      "        p_b += K * H",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[None, :]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/250.py"
  },
  {
    "name": "fused_recurrent_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_DHT': lambda args: args['dht'] is not None, 'USE_DH0': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3]], key=['BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "ha",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "da",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "dha",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DH0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DHT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    a,",
      "    b,",
      "    ha,",
      "    dht,",
      "    dh0,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    da,",
      "    db,",
      "    dha,",
      "    h0,",
      "    scale,",
      "    cu_seqlens,",
      "    B,",
      "    H,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_DH0: tl.constexpr,",
      "    USE_DHT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    dk += i_v * B * H * K * T",
      "    db += i_v * B * H * K * T",
      "    dq += i_v * B * H * K * T",
      "    da += i_v * B * H * K * T",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    mask_k = tl.arange(0, BK) < K",
      "    mask_v = (tl.arange(0, BV) + i_v * BV) < V",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V + i_v * BV",
      "    ha += (bos * H + i_h) * V + i_v * BV",
      "    a += (bos * H + i_h) * K",
      "    b += (bos * H + i_h) * K",
      "    do += (bos * H + i_h) * V + i_v * BV",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    dv += (bos * H + i_h) * V + i_v * BV",
      "    da += (bos * H + i_h) * K",
      "    db += (bos * H + i_h) * K",
      "    dha += (bos * H + i_h) * V + i_v * BV",
      "",
      "    p_q = q + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_k = k + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_v = v + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_ha = ha + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_a = a + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_b = b + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_do = do + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_dk = dk + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_dv = dv + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_dha = dha + tl.arange(0, BV) + (T - 1) * H * V",
      "    p_db = db + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_da = da + tl.arange(0, BK) + (T - 1) * H * K",
      "    p_dq = dq + tl.arange(0, BK) + (T - 1) * H * K",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_DHT:",
      "        p_ht = (",
      "            dht",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[:, None]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[None, :])",
      "        )",
      "        b_dh += tl.load(p_ht, mask=mask_k[:, None] & mask_v[None, :], other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "    for _ in range(T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)",
      "",
      "        b_dh += b_q[:, None] * b_do[None, :]",
      "        d_k = tl.sum(b_dh * b_v[None, :], axis=1)",
      "        d_v = tl.sum(b_dh * b_k[:, None], axis=0)",
      "        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_v)",
      "",
      "        b_dha = tl.sum(b_dh * b_b[:, None], axis=0)",
      "        tl.store(p_dha, b_dha.to(p_dha.dtype.element_ty), mask=mask_v)",
      "        b_db = tl.sum(b_dh * b_ha[None, :], axis=1)",
      "        tl.store(p_db, b_db.to(p_db.dtype.element_ty), mask=mask_k)",
      "",
      "        b_dh += b_dha[None, :] * b_a[:, None]",
      "        p_do -= H * V",
      "        p_q -= H * K",
      "        p_k -= H * K",
      "        p_v -= H * V",
      "        p_dk -= H * K",
      "        p_dv -= H * V",
      "        p_b -= H * K",
      "        p_db -= H * K",
      "        p_a -= H * K",
      "        p_dha -= H * V",
      "        p_ha -= H * V",
      "",
      "    if USE_DH0:",
      "        p_dh0 = (",
      "            dh0",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        tl.store(",
      "            p_dh0,",
      "            b_dh.to(p_dh0.dtype.element_ty),",
      "            mask=mask_k[:, None] & mask_v[None, :],",
      "        )",
      "",
      "    tl.debug_barrier()",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        mask_kv = mask_k[:, None] & mask_v[None, :]",
      "        p_h0 = (",
      "            h0",
      "            + i_nh * K * V",
      "            + (tl.arange(0, BK)[:, None]) * V",
      "            + ((i_v * BV + tl.arange(0, BV))[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)",
      "",
      "    p_k = k + tl.arange(0, BK)",
      "    p_v = v + tl.arange(0, BV)",
      "    p_ha = ha + tl.arange(0, BV)",
      "    p_do = do + tl.arange(0, BV)",
      "    p_dha = dha + tl.arange(0, BV)",
      "    p_da = da + tl.arange(0, BK)",
      "    p_dq = dq + tl.arange(0, BK)",
      "    p_b = b + tl.arange(0, BK)",
      "",
      "    for i in range(0, T):",
      "        b_dha = tl.load(p_dha, mask=mask_v, other=0).to(tl.float32)",
      "        d_a = tl.sum(b_dha[None, :] * b_h, axis=1)",
      "        tl.store(p_da, d_a.to(p_da.dtype.element_ty), mask=mask_k)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_b = tl.load(p_b, mask=mask_k, other=0).to(tl.float32)",
      "        b_ha = tl.load(p_ha, mask=mask_v, other=0).to(tl.float32)",
      "        b_h += b_k[:, None] * b_v[None, :] + b_b[:, None] * b_ha[None, :]",
      "        _d_q = b_h * b_do[None, :]",
      "        d_q = tl.sum(_d_q, axis=1) * scale",
      "        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += H * K",
      "        p_do += H * V",
      "        p_v += H * V",
      "        p_da += H * K",
      "        p_dha += H * V",
      "        p_ha += H * V",
      "        p_dq += H * K",
      "        p_b += H * K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/250.py"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk32",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BK'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk32(",
      "    a,",
      "    b,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_a = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_b = tl.make_block_ptr(",
      "            b + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, K * H),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        b_a = tl.load(p_a, boundary_check=(0, 1))",
      "        b_b = tl.load(p_b, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_a, b_b)",
      "",
      "    b_A = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_A, 0)",
      "    for i in range(1, BT):",
      "        mask = tl.arange(0, BT) == i",
      "        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BT) < i)",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "    b_A += tl.arange(0, BT)[:, None] == tl.arange(0, BT)[None, :]",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/251.py"
  },
  {
    "name": "prepare_wy_repr_fwd_kernel_chunk64",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BK'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_wy_repr_fwd_kernel_chunk64(",
      "    a,",
      "    b,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    b_A2 = tl.zeros([BC, BC], dtype=tl.float32)",
      "    b_A3 = tl.zeros([BC, BC], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_a1 = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_a2 = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_b1 = tl.make_block_ptr(",
      "            b + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, K * H),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_b2 = tl.make_block_ptr(",
      "            b + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, K * H),",
      "            (i_k * BK, i_t * BT + BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        b_a1 = tl.load(p_a1, boundary_check=(0, 1))",
      "        b_a2 = tl.load(p_a2, boundary_check=(0, 1))",
      "        b_b1 = tl.load(p_b1, boundary_check=(0, 1))",
      "        b_b2 = tl.load(p_b2, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_a1, b_b1, allow_tf32=False)",
      "        b_A2 += tl.dot(b_a2, b_b2, allow_tf32=False)",
      "        b_A3 += tl.dot(b_a2, b_b1, allow_tf32=False)",
      "",
      "    b_A = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A, 0)",
      "    b_A2 = tl.where(tl.arange(0, BC)[:, None] > tl.arange(0, BC)[None, :], b_A2, 0)",
      "",
      "    for i in range(1, BC):",
      "        mask = tl.arange(0, BC) == i",
      "        b_a = tl.sum(tl.where(mask[:, None], b_A, 0), 0)",
      "        b_a2 = tl.sum(tl.where(mask[:, None], b_A2, 0), 0)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0) * (tl.arange(0, BC) < i)",
      "        b_a2 = b_a2 + tl.sum(b_a2[:, None] * b_A2, 0) * (tl.arange(0, BC) < i)",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "        b_A2 = tl.where(mask[:, None], b_a2, b_A2)",
      "",
      "    b_A += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A2 += tl.arange(0, BC)[:, None] == tl.arange(0, BC)[None, :]",
      "    b_A3 = tl.dot(tl.dot(b_A2, b_A3, allow_tf32=False), b_A, allow_tf32=False)",
      "",
      "    p_A1 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BC, BC), (1, 0)",
      "    )",
      "    p_A2 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A3 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + BC, 0),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    p_A4 = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, BC), (BC, BC), (1, 0)",
      "    )",
      "    tl.store(p_A1, b_A.to(p_A1.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_A2, b_A2.to(p_A2.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_A3, b_A3.to(p_A3.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.store(",
      "        p_A4,",
      "        tl.zeros([BC, BC], dtype=tl.float32).to(p_A4.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/251.py"
  },
  {
    "name": "wu_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in NUM_WARPS], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def wu_fwd_kernel(",
      "    w,",
      "    u,",
      "    a,",
      "    k,",
      "    v,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_Aak = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_a = tl.make_block_ptr(",
      "            a + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_w = tl.make_block_ptr(",
      "            w + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_a = tl.load(p_a, boundary_check=(0, 1))",
      "        b_w = tl.dot(b_A, b_a)",
      "        b_Aak += tl.dot(b_a, tl.trans(b_k))",
      "        tl.store(p_w, b_w.to(p_w.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_Aak = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_Aak, 0)",
      "    b_Aak = b_Aak.to(k.dtype.element_ty)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_u = tl.make_block_ptr(",
      "            u + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_v = tl.dot(b_Aak, b_v).to(v.dtype.element_ty)",
      "        b_u = tl.dot(b_A, b_v)",
      "        tl.store(p_u, b_u.to(p_u.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/generalized_delta_rule/iplr/251.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BC'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_inter(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_i, i_j = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "    if i_i <= i_j:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_qg = b_q * exp(b_g - b_gn[None, :]) * scale",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_kg = b_k * exp(b_gn[:, None] - b_gk)",
      "",
      "        b_A += tl.dot(b_qg, b_kg)",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BK', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i >= j, b_A * scale, 0.0)",
      "",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_k += H * K",
      "        p_gk += H * K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC', 'BK'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra_split(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_tc // NC, i_tc % NC",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "",
      "    o_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = g + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_A = tl.zeros([BC], dtype=tl.float32)",
      "        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A += tl.sum(b_q * b_k[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i >= j, b_A * scale, 0.0)",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_k += H * K",
      "        p_gk += H * K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra_merge",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra_merge(",
      "    A,",
      "    A2,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    NK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_c * BC >= T:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(0, NK):",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_k * all + bos) * H * BC + i_h * BC,",
      "            (T, BC),",
      "            (H * BC, 1),",
      "            (i_t * BT + i_c * BC, 0),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        b_A += tl.load(p_A, boundary_check=(0, 1))",
      "    p_A2 = tl.make_block_ptr(",
      "        A2 + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_c * BC, i_c * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in [32, 64] for BV in [64, 128] for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_kernel_o(",
      "    q,",
      "    v,",
      "    g,",
      "    h,",
      "    o,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "",
      "        b_qg = (b_q * exp(b_g)).to(b_q.dtype)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        if i_k >= 0:",
      "            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)",
      "    b_o += tl.dot(b_A, b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_bwd_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BK', 'NC', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_intra(",
      "    q,",
      "    k,",
      "    g,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    if i_i > 0:",
      "        p_gn = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(0, i_i):",
      "            p_k = tl.make_block_ptr(",
      "                k + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gk = tl.make_block_ptr(",
      "                g + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (T, BT),",
      "                (H * BT, 1),",
      "                (i_t * BT + i_i * BC, i_j * BC),",
      "                (BC, BC),",
      "                (1, 0),",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_kg = b_k * exp(b_gn[None, :] - b_gk)",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dq += tl.dot(b_dA, b_kg)",
      "        b_dq *= exp(b_g - b_gn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "    )",
      "    p_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gkj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)",
      "",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] >= j",
      "",
      "        b_dq += tl.where(",
      "            m_i, b_dA[:, None] * b_kj[None, :] * exp(b_g - b_gkj[None, :]), 0.0",
      "        )",
      "        p_kj += H * K",
      "        p_gkj += H * K",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_gk = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    NC = min(NC, tl.cdiv(T - i_t * BT, BC))",
      "    if i_i < NC - 1:",
      "        p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(i_i + 1, NC):",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gq = tl.make_block_ptr(",
      "                g + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (BT, T),",
      "                (1, H * BT),",
      "                (i_i * BC, i_t * BT + i_j * BC),",
      "                (BC, BC),",
      "                (0, 1),",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_gq = tl.load(p_gq, boundary_check=(0, 1))",
      "            b_qg = b_q * safe_exp(b_gq - b_gn[None, :])",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dk += tl.dot(b_dA, b_qg)",
      "        b_dk *= exp(b_gn[None, :] - b_gk)",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "        + tl.arange(0, BC)",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gqj = g + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j * H * BT)",
      "",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dk += tl.where(",
      "            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0",
      "        )",
      "        p_qj += H * K",
      "        p_gqj += H * K",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_bwd_kernel_dA",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BV', 'BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_dA(",
      "    v,",
      "    do,",
      "    dA,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "",
      "    b_dA = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dA += tl.dot(b_do, b_v)",
      "    p_dA = tl.make_block_ptr(",
      "        dA + (bos * H + i_h) * BT, (T, BT), (H * BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "    b_dA = tl.where(m_s, b_dA * scale, 0.0)",
      "    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_bwd_kernel_dv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in BK_LIST for BV in BV_LIST for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_dv(",
      "    k,",
      "    g,",
      "    A,",
      "    do,",
      "    dh,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT, (BT, T), (1, H * BT), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_A = tl.where(tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :], b_A, 0.0)",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_dv = tl.dot(b_A, b_do.to(b_A.dtype), allow_tf32=False)",
      "",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_gn = g + (bos + min(i_t * BT + BT, T) - 1) * H * K + i_h * K + o_k",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_gn = exp(tl.load(p_gn, mask=m_k, other=0)[None, :] - b_gk)",
      "        b_k = (b_k * b_gn).to(b_k.dtype)",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dv += tl.dot(b_k, b_dh.to(b_k.dtype))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "chunk_gla_bwd_kernel_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in BK_LIST for BV in BV_LIST for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dq2",
        "annotation": null
      },
      {
        "name": "dk2",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_bwd_kernel_inter(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dq2,",
      "    dk2,",
      "    dg,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_gk = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_gn = g + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k",
      "    b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dgk = tl.zeros(",
      "        [",
      "            BK,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dgk += tl.sum(b_h * b_dh, axis=0)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "    b_dgk *= exp(b_gn)",
      "    b_dq *= scale",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_dq = b_dq * exp(b_gk)",
      "    b_dk = b_dk * exp(b_gn[None, :] - b_gk)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dgk += tl.sum(b_dk * b_k, axis=0)",
      "    b_dq += tl.load(p_dq, boundary_check=(0, 1))",
      "    b_dk += tl.load(p_dk, boundary_check=(0, 1))",
      "    b_dg = b_q * b_dq - b_k * b_dk",
      "",
      "    b_dg = (",
      "        b_dg - tl.cumsum(b_dg, axis=0) + tl.sum(b_dg, axis=0)[None, :] + b_dgk[None, :]",
      "    )",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/252.py"
  },
  {
    "name": "prepare_qg_kg",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "qg",
        "annotation": null
      },
      {
        "name": "kg",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_qg_kg(",
      "    q, k, g, qg, kg, scale, T, K: tl.constexpr, BT: tl.constexpr, BK: tl.constexpr",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    p_q = q + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_g = g + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_qg = qg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "    p_kg = kg + i_bh * T * K + i_c * BT * K + i_k * BK + tl.arange(0, BK)",
      "",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "",
      "    last_decay = tl.load(",
      "        g + i_bh * T * K + (i_c * BT + BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "    )",
      "",
      "    for _ in range(BT):",
      "        b_q = tl.load(p_q, mask=mask, other=0)",
      "        b_k = tl.load(p_k, mask=mask, other=0)",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_q *= exp(b_g) * scale",
      "        b_k *= exp(last_decay - b_g)",
      "        tl.store(p_kg, b_k.to(p_kg.dtype.element_ty), mask=mask)",
      "        tl.store(p_qg, b_q.to(p_qg.dtype.element_ty), mask=mask)",
      "        p_q += K",
      "        p_g += K",
      "        p_k += K",
      "        p_kg += K",
      "        p_qg += K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/253.py"
  },
  {
    "name": "bwd_decay_global_cumsum",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "dq_inner",
        "annotation": null
      },
      {
        "name": "dq_inter",
        "annotation": null
      },
      {
        "name": "dk_inner",
        "annotation": null
      },
      {
        "name": "dk_inter",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bwd_decay_global_cumsum(",
      "    dq_inner,",
      "    dq_inter,",
      "    dk_inner,",
      "    dk_inter,",
      "    q,",
      "    k,",
      "    g,",
      "    dg,",
      "    T,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_g = g + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_dg = dg + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    p_dq_inner = (",
      "        dq_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    p_dk_inner = (",
      "        dk_inner + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    p_dq_inter = (",
      "        dq_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    p_dk_inter = (",
      "        dk_inter + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (i_c * BT + BT - 1) * K",
      "    )",
      "    cum_grad_dg = tl.zeros([BK], dtype=tl.float32)",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "    last_g = tl.zeros([BK], dtype=tl.float32)",
      "    for j in range(BT - 1, -1, -1):",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        if j == (BT - 1):",
      "            last_g = b_g",
      "        b_dq1 = tl.load(p_dq_inner, mask=mask, other=0)",
      "        b_dq2 = tl.load(p_dq_inter, mask=mask, other=0)",
      "        b_dq2 *= exp(b_g)",
      "        b_dq = b_dq1 + b_dq2",
      "        tl.store(p_dq_inter, b_dq, mask=mask)",
      "        b_dk1 = tl.load(p_dk_inner, mask=mask, other=0)",
      "        b_dk2 = tl.load(p_dk_inter, mask=mask, other=0)",
      "        b_dk2 *= safe_exp(last_g - b_g)",
      "        b_dk = b_dk1 + b_dk2",
      "        tl.store(p_dk_inter, b_dk, mask=mask)",
      "        b_q = tl.load(p_q, mask=mask, other=0)",
      "        b_k = tl.load(p_k, mask=mask, other=0)",
      "        b_dg = b_dq * b_q - b_dk * b_k",
      "        cum_grad_dg += b_dg",
      "        tl.store(p_dg, cum_grad_dg.to(p_dg.dtype.element_ty), mask=mask)",
      "        p_g -= K",
      "        p_k -= K",
      "        p_q -= K",
      "        p_dq_inner -= K",
      "        p_dk_inner -= K",
      "        p_dq_inter -= K",
      "        p_dk_inter -= K",
      "        p_dg -= K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/253.py"
  },
  {
    "name": "fused_chunk_gla_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_gla_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    o,",
      "    h0,",
      "    ht,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_gn = g + i_bh * T * K + (BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + i_k * B * H) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (0, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)",
      "        if CHECK and i == 0:",
      "            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[:, None] + tl.dot(",
      "                b_k.to(b_v.dtype), b_v, allow_tf32=False",
      "            )",
      "        else:",
      "            b_o = tl.dot(b_q.to(b_v.dtype), b_h.to(b_v.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[:, None] + tl.dot(",
      "                b_k.to(b_v.dtype), b_v, allow_tf32=False",
      "            )",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "        p_q = tl.advance(p_q, (BT, 0))",
      "        p_k = tl.advance(p_k, (0, BT))",
      "        p_v = tl.advance(p_v, (BT, 0))",
      "        p_o = tl.advance(p_o, (BT, 0))",
      "        p_gn += BT * K",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_final = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_final, b_h.to(p_final.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/253.py"
  },
  {
    "name": "fused_chunk_gla_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_gla_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        b_h += tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    mask = (i_k * BK + tl.arange(0, BK)) < K",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_gn = g + i_bh * T * K + ((i + 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (i_bh + i_v * B * H) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gn = tl.load(p_gn, mask=mask, other=0).to(tl.float32)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        if CHECK and i == 0:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[None, :] + tl.dot(",
      "                b_v, b_k.to(b_v.dtype), allow_tf32=False",
      "            )",
      "        else:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h * exp(b_gn)[None, :] + tl.dot(",
      "                b_v, b_k.to(b_v.dtype), allow_tf32=False",
      "            )",
      "        b_dq *= scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_h = None",
      "    tl.debug_barrier()",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for i in range(1, tl.cdiv(T, BT) + 1):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_gn = (",
      "            g + i_bh * T * K + (T - (i - 1) * BT - 1) * K + i_k * BK + tl.arange(0, BK)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (i_bh + i_v * B * H) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (T - i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_bh + i_k * B * H) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (T - i * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_db = tl.load(p_gn, mask=mask, other=0).to(tl.float32)",
      "",
      "        if CHECK and i == 1:",
      "            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))",
      "            b_dv = tl.dot((b_k).to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)",
      "            b_dh = b_dh * exp(b_db)[:, None] + tl.dot(",
      "                b_q.to(b_do.dtype), b_do, allow_tf32=False",
      "            )",
      "        else:",
      "            b_dk = tl.trans(tl.dot(b_dh.to(b_v.dtype), tl.trans(b_v), allow_tf32=False))",
      "            b_dv = tl.dot((b_k).to(b_v.dtype), b_dh.to(b_v.dtype), allow_tf32=False)",
      "            b_dh = b_dh * exp(b_db)[:, None] + tl.dot(",
      "                b_q.to(b_do.dtype), b_do, allow_tf32=False",
      "            )",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gla/253.py"
  },
  {
    "name": "chunk_gsa_fwd_k_kernel_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_fwd_k_kernel_inter(",
      "    q,",
      "    k,",
      "    h,",
      "    g,",
      "    o,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (T, K),",
      "            (HQ * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_o += tl.dot(b_q, b_h)",
      "",
      "        b_A += tl.dot(b_q, b_k)",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    b_o = b_o * exp(b_g)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    if i_v == 0:",
      "        tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gsa/254.py"
  },
  {
    "name": "chunk_gsa_fwd_k_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_fwd_k_kernel_intra(",
      "    v,",
      "    g,",
      "    o,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    m_v = o_v < V",
      "",
      "    if i_t * BT + i_i * BC > T:",
      "        return",
      "",
      "    p_g = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_gn = g + (bos + min(i_t * BT + i_i * BC, T)) * H * V + i_h * V + o_v",
      "",
      "    b_gn = tl.load(p_gn, mask=m_v, other=0)",
      "",
      "    b_o = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(0, i_i):",
      "        p_A = tl.make_block_ptr(",
      "            A + (bos * HQ + i_hq) * BT,",
      "            (T, BT),",
      "            (HQ * BT, 1),",
      "            (i_t * BT + i_i * BC, i_j * BC),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_gv = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_vg = (b_v * exp(b_gn[None, :] - b_gv)).to(b_v.dtype)",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_A, b_vg)",
      "",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    b_o *= exp(b_g - b_gn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_A = (",
      "        (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * HQ * BT + i_hq * BT + i_i * BC",
      "    )",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        p_v = v + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v",
      "        p_gv = g + (bos + i_t * BT + i_i * BC + j) * H * V + i_h * V + o_v",
      "",
      "        b_A = tl.load(A + o_A + j, mask=m_A, other=0)",
      "",
      "        b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)",
      "        b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)",
      "",
      "        b_vg = b_v[None, :] * exp(b_g - b_gv[None, :])",
      "",
      "        b_o += tl.where(o_i[:, None] >= j, b_A[:, None] * b_vg, 0.0)",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    b_o += tl.load(p_o, boundary_check=(0, 1))",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gsa/254.py"
  },
  {
    "name": "chunk_gsa_bwd_k_kernel_dA",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_bwd_k_kernel_dA(",
      "    v,",
      "    g,",
      "    do,",
      "    dA,",
      "    chunk_indices,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    i_t, i_i, i_j = i_c // (NC * NC), (i_c % (NC * NC)) // NC, (i_c % (NC * NC)) % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    m_v = o_v < V",
      "",
      "    if i_t * BT + i_i * BC > T:",
      "        return",
      "",
      "    p_dA = tl.make_block_ptr(",
      "        dA + ((i_v * all + bos) * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "",
      "    b_dA = tl.zeros([BC, BC], dtype=tl.float32)",
      "    if i_i > i_j:",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT + i_j * BC),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "        p_gv = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT + i_j * BC),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_gn = tl.load(p_gn, mask=m_v, other=0.0)",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_g - b_gn[None, :]) * scale).to(b_do.dtype)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "        b_vg = (b_v * exp(b_gn[:, None] - b_gv)).to(b_v.dtype)",
      "",
      "        b_dA = tl.dot(b_do, b_vg)",
      "    elif i_i == i_j:",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT + i_i * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = v + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v",
      "        p_gv = g + (bos + i_t * BT + i_j * BC) * H * V + i_h * V + o_v",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)) * scale",
      "        m_v = o_v < V",
      "",
      "        o_i = tl.arange(0, BC)",
      "",
      "        m_dA = o_i[:, None] >= o_i[None, :]",
      "        for j in range(0, min(BC, T - i_t * BT - i_j * BC)):",
      "",
      "            b_v = tl.load(p_v, mask=m_v, other=0).to(tl.float32)",
      "            b_gv = tl.load(p_gv, mask=m_v, other=0).to(tl.float32)",
      "",
      "            b_dAj = tl.sum(b_do * b_v[None, :] * exp(b_g - b_gv[None, :]), 1)",
      "            b_dA = tl.where((o_i == j)[None, :], b_dAj[:, None], b_dA)",
      "",
      "            p_v += H * V",
      "            p_gv += H * V",
      "        b_dA = tl.where(m_dA, b_dA, 0.0)",
      "    tl.store(p_dA, b_dA.to(dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gsa/254.py"
  },
  {
    "name": "chunk_gsa_bwd_k_kernel_dqkvg",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4] for num_stages in [2, 3, 4]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dgv",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_bwd_k_kernel_dqkvg(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    A,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    dg,",
      "    dgv,",
      "    dA,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    o_t = min(i_t * BT + BT, T)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (HQ * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + ((i_k * all + bos) * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "    b_A = tl.dot((b_q * scale).to(b_q.dtype), tl.trans(b_k))",
      "    b_A = tl.where(m_s, b_A, 0.0)",
      "    tl.store(p_A, b_A.to(p_A.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        o_v = i_v * BV + tl.arange(0, BV)",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_gn = g + (bos + o_t - 1) * H * V + i_h * V + o_v",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + ((i_k * all + bos) * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dg = tl.make_block_ptr(",
      "            dg + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dgv = tl.make_block_ptr(",
      "            dgv + ((i_k * all + bos) * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * HQ + i_hq) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        m_v = o_v < V",
      "",
      "        b_gn = tl.load(p_gn, mask=m_v, other=0)",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_gv = exp(b_gn[None, :] - b_g)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_do = (b_do * exp(b_g) * scale).to(b_do.dtype)",
      "",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dg = tl.sum(tl.trans(b_h) * b_dh, 0) * exp(b_gn)",
      "",
      "        b_dh = b_dh.to(b_k.dtype)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_k.dtype))",
      "        b_dk += tl.dot((b_v * b_gv).to(b_v.dtype), tl.trans(b_dh))",
      "",
      "        b_dv = tl.dot(b_k, b_dh) * b_gv",
      "",
      "        b_dg += tl.sum(b_dv * b_v, 0)",
      "",
      "        if i_k == 0:",
      "            b_dgv = tl.load(p_dg, boundary_check=(0, 1)) + b_dg[None, :]",
      "        else:",
      "            b_dgv = tl.zeros([BT, BV], dtype=tl.float32) + b_dg[None, :]",
      "",
      "        tl.store(p_dgv, b_dgv.to(p_dgv.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dA = tl.make_block_ptr(",
      "        dA + (bos * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (HQ * BT, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (HQ * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (HQ * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "    b_dq += tl.dot(b_dA, b_k)",
      "    b_dk += tl.dot(tl.trans(b_dA).to(b_k.dtype), b_q)",
      "",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gsa/254.py"
  },
  {
    "name": "chunk_gsa_bwd_k_kernel_intra_dvg",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gsa_bwd_k_kernel_intra_dvg(",
      "    v,",
      "    g,",
      "    o,",
      "    A,",
      "    do,",
      "    dv,",
      "    dg,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // NG",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    m_v = o_v < V",
      "",
      "    if i_t * BT + i_i * BC > T:",
      "        return",
      "",
      "    p_gv = tl.make_block_ptr(",
      "        g + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_gn = g + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * V + i_h * V + o_v",
      "",
      "    b_gn = tl.load(p_gn, mask=m_v, other=0)",
      "",
      "    b_gv = tl.load(p_gv, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "    for i_j in range(i_i + 1, NC):",
      "        p_g = tl.make_block_ptr(",
      "            g + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "        p_A = tl.make_block_ptr(",
      "            A + (bos * HQ + i_hq) * BT,",
      "            (BT, T),",
      "            (1, HQ * BT),",
      "            (i_i * BC, i_t * BT + i_j * BC),",
      "            (BC, BC),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT + i_j * BC, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)) * safe_exp(b_g - b_gn[None, :])",
      "",
      "        b_A = tl.load(p_A, boundary_check=(0, 1))",
      "",
      "        b_dv += tl.dot(b_A, b_do.to(b_A.dtype))",
      "    b_dv *= exp(b_gn[None, :] - b_gv)",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_c = i_i * BC + tl.arange(0, BC)",
      "",
      "    p_g = g + (bos + i_t * BT + i_i * BC) * H * V + i_h * V + o_v",
      "    p_A = A + (bos + i_t * BT + i_i * BC) * HQ * BT + i_hq * BT + o_c",
      "    p_do = do + (bos + i_t * BT + i_i * BC) * HQ * V + i_hq * V + o_v",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_A = tl.load(p_A)",
      "",
      "        b_g = tl.load(p_g, mask=m_v, other=0)",
      "        b_do = tl.load(p_do, mask=m_v, other=0)",
      "",
      "        m_i = o_i[:, None] <= j",
      "        b_dv += tl.where(",
      "            m_i, exp(b_g[None, :] - b_gv) * b_A[:, None] * b_do[None, :], 0.0",
      "        )",
      "",
      "        p_g += H * V",
      "        p_A += HQ * BT",
      "        p_do += HQ * V",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT + i_i * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)",
      "    b_v = tl.load(p_v, boundary_check=(0, 1)).to(tl.float32)",
      "    b_do = tl.load(p_do, boundary_check=(0, 1)).to(tl.float32)",
      "    b_dv = b_dv + tl.load(p_dv, boundary_check=(0, 1)).to(tl.float32)",
      "    b_dg = b_o * b_do - b_v * b_dv",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/gsa/254.py"
  },
  {
    "name": "chunk_hgrn_fwd_kernel_h",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4), triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64}, num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton.Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps=2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128}, num_warps=8)], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_fwd_kernel_h(",
      "    x,",
      "    g,",
      "    gc,",
      "    o,",
      "    h0,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_x = x + i_b * T * D + i_t * BT * D + o_d",
      "    p_g = g + i_b * T * D + i_t * BT * D + o_d",
      "    p_gc = gc + i_b * T * D + i_t * BT * D + o_d",
      "    p_o = o + i_b * T * D + i_t * BT * D + o_d",
      "",
      "    b_h = tl.zeros([BD], dtype=tl.float32)",
      "    b_gc = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        if i_t == 0:",
      "            b_h += tl.load(h0 + i_b * D + o_d, mask=mask, other=0).to(tl.float32)",
      "    for i in range(0, BT):",
      "        mask_t = mask & ((i_t * BT + i) < T)",
      "        b_x = tl.load(p_x, mask=mask_t, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g, mask=mask_t, other=0).to(tl.float32)",
      "        b_h = exp(b_g) * b_h + b_x",
      "        b_gc = b_gc + b_g",
      "        tl.store(p_gc, b_gc.to(p_o.dtype.element_ty), mask=mask_t)",
      "        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask_t)",
      "",
      "        p_x += D",
      "        p_g += D",
      "        p_gc += D",
      "        p_o += D"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/hgrn/256.py"
  },
  {
    "name": "chunk_hgrn_fwd_kernel_o",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "s_b",
        "annotation": null
      },
      {
        "name": "s_t",
        "annotation": null
      },
      {
        "name": "s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_fwd_kernel_o(",
      "    gc, o, s_b, s_t, s_d, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr",
      "):",
      "    i_d, i_b = tl.program_id(0), tl.program_id(1)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    for i_t in range(1, tl.cdiv(T, BT)):",
      "        p_gc = tl.make_block_ptr(",
      "            gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        b_h0 = tl.load(o + i_b * T * D + i_t * BT * D - D + o_d, mask=mask, other=0).to(",
      "            tl.float32",
      "        )",
      "",
      "        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)",
      "        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)",
      "        b_o = b_o + exp(b_gc) * b_h0[None, :]",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/hgrn/256.py"
  },
  {
    "name": "chunk_hgrn_bwd_kernel_h",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_bwd_kernel_h(",
      "    g, gc, dx, do, T, D: tl.constexpr, BT: tl.constexpr, BD: tl.constexpr",
      "):",
      "    i_d, i_t, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "    BC = min(BT, T - i_t * BT)",
      "    NT = tl.num_programs(1)",
      "",
      "    p_g = g + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "    p_gc = gc + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "    p_dx = dx + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "    p_do = do + (i_b * T + i_t * BT + BC - 1) * D + o_d",
      "",
      "    if i_t == NT - 1:",
      "        b_gc = tl.zeros([BD], dtype=tl.float32)",
      "    else:",
      "        b_gc = tl.load(g + (i_b * T + i_t * BT + BT) * D + o_d, mask=mask, other=0).to(",
      "            tl.float32",
      "        )",
      "    b_dh = tl.zeros([BD], dtype=tl.float32)",
      "    for _ in range(BC - 1, -1, -1):",
      "        tl.store(p_gc, b_gc.to(p_gc.dtype.element_ty), mask=mask)",
      "",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)",
      "",
      "        b_gc = b_gc + b_g",
      "        b_dh = b_dh + b_do",
      "        b_dx = b_dh",
      "        b_dh = b_dh * exp(b_g)",
      "",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)",
      "",
      "        p_g -= D",
      "        p_gc -= D",
      "        p_dx -= D",
      "        p_do -= D"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/hgrn/256.py"
  },
  {
    "name": "chunk_hgrn_bwd_kernel_o",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "gc",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "s_b",
        "annotation": null
      },
      {
        "name": "s_t",
        "annotation": null
      },
      {
        "name": "s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_hgrn_bwd_kernel_o(",
      "    g,",
      "    gc,",
      "    o,",
      "    dx,",
      "    dg,",
      "    s_b,",
      "    s_t,",
      "    s_d,",
      "    T,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_d, i_b = tl.program_id(0), tl.program_id(1)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):",
      "        p_g = tl.make_block_ptr(",
      "            g + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_gc = tl.make_block_ptr(",
      "            gc + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + i_b * s_b,",
      "            (T, D),",
      "            (s_t, s_d),",
      "            (i_t * BT - 1, i_d * BD),",
      "            (BT, BD),",
      "            (1, 0),",
      "        )",
      "        p_dx = tl.make_block_ptr(",
      "            dx + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "        p_dg = tl.make_block_ptr(",
      "            dg + i_b * s_b, (T, D), (s_t, s_d), (i_t * BT, i_d * BD), (BT, BD), (1, 0)",
      "        )",
      "",
      "        mask_t = mask & ((i_t + 1) * BT < T)",
      "        b_ht = tl.load(",
      "            dx + i_b * T * D + (i_t + 1) * BT * D + o_d, mask=mask_t, other=0",
      "        ).to(tl.float32)",
      "",
      "        b_g = tl.load(p_g, boundary_check=(0, 1)).to(tl.float32)",
      "        b_gc = tl.load(p_gc, boundary_check=(0, 1)).to(tl.float32)",
      "        b_o = tl.load(p_o, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dx = tl.load(p_dx, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        b_dx = b_dx + exp(b_gc) * b_ht[None, :]",
      "        b_dg = b_o * b_dx * exp(b_g)",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/hgrn/256.py"
  },
  {
    "name": "fused_recurrent_hgrn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_hgrn_fwd_kernel(",
      "    x,",
      "    g,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_n = tl.program_id(0), tl.program_id(1)",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_x = x + bos * D + o_d",
      "    p_g = g + bos * D + o_d",
      "    p_o = o + bos * D + o_d",
      "",
      "    b_h = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_n * D + o_d",
      "        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)",
      "    for _ in range(0, T):",
      "        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_h = exp(b_g) * b_h + b_x",
      "        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)",
      "",
      "        p_x += D",
      "        p_g += D",
      "        p_o += D",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_n * D + o_d",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/hgrn/257.py"
  },
  {
    "name": "fused_recurrent_hgrn_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['D'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_hgrn_bwd_kernel(",
      "    g,",
      "    o,",
      "    h0,",
      "    dx,",
      "    dg,",
      "    do,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_n = tl.program_id(0), tl.program_id(1)",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_g = g + (bos + T - 1) * D + o_d",
      "    p_o = o + (bos + T - 2) * D + o_d",
      "    p_dx = dx + (bos + T - 1) * D + o_d",
      "    p_dg = dg + (bos + T - 1) * D + o_d",
      "    p_do = do + (bos + T - 1) * D + o_d",
      "",
      "    b_dh = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = dht + i_n * D + o_d",
      "        b_dh += tl.load(p_dht, mask=mask, other=0).to(tl.float32)",
      "",
      "    for i in range(T - 1, -1, -1):",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)",
      "        if i > 0:",
      "            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)",
      "        elif USE_INITIAL_STATE:",
      "            b_o = tl.load(h0 + i_n * D + o_d, mask=mask, other=0).to(tl.float32)",
      "        else:",
      "            b_o = tl.zeros([BD], dtype=tl.float32)",
      "",
      "        b_dh = b_dh + b_do",
      "        b_dx = b_dh",
      "        b_dh = b_dh * exp(b_g)",
      "        b_dg = b_dh * b_o",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)",
      "",
      "        p_g -= D",
      "        p_o -= D",
      "        p_dx -= D",
      "        p_dg -= D",
      "        p_do -= D",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = dh0 + i_n * D + o_d",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/hgrn/257.py"
  },
  {
    "name": "fused_chunk_linear_attn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [4] for num_stages in [1]], key=['B', 'H', 'K', 'V', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_linear_attn_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    h0,",
      "    ht,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    o_i = tl.arange(0, BT)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(0, tl.cdiv(T, BT)):",
      "        p_q = tl.make_block_ptr(",
      "            q + (i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (i_b * T * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (i_k * B * T * H + i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = tl.where(m_s, b_s, 0)",
      "",
      "        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "        if CHECK and i_t == 0:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)",
      "        else:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_k, b_v, allow_tf32=False)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/linear_attn/258.py"
  },
  {
    "name": "fused_chunk_linear_attn_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [4] for num_stages in [1]], key=['B', 'H', 'K', 'V', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_linear_attn_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    o_i = tl.arange(0, BT)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(0, tl.cdiv(T, BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + (i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (i_b * T * H + i_h) * V,",
      "            (V, T),",
      "            (1, H * V),",
      "            (i_v * BV, i_t * BT),",
      "            (BV, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (i_v * B * T * H + i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        b_ds = tl.where(m_s, b_ds, 0)",
      "",
      "        b_dq = tl.dot(b_ds.to(b_k.dtype), b_k, allow_tf32=False)",
      "",
      "        if CHECK and i_t == 0:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)",
      "        else:",
      "            b_dq += tl.dot(b_do, b_h.to(b_do.dtype), allow_tf32=False)",
      "            b_h = b_h + tl.dot(b_v, b_k, allow_tf32=False)",
      "        b_dq *= scale",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_h = None",
      "    tl.debug_barrier()",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    m_s = o_i[:, None] <= o_i[None, :]",
      "    for i_t in range(1, tl.cdiv(T, BT) + 1):",
      "        p_q = tl.make_block_ptr(",
      "            q + (i_b * T * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, T - i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (T - i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (T - i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (T - i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (i_v * B * T * H + i_b * T * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (T - i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * B * T * H + i_b * T * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (T - i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_k, b_q, allow_tf32=False)",
      "        b_s = tl.where(m_s, b_s, 0).to(b_q.dtype)",
      "",
      "        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)",
      "        b_ds = tl.where(m_s, b_ds, 0).to(b_q.dtype)",
      "",
      "        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)",
      "",
      "        b_dv = tl.dot(b_s, b_do, allow_tf32=False)",
      "        if CHECK and i_t == 1:",
      "            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)",
      "            b_dh += tl.dot(b_q, b_do, allow_tf32=False)",
      "        else:",
      "            b_dk += tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False)",
      "            b_dh += tl.dot(b_q, b_do, allow_tf32=False)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/linear_attn/258.py"
  },
  {
    "name": "fused_recurrent_linear_attn_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_linear_attn_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    h0,",
      "    ht,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)",
      "    p_o = o + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    mask_bk = (i_k * BK + tl.arange(0, BK)) < K",
      "    mask_bv = (i_v * BV + tl.arange(0, BV)) < V",
      "    mask_kv = mask_bk[None, :] & mask_bv[:, None]",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = (",
      "            h0",
      "            + i_bh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale",
      "",
      "        b_h += b_k[None, :] * b_v[:, None]",
      "        b_o = b_h * b_q[None, :]",
      "        b_o = tl.sum(b_o, axis=1)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_bv)",
      "",
      "        p_q += K",
      "        p_k += K",
      "        p_o += V",
      "        p_v += V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = (",
      "            ht",
      "            + i_bh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[None, :]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[:, None])",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_kv)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/linear_attn/259.py"
  },
  {
    "name": "fused_recurrent_linear_attn_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_linear_attn_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK)",
      "    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV)",
      "    p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV)",
      "",
      "    p_dq = dq + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK)",
      "    mask_bk = i_k * BK + tl.arange(0, BK) < K",
      "    mask_bv = i_v * BV + tl.arange(0, BV) < V",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        mask_kv = mask_bk[:, None] & mask_bv[None, :]",
      "        p_h0 = (",
      "            h0",
      "            + i_bh * K * V",
      "            + (i_k * BK + tl.arange(0, BK)[:, None]) * V",
      "            + (i_v * BV + tl.arange(0, BV)[None, :])",
      "        )",
      "        b_h += tl.load(p_h0, mask=mask_kv, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)",
      "",
      "        b_h += b_k[:, None] * b_v[None, :]",
      "        _d_q = b_h * b_do[None, :]",
      "        d_q = tl.sum(_d_q, axis=1) * scale",
      "        tl.store(p_dq, d_q.to(p_dq.dtype.element_ty), mask=mask_bk)",
      "",
      "        p_k += K",
      "        p_do += V",
      "        p_v += V",
      "        p_dq += K",
      "",
      "    tl.debug_barrier()",
      "",
      "    p_q = q + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K",
      "    p_k = k + i_bh * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K",
      "    p_do = do + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V",
      "    p_v = v + i_bh * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V",
      "    p_dk = dk + (i_bh + i_v * B * H) * T * K + i_k * BK + tl.arange(0, BK) + (T - 1) * K",
      "    p_dv = dv + (i_bh + i_k * B * H) * T * V + i_v * BV + tl.arange(0, BV) + (T - 1) * V",
      "    d_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for _ in range(T):",
      "        b_do = tl.load(p_do, mask=mask_bv, other=0).to(tl.float32)",
      "        b_q = tl.load(p_q, mask=mask_bk, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_bk, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_bv, other=0).to(tl.float32)",
      "        d_h += b_q[:, None] * b_do[None, :]",
      "        d_k = tl.sum(d_h * b_v[None, :], axis=1)",
      "        d_v = tl.sum(d_h * b_k[:, None], axis=0)",
      "",
      "        tl.store(p_dk, d_k.to(p_dk.dtype.element_ty), mask=mask_bk)",
      "        tl.store(p_dv, d_v.to(p_dv.dtype.element_ty), mask=mask_bv)",
      "",
      "        p_do -= V",
      "        p_q -= K",
      "        p_k -= K",
      "        p_v -= V",
      "        p_dk -= K",
      "        p_dv -= V"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/linear_attn/259.py"
  },
  {
    "name": "chunk_update_once",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "b_p",
        "annotation": null
      },
      {
        "name": "b_k",
        "annotation": null
      },
      {
        "name": "b_v",
        "annotation": null
      },
      {
        "name": "b_m",
        "annotation": null
      },
      {
        "name": "b_g_exp_q",
        "annotation": null
      },
      {
        "name": "b_h",
        "annotation": null
      },
      {
        "name": "b_lamb",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_update_once(b_p, b_k, b_v, b_m, b_g_exp_q, b_h, b_lamb):",
      "    b_o = tl.dot((tl.dot(b_p.to(b_k.dtype), tl.trans(b_k)) * b_m).to(b_v.dtype), b_v)",
      "    b_o += tl.dot((b_p * b_g_exp_q).to(b_h.dtype), b_h)",
      "    if b_lamb is not None:",
      "        b_o += b_lamb[None, :] * b_p",
      "    return b_o"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/mesa_net/260.py"
  },
  {
    "name": "chunk_mesa_net_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h_init'] is not None, 'STORE_FINAL_STATE': lambda args: args['h_final'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h_kv",
        "annotation": null
      },
      {
        "name": "h_init",
        "annotation": null
      },
      {
        "name": "h_kv_init",
        "annotation": null
      },
      {
        "name": "h_final",
        "annotation": null
      },
      {
        "name": "h_kv_final",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_mesa_net_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    beta,",
      "    g,",
      "    h,",
      "    h_kv,",
      "    h_init,",
      "    h_kv_init,",
      "    h_final,",
      "    h_kv_final,",
      "    cu_seqlens,",
      "    split_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        NS = tl.cdiv(T, BS)",
      "        boh = i_n * NS",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    b_h_kv = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h_init + i_nh * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "        p_h_kv0 = tl.make_block_ptr(",
      "            h_kv_init + i_nh * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h_kv = tl.load(p_h_kv0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        i_s = i_t // (BS // BT)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k2 = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_beta = tl.make_block_ptr(",
      "            beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_beta = tl.load(p_beta, boundary_check=(0,))",
      "",
      "        o_h = ((boh + i_s) * H + i_h).to(tl.int64) * K * V",
      "        p_h = tl.make_block_ptr(",
      "            h + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_h_kv = tl.make_block_ptr(",
      "            h_kv + o_h, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "",
      "        if i_t % (BS // BT) == 0:",
      "            tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "            tl.store(p_h_kv, b_h_kv.to(p_h_kv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_k2 = tl.load(p_k2, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        last_idx = min((i_t + 1) * BT, T) - 1",
      "",
      "        b_g_last = tl.load(g + bos * H + last_idx * H + i_h)",
      "        p_g = g + bos * H + (i_t * BT + tl.arange(0, BT)) * H + i_h",
      "        b_h *= exp(b_g_last)",
      "        b_h_kv *= exp(b_g_last)",
      "        b_g = tl.load(p_g, mask=(i_t * BT + tl.arange(0, BT) < T), other=0.0)",
      "        b_k_decay = ((b_k * exp(b_g_last - b_g)[:, None]) * b_beta[:, None]).to(",
      "            b_k.dtype",
      "        )",
      "        b_h += tl.dot(tl.trans(b_k_decay), b_k2)",
      "        b_h_kv += tl.dot(tl.trans(b_k_decay), b_v)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            h_final + i_nh * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        p_h_kv_final = tl.make_block_ptr(",
      "            h_kv_final + i_nh * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(",
      "            p_h_kv_final,",
      "            b_h_kv.to(p_h_kv_final.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/mesa_net/261.py"
  },
  {
    "name": "chunk_mesa_net_h_kk_bwd_intra_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "q_star",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "dk_beta",
        "annotation": null
      },
      {
        "name": "dlamb",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_mesa_net_h_kk_bwd_intra_kernel(",
      "    k,",
      "    beta,",
      "    h,",
      "    dh,",
      "    g,",
      "    q_star,",
      "    dq,",
      "    dk,",
      "    dg,",
      "    dbeta,",
      "    dk_beta,",
      "    dlamb,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q_star += (bos * H + i_h) * V",
      "    dq += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    dh += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    k += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    dk_beta += (bos * H + i_h) * K",
      "    dlamb += (i_tg * H + i_h).to(tl.int64) * K",
      "    beta += bos * H + i_h",
      "    dbeta += bos * H + i_h",
      "    g += bos * H + i_h",
      "    dg += bos * H + i_h",
      "",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dv = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dbeta = tl.zeros(",
      "        [",
      "            BT,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    b_dg_last = tl.zeros(",
      "        [",
      "            1,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    b_dg = tl.zeros(",
      "        [",
      "            BT,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_g = tl.load(p_g, boundary_check=(0,))",
      "    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)",
      "    b_gk = safe_exp(b_g_last - b_g)",
      "",
      "    p_q_star = tl.make_block_ptr(",
      "        q_star, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_q_star = tl.load(p_q_star, boundary_check=(0, 1))",
      "    p_dq = tl.make_block_ptr(dq, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_dq = tl.load(p_dq, boundary_check=(0, 1))",
      "    b_dlamb = -tl.sum(b_q_star * b_dq, axis=0)",
      "    p_dlamb = tl.make_block_ptr(dlamb, (K,), (1,), (0,), (BK,), (0,))",
      "    tl.store(p_dlamb, b_dlamb.to(p_dlamb.dtype.element_ty), boundary_check=(0,))",
      "",
      "    p_h = tl.make_block_ptr(h, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))",
      "    p_dh = tl.make_block_ptr(dh, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))",
      "    b_h = tl.load(p_h, boundary_check=(0, 1))",
      "    b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_v = tl.load(p_k, boundary_check=(0, 1))",
      "    b_k = b_v * b_beta[:, None]",
      "",
      "    b_dg_last = tl.sum(b_h * b_dh)",
      "    b_dg_last *= exp(b_g_last)",
      "    o_t = tl.arange(0, BT)",
      "    b_m = tl.where(",
      "        o_t[:, None] >= o_t[None, :], safe_exp(b_g[:, None] - b_g[None, :]), 0",
      "    )",
      "    b_s = tl.dot(b_q_star, tl.trans(b_k)) * b_m",
      "    b_ds = tl.dot(b_dq, tl.trans(b_v))",
      "    b_dm = b_s * b_ds",
      "    b_dm = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dm, 0)",
      "    b_dg += tl.sum(b_dm, axis=1)",
      "    b_dg -= tl.sum(b_dm, axis=0)",
      "    b_ds = b_ds * b_m",
      "    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype)) * b_gk[:, None]",
      "",
      "    b_dg_last += tl.sum(b_dk * b_k)",
      "    b_dg -= tl.sum(b_dk * b_k, axis=1)",
      "    b_dg += tl.sum(",
      "        tl.dot(b_dq, tl.trans(b_h)) * tl.exp(b_g)[:, None] * b_q_star, axis=1",
      "    )",
      "    b_dv += tl.dot(b_k, tl.trans(b_dh).to(b_k.dtype)) * b_gk[:, None] + tl.dot(",
      "        tl.trans(b_s.to(b_dq.dtype)), b_dq",
      "    )",
      "    b_dk += tl.dot(tl.trans(b_ds.to(b_q_star.dtype)), b_q_star)",
      "",
      "    p_dk_beta = tl.make_block_ptr(",
      "        dk_beta, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_dk -= tl.load(p_dk_beta, boundary_check=(0, 1))",
      "    b_dbeta = tl.sum(b_dk * b_v, axis=1)",
      "    b_dk = b_dk * b_beta[:, None] + b_dv",
      "    b_dk = -b_dk",
      "",
      "    b_dg = tl.where(o_t < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)",
      "    p_dk = tl.make_block_ptr(dk, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_dg, -b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "    p_dbeta = tl.make_block_ptr(dbeta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_dbeta, -b_dbeta.to(p_dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/mesa_net/262.py"
  },
  {
    "name": "chunk_mesa_net_h_kv_bwd_intra_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in NUM_WARPS for num_stages in [2, 3, 4]], key=['H', 'K', 'V', 'BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q_star",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "h_kv",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh_kv",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk_beta",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_mesa_net_h_kv_bwd_intra_kernel(",
      "    q_star,",
      "    k,",
      "    v,",
      "    beta,",
      "    h_kv,",
      "    g,",
      "    do,",
      "    dh_kv,",
      "    dq,",
      "    dk_beta,",
      "    dg,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h_kv += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    dh_kv += (i_tg * H + i_h).to(tl.int64) * K * V",
      "    q_star += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    beta += bos * H + i_h",
      "    g += bos * H + i_h",
      "    dg += bos * H + i_h",
      "    dq += (bos * H + i_h) * K",
      "    dk_beta += (bos * H + i_h) * K",
      "    dv += (bos * H + i_h) * V",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dv = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dg_last = tl.zeros(",
      "        [",
      "            1,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    b_dg = tl.zeros(",
      "        [",
      "            BT,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    p_do = tl.make_block_ptr(do, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    p_h = tl.make_block_ptr(h_kv, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))",
      "    p_dh = tl.make_block_ptr(dh_kv, (V, K), (1, V), (0, 0), (BV, BK), (0, 1))",
      "    p_q = tl.make_block_ptr(q_star, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_g = tl.make_block_ptr(g, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_g = tl.load(p_g, boundary_check=(0,))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_h = tl.load(p_h, boundary_check=(0, 1))",
      "    b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "    b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * H)",
      "",
      "    b_dg_last += tl.sum(b_h * b_dh)",
      "    b_dg_last *= exp(b_g_last)",
      "    o_t = tl.arange(0, BT)",
      "    b_m = tl.where(",
      "        o_t[:, None] >= o_t[None, :], safe_exp(b_g[:, None] - b_g[None, :]), 0",
      "    )",
      "    b_k = b_k * b_beta[:, None]",
      "    b_s = tl.dot(b_q, tl.trans(b_k)) * b_m",
      "",
      "    b_ds = tl.dot(b_do, tl.trans(b_v))",
      "    b_dm = b_s * b_ds",
      "    b_dm = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dm, 0)",
      "",
      "    b_dg += tl.sum(b_dm, axis=1)",
      "    b_dg -= tl.sum(b_dm, axis=0)",
      "",
      "    b_g_exp_q = exp(b_g)",
      "    b_g_exp_k = safe_exp(-b_g + b_g_last)",
      "    b_ds = b_ds * b_m",
      "    b_dq += tl.dot(b_do, b_h.to(b_do.dtype)) * b_g_exp_q[:, None]",
      "    b_dk += tl.dot(b_v, b_dh.to(b_v.dtype)) * b_g_exp_k[:, None]",
      "    b_dg_last += tl.sum(b_dk * b_k)",
      "    b_dg -= tl.sum(b_dk * b_k, axis=1)",
      "    b_dg += tl.sum(b_dq * b_q, axis=1)",
      "    b_dq += tl.dot(b_ds.to(b_k.dtype), b_k)",
      "    b_dv += tl.dot(b_k, tl.trans(b_dh).to(b_k.dtype)) * b_g_exp_k[:, None] + tl.dot(",
      "        tl.trans(b_s.to(b_do.dtype)), b_do",
      "    )",
      "    b_dk += tl.dot(tl.trans(b_ds.to(b_q.dtype)), b_q)",
      "",
      "    b_dg = tl.where(o_t < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)",
      "    p_dq = tl.make_block_ptr(dq, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_dk = tl.make_block_ptr(",
      "        dk_beta, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(dv, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    p_dg = tl.make_block_ptr(dg, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/mesa_net/263.py"
  },
  {
    "name": "parallel_nsa_compression_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    cu_seqlens,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (TC, V),",
      "            (H * V, 1),",
      "            (i_c, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = exp(b_mp - b_m)",
      "",
      "        b_p = exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "        b_mp = b_m",
      "    if NC == 0:",
      "        b_lse = tl.zeros([G], dtype=tl.float32)",
      "    else:",
      "        b_o = b_o / b_acc[:, None]",
      "        b_lse = b_m + log(b_acc)",
      "",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    if i_v == 0:",
      "        tl.store(",
      "            lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G),",
      "            b_lse.to(lse.dtype.element_ty),",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/265.py"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    cu_seqlens,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (V, TC),",
      "            (1, H * V),",
      "            (i_v * BV, i_c),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_p = exp(b_s - b_lse[:, None])",
      "        b_p = tl.where((o_c < NC)[None, :], b_p, 0)",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/265.py"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_c = tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_c * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + boc * H + i_h) * K,",
      "        (TC, K),",
      "        (H * K, 1),",
      "        (i_c * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (i_v * B * T * H + boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_c * BC * BS, T):",
      "        o_c = i_c * BC + tl.arange(0, BC)",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos + i) * HQ * V,",
      "            (HQ, V),",
      "            (V, 1),",
      "            (i_h * G, i_v * BV),",
      "            (G, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse)",
      "        b_delta = tl.load(p_delta)",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        b_p = exp(b_s - b_lse[None, :])",
      "        b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/265.py"
  },
  {
    "name": "parallel_nsa_kernel_topk",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_topk(",
      "    q,",
      "    k,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    cu_seqlens,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    if lse is not None:",
      "        b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))",
      "    else:",
      "",
      "        b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "        b_acc = tl.zeros([G], dtype=tl.float32)",
      "        for i_c in range(0, NC, BC):",
      "            o_c = i_c + tl.arange(0, BC)",
      "",
      "            p_k = tl.make_block_ptr(",
      "                k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = exp(b_mp - b_m)",
      "",
      "            b_p = exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_mp = b_m",
      "        if NC == 0:",
      "            b_lse = tl.zeros([G], dtype=tl.float32)",
      "        else:",
      "            b_lse = b_m + log(b_acc)",
      "",
      "    b_i = tl.full([BC], -1, dtype=tl.float32)",
      "    o_i = tl.zeros([BC], dtype=tl.int32)",
      "    m_i = tl.arange(0, BC) < BC // 2",
      "    for i_c in range(0, i_t // BS + 1, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_p = tl.where(",
      "            (i_t // BS == o_c)[None, :], float(1.0), exp(b_s - b_lse[:, None])",
      "        )",
      "",
      "        b_i, b_ip = tl.sum(b_p, 0), b_i",
      "        o_i, o_ip = tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i",
      "",
      "        n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])",
      "        for i in tl.static_range(1, n_dims):",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)",
      "",
      "        if i_c != 0:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)",
      "            b_i_new = b_ip * m_i + b_i * (1 - m_i)",
      "            o_i_new = o_ip * m_i + o_i * (1 - m_i)",
      "            b_i, o_i = _bitonic_merge(",
      "                b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims",
      "            )",
      "        else:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)",
      "",
      "    m_top = tl.arange(0, BC // S) == 0",
      "    b_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)",
      "",
      "    p_b = tl.make_block_ptr(",
      "        block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,)",
      "    )",
      "    tl.store(p_b, b_top.to(p_b.dtype.element_ty))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/266.py"
  },
  {
    "name": "parallel_nsa_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    cu_seqlens,",
      "    token_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "    p_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where(",
      "                (i_t >= (i_s + tl.arange(0, BS)))[None, :], b_s, float(\"-inf\")",
      "            )",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = exp(b_mp - b_m)",
      "",
      "            b_p = exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "            b_mp = b_m",
      "    b_o = b_o / b_acc[:, None]",
      "    b_m += log(b_acc)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/266.py"
  },
  {
    "name": "parallel_nsa_kernel_mask",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_mask(",
      "    block_indices,",
      "    block_counts,",
      "    block_mask,",
      "    T,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_b, i_hs = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h, i_s = i_hs // S, i_hs % S",
      "",
      "    b_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)",
      "    if USE_BLOCK_COUNTS:",
      "        b_m = b_i * BS <= i_t and i_s < tl.load(",
      "            block_counts + i_b * T * H + i_t * H + i_h",
      "        )",
      "    else:",
      "        b_m = b_i * BS <= i_t",
      "",
      "    if b_i < NS and b_i >= 0:",
      "        tl.store(",
      "            block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i,",
      "            b_m.to(block_mask.dtype.element_ty),",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/266.py"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    cu_seqlens,",
      "    token_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_p = exp(b_s - b_lse[:, None])",
      "            b_p = tl.where((i_t >= (i_s + tl.arange(0, BS)))[None, :], b_p, 0)",
      "",
      "            b_dp = tl.dot(b_do, b_v)",
      "            b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "            b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/266.py"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    block_mask,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    M: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_s, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_s * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_s * BS, 0),",
      "        (BS, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BS, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BS, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_s * BS, T):",
      "        b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)",
      "        if b_m:",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "            p_do = tl.make_block_ptr(",
      "                do + (bos + i) * HQ * V,",
      "                (HQ, V),",
      "                (V, 1),",
      "                (i_h * G, i_v * BV),",
      "                (G, BV),",
      "                (1, 0),",
      "            )",
      "            p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "            p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "            b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "            b_lse = tl.load(p_lse)",
      "            b_delta = tl.load(p_delta)",
      "",
      "            b_s = tl.dot(b_k, tl.trans(b_q))",
      "            b_p = exp(b_s - b_lse[None, :])",
      "            b_p = tl.where((i >= (i_s * BS + tl.arange(0, BS)))[:, None], b_p, 0)",
      "",
      "            b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "            b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "            b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "            b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/nsa/266.py"
  },
  {
    "name": "chunk_cumprod_householder_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hc_suffix",
        "annotation": null
      },
      {
        "name": "dhc_whole",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dk_new",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_cumprod_householder_bwd_kernel(",
      "    h,",
      "    hc_suffix,",
      "    dhc_whole,",
      "    dh,",
      "    k,",
      "    dk,",
      "    dk_new,",
      "    cu_seqlens,",
      "    split_indices,",
      "    chunk_offsets,",
      "    split_offsets,",
      "    BT: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    G: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_ss, i_hq = tl.program_id(0), tl.program_id(1)",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh = i_n * tl.cdiv(T, BT)",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    h += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    k += (bos * H + i_h) * K",
      "",
      "    dh += ((boh + tl.cdiv(i_s * S, BT)) * HQ + i_hq) * K * K",
      "    dhc_whole += ((boh_large + i_s) * HQ + i_hq) * K * K",
      "    dk += (bos * HQ + i_hq) * K",
      "    dk_new += (bos * HQ + i_hq) * K",
      "",
      "    stride_hq = HQ * K * K",
      "    stride_h = H * K * K",
      "    NT_small = tl.cdiv(min(S, T - i_s * S), BT)",
      "    p_dhc_whole = tl.make_block_ptr(dhc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "    b_dhc = tl.zeros([BK, BK], dtype=tl.float32)",
      "    b_dhc += tl.load(p_dhc_whole, boundary_check=(0, 1))",
      "",
      "    for i_t_small in range(0, NT_small):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_dk_new = tl.make_block_ptr(",
      "            dk_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_hc = tl.make_block_ptr(",
      "            hc_suffix + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_t_small * stride_hq, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_hc = tl.load(p_hc, boundary_check=(0, 1))",
      "        b_dk_new = b_dk - tl.dot(b_dk.to(b_hc.dtype), b_hc)",
      "        tl.store(p_dk_new, b_dk_new.to(dk.dtype.element_ty), boundary_check=(0, 1))",
      "        b_dh = b_dhc - tl.dot(tl.trans(b_hc), b_dhc.to(b_hc.dtype))",
      "        tl.store(p_dh, b_dh.to(dh.dtype.element_ty), boundary_check=(0, 1))",
      "        b_dhc = b_dhc - tl.dot(b_dhc.to(b_h.dtype), tl.trans(b_h))",
      "        b_dhc -= tl.dot(tl.trans(b_dk).to(b_k.dtype), b_k)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/268.py"
  },
  {
    "name": "chunk_cumprod_householder_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hc_suffix",
        "annotation": null
      },
      {
        "name": "hc_prefix",
        "annotation": null
      },
      {
        "name": "hc_whole",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "split_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_cumprod_householder_fwd_kernel(",
      "    q,",
      "    q_new,",
      "    k,",
      "    k_new,",
      "    h,",
      "    hc_suffix,",
      "    hc_prefix,",
      "    hc_whole,",
      "    cu_seqlens,",
      "    split_indices,",
      "    chunk_offsets,",
      "    split_offsets,",
      "    BT: tl.constexpr,",
      "    K: tl.constexpr,",
      "    G: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_ss, i_hq = tl.program_id(0), tl.program_id(1)",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_s = tl.load(split_indices + i_ss * 2).to(tl.int32), tl.load(",
      "            split_indices + i_ss * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NS = tl.cdiv(T, S)",
      "",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "    else:",
      "        NS = tl.cdiv(T, S)",
      "        i_n, i_s = i_ss // NS, i_ss % NS",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "        boh = i_n * tl.cdiv(T, BT)",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    NT_small = tl.cdiv(min(S, T - i_s * S), BT)",
      "    stride_h = H * K * K",
      "",
      "    h += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_suffix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_prefix += ((boh + tl.cdiv(i_s * S, BT)) * H + i_h) * K * K",
      "    hc_whole += ((boh_large + i_s) * H + i_h) * K * K",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    q_new += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "",
      "    p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "    b_h = tl.zeros([BK, BK], dtype=tl.float32)",
      "    b_h += tl.load(p_h, boundary_check=(0, 1))",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new, (T, K), (HQ * K, 1), (i_s * S, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_hc_prefix = tl.make_block_ptr(hc_prefix, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "    tl.store(",
      "        p_hc_prefix,",
      "        tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_prefix.dtype.element_ty),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    for i_t_small in range(1, NT_small):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q - tl.dot(b_q, b_h.to(b_q.dtype))).to(b_q.dtype)",
      "        p_q_new = tl.make_block_ptr(",
      "            q_new, (T, K), (HQ * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        tl.store(p_q_new, b_q.to(q_new.dtype.element_ty), boundary_check=(0, 1))",
      "        if HQ % G == 0:",
      "            p_hc_prefix = tl.make_block_ptr(",
      "                hc_prefix + i_t_small * stride_h,",
      "                (K, K),",
      "                (K, 1),",
      "                (0, 0),",
      "                (BK, BK),",
      "                (1, 0),",
      "            )",
      "            tl.store(",
      "                p_hc_prefix, b_h.to(hc_prefix.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "        p_h_new = tl.make_block_ptr(",
      "            h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_h_new = tl.load(p_h_new, boundary_check=(0, 1))",
      "        b_h = b_h + b_h_new - tl.dot(b_h_new, b_h.to(b_h_new.dtype))",
      "",
      "    tl.debug_barrier()",
      "",
      "    if HQ % G == 0:",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (H * K, 1), (i_s * S + (NT_small - 1) * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        p_k_new = tl.make_block_ptr(",
      "            k_new,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_s * S + (NT_small - 1) * BT, 0),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))",
      "        p_hc_suffix = tl.make_block_ptr(",
      "            hc_suffix + (NT_small - 1) * stride_h,",
      "            (K, K),",
      "            (K, 1),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (1, 0),",
      "        )",
      "        tl.store(",
      "            p_hc_suffix,",
      "            tl.zeros([BK, BK], dtype=tl.float32).to(p_hc_suffix.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "        p_h = tl.make_block_ptr(",
      "            h + (NT_small - 1) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_h = tl.zeros([BK, BK], dtype=tl.float32)",
      "        b_h += tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        for i_t_small in range(NT_small - 2, -1, -1):",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (H * K, 1), (i_s * S + i_t_small * BT, 0), (BT, BK), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_k = (b_k - tl.dot(b_k, tl.trans(b_h).to(b_k.dtype))).to(b_k.dtype)",
      "            p_k_new = tl.make_block_ptr(",
      "                k_new,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_s * S + i_t_small * BT, 0),",
      "                (BT, BK),",
      "                (1, 0),",
      "            )",
      "            tl.store(p_k_new, b_k.to(k_new.dtype.element_ty), boundary_check=(0, 1))",
      "            p_hc_suffix = tl.make_block_ptr(",
      "                hc_suffix + i_t_small * stride_h,",
      "                (K, K),",
      "                (K, 1),",
      "                (0, 0),",
      "                (BK, BK),",
      "                (1, 0),",
      "            )",
      "            tl.store(",
      "                p_hc_suffix, b_h.to(hc_suffix.dtype.element_ty), boundary_check=(0, 1)",
      "            )",
      "            p_h_new = tl.make_block_ptr(",
      "                h + i_t_small * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "            )",
      "            b_h_new = tl.load(p_h_new, boundary_check=(0, 1))",
      "            b_h = b_h + b_h_new - tl.dot(b_h.to(b_h_new.dtype), b_h_new)",
      "",
      "        p_hc_whole = tl.make_block_ptr(",
      "            hc_whole, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        tl.store(p_hc_whole, b_h.to(hc_whole.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/269.py"
  },
  {
    "name": "intra_chunk_preprocess_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "AT",
        "annotation": null
      },
      {
        "name": "dA_local",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dq_new",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dk_new",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "dbeta",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def intra_chunk_preprocess_bwd_kernel(",
      "    q,",
      "    k,",
      "    w,",
      "    beta,",
      "    AT,",
      "    dA_local,",
      "    dq,",
      "    dq_new,",
      "    dk,",
      "    dk_new,",
      "    dw,",
      "    dbeta,",
      "    dh,",
      "    T,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dw_beta = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dw = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dT = tl.zeros([BT, BT], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_w = tl.make_block_ptr(",
      "        w + (bos * H + i_h) * K, (T, K), (K * H, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    p_beta = tl.make_block_ptr(",
      "        beta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    p_T = tl.make_block_ptr(",
      "        AT + (bos * H + i_h) * BT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_w = tl.load(p_w, boundary_check=(0, 1))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_k.dtype)",
      "    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    b_qw = tl.where(o_i[:, None] >= o_i[None, :], tl.dot(b_q, tl.trans(b_w)), 0).to(",
      "        b_q.dtype",
      "    )",
      "    b_wbk = tl.where(",
      "        o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, tl.trans(b_k)), 0",
      "    ).to(b_k.dtype)",
      "    b_Twb = tl.dot(b_T, b_w_beta).to(b_w.dtype)",
      "    b_Twbk = tl.dot(b_T, b_wbk).to(b_w.dtype)",
      "",
      "    p_dA_local = tl.make_block_ptr(",
      "        dA_local + (bos * HQ + i_hq) * BT,",
      "        (T, BT),",
      "        (BT * HQ, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BT),",
      "        (1, 0),",
      "    )",
      "    b_dA_local = tl.load(p_dA_local, boundary_check=(0, 1))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(b_w.dtype)",
      "",
      "    p_dh = tl.make_block_ptr(",
      "        dh + ((boh + i_t) * HQ + i_hq) * K * K, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "    )",
      "    b_dh = tl.load(p_dh, boundary_check=(0, 1)).to(b_w.dtype)",
      "    b_dw += tl.dot(b_Twb, tl.trans(b_dh))",
      "    b_dqw = -tl.dot(b_dA_local, tl.trans(b_Twbk)) - tl.dot(",
      "        b_dq.to(b_Twb.dtype), tl.trans(b_Twb)",
      "    )",
      "    b_dTwb = (-tl.dot(tl.trans(b_qw), b_dq) + tl.dot(b_w, b_dh)).to(b_w.dtype)",
      "    b_dT += tl.dot(b_dTwb, tl.trans(b_w_beta))",
      "    b_dw_beta += tl.dot(tl.trans(b_T), b_dTwb)",
      "",
      "    b_dqw = tl.where(tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :], b_dqw, 0)",
      "    b_dq += tl.dot(b_dA_local.to(b_k.dtype), b_k)",
      "    b_dq += tl.dot(b_dqw.to(b_w.dtype), b_w)",
      "    b_dw += tl.dot(tl.trans(b_dqw.to(b_q.dtype)), b_q)",
      "    p_q_new = tl.make_block_ptr(",
      "        dq_new + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (K * HQ, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_q_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_dk = tl.load(p_dk, boundary_check=(0, 1))",
      "    b_dTwbk = -tl.dot(tl.trans(b_qw), b_dA_local.to(b_qw.dtype)) - tl.dot(",
      "        b_w, tl.trans(b_dk.to(b_w.dtype))",
      "    )",
      "    b_dw -= tl.dot(b_Twbk, b_dk.to(b_w.dtype))",
      "    b_dT += tl.dot(b_dTwbk.to(b_wbk.dtype), tl.trans(b_wbk))",
      "    b_dwbk = tl.where(",
      "        o_i[:, None] > o_i[None, :], tl.dot(tl.trans(b_T), b_dTwbk.to(b_T.dtype)), 0",
      "    ).to(b_w.dtype)",
      "    b_dw_beta += tl.dot(b_dwbk, b_k)",
      "",
      "    b_dk += tl.dot(tl.trans(b_dwbk), b_w_beta)",
      "    b_dk += tl.dot(tl.trans(b_dA_local), b_q)",
      "    p_dk_new = tl.make_block_ptr(",
      "        dk_new + (bos * HQ + i_hq) * K,",
      "        (T, K),",
      "        (K * HQ, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dk_new, b_dk.to(dk_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_T = tl.make_block_ptr(",
      "        AT + (bos * H + i_h) * BT, (BT, T), (1, BT * H), (0, i_t * BT), (BT, BT), (0, 1)",
      "    )",
      "    b_Tt = tl.load(p_T, boundary_check=(0, 1))",
      "    b_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], b_dT, 0).to(",
      "        b_w.dtype",
      "    )",
      "    b_dT = tl.dot(b_Tt, b_dT).to(b_w.dtype)",
      "    b_dT = tl.dot(b_dT, b_Tt)",
      "    b_dT = tl.where(tl.arange(0, BT)[:, None] > tl.arange(0, BT)[None, :], -b_dT, 0).to(",
      "        b_k.dtype",
      "    )",
      "",
      "    b_dw_beta += tl.dot(b_dT, b_w)",
      "    b_dw += tl.dot(tl.trans(b_dT), b_w_beta)",
      "    b_dw += b_dw_beta * b_beta[:, None]",
      "    b_dbeta = tl.sum(b_dw_beta * b_w, axis=1)",
      "",
      "    p_dw = tl.make_block_ptr(",
      "        dw + (bos * HQ + i_hq) * K, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dw, b_dw.to(dw.dtype.element_ty), boundary_check=(0, 1))",
      "    p_dbeta = tl.make_block_ptr(",
      "        dbeta + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dbeta, b_dbeta.to(dbeta.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/270.py"
  },
  {
    "name": "chunk_transform_qk_bwd_kernel_prepare",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_GATE': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "AT",
        "annotation": null
      },
      {
        "name": "dA_local",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_transform_qk_bwd_kernel_prepare(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    beta,",
      "    g_cumsum,",
      "    L,",
      "    D,",
      "    h,",
      "    q_new,",
      "    k_new,",
      "    AT,",
      "    dA_local,",
      "    dv,",
      "    do,",
      "    dg_cumsum,",
      "    scale,",
      "    indices,",
      "    offsets,",
      "    chunk_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    sm_scale = scale * 1.44269504",
      "",
      "    dA_local += (bos * HQ + i_hq) * BT",
      "    AT += (bos * H + i_h) * BT",
      "    q += (bos * HQ + i_hq) * K",
      "    q_new += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * HQ + i_hq) * V",
      "    dv += (bos * HQ + i_hq) * V",
      "    beta += bos * H + i_h",
      "    h += ((boh + i_t) * H + i_h) * K * K",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))",
      "    p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_kt = tl.load(p_k, boundary_check=(0, 1))",
      "    b_w = tl.load(p_w, boundary_check=(0, 1))",
      "    p_T = tl.make_block_ptr(AT, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))",
      "    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_t = o_i[:, None] >= o_i[None, :]",
      "    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)",
      "",
      "    b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)",
      "",
      "    b_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)",
      "    b_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)",
      "    b_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(",
      "        b_w.dtype",
      "    )",
      "    b_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)",
      "",
      "    b_q = b_q - tl.dot(b_qwT, b_w_beta)",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0)",
      "    )",
      "    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if i_hq % G == 0:",
      "        b_h = tl.dot(tl.trans(b_w), b_Twb)",
      "        p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)",
      "        p_k_new = tl.make_block_ptr(",
      "            k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        tl.store(",
      "            p_k_new,",
      "            (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "    if USE_GATE:",
      "        p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))",
      "        b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])",
      "        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\"))",
      "",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_l, boundary_check=(0,))",
      "    p_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    delta = tl.load(p_delta, boundary_check=(0,))",
      "",
      "    b_A_softmax = tl.exp2(",
      "        tl.where(",
      "            o_i[:, None] >= o_i[None, :], b_A * sm_scale - b_l[:, None], float(\"-inf\")",
      "        )",
      "    )",
      "    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "    b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "    p_dv = tl.make_block_ptr(dv, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    p_v = tl.make_block_ptr(v, (V, T), (1, H * V), (0, i_t * BT), (BV, BT), (0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dp = tl.dot(b_do, b_v)",
      "",
      "    b_dA = (b_dp - delta[:, None]) * b_A_softmax * scale",
      "    b_dgq = tl.sum(b_dA, axis=1) - tl.sum(b_dA, axis=0)",
      "    b_dA = b_dA.to(b_v.dtype)",
      "",
      "    if USE_GATE:",
      "        p_dg = tl.make_block_ptr(dg_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_dg, b_dgq.to(p_dg.dtype.element_ty), boundary_check=(0,))",
      "",
      "    p_dA = tl.make_block_ptr(",
      "        dA_local, (T, BT), (BT * HQ, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    tl.store(p_dA, b_dA.to(p_dA.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/271.py"
  },
  {
    "name": "intra_chunk_preprocess_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_G': lambda args: args['g_cumsum'] is not None, 'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "q_new",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def intra_chunk_preprocess_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    beta,",
      "    g_cumsum,",
      "    o,",
      "    A,",
      "    L,",
      "    M,",
      "    h,",
      "    q_new,",
      "    k_new,",
      "    scale,",
      "    indices,",
      "    offsets,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    sm_scale = scale * 1.44269504",
      "",
      "    A += (bos * H + i_h) * BT",
      "    q += (bos * HQ + i_hq) * K",
      "    q_new += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    w += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * HQ + i_hq) * V",
      "    beta += bos * H + i_h",
      "    h += ((boh + i_t) * H + i_h) * K * K",
      "    if USE_G:",
      "        g_cumsum += bos * HQ + i_hq",
      "    L += bos * HQ + i_hq",
      "    M += bos * HQ + i_hq",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1))",
      "    p_w = tl.make_block_ptr(w, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_v = tl.make_block_ptr(v, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_kt = tl.load(p_k, boundary_check=(0, 1))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_w = tl.load(p_w, boundary_check=(0, 1))",
      "    p_T = tl.make_block_ptr(A, (T, BT), (BT * H, 1), (i_t * BT, 0), (BT, BT), (1, 0))",
      "    b_T = tl.load(p_T, boundary_check=(0, 1)).to(b_q.dtype)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_t = o_i[:, None] >= o_i[None, :]",
      "    p_beta = tl.make_block_ptr(beta, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "    b_beta = tl.load(p_beta, boundary_check=(0,))",
      "    b_w_beta = (b_w * b_beta[:, None]).to(b_w.dtype)",
      "",
      "    b_qw = tl.where(m_t, tl.dot(b_q, tl.trans(b_w)), 0).to(b_q.dtype)",
      "    b_qwT = tl.dot(b_qw, b_T).to(b_q.dtype)",
      "    b_wbk = tl.where(o_i[:, None] > o_i[None, :], tl.dot(b_w_beta, b_kt), 0).to(",
      "        b_w.dtype",
      "    )",
      "    b_A = tl.where(m_t, tl.dot(b_q, b_kt) - tl.dot(b_qwT, b_wbk), 0)",
      "",
      "    b_q = b_q - tl.dot(b_qwT, b_w_beta)",
      "    p_q_new = tl.make_block_ptr(",
      "        q_new, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, K), (1, 0)",
      "    )",
      "    tl.store(p_q_new, b_q.to(p_q_new.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if i_hq % G == 0:",
      "        b_Twb = tl.dot(b_T.to(b_w_beta.dtype), b_w_beta).to(b_w_beta.dtype)",
      "        b_h = tl.dot(tl.trans(b_w), b_Twb)",
      "        p_h = tl.make_block_ptr(h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0))",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_T_wbk = tl.dot(b_T, b_wbk).to(b_w.dtype)",
      "        p_k_new = tl.make_block_ptr(",
      "            k_new, (K, T), (1, K * H), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        tl.store(",
      "            p_k_new,",
      "            (b_kt - tl.dot(tl.trans(b_w), b_T_wbk)).to(p_k_new.dtype.element_ty),",
      "            boundary_check=(0, 1),",
      "        )",
      "",
      "    if USE_G:",
      "        p_g_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        b_g_cumsum = tl.load(p_g_cumsum, boundary_check=(0,))",
      "        b_A = b_A + (b_g_cumsum[:, None] - b_g_cumsum[None, :])",
      "        b_A = tl.where((i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\"))",
      "",
      "    b_qkT_softmax = tl.where(",
      "        o_i[:, None] >= o_i[None, :], b_A * sm_scale, float(\"-inf\")",
      "    )",
      "    m_i = tl.max(b_qkT_softmax, 1)",
      "    b_qkT_softmax = tl.math.exp2(b_qkT_softmax - m_i[:, None])",
      "    l_i = tl.sum(b_qkT_softmax, 1)",
      "    b_o = tl.dot(b_qkT_softmax.to(b_v.dtype), b_v)",
      "    p_o = tl.make_block_ptr(o, (T, V), (V * HQ, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    p_m = tl.make_block_ptr(M, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    tl.store(p_m, m_i.to(p_m.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_l, l_i.to(p_l.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/272.py"
  },
  {
    "name": "parallel_path_bwd_dkv_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "hc_whole",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_bwd_dkv_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    hc_whole,",
      "    scale,",
      "    L,",
      "    D,",
      "    dk,",
      "    dv,",
      "    do,",
      "    dg_cumsum,",
      "    cu_seqlens,",
      "    indices,",
      "    split_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    do += (bos * HQ + i_hq) * V",
      "    dk += (bos * HQ + i_hq) * K",
      "    dv += (bos * HQ + i_hq) * K",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    hc_whole += (boh_large * H + i_h) * K * K",
      "",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "",
      "    stride_h = H * K * K",
      "    sm_scale = scale * 1.44269504",
      "",
      "    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_k_origin = tl.load(p_k, boundary_check=(0, 1))",
      "    p_v = tl.make_block_ptr(v, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "    if USE_GATE:",
      "        b_g_cumsum_k = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        p_g_cumsum_k = tl.make_block_ptr(",
      "            g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g_cumsum_k += tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "        b_dg_cumsum_k = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_g_cumsum_k = None",
      "        b_dg_cumsum_k = None",
      "",
      "    b_dk = tl.zeros([BT, K], dtype=tl.float32)",
      "    b_dv = tl.zeros([BT, K], dtype=tl.float32)",
      "    idx_i = (i_t * BT // S).to(tl.int32)",
      "",
      "    last_chunk_start = tl.floor(T / S).to(tl.int32) * S",
      "",
      "    if i_t * BT < last_chunk_start:",
      "",
      "        if T % S != 0:",
      "            idx_j = last_chunk_start // S",
      "            b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)",
      "            b_k_accum += b_k_origin",
      "            for i in range(idx_i + 1, idx_j):",
      "                p_h = tl.make_block_ptr(",
      "                    hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "                )",
      "                b_h = tl.load(p_h, boundary_check=(0, 1))",
      "                b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))",
      "            b_k = b_k_accum.to(b_k_origin.dtype)",
      "",
      "            for offset in range(",
      "                tl.ceil(T / BS).to(tl.int32) * BS - BS, last_chunk_start - BS, -BS",
      "            ):",
      "                p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                b_delta = tl.load(p_delta, boundary_check=(0,))",
      "                b_l = tl.load(p_l, boundary_check=(0,))",
      "                p_q = tl.make_block_ptr(",
      "                    q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0)",
      "                )",
      "                b_q = tl.load(p_q, boundary_check=(0, 1))",
      "                b_A = tl.dot(b_q, tl.trans(b_k))",
      "                if USE_GATE:",
      "                    p_g_cumsum_q = tl.make_block_ptr(",
      "                        g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)",
      "                    )",
      "                    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "                    b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "                    b_A = tl.where(",
      "                        (offset + tl.arange(0, BS) < T)[:, None], b_A, float(\"-inf\")",
      "                    )",
      "                b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "                p_do = tl.make_block_ptr(",
      "                    do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0)",
      "                )",
      "                b_do = tl.load(p_do, boundary_check=(0, 1))",
      "                b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "                b_dp = tl.dot(b_do, tl.trans(b_v))",
      "                b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "                if USE_GATE:",
      "                    b_dg_cumsum_k -= tl.sum(b_dA, axis=0)",
      "                b_dA = b_dA.to(b_v.dtype)",
      "                b_dk += tl.dot(tl.trans(b_dA), b_q)",
      "",
      "        for offset_outer in range(last_chunk_start, i_t * BT + S, -S):",
      "            idx_j = (offset_outer // S) - 1",
      "            b_k_accum = tl.zeros([BT, BK], dtype=tl.float32)",
      "            b_k_accum += b_k_origin",
      "            for i in range(idx_i + 1, idx_j):",
      "                p_h = tl.make_block_ptr(",
      "                    hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "                )",
      "                b_h = tl.load(p_h, boundary_check=(0, 1))",
      "                b_k_accum = b_k_accum - tl.dot(b_k_accum.to(b_h.dtype), tl.trans(b_h))",
      "            b_k = b_k_accum.to(b_k_origin.dtype)",
      "",
      "            p_h = tl.make_block_ptr(",
      "                hc_whole + (idx_j) * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "            )",
      "            b_h = tl.load(p_h, boundary_check=(0, 1))",
      "            b_dk = b_dk - tl.dot(b_dk.to(b_h.dtype), b_h)",
      "",
      "            for offset in range(offset_outer - BS, offset_outer - S - BS, -BS):",
      "                p_delta = tl.make_block_ptr(D, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                p_l = tl.make_block_ptr(L, (T,), (HQ,), (offset,), (BS,), (0,))",
      "                b_delta = tl.load(p_delta, boundary_check=(0,))",
      "                b_l = tl.load(p_l, boundary_check=(0,))",
      "                p_q = tl.make_block_ptr(",
      "                    q, (T, K), (HQ * K, 1), (offset, 0), (BS, BK), (1, 0)",
      "                )",
      "                b_q = tl.load(p_q, boundary_check=(0, 1))",
      "                b_A = tl.dot(b_q, tl.trans(b_k))",
      "                if USE_GATE:",
      "                    p_g_cumsum_q = tl.make_block_ptr(",
      "                        g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)",
      "                    )",
      "                    b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "                    b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "                    b_A = tl.where(",
      "                        (offset + tl.arange(0, BS) < T)[:, None], b_A, float(\"-inf\")",
      "                    )",
      "                b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "                p_do = tl.make_block_ptr(",
      "                    do, (T, V), (HQ * V, 1), (offset, 0), (BS, BV), (1, 0)",
      "                )",
      "                b_do = tl.load(p_do, boundary_check=(0, 1))",
      "                b_dv += tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "                b_dp = tl.dot(b_do, tl.trans(b_v))",
      "",
      "                b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "                if USE_GATE:",
      "                    b_dg_cumsum_k -= tl.sum(b_dA, axis=0)",
      "                b_dA = b_dA.to(b_v.dtype)",
      "                b_dk += tl.dot(tl.trans(b_dA), b_q)",
      "",
      "    p_dk = tl.make_block_ptr(dk, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    tl.store(p_dk, b_dk.to(dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.atomic_add(",
      "        dv + (i_t * BT + tl.arange(0, BT))[:, None] * HQ * K + tl.arange(0, K)[None, :],",
      "        b_dv,",
      "        sem=\"relaxed\",",
      "    )",
      "    if USE_GATE:",
      "        tl.atomic_add(",
      "            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_k, sem=\"relaxed\"",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/273.py"
  },
  {
    "name": "parallel_path_bwd_dq_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "hc_whole",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dhc_whole",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "split_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_bwd_dq_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    hc_whole,",
      "    scale,",
      "    L,",
      "    D,",
      "    dq,",
      "    do,",
      "    dhc_whole,",
      "    dg_cumsum,",
      "    cu_seqlens,",
      "    indices,",
      "    split_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh_large = tl.load(split_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh_large = i_n * tl.cdiv(T, S)",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    dq += (bos * HQ + i_hq) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * HQ + i_hq) * V",
      "    hc_whole += (boh_large * H + i_h) * K * K",
      "    dhc_whole += (boh_large * HQ + i_hq) * K * K",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "",
      "    stride_h = H * K * K",
      "    stride_hq = HQ * K * K",
      "    sm_scale = scale * 1.44269504",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_q_origin = tl.load(p_q, boundary_check=(0, 1))",
      "    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    p_d = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_l, boundary_check=(0,))",
      "    b_delta = tl.load(p_d, boundary_check=(0,))",
      "",
      "    if USE_GATE:",
      "        b_g_cumsum_q = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "        p_g_cumsum_q = tl.make_block_ptr(",
      "            g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g_cumsum_q += tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "        b_dg_cumsum_q = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_g_cumsum_q = None",
      "        b_dg_cumsum_q = None",
      "",
      "    idx_i = i_t * BT // S",
      "    curr_end = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)",
      "    b_dq = tl.zeros([BT, K], dtype=tl.float32)",
      "",
      "    for offset_outer in range(0, curr_end, S):",
      "        idx_j = offset_outer // S",
      "        b_q_accum = tl.zeros([BT, BK], dtype=tl.float32)",
      "        b_q_accum += b_q_origin",
      "        for i in range(idx_i - 1, idx_j, -1):",
      "            p_h = tl.make_block_ptr(",
      "                hc_whole + i * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "            )",
      "            b_h = tl.load(p_h, boundary_check=(0, 1))",
      "            b_q_accum = b_q_accum - tl.dot(b_q_accum.to(b_h.dtype), b_h)",
      "        b_q = b_q_accum.to(b_q_origin.dtype)",
      "        b_dh = -tl.dot(tl.trans(b_q), b_dq.to(b_q.dtype))",
      "",
      "        tl.atomic_add(",
      "            dhc_whole",
      "            + idx_j * stride_hq",
      "            + tl.arange(0, K)[:, None] * K",
      "            + tl.arange(0, K)[None, :],",
      "            b_dh,",
      "            sem=\"relaxed\",",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            hc_whole + idx_j * stride_h, (K, K), (K, 1), (0, 0), (BK, BK), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dq = b_dq - tl.dot(b_dq.to(b_h.dtype), tl.trans(b_h))",
      "",
      "        for offset in range(offset_outer, min(offset_outer + S, i_t * BT), BS):",
      "            p_k = tl.make_block_ptr(",
      "                k, (T, K), (H * K, 1), (offset, 0), (BS, BK), (1, 0)",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_A = tl.dot(b_q, tl.trans(b_k))",
      "            if USE_GATE:",
      "                p_g_cumsum_k = tl.make_block_ptr(",
      "                    g_cumsum, (T,), (HQ,), (offset,), (BS,), (0,)",
      "                )",
      "                b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "                b_A = b_A + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "                b_A = tl.where(",
      "                    (i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\")",
      "                )",
      "            b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "            p_v = tl.make_block_ptr(",
      "                v, (V, T), (1, V * H), (0, offset), (BK, BS), (0, 1)",
      "            )",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_dp = tl.dot(b_do, b_v)",
      "            b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "            b_dq += tl.dot(b_dA.to(b_k.dtype), b_k)",
      "            if USE_GATE:",
      "                b_dg_cumsum_q += tl.sum(b_dA, axis=1)",
      "",
      "    p_dq = tl.make_block_ptr(dq, (T, K), (K * HQ, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    tl.store(p_dq, b_dq.to(dq.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_GATE:",
      "        tl.atomic_add(",
      "            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dg_cumsum_q, sem=\"relaxed\"",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/274.py"
  },
  {
    "name": "parallel_path_bwd_intra_chunk_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dq_new",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dg_cumsum",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_bwd_intra_chunk_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g_cumsum,",
      "    h,",
      "    L,",
      "    D,",
      "    dq,",
      "    dq_new,",
      "    dk,",
      "    dv,",
      "    dh,",
      "    do,",
      "    dg_cumsum,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    T,",
      "    scale,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    S: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh = i_n * tl.cdiv(T, BT)",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    h += (boh * H + i_h) * K * K",
      "",
      "    q += (bos * HQ + i_hq) * K",
      "    dq += (bos * HQ + i_hq) * K",
      "    dq_new += (bos * HQ + i_hq) * K",
      "    dk += (bos * HQ + i_hq) * K",
      "    dv += (bos * HQ + i_hq) * V",
      "    do += (bos * HQ + i_hq) * V",
      "    dh += (boh * HQ + i_hq) * K * K",
      "    L += bos * HQ + i_hq",
      "    D += bos * HQ + i_hq",
      "    if USE_GATE:",
      "        g_cumsum += bos * HQ + i_hq",
      "        dg_cumsum += bos * HQ + i_hq",
      "",
      "    sm_scale = scale * 1.44269504",
      "",
      "    p_do = tl.make_block_ptr(do, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0))",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    p_delta = tl.make_block_ptr(D, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_delta = tl.load(p_delta, boundary_check=(0,))",
      "    p_l = tl.make_block_ptr(L, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_l, boundary_check=(0,))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    p_dq = tl.make_block_ptr(dq, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_dq += tl.load(p_dq, boundary_check=(0, 1))",
      "    p_q = tl.make_block_ptr(q, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "    if USE_GATE:",
      "        p_gq_cumsum = tl.make_block_ptr(g_cumsum, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "        b_gq_cumsum = tl.load(p_gq_cumsum, boundary_check=(0,))",
      "        b_dgq = tl.zeros(",
      "            [",
      "                BT,",
      "            ],",
      "            dtype=tl.float32,",
      "        )",
      "    else:",
      "        b_dgq = None",
      "",
      "    curr_start = (tl.floor(i_t * BT / S).to(tl.int32) * S).to(tl.int32)",
      "",
      "    for offset in range(curr_start, i_t * BT, BT):",
      "        p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (offset, 0), (BT, BK), (1, 0))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_q_tmp = tl.zeros([BT, BK], dtype=tl.float32)",
      "        b_q_tmp += b_q",
      "",
      "        for i_t_small in range(i_t * BT - BT, offset, -BT):",
      "            p_h = tl.make_block_ptr(",
      "                h + tl.cdiv(i_t_small, BT) * H * K * K,",
      "                (K, K),",
      "                (K, 1),",
      "                (0, 0),",
      "                (BK, BK),",
      "                (1, 0),",
      "            )",
      "            b_h = tl.load(p_h, boundary_check=(0, 1))",
      "            b_q_tmp -= tl.dot(b_q_tmp.to(b_h.dtype), b_h)",
      "",
      "        b_q2 = b_q_tmp.to(b_k.dtype)",
      "        b_dh = -tl.dot(tl.trans(b_q2), b_dq.to(b_q2.dtype))",
      "        tl.atomic_add(",
      "            dh",
      "            + tl.cdiv(offset, BT) * HQ * K * K",
      "            + tl.arange(0, K)[:, None] * K",
      "            + tl.arange(0, K)[None, :],",
      "            b_dh,",
      "            sem=\"relaxed\",",
      "        )",
      "",
      "        b_A = tl.dot(b_q2, tl.trans(b_k))",
      "        if USE_GATE:",
      "            p_gk_cumsum = tl.make_block_ptr(",
      "                g_cumsum, (T,), (HQ,), (offset,), (BT,), (0,)",
      "            )",
      "            b_gk_cumsum = tl.load(p_gk_cumsum, boundary_check=(0,))",
      "            b_A = b_A + b_gq_cumsum[:, None] - b_gk_cumsum[None, :]",
      "            b_A = tl.where(",
      "                (i_t * BT + tl.arange(0, BT) < T)[:, None], b_A, float(\"-inf\")",
      "            )",
      "",
      "        b_A_softmax = tl.math.exp2(b_A * sm_scale - b_l[:, None])",
      "        b_dv = tl.dot(tl.trans(b_A_softmax.to(b_do.dtype)), b_do)",
      "",
      "        tl.atomic_add(",
      "            dv",
      "            + ((offset + tl.arange(0, BT)) * HQ * V)[:, None]",
      "            + tl.arange(0, BV)[None, :],",
      "            b_dv.to(dv.dtype.element_ty),",
      "            sem=\"relaxed\",",
      "        )",
      "",
      "        p_v = tl.make_block_ptr(v, (T, V), (V * H, 1), (offset, 0), (BT, BV), (1, 0))",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_dp = tl.dot(b_do, tl.trans(b_v))",
      "        b_dA = (b_dp - b_delta[:, None]) * b_A_softmax * scale",
      "        if USE_GATE:",
      "            b_dgk = -tl.sum(b_dA, axis=0)",
      "            tl.atomic_add(",
      "                dg_cumsum + (offset + tl.arange(0, BT)) * HQ, b_dgk, sem=\"relaxed\"",
      "            )",
      "            b_dgq += tl.sum(b_dA, axis=1)",
      "",
      "        b_dA = b_dA.to(b_k.dtype)",
      "        p_h = tl.make_block_ptr(",
      "            h + tl.cdiv(offset, BT) * H * K * K,",
      "            (K, K),",
      "            (1, K),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (0, 1),",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dk = tl.dot(tl.trans(b_dA), b_q2)",
      "        tl.atomic_add(",
      "            dk",
      "            + (offset + tl.arange(0, BT))[:, None] * HQ * K",
      "            + tl.arange(0, BK)[None, :],",
      "            b_dk,",
      "            sem=\"relaxed\",",
      "        )",
      "        b_dq -= tl.dot(b_dq.to(b_h.dtype), b_h)",
      "        b_dq += tl.dot(b_dA, b_k)",
      "",
      "    p_dq_new = tl.make_block_ptr(",
      "        dq_new, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dq_new, b_dq.to(dq_new.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_GATE:",
      "        tl.atomic_add(",
      "            dg_cumsum + (i_t * BT + tl.arange(0, BT)) * HQ, b_dgq, sem=\"relaxed\"",
      "        )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/275.py"
  },
  {
    "name": "parallel_path_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None, 'USE_GATE': lambda args: args['g_cumsum'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_stages=num_stages, num_warps=4) for num_stages in [2, 3, 4, 5, 6]], key=['BK', 'BS', 'BT', 'USE_GATE', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "o_new",
        "annotation": null
      },
      {
        "name": "g_cumsum",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "L_new",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_GATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    o_new,",
      "    g_cumsum,",
      "    h,",
      "    scale,",
      "    L,",
      "    L_new,",
      "    M,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    T,",
      "    G: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_GATE: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_hq = i_bh // HQ, i_bh % HQ",
      "    i_h = i_hq // G",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        boh = i_n * tl.cdiv(T, BS)",
      "    sm_scale = scale * 1.44269504",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * HQ + i_hq) * K, (T, K), (HQ * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    b_q = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_q += tl.load(p_q, boundary_check=(0, 1))",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos * HQ + i_hq) * V, (T, V), (HQ * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "    )",
      "    b_o += tl.load(p_o, boundary_check=(0, 1))",
      "",
      "    p_L = tl.make_block_ptr(L + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    p_M = tl.make_block_ptr(M + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,))",
      "    b_l = tl.load(p_L, boundary_check=(0,))",
      "    b_m = tl.load(p_M, boundary_check=(0,))",
      "",
      "    if USE_GATE:",
      "        p_g_cumsum_q = tl.make_block_ptr(",
      "            g_cumsum + bos * HQ + i_hq, (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        b_g_cumsum_q = tl.load(p_g_cumsum_q, boundary_check=(0,))",
      "    else:",
      "        b_g_cumsum_q = None",
      "",
      "    for offset in range((i_t + 1) * BT - 2 * BS, i_t * BT - BS, -BS):",
      "        i_tk = tl.cdiv(offset, BS)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_tk) * H + i_h) * K * K,",
      "            (K, K),",
      "            (K, 1),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        m_s = i_t * BT + tl.arange(0, BT) >= (offset + BS)",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k)",
      "        b_q_minus = tl.dot(b_q.to(b_h.dtype), b_h)",
      "        b_q = tl.where(m_s[:, None], b_q - b_q_minus, b_q)",
      "        if USE_GATE:",
      "            p_g_cumsum_k = tl.make_block_ptr(",
      "                g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,)",
      "            )",
      "            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "            b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "        b_s = tl.where(m_s[:, None], b_s * sm_scale, float(\"-inf\"))",
      "        b_m_new = tl.maximum(b_m, tl.max(b_s, 1))",
      "        alpha = tl.math.exp2((b_m - b_m_new))",
      "        b_s = tl.math.exp2(b_s - b_m_new[:, None])",
      "        b_s = tl.where(m_s[:, None], b_s, 0)",
      "        b_o *= alpha[:, None]",
      "        b_l = b_l * alpha + tl.sum(b_s, 1)",
      "        b_m = b_m_new",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v)",
      "",
      "    tl.debug_barrier()",
      "",
      "    for offset in range(i_t * BT - BS, -BS, -BS):",
      "        i_tk = tl.cdiv(offset, BS)",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, K * H), (0, offset), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (V * H, 1), (offset, 0), (BS, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_tk) * H + i_h) * K * K,",
      "            (K, K),",
      "            (K, 1),",
      "            (0, 0),",
      "            (BK, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q.to(b_k.dtype), b_k)",
      "        b_q -= tl.dot(b_q.to(b_h.dtype), b_h)",
      "        if USE_GATE:",
      "            p_g_cumsum_k = tl.make_block_ptr(",
      "                g_cumsum + (bos * HQ + i_hq), (T,), (HQ,), (offset,), (BS,), (0,)",
      "            )",
      "            b_g_cumsum_k = tl.load(p_g_cumsum_k, boundary_check=(0,))",
      "            b_s = b_s + b_g_cumsum_q[:, None] - b_g_cumsum_k[None, :]",
      "        b_s = b_s * sm_scale",
      "        b_m_new = tl.maximum(b_m, tl.max(b_s, 1))",
      "        alpha = tl.math.exp2((b_m - b_m_new))",
      "        b_s = tl.math.exp2(b_s - b_m_new[:, None])",
      "        b_o *= alpha[:, None]",
      "        b_l = b_l * alpha + tl.sum(b_s, 1)",
      "        b_m = b_m_new",
      "        b_o += tl.dot(b_s.to(b_v.dtype), b_v)",
      "",
      "    b_o = b_o / b_l[:, None]",
      "    p_o_new = tl.make_block_ptr(",
      "        o_new + (bos * HQ + i_hq) * V,",
      "        (T, V),",
      "        (HQ * V, 1),",
      "        (i_t * BT, 0),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_o_new, b_o.to(p_o_new.dtype.element_ty), boundary_check=(0, 1))",
      "    b_l = tl.math.log2(b_l) + b_m",
      "    p_L_new = tl.make_block_ptr(",
      "        L_new + (bos * HQ + i_hq), (T,), (HQ,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_L_new, b_l.to(p_L_new.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/276.py"
  },
  {
    "name": "parallel_path_fwd_kernel_prepare_k_cache",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['offsets'] is not None})",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "k_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_path_fwd_kernel_prepare_k_cache(",
      "    k,",
      "    k_new,",
      "    h,",
      "    offsets,",
      "    indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(indices + i_t * 2).to(tl.int32), tl.load(",
      "            indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        i_n = i_b",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = triton.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    k += (bos * H + i_h) * K",
      "    k_new += (bos * H + i_h) * K",
      "    h += (boh * H + i_h) * K * K",
      "",
      "    stride_h = H * K * K",
      "    p_k = tl.make_block_ptr(k, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    b_k = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_k += tl.load(p_k, boundary_check=(0, 1))",
      "    for k_block_idx in range(i_t + 1, tl.cdiv(T, BT)):",
      "        p_h = tl.make_block_ptr(",
      "            h + k_block_idx * stride_h, (K, K), (1, K), (0, 0), (BK, BK), (0, 1)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_k_minus = tl.dot(b_k.to(b_h.dtype), b_h)",
      "        b_k = b_k - b_k_minus",
      "    p_k_new = tl.make_block_ptr(",
      "        k_new, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_k_new, b_k.to(p_k_new.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/path_attn/277.py"
  },
  {
    "name": "parallel_rebased_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_rebased_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    z,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    i_kv, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    NV = tl.cdiv(V, BV)",
      "    i_k = i_kv // (NV)",
      "    i_v = i_kv % (NV)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_o = tl.zeros([BTL, BV], dtype=tl.float32)",
      "    b_z = tl.zeros([BTL], dtype=tl.float32)",
      "",
      "    for _ in range(0, i_c * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, (b_k), allow_tf32=False)",
      "        b_s = b_s * b_s",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o = b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "",
      "    tl.debug_barrier()",
      "    o_q = tl.arange(0, BTL)",
      "",
      "    o_k = tl.arange(0, BTS)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, i_c * BTL), (BK, BTS), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTS, BV), (1, 0)",
      "    )",
      "",
      "    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_s = b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_z += tl.sum(b_s, axis=1)",
      "",
      "        b_o += tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (0, BTS))",
      "        p_v = tl.advance(p_v, (BTS, 0))",
      "        o_k += BTS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_bh + B * H * i_k) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_c * BTL, i_v * BV),",
      "        (BTL, BV),",
      "        (1, 0),",
      "    )",
      "    p_z = z + (i_bh + B * H * i_k) * T + i_c * BTL + tl.arange(0, BTL)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(",
      "        p_z, b_z.to(p_z.dtype.element_ty), mask=((i_c * BTL + tl.arange(0, BTL)) < T)",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rebased/278.py"
  },
  {
    "name": "_parallel_rebased_bwd_dq",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_bh",
        "annotation": null
      },
      {
        "name": "i_c",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "i_h",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _parallel_rebased_bwd_dq(",
      "    i_bh,",
      "    i_c,",
      "    i_k,",
      "    i_v,",
      "    i_h,",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dq,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    p_do = tl.make_block_ptr(",
      "        do + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0)",
      "    )",
      "    p_q = tl.make_block_ptr(",
      "        q + (i_bh) * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_dq = tl.zeros([BTL, BK], dtype=tl.float32)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BTS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (V, T), (1, V), (i_v * BV, 0), (BV, BTS), (0, 1)",
      "    )",
      "    p_dz = dz + i_bh * T + i_c * BTL + tl.arange(0, BTL)",
      "    b_dz = tl.load(p_dz, mask=(i_c * BTL + tl.arange(0, BTL)) < T)",
      "",
      "    for _ in range(0, i_c * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        if i_v == 0:",
      "            b_ds += b_dz[:, None]",
      "        else:",
      "            b_ds = b_ds",
      "        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)",
      "",
      "        b_dq += tl.dot((2 * b_ds * b_s).to(b_v.dtype), b_k, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (BTS, 0))",
      "        p_v = tl.advance(p_v, (0, BTS))",
      "",
      "    b_dq *= scale",
      "    o_q = tl.arange(0, BTL)",
      "    o_k = tl.arange(0, BTS)",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i_c * BTL), (BV, BTS), (0, 1)",
      "    )",
      "",
      "    for _ in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        if i_v == 0:",
      "            b_ds += b_dz[:, None]",
      "        else:",
      "            b_ds = b_ds",
      "        b_ds = tl.where(m_s, b_ds, 0) * scale",
      "        b_s = tl.dot(b_q, tl.trans(b_k), allow_tf32=False)",
      "        b_s = tl.where(m_s, b_s, 0)",
      "",
      "        b_dq += tl.dot((2 * b_ds * b_s).to(b_k.dtype), b_k, allow_tf32=False)",
      "        p_k = tl.advance(p_k, (BTS, 0))",
      "        p_v = tl.advance(p_v, (0, BTS))",
      "        o_k += BTS",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (i_bh + B * H * i_v) * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_c * BTL, i_k * BK),",
      "        (BTL, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    return"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rebased/278.py"
  },
  {
    "name": "_parallel_rebased_bwd_dkv",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_bh",
        "annotation": null
      },
      {
        "name": "i_c",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "i_h",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dz",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _parallel_rebased_bwd_dkv(",
      "    i_bh,",
      "    i_c,",
      "    i_k,",
      "    i_v,",
      "    i_h,",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dz,",
      "    dk,",
      "    dv,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BTL: tl.constexpr,",
      "    BTS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (T, K), (K, 1), (i_c * BTL, i_k * BK), (BTL, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (i_c * BTL, i_v * BV), (BTL, BV), (1, 0)",
      "    )",
      "    b_k, b_v = tl.load(p_k, boundary_check=(0, 1)), tl.load(p_v, boundary_check=(0, 1))",
      "    b_dk, b_dv = tl.zeros([BTL, BK], dtype=tl.float32), tl.zeros(",
      "        [BTL, BV], dtype=tl.float32",
      "    )",
      "",
      "    for i in range((tl.cdiv(T, BTS) * BTS) - BTS, (i_c + 1) * BTL - BTS, -BTS):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1)",
      "        )",
      "        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)",
      "        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)",
      "",
      "        b_s = tl.dot(b_k.to(b_q.dtype), b_q, allow_tf32=False) * scale",
      "        b_s2 = b_s * b_s",
      "        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)",
      "        b_ds = tl.dot(b_v, b_do, allow_tf32=False) * scale",
      "        if i_v == 0:",
      "            b_ds += b_dz[None, :] * scale",
      "        else:",
      "            b_ds = b_ds",
      "        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)",
      "",
      "    tl.debug_barrier()",
      "    o_q, o_k = tl.arange(0, BTS), tl.arange(0, BTL)",
      "    for i in range(i_c * BTL, (i_c + 1) * BTL, BTS):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, i), (BK, BTS), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (V, T), (1, V), (i_v * BV, i), (BV, BTS), (0, 1)",
      "        )",
      "        p_dz = dz + i_bh * T + i + tl.arange(0, BTS)",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1)).to(b_q.dtype)",
      "        b_dz = tl.load(p_dz, mask=(i + tl.arange(0, BTS)) < T)",
      "",
      "        m_s = o_k[:, None] <= o_q[None, :]",
      "        b_s = tl.dot(b_k, b_q, allow_tf32=False) * scale",
      "        b_s2 = b_s * b_s",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_s2 = tl.where(m_s, b_s2, 0)",
      "",
      "        b_ds = tl.dot(b_v, b_do, allow_tf32=False)",
      "        if i_v == 0:",
      "            b_ds += b_dz[None, :]",
      "        else:",
      "            b_ds = b_ds",
      "        b_ds = tl.where(m_s, b_ds, 0) * scale",
      "",
      "        b_dv += tl.dot(b_s2.to(b_q.dtype), tl.trans(b_do), allow_tf32=False)",
      "        b_dk += tl.dot((2 * b_ds * b_s).to(b_q.dtype), tl.trans(b_q), allow_tf32=False)",
      "        o_q += BTS",
      "",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_bh + B * H * i_v) * T * K,",
      "        (T, K),",
      "        (K, 1),",
      "        (i_c * BTL, i_k * BK),",
      "        (BTL, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (i_bh + B * H * i_k) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (i_c * BTL, i_v * BV),",
      "        (BTL, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    return"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rebased/278.py"
  },
  {
    "name": "fused_chunk_retention_fwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_retention_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    h0,",
      "    ht,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "",
      "    o_i = tl.arange(0, BT)",
      "",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "",
      "    d_b, d_o, d_h = (",
      "        tl.math.exp2(BT * b_b),",
      "        tl.math.exp2((o_i + 1) * b_b),",
      "        tl.math.exp2((BT - o_i - 1) * b_b),",
      "    )",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * T * K, (T, K), (K, 1), (0, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * T * K, (K, T), (1, K), (i_k * BK, 0), (BK, BT), (0, 1)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * T * V, (T, V), (V, 1), (0, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_k * B * H + i_bh).to(tl.int64) * T * V,",
      "        (T, V),",
      "        (V, 1),",
      "        (0, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    NT = tl.cdiv(T, BT)",
      "    for i in range(0, NT):",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k, allow_tf32=False) * d_s",
      "",
      "        b_o = tl.dot(b_s.to(b_q.dtype), b_v, allow_tf32=False)",
      "        if CHECK and i == 0:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]",
      "            b_h = d_b * b_h + tl.dot(",
      "                b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False",
      "            )",
      "        else:",
      "            b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False) * d_o[:, None]",
      "            if i == NT - 1 and (T % BT) != 0:",
      "                d_b = tl.math.exp2((T % BT) * b_b)",
      "                d_h = tl.math.exp2(((T % BT) - o_i - 1) * b_b)",
      "            b_h = d_b * b_h + tl.dot(",
      "                b_k, (b_v * d_h[:, None]).to(b_k.dtype), allow_tf32=False",
      "            )",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        p_q = tl.advance(p_q, (BT, 0))",
      "        p_k = tl.advance(p_k, (0, BT))",
      "        p_v = tl.advance(p_v, (BT, 0))",
      "        p_o = tl.advance(p_o, (BT, 0))",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/retention/279.py"
  },
  {
    "name": "fused_chunk_retention_bwd_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CHECK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_retention_bwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    do,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    h0,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    CHECK: tl.constexpr,",
      "):",
      "    i_v, i_k, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "",
      "    o_i = tl.arange(0, BT)",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "    d_q, d_k = tl.math.exp2((o_i + 1) * b_b) * scale, tl.math.exp2((BT - o_i - 1) * b_b)",
      "    d_b = tl.math.exp2(BT * b_b)",
      "",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale",
      "",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        b_h = tl.load(p_h, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i in range(0, tl.cdiv(T, BT)):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (V, T), (1, V), (i_v * BV, i * BT), (BV, BT), (0, 1)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (i_bh + i_v * B * H).to(tl.int64) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)",
      "",
      "        b_ds = tl.dot(b_do, b_v, allow_tf32=False)",
      "        b_ds = (b_ds * d_s).to(b_k.dtype)",
      "",
      "        b_dq = tl.dot(b_ds, b_k, allow_tf32=False)",
      "",
      "        if CHECK and i == 0:",
      "            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)",
      "            b_h = d_b * b_h + tl.dot(",
      "                (b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False",
      "            )",
      "        else:",
      "            b_dq += tl.dot(b_dd, b_h.to(b_k.dtype), allow_tf32=False)",
      "            b_h = d_b * b_h + tl.dot(",
      "                (b_v * d_k[None, :]).to(b_k.dtype), b_k, allow_tf32=False",
      "            )",
      "",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    b_h = None",
      "    tl.debug_barrier()",
      "    d_s = tl.trans(d_s)",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i in range(1, tl.cdiv(T, BT) + 1):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * T * K, (K, T), (1, K), (i_k * BK, T - i * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * T * K, (T, K), (K, 1), (T - i * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * T * V, (T, V), (V, 1), (T - i * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (i_bh + i_v * B * H).to(tl.int64) * T * K,",
      "            (T, K),",
      "            (K, 1),",
      "            (T - i * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_bh + i_k * B * H).to(tl.int64) * T * V,",
      "            (T, V),",
      "            (V, 1),",
      "            (T - i * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dd = (b_do * d_q[:, None]).to(b_do.dtype)",
      "",
      "        b_ds = tl.dot(b_v, tl.trans(b_do), allow_tf32=False)",
      "        b_ds = (b_ds * d_s).to(b_k.dtype)",
      "",
      "        b_s = tl.dot(b_k, b_q, allow_tf32=False) * d_s",
      "",
      "        b_dk = tl.dot(b_ds, tl.trans(b_q), allow_tf32=False)",
      "",
      "        b_dv = tl.dot(b_s.to(b_q.dtype), b_do, allow_tf32=False)",
      "        if CHECK and i == 1:",
      "            b_dk += (",
      "                tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "                * d_k[:, None]",
      "            )",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]",
      "            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)",
      "        else:",
      "            b_dk += (",
      "                tl.dot(b_v, tl.trans(b_dh).to(b_v.dtype), allow_tf32=False)",
      "                * d_k[:, None]",
      "            )",
      "            b_dv += tl.dot(b_k, b_dh.to(b_k.dtype), allow_tf32=False) * d_k[:, None]",
      "            b_dh = d_b * b_dh + tl.dot(b_q, b_dd, allow_tf32=False)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/retention/279.py"
  },
  {
    "name": "chunk_rwkv6_fwd_cumsum_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BS': BS}, num_warps=num_warps, num_stages=num_stages) for BS in [16, 32, 64] for num_warps in [4, 8, 16] for num_stages in [2, 3, 4]], key=['S', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "oi",
        "annotation": null
      },
      {
        "name": "oe",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_cumsum_kernel(",
      "    s,",
      "    oi,",
      "    oe,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_i = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0).to(tl.float32)",
      "    m_e = tl.where(o_i[:, None] > o_i[None, :], 1.0, 0.0).to(tl.float32)",
      "",
      "    p_s = tl.make_block_ptr(",
      "        s + (bos * H + i_h) * S,",
      "        (T, S),",
      "        (H * S, 1),",
      "        (i_t * BT, i_s * BS),",
      "        (BT, BS),",
      "        (1, 0),",
      "    )",
      "    p_oi = tl.make_block_ptr(",
      "        oi + (bos * H + i_h) * S,",
      "        (T, S),",
      "        (H * S, 1),",
      "        (i_t * BT, i_s * BS),",
      "        (BT, BS),",
      "        (1, 0),",
      "    )",
      "    p_oe = tl.make_block_ptr(",
      "        oe + (bos * H + i_h) * S,",
      "        (T, S),",
      "        (H * S, 1),",
      "        (i_t * BT, i_s * BS),",
      "        (BT, BS),",
      "        (1, 0),",
      "    )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "    b_oi = tl.dot(m_i, b_s)",
      "    b_oe = tl.dot(m_e, b_s)",
      "    tl.store(",
      "        p_oi,",
      "        b_oi.to(p_oi.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_oe,",
      "        b_oe.to(p_oe.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BC'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_inter(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_i, i_j = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "    if i_i <= i_j:",
      "        return",
      "",
      "    m_i = i_t * BT + i_i * BC + tl.arange(0, BC) < T",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_gq = tl.make_block_ptr(",
      "            ge + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            gi + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_gq = tl.where(",
      "            m_i[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float(\"-inf\")",
      "        )",
      "        b_qg = b_q * exp(b_gq - b_gn[None, :]) * scale",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_kg = b_k * exp(b_gn[:, None] - b_gk)",
      "",
      "        b_A += tl.dot(b_qg, b_kg)",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BK', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    u,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_A = (bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT + i_h * BT + i_j * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "",
      "    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (0,), (BK,), (0,))",
      "    b_u = tl.load(p_u, boundary_check=(0,))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i > j, b_A * scale, 0.0)",
      "        b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_qj += H * K",
      "        p_kj += H * K",
      "        p_gk += H * K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC', 'BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_split(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    u,",
      "    A,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    B: tl.constexpr,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_tc // NC, i_tc % NC",
      "    i_j = i_i",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "",
      "    o_A = (i_k * all + bos + i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BC + i_h * BC",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_kj = k + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "    p_gk = gi + (bos + i_t * BT + i_j * BC) * H * K + i_h * K + o_k",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "",
      "    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK), (BK,), (0,))",
      "    b_u = tl.load(p_u, boundary_check=(0,))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_kj[None, :] * exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i > j, b_A * scale, 0.0)",
      "        b_A = tl.where(o_i != j, b_A, tl.sum(b_qj * b_kj * b_u * scale))",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_qj += H * K",
      "        p_kj += H * K",
      "        p_gk += H * K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A2",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_fwd_A_kernel_intra_sub_intra_merge(",
      "    A,",
      "    A2,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    NK: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        all = B * T",
      "",
      "    if i_t * BT + i_c * BC >= T:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(0, NK):",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_k * all + bos) * H * BC + i_h * BC,",
      "            (T, BC),",
      "            (H * BC, 1),",
      "            (i_t * BT + i_c * BC, 0),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        b_A += tl.load(p_A, boundary_check=(0, 1))",
      "    p_A2 = tl.make_block_ptr(",
      "        A2 + (bos * H + i_h) * BT,",
      "        (T, BT),",
      "        (H * BT, 1),",
      "        (i_t * BT + i_c * BC, i_c * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_bwd_kernel_dh",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'STORE_INITIAL_STATE_GRADIENT': lambda args: args['dh0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in BK_LIST for BV in BV_LIST for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_INITIAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_bwd_kernel_dh(",
      "    q,",
      "    gi,",
      "    ge,",
      "    do,",
      "    dh,",
      "    dht,",
      "    dh0,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    HQ: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NG: tl.constexpr,",
      "    STORE_INITIAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_hq = i_nh // HQ, i_nh % HQ",
      "    i_h = i_hq // NG",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        last_idx = min(i_t * BT + BT, T) - 1",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * HQ + i_hq) * K,",
      "            (K, T),",
      "            (1, HQ * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * HQ + i_hq) * V,",
      "            (T, V),",
      "            (HQ * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        p_gk = tl.make_block_ptr(",
      "            ge + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_gk_last = (",
      "            gi + (bos + last_idx) * H * K + i_h * K + i_k * BK + tl.arange(0, BK)",
      "        )",
      "",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_q = (b_q * exp(b_gk) * scale).to(b_q.dtype)",
      "        b_gk_last = tl.load(",
      "            p_gk_last, mask=(i_k * BK + tl.arange(0, BK) < K), other=0.0",
      "        )",
      "        b_dh *= exp(b_gk_last)[:, None]",
      "        b_dh += tl.dot(b_q, b_do)",
      "",
      "    if STORE_INITIAL_STATE_GRADIENT:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_bwd_kernel_intra",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BK', 'NC', 'BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_bwd_kernel_intra(",
      "    q,",
      "    k,",
      "    gi,",
      "    ge,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    i_t, i_i = i_c // NC, i_c % NC",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "    T = eos - bos",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_ge = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_ge = tl.load(p_ge, boundary_check=(0, 1))",
      "    b_dq = tl.zeros([BC, BK], dtype=tl.float32)",
      "    if i_i > 0:",
      "        p_gn = gi + (bos + i_t * BT + i_i * BC - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(0, i_i):",
      "            p_k = tl.make_block_ptr(",
      "                k + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gk = tl.make_block_ptr(",
      "                gi + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (T, BT),",
      "                (H * BT, 1),",
      "                (i_t * BT + i_i * BC, i_j * BC),",
      "                (BC, BC),",
      "                (1, 0),",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "            b_kg = b_k * exp(b_gn[None, :] - b_gk)",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dq += tl.dot(b_dA, b_kg)",
      "        b_dq *= exp(b_ge - b_gn[None, :])",
      "",
      "    o_i = tl.arange(0, BC)",
      "    m_dA = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC + tl.arange(0, BC)) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "    )",
      "    p_kj = k + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gkj = gi + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j, mask=m_dA, other=0)",
      "",
      "        b_kj = tl.load(p_kj, mask=m_k, other=0).to(tl.float32)",
      "        b_gkj = tl.load(p_gkj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] > j",
      "",
      "        b_dq += tl.where(",
      "            m_i, b_dA[:, None] * b_kj[None, :] * exp(b_ge - b_gkj[None, :]), 0.0",
      "        )",
      "        p_kj += H * K",
      "        p_gkj += H * K",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    tl.debug_barrier()",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_gk = tl.make_block_ptr(",
      "        gi + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    NC = min(NC, tl.cdiv(T - i_t * BT, BC))",
      "    if i_i < NC - 1:",
      "        p_gn = gi + (bos + min(i_t * BT + i_i * BC + BC, T) - 1) * H * K + i_h * K + o_k",
      "",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        for i_j in range(i_i + 1, NC):",
      "            m_j = (i_t * BT + i_j * BC + tl.arange(0, BC)) < T",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_gq = tl.make_block_ptr(",
      "                ge + (bos * H + i_h) * K,",
      "                (T, K),",
      "                (H * K, 1),",
      "                (i_t * BT + i_j * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_dA = tl.make_block_ptr(",
      "                dA + (bos * H + i_h) * BT,",
      "                (BT, T),",
      "                (1, H * BT),",
      "                (i_i * BC, i_t * BT + i_j * BC),",
      "                (BC, BC),",
      "                (0, 1),",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_gq = tl.where(",
      "                m_j[:, None] & m_k, tl.load(p_gq, boundary_check=(0, 1)), float(\"-inf\")",
      "            )",
      "            b_qg = b_q * exp(b_gq - b_gn[None, :])",
      "",
      "            b_dA = tl.load(p_dA, boundary_check=(0, 1))",
      "",
      "            b_dk += tl.dot(b_dA, b_qg)",
      "        b_dk *= exp(b_gn[None, :] - b_gk)",
      "    o_dA = (",
      "        bos * H * BT",
      "        + (i_t * BT + i_i * BC) * H * BT",
      "        + i_h * BT",
      "        + i_i * BC",
      "        + tl.arange(0, BC)",
      "    )",
      "    p_qj = q + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_gqj = ge + (bos + i_t * BT + i_i * BC) * H * K + i_h * K + o_k",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "",
      "        b_dA = tl.load(dA + o_dA + j * H * BT)",
      "",
      "        b_qj = tl.load(p_qj, mask=m_k, other=0).to(tl.float32)",
      "        b_gqj = tl.load(p_gqj, mask=m_k, other=0).to(tl.float32)",
      "",
      "        m_i = o_i[:, None] < j",
      "        b_dk += tl.where(",
      "            m_i, b_dA[:, None] * b_qj[None, :] * exp(b_gqj[None, :] - b_gk), 0.0",
      "        )",
      "        p_qj += H * K",
      "        p_gqj += H * K",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "chunk_rwkv6_bwd_kernel_inter",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps) for BK in BK_LIST for BV in BV_LIST for num_warps in [2, 4, 8]], key=['BT'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "gi",
        "annotation": null
      },
      {
        "name": "ge",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dA",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dq2",
        "annotation": null
      },
      {
        "name": "dk2",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "du",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_rwkv6_bwd_kernel_inter(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    gi,",
      "    ge,",
      "    u,",
      "    do,",
      "    dh,",
      "    dA,",
      "    dq,",
      "    dk,",
      "    dq2,",
      "    dk2,",
      "    dg,",
      "    du,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    m_k = o_k < K",
      "",
      "    p_gk = tl.make_block_ptr(",
      "        ge + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_gi = tl.make_block_ptr(",
      "        gi + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_gn = gi + (bos + min(T, i_t * BT + BT) - 1) * H * K + i_h * K + o_k",
      "    b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dgk = tl.zeros(",
      "        [",
      "            BK,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + (i_tg * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dgk += tl.sum(b_h * b_dh, axis=0)",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "    b_dgk *= exp(b_gn)",
      "    b_dq *= scale",
      "    b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "    b_gi = tl.load(p_gi, boundary_check=(0, 1))",
      "    b_dq = b_dq * exp(b_gk)",
      "    b_dk = b_dk * exp(b_gn[None, :] - b_gi)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dq = tl.make_block_ptr(",
      "        dq + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dA_dig = dA + ((bos + i_t * BT + o_i) * H + i_h) * BT + o_i",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dgk += tl.sum(b_dk * b_k, axis=0)",
      "",
      "    b_dq += tl.load(p_dq, boundary_check=(0, 1))",
      "    b_dk += tl.load(p_dk, boundary_check=(0, 1))",
      "    b_dg = b_q * b_dq - b_k * b_dk",
      "    b_dg = (",
      "        b_dg",
      "        - tl.cumsum(b_dg, axis=0)",
      "        + tl.sum(b_dg, axis=0)[None, :]",
      "        + b_dgk[None, :]",
      "        - b_q * b_dq",
      "    )",
      "",
      "    b_dA_dig = tl.load(p_dA_dig, mask=(i_t * BT + o_i) < T, other=0)",
      "",
      "    p_u = tl.make_block_ptr(u + i_h * K, (K,), (1,), (i_k * BK,), (BK,), (0,))",
      "    b_u = tl.load(p_u, boundary_check=(0,))",
      "",
      "    b_dq += b_dA_dig[:, None] * b_u[None, :] * b_k",
      "    b_dk += b_dA_dig[:, None] * b_u[None, :] * b_q",
      "    b_du = tl.sum(b_dA_dig[:, None] * b_q * b_k, axis=0)",
      "    p_du = tl.make_block_ptr(",
      "        du + (i_tg * H + i_h) * K, (K,), (1,), (i_k * BK,), (BK,), (0,)",
      "    )",
      "    tl.store(p_du, b_du, boundary_check=(0,))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk2 + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/281.py"
  },
  {
    "name": "fused_recurrent_rwkv6_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16]], key=['BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    u,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_q = q + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_o = o + ((i_k * all + bos) + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_u = u + i_h * K + o_k",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_kv = b_k[:, None] * b_v[None, :]",
      "        b_o = tl.sum((b_h + b_kv * b_u[:, None]) * b_q[:, None], 0)",
      "        b_h = b_h * exp(b_w)[:, None] + b_kv",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "        p_q += (-1 if REVERSE else 1) * H * K",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_w += (-1 if REVERSE else 1) * H * K",
      "        p_o += (-1 if REVERSE else 1) * H * V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/282.py"
  },
  {
    "name": "fused_recurrent_rwkv6_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dq1",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_bwd_kernel_dq(",
      "    k,",
      "    v,",
      "    w,",
      "    u,",
      "    do,",
      "    dq,",
      "    dq1,",
      "    h0,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_do = do + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_dq = (",
      "        dq + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    )",
      "    p_dq1 = (",
      "        dq1 + ((i_v * all + bos) + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    )",
      "    p_u = u + i_h * K + o_k",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    for _ in range(0, T):",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_kv = b_k[:, None] * b_v[None, :]",
      "",
      "        b_hq = b_h * b_do[None, :]",
      "        b_dq = tl.sum(b_hq + b_kv * b_u[:, None] * b_do[None, :], 1) * scale",
      "        b_dq1 = tl.sum(b_hq, 1)",
      "        b_h = b_h * exp(b_w)[:, None]",
      "        b_h += b_kv",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dq1, b_dq1.to(p_dq1.dtype.element_ty), mask=mask_k)",
      "",
      "        p_k += (-1 if REVERSE else 1) * H * K",
      "        p_v += (-1 if REVERSE else 1) * H * V",
      "        p_w += (-1 if REVERSE else 1) * H * K",
      "        p_do += (-1 if REVERSE else 1) * H * V",
      "        p_dq += (-1 if REVERSE else 1) * H * K",
      "        p_dq1 += (-1 if REVERSE else 1) * H * K"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/282.py"
  },
  {
    "name": "fused_recurrent_rwkv6_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "u",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dk1",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    w,",
      "    u,",
      "    do,",
      "    dk,",
      "    dk1,",
      "    dv,",
      "    dh0,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_k, i_nh = (",
      "        tl.program_id(0).to(tl.int64),",
      "        tl.program_id(1).to(tl.int64),",
      "        tl.program_id(2).to(tl.int64),",
      "    )",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        all = T",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        all = B * T",
      "",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_q = q + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if not REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_w = w + (bos + ((T - 1) if not REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_do = do + (bos + ((T - 1) if not REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_dk = (",
      "        dk",
      "        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + o_k",
      "    )",
      "    p_dk1 = (",
      "        dk1",
      "        + ((i_v * all + bos) + ((T - 1) if not REVERSE else 0)) * H * K",
      "        + i_h * K",
      "        + o_k",
      "    )",
      "    p_dv = (",
      "        dv",
      "        + ((i_k * all + bos) + ((T - 1) if not REVERSE else 0)) * H * V",
      "        + i_h * V",
      "        + o_v",
      "    )",
      "    p_u = u + i_h * K + o_k",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[:, None] & mask_v[None, :]",
      "",
      "    b_u = tl.load(p_u, mask=mask_k, other=0).to(tl.float32)",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for _ in range(T - 1, -1, -1):",
      "        b_q = tl.load(p_q, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask_v, other=0).to(tl.float32)",
      "        b_dkv = b_q[:, None] * b_do[None, :]",
      "        b_dk = tl.sum(b_dh * b_v[None, :], 1)",
      "        tl.store(p_dk1, b_dk.to(p_dk1.dtype.element_ty), mask=mask_k)",
      "        b_dk += tl.sum(b_dkv * b_u[:, None] * b_v[None, :], 1)",
      "        b_dv = tl.sum((b_dh + (b_dkv * b_u[:, None])) * b_k[:, None], 0)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), mask=mask_k)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), mask=mask_v)",
      "        b_dh *= exp(b_w)[:, None]",
      "        b_dh += b_dkv",
      "",
      "        p_q += (-1 if not REVERSE else 1) * H * K",
      "        p_k += (-1 if not REVERSE else 1) * H * K",
      "        p_v += (-1 if not REVERSE else 1) * H * V",
      "        p_w += (-1 if not REVERSE else 1) * H * K",
      "        p_do += (-1 if not REVERSE else 1) * H * V",
      "        p_dk += (-1 if not REVERSE else 1) * H * K",
      "        p_dk1 += (-1 if not REVERSE else 1) * H * K",
      "        p_dv += (-1 if not REVERSE else 1) * H * V",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = dh0 + i_nh * K * V + o_k[:, None] * V + o_v[None, :]",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/282.py"
  },
  {
    "name": "fused_recurrent_rwkv6_bwd_kernel_dw",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT, 'BK': BK}, num_warps=num_warps) for BT in [16, 32, 64] for BK in [32, 64] for num_warps in [1, 2, 4, 8]], key=['K'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv6_bwd_kernel_dw(",
      "    q,",
      "    k,",
      "    dq,",
      "    dk,",
      "    dw,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    T = eos - bos",
      "    NT = tl.cdiv(T, BT)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_i = (",
      "        tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "        if not REVERSE",
      "        else tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "    )",
      "",
      "    b_z = tl.zeros([BK], dtype=tl.float32)",
      "",
      "    i_t = 0 if not REVERSE else NT - 1",
      "    for _ in range(NT):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + 1, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T - 1, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT + 1, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T - 1, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_dw = tl.make_block_ptr(",
      "            dw + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dq = tl.load(p_dq, boundary_check=(0, 1)).to(tl.float32)",
      "        b_k = tl.load(p_k, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dk = tl.load(p_dk, boundary_check=(0, 1)).to(tl.float32)",
      "        b_dw = (b_q * b_dq * scale) - b_k * b_dk",
      "        b_c = b_z[None, :] + tl.dot(m_i, b_dw, allow_tf32=False)",
      "        tl.store(p_dw, b_c.to(p_dw.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_t >= 0:",
      "            b_z += tl.sum(b_dw, 0)",
      "",
      "        i_t += 1 if not REVERSE else -1"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv6/282.py"
  },
  {
    "name": "rwkv_seq_mix_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}) for block_size in [128, 256, 512, 1024, 2048, 4096, 8192]], key=['hidden_dim'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "x_prev_ptr",
        "annotation": null
      },
      {
        "name": "mix_k_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "batch_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "token_length",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rwkv_seq_mix_kernel(",
      "    x_ptr,",
      "    x_prev_ptr,",
      "    mix_k_ptr,",
      "    output_ptr,",
      "    batch_size: tl.constexpr,",
      "    token_length,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    block_start = tl.program_id(0) * BLOCK_SIZE",
      "    block_idx = block_start + tl.arange(0, BLOCK_SIZE)[:]",
      "",
      "    total_seq_dim = token_length * hidden_dim",
      "    batch_idx = block_idx // total_seq_dim",
      "    seq_and_feat = block_idx % total_seq_dim",
      "    seq_idx = seq_and_feat // hidden_dim",
      "    feat_idx = seq_and_feat % hidden_dim",
      "",
      "    is_valid = (batch_idx < batch_size) & (seq_idx < token_length)",
      "",
      "    x_idx = batch_idx * total_seq_dim + seq_idx * hidden_dim + feat_idx",
      "",
      "    curr_x = tl.load(x_ptr + x_idx, mask=is_valid, other=0.0).to(tl.float32)",
      "    k_value = tl.load(mix_k_ptr + feat_idx).to(tl.float32)",
      "",
      "    is_first = seq_idx < 1",
      "    prev_state_idx = batch_idx * hidden_dim + feat_idx",
      "    prev_state = tl.load(",
      "        x_prev_ptr + prev_state_idx, mask=(is_first & is_valid), other=0.0",
      "    ).to(tl.float32)",
      "",
      "    prev_x_idx = x_idx - hidden_dim",
      "    prev_x = tl.load(x_ptr + prev_x_idx, mask=(~is_first & is_valid), other=0.0).to(",
      "        tl.float32",
      "    )",
      "",
      "    prev_value = tl.where(is_first, prev_state, prev_x)",
      "    state_diff = prev_value - curr_x",
      "    mixed = state_diff * k_value",
      "    result = tl.cast(",
      "        curr_x + mixed, dtype=output_ptr.dtype.element_ty, fp_downcast_rounding=\"rtne\"",
      "    )",
      "    tl.store(output_ptr + x_idx, result, mask=is_valid)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/283.py"
  },
  {
    "name": "rwkv_mix_bwd_kenel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}) for block_size in [128, 256, 512, 1024, 2048, 4096, 8192]], key=['hidden_dim'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "dk1_ptr0",
        "annotation": null
      },
      {
        "name": "xk_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "dx_prev_ptr",
        "annotation": null
      },
      {
        "name": "batch_size",
        "annotation": null
      },
      {
        "name": "token_length",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rwkv_mix_bwd_kenel(",
      "    dk1_ptr0,",
      "    xk_ptr,",
      "    dx_ptr,",
      "    dx_prev_ptr,",
      "    batch_size,",
      "    token_length,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "",
      "    batch_idx = offsets // (token_length * hidden_dim)",
      "    seq_feat = offsets % (token_length * hidden_dim)",
      "    seq_idx = seq_feat // hidden_dim",
      "    feat_idx = seq_feat % hidden_dim",
      "",
      "    is_valid = offsets < (batch_size * token_length * hidden_dim)",
      "",
      "    dk1 = tl.load(dk1_ptr0 + offsets, mask=is_valid)",
      "    xk = tl.load(xk_ptr + feat_idx, mask=is_valid)",
      "    prod = dk1 * xk",
      "",
      "    mask_next = seq_idx < (token_length - 1)",
      "    next_offset = offsets + hidden_dim",
      "    dk1_next = tl.load(dk1_ptr0 + next_offset, mask=mask_next & is_valid, other=0.0)",
      "    prod_next = dk1_next * xk",
      "    dx_val = dk1 - prod + tl.where(mask_next, prod_next, 0.0)",
      "    dx_val = tl.cast(dx_val, dtype=dx_ptr.dtype.element_ty, fp_downcast_rounding=\"rtne\")",
      "    tl.store(dx_ptr + offsets, dx_val, mask=is_valid)",
      "",
      "    dx_prev_offset = batch_idx * hidden_dim + feat_idx",
      "    is_first_step = seq_idx == 0",
      "",
      "    tl.store(",
      "        dx_prev_ptr + dx_prev_offset,",
      "        tl.cast(prod, dtype=dx_prev_ptr.dtype.element_ty),",
      "        mask=is_first_step,",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/283.py"
  },
  {
    "name": "fused_addcmul_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "hidden",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "ixr",
        "annotation": null
      },
      {
        "name": "ixw",
        "annotation": null
      },
      {
        "name": "ixk",
        "annotation": null
      },
      {
        "name": "ixv",
        "annotation": null
      },
      {
        "name": "ixa",
        "annotation": null
      },
      {
        "name": "ixg",
        "annotation": null
      },
      {
        "name": "oxr",
        "annotation": null
      },
      {
        "name": "oxw",
        "annotation": null
      },
      {
        "name": "oxk",
        "annotation": null
      },
      {
        "name": "oxv",
        "annotation": null
      },
      {
        "name": "oxa",
        "annotation": null
      },
      {
        "name": "oxg",
        "annotation": null
      },
      {
        "name": "use_xg",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_addcmul_fwd_kernel(",
      "    hidden,",
      "    delta,",
      "    ixr,",
      "    ixw,",
      "    ixk,",
      "    ixv,",
      "    ixa,",
      "    ixg,",
      "    oxr,",
      "    oxw,",
      "    oxk,",
      "    oxv,",
      "    oxa,",
      "    oxg,",
      "    use_xg: tl.constexpr,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "    xoffset = i_b * T * D + i_t * D",
      "    indices = tl.arange(0, BD)",
      "    xindex = xoffset + indices",
      "    xmask = indices < D",
      "    b_hiddn = tl.load(hidden + xindex, xmask)",
      "    b_x = tl.load(delta + xindex, xmask)",
      "    b_ixr = tl.load(ixr + indices, xmask)",
      "    b_ixw = tl.load(ixw + indices, xmask)",
      "    b_ixk = tl.load(ixk + indices, xmask)",
      "    b_ixv = tl.load(ixv + indices, xmask)",
      "    b_ixa = tl.load(ixa + indices, xmask)",
      "    b_oxr = tl.fma(b_x, b_ixr, b_hiddn)",
      "    b_oxw = tl.fma(b_x, b_ixw, b_hiddn)",
      "    b_oxk = tl.fma(b_x, b_ixk, b_hiddn)",
      "    b_oxv = tl.fma(b_x, b_ixv, b_hiddn)",
      "    b_oxa = tl.fma(b_x, b_ixa, b_hiddn)",
      "",
      "    tl.store(oxr + xindex, b_oxr.to(oxr.dtype.element_ty), xmask)",
      "    tl.store(oxw + xindex, b_oxw.to(oxw.dtype.element_ty), xmask)",
      "    tl.store(oxk + xindex, b_oxk.to(oxk.dtype.element_ty), xmask)",
      "    tl.store(oxv + xindex, b_oxv.to(oxv.dtype.element_ty), xmask)",
      "    tl.store(oxa + xindex, b_oxa.to(oxa.dtype.element_ty), xmask)",
      "",
      "    if use_xg:",
      "        b_ixg = tl.load(ixg + indices)",
      "        b_oxg = tl.fma(b_x, b_ixg, b_hiddn)",
      "        tl.store(oxg + xindex, b_oxg.to(oxg.dtype.element_ty), xmask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/284.py"
  },
  {
    "name": "addcmul_bwd_kernel1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'], use_cuda_graph=use_cuda_graph)"
    ],
    "args": [
      {
        "name": "ixr",
        "annotation": null
      },
      {
        "name": "ixw",
        "annotation": null
      },
      {
        "name": "ixk",
        "annotation": null
      },
      {
        "name": "ixv",
        "annotation": null
      },
      {
        "name": "ixa",
        "annotation": null
      },
      {
        "name": "ixg",
        "annotation": null
      },
      {
        "name": "dxr",
        "annotation": null
      },
      {
        "name": "dxw",
        "annotation": null
      },
      {
        "name": "dxk",
        "annotation": null
      },
      {
        "name": "dxv",
        "annotation": null
      },
      {
        "name": "dxa",
        "annotation": null
      },
      {
        "name": "dxg",
        "annotation": null
      },
      {
        "name": "ghidden",
        "annotation": null
      },
      {
        "name": "gx",
        "annotation": null
      },
      {
        "name": "use_xg",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DTYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def addcmul_bwd_kernel1(",
      "    ixr,",
      "    ixw,",
      "    ixk,",
      "    ixv,",
      "    ixa,",
      "    ixg,",
      "    dxr,",
      "    dxw,",
      "    dxk,",
      "    dxv,",
      "    dxa,",
      "    dxg,",
      "    ghidden,",
      "    gx,",
      "    use_xg: tl.constexpr,",
      "    T,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    DTYPE: tl.constexpr,",
      "):",
      "    i_b, i_t = tl.program_id(0), tl.program_id(1)",
      "    xoffset = i_b * T * D + i_t * D",
      "    indices = tl.arange(0, BD)",
      "    xindex = xoffset + indices",
      "    xmask = indices < D",
      "    b_dxr = tl.load(dxr + xindex, xmask).to(DTYPE)",
      "    b_dxw = tl.load(dxw + xindex, xmask).to(DTYPE)",
      "    b_dxk = tl.load(dxk + xindex, xmask).to(DTYPE)",
      "    b_dxv = tl.load(dxv + xindex, xmask).to(DTYPE)",
      "    b_dxa = tl.load(dxa + xindex, xmask).to(DTYPE)",
      "    b_ixr = tl.load(ixr + indices, xmask).to(DTYPE)",
      "    b_ixw = tl.load(ixw + indices, xmask).to(DTYPE)",
      "    b_iwk = tl.load(ixk + indices, xmask).to(DTYPE)",
      "    b_ixv = tl.load(ixv + indices, xmask).to(DTYPE)",
      "    b_ixa = tl.load(ixa + indices, xmask).to(DTYPE)",
      "",
      "    if use_xg:",
      "        b_dxg = tl.load(dxg + xindex, xmask).to(DTYPE)",
      "        b_ixg = tl.load(ixg + indices, xmask).to(DTYPE)",
      "        g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa + b_dxg",
      "        g_x = (",
      "            b_dxr * b_ixr",
      "            + b_dxw * b_ixw",
      "            + b_dxk * b_iwk",
      "            + b_dxv * b_ixv",
      "            + b_dxa * b_ixa",
      "            + b_dxg * b_ixg",
      "        )",
      "    else:",
      "        g_hidden = b_dxr + b_dxw + b_dxk + b_dxv + b_dxa",
      "        g_x = (",
      "            b_dxr * b_ixr",
      "            + b_dxw * b_ixw",
      "            + b_dxk * b_iwk",
      "            + b_dxv * b_ixv",
      "            + b_dxa * b_ixa",
      "        )",
      "",
      "    tl.store(ghidden + xindex, g_hidden.to(ghidden.dtype.element_ty), xmask)",
      "    tl.store(gx + xindex, g_x.to(gx.dtype.element_ty), xmask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/284.py"
  },
  {
    "name": "k_update_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size in [1024, 2048, 4096, 8192] for num_warps in [2, 4, 8, 16, 32]], key=['hidden_dim'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "ka",
        "annotation": null
      },
      {
        "name": "out",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def k_update_fwd_kernel(",
      "    k,",
      "    a,",
      "    ka,",
      "    out,",
      "    xnumel,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    xoffset = tl.program_id(0) * BLOCK_SIZE",
      "    xindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]",
      "",
      "    xmask = xindex < xnumel",
      "    x0 = xindex % hidden_dim",
      "",
      "    b_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_ka = tl.load(ka + x0, eviction_policy=\"evict_last\").to(tl.float32)",
      "",
      "    output = b_k * (1 + (b_a - 1) * b_ka)",
      "",
      "    tl.store(out + xindex, output.to(out.dtype.element_ty), xmask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/285.py"
  },
  {
    "name": "k_update_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size in [1024, 2048, 4096, 8192] for num_warps in [2, 4, 8, 16, 32]], key=['hidden_dim'])"
    ],
    "args": [
      {
        "name": "grad_output",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "ka",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "da",
        "annotation": null
      },
      {
        "name": "dka",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "hidden_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def k_update_bwd_kernel(",
      "    grad_output,",
      "    k,",
      "    a,",
      "    ka,",
      "    dk,",
      "    da,",
      "    dka,",
      "    xnumel,",
      "    hidden_dim: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    xoffset = tl.program_id(0) * BLOCK_SIZE",
      "    xindex = xoffset + tl.arange(0, BLOCK_SIZE)[:]",
      "",
      "    xmask = xindex < xnumel",
      "    x0 = xindex % hidden_dim",
      "",
      "    b_grad_output = tl.load(grad_output + xindex, xmask, other=0.0)",
      "    b_k = tl.load(k + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_a = tl.load(a + xindex, xmask, other=0.0).to(tl.float32)",
      "    b_ka = tl.load(ka + x0, eviction_policy=\"evict_last\").to(tl.float32)",
      "",
      "    b_dk = b_grad_output * (1 + (b_a - 1) * b_ka)",
      "    b_da = b_grad_output * b_k * b_ka",
      "    b_dka = b_grad_output * b_k * (b_a - 1)",
      "",
      "    tl.store(dk + xindex, b_dk.to(dk.dtype.element_ty), xmask)",
      "    tl.store(da + xindex, b_da.to(da.dtype.element_ty), xmask)",
      "    tl.store(dka + xindex, b_dka.to(dka.dtype.element_ty), xmask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/285.py"
  },
  {
    "name": "fused_recurrent_rwkv7_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BV in [16, 32, 64] for num_warps in [2, 4, 8, 16, 32] for num_stages in [2, 3, 4]], key=['BK'], use_cuda_graph=use_cuda_graph)",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "kk",
        "annotation": null
      },
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_DECODE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_rwkv7_fwd_kernel(",
      "    r,",
      "    w,",
      "    k,",
      "    v,",
      "    kk,",
      "    a,",
      "    o,",
      "    h0,",
      "    ht,",
      "    cu_seqlens,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    IS_DECODE: tl.constexpr,",
      "):",
      "    i_v, i_nh = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int64), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int64)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "",
      "    o_k = tl.arange(0, BK)",
      "    o_v = i_v * BV + tl.arange(0, BV)",
      "    p_r = r + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_w = w + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_k = k + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_v = v + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "    p_a = a + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "    p_kk = kk + (bos + ((T - 1) if REVERSE else 0)) * H * K + i_h * K + o_k",
      "",
      "    p_o = o + (bos + ((T - 1) if REVERSE else 0)) * H * V + i_h * V + o_v",
      "",
      "    mask_k = o_k < K",
      "    mask_v = o_v < V",
      "    mask_h = mask_k[None, :] & mask_v[:, None]",
      "    b_h = tl.zeros([BV, BK], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        b_h += tl.load(p_h0, mask=mask_h, other=0).to(tl.float32)",
      "",
      "    if IS_DECODE:",
      "        b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale",
      "        b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "        b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "        b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "        b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "        b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)",
      "        b_act_a = -b_kk",
      "        b_b = b_kk * b_a",
      "",
      "        tmp = tl.sum(b_h * b_act_a[None, :], axis=1)",
      "        b_h = exp(b_w)[None, :] * b_h + (",
      "            tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "        )",
      "        b_o = tl.sum(b_h * b_r[None, :], axis=1)",
      "",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "    else:",
      "        for _ in range(0, T):",
      "            b_r = tl.load(p_r, mask=mask_k, other=0).to(tl.float32) * scale",
      "            b_w = tl.load(p_w, mask=mask_k, other=0).to(tl.float32)",
      "            b_k = tl.load(p_k, mask=mask_k, other=0).to(tl.float32)",
      "            b_v = tl.load(p_v, mask=mask_v, other=0).to(tl.float32)",
      "            b_a = tl.load(p_a, mask=mask_k, other=0).to(tl.float32)",
      "            b_kk = tl.load(p_kk, mask=mask_k, other=0).to(tl.float32)",
      "            b_act_a = -b_kk",
      "            b_b = b_kk * b_a",
      "",
      "            tmp = tl.sum(b_h * b_act_a[None, :], axis=1)",
      "            b_h = exp(b_w)[None, :] * b_h + (",
      "                tmp[:, None] * b_b[None, :] + b_k[None, :] * b_v[:, None]",
      "            )",
      "            b_o = tl.sum(b_h * b_r[None, :], axis=1)",
      "",
      "            tl.store(p_o, b_o.to(p_o.dtype.element_ty), mask=mask_v)",
      "            p_r += (-1 if REVERSE else 1) * H * K",
      "            p_w += (-1 if REVERSE else 1) * H * K",
      "            p_k += (-1 if REVERSE else 1) * H * K",
      "            p_v += (-1 if REVERSE else 1) * H * V",
      "            p_a += (-1 if REVERSE else 1) * H * K",
      "            p_kk += (-1 if REVERSE else 1) * H * K",
      "            p_o += (-1 if REVERSE else 1) * H * V",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_nh * K * V + o_k[None, :] * V + o_v[:, None]",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask_h)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/rwkv7/286.py"
  },
  {
    "name": "parallel_simple_gla_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'NV': lambda args: triton.cdiv(args['V'], args['BV']), 'OUTPUT_ATTENTIONS': lambda args: args['attn'] is not None, 'USE_G': lambda args: args['g'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8, 16] for num_stages in [2, 3, 4]], key=['BT', 'BS', 'BK', 'BV', 'USE_G'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "attn",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "OUTPUT_ATTENTIONS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    o,",
      "    attn,",
      "    scale,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NV: tl.constexpr,",
      "    OUTPUT_ATTENTIONS: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    i_kv, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_k, i_v = i_kv // NV, i_kv % NV",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    o += i_k * B * T * H * V",
      "",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    o += (bos * H + i_h) * V",
      "    if USE_G:",
      "        g += bos * H + i_h",
      "    if OUTPUT_ATTENTIONS:",
      "        attn += (bos * H + i_h * T) * T + i_k * B * H * T * T",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_g = H",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "",
      "    o_k = i_t * BT + tl.arange(0, BS)",
      "",
      "    if USE_G:",
      "        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "",
      "        b_gq = tl.load(p_gq, boundary_check=(0,)).to(tl.float32)",
      "",
      "    else:",
      "        b_gq = None",
      "",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        m_s = o_q[:, None] >= o_k[None, :]",
      "        b_s = tl.dot(b_q, b_k)",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gk = tl.load(p_gk, boundary_check=(0,))",
      "            b_s *= safe_exp(b_gq[:, None] - b_gk[None, :])",
      "            b_s = tl.where(m_s, b_s, 0)",
      "        else:",
      "            b_s = tl.where(m_s, b_s, 0)",
      "",
      "        if i_s >= 0:",
      "            b_o += tl.dot(b_s.to(b_q.dtype), b_v)",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0)",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "        o_k += BS",
      "",
      "    for i_s in range(i_t * BT - BS, -BS, -BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (K, T), (1, stride_qk), (i_k * BK, i_s), (BK, BS), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_s = tl.dot(b_q, b_k)",
      "        if USE_G:",
      "            p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_g = tl.load(p_g, boundary_check=(0,))",
      "            b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)",
      "            b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0",
      "",
      "            b_s *= safe_exp(b_gq[:, None] + (b_gn - b_g)[None, :])",
      "            b_gq += b_gn - b_gp",
      "        if OUTPUT_ATTENTIONS:",
      "            p_a = tl.make_block_ptr(",
      "                attn, (T, T), (T, 1), (i_t * BT, i_s), (BT, BS), (1, 0)",
      "            )",
      "            tl.store(p_a, b_s.to(p_a.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_s >= 0:",
      "            b_o += tl.dot(b_s.to(b_v.dtype), b_v)",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/simple_gla/287.py"
  },
  {
    "name": "parallel_simple_gla_bwd_kernel_dq",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_t",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_vo",
        "annotation": null
      },
      {
        "name": "stride_g",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_bwd_kernel_dq(",
      "    i_t,",
      "    i_k,",
      "    i_v,",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dq,",
      "    dg,",
      "    stride_qk,",
      "    stride_vo,",
      "    stride_g,",
      "    scale,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "    p_do = tl.make_block_ptr(",
      "        do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "    for i_s in range(0, i_t * BT, BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v)",
      "        if USE_G:",
      "            p_g = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_g = tl.load(p_g, boundary_check=(0,))",
      "            b_gn = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)",
      "            b_gp = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0",
      "            b_ds *= safe_exp(b_gn - b_g)[None, :]",
      "            if i_s > 0:",
      "                b_dq *= safe_exp(b_gn - b_gp)",
      "",
      "        b_dq += tl.dot(b_ds.to(b_v.dtype), b_k)",
      "",
      "    if USE_G:",
      "        p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "",
      "        b_gq = tl.load(p_gq, boundary_check=(0,))",
      "",
      "        b_dq *= safe_exp(b_gq)[:, None]",
      "",
      "    o_q = i_t * BT + tl.arange(0, BT)",
      "",
      "    o_k = i_t * BT + tl.arange(0, BS)",
      "",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v, (V, T), (1, stride_vo), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_do, b_v)",
      "        if USE_G:",
      "            p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gk = tl.load(p_gk, boundary_check=(0,))",
      "            b_ds *= safe_exp(b_gq[:, None] - b_gk[None, :])",
      "        b_ds = tl.where(o_q[:, None] >= o_k[None, :], b_ds, 0)",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), b_k)",
      "        o_k += BS",
      "",
      "    b_dq *= scale",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_dg = tl.sum(b_dq * b_q, 1)",
      "        p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/simple_gla/287.py"
  },
  {
    "name": "parallel_simple_gla_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "i_t",
        "annotation": null
      },
      {
        "name": "i_k",
        "annotation": null
      },
      {
        "name": "i_v",
        "annotation": null
      },
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_vo",
        "annotation": null
      },
      {
        "name": "stride_g",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_G",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_simple_gla_bwd_kernel_dkv(",
      "    i_t,",
      "    i_k,",
      "    i_v,",
      "    q,",
      "    k,",
      "    v,",
      "    g,",
      "    do,",
      "    dk,",
      "    dv,",
      "    dg,",
      "    scale,",
      "    stride_qk,",
      "    stride_vo,",
      "    stride_g,",
      "    T,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_G: tl.constexpr,",
      "):",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BT, BV], dtype=tl.float32)",
      "    if USE_G:",
      "        p_gk = tl.make_block_ptr(g, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "        b_gk = tl.load(p_gk, boundary_check=(0,))",
      "    NTS = tl.cdiv(T, BS)",
      "",
      "    for i_s in range(NTS * BS - BS, (i_t + 1) * BT - BS, -BS):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_ds = tl.dot(b_v, tl.trans(b_do))",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gq = tl.load(p_gq, boundary_check=(0,))",
      "            b_gp = tl.load(g + (min(i_s + BS, T) - 1) * stride_g)",
      "            b_gn = tl.load(g + (i_s - 1) * stride_g) if i_s % BT > 0 else 0.0",
      "            if i_s >= 0:",
      "                tmp = safe_exp(b_gp - b_gn)",
      "                b_dk *= tmp",
      "                b_dv *= tmp",
      "                tmp2 = safe_exp(b_gq - b_gn)",
      "                b_ds *= tmp2[None, :]",
      "                b_s *= tmp2[None, :]",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "        b_dv += tl.dot(b_s.to(b_do.dtype), b_do)",
      "",
      "    if USE_G:",
      "        b_g_last = tl.load(g + (min(i_t * BT + BT, T) - 1) * stride_g)",
      "        if i_t >= 0:",
      "            tmp2 = safe_exp(b_g_last - b_gk)[:, None]",
      "            b_dk *= tmp2",
      "            b_dv *= tmp2",
      "",
      "    o_q = i_t * BT + tl.arange(0, BS)",
      "    o_k = i_t * BT + tl.arange(0, BT)",
      "    for i_s in range(i_t * BT, min((i_t + 1) * BT, T), BS):",
      "        p_q = tl.make_block_ptr(",
      "            q, (T, K), (stride_qk, 1), (i_s, i_k * BK), (BS, BK), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_ds = tl.dot(b_v, tl.trans(b_do))",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        if USE_G:",
      "            p_gq = tl.make_block_ptr(g, (T,), (stride_g,), (i_s,), (BS,), (0,))",
      "            b_gq = tl.load(p_gq, boundary_check=(0,))",
      "            if i_s >= 0:",
      "                tmp = safe_exp(-b_gk[:, None] + b_gq[None, :])",
      "                b_ds *= tmp",
      "                b_s *= tmp",
      "        m_s = o_k[:, None] <= o_q[None, :]",
      "        b_s = tl.where(m_s, b_s, 0)",
      "        b_ds = tl.where(m_s, b_ds, 0)",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "        b_dv += tl.dot(b_s.to(b_do.dtype), b_do)",
      "        o_q += BS",
      "    b_dk *= scale",
      "    b_dv *= scale",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_G:",
      "        p_dg = tl.make_block_ptr(dg, (T,), (stride_g,), (i_t * BT,), (BT,), (0,))",
      "        b_dg = tl.load(p_dg, boundary_check=(0,))",
      "        b_dg -= tl.sum(b_dk * b_k, 1)",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/simple_gla/287.py"
  },
  {
    "name": "chunk_ttt_linear_fwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hb",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "hbt",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    v_new,",
      "    eta,",
      "    w,",
      "    b,",
      "    eps,",
      "    h,",
      "    hb,",
      "    h0,",
      "    hb0,",
      "    ht,",
      "    hbt,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    offs = tl.arange(0, BV)",
      "    b_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_hb = tl.make_block_ptr(",
      "            hb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_hb, b_hb.to(p_hb.dtype.element_ty), boundary_check=(0,))",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_eta_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "        b_eta_last = tl.load(p_eta_last)",
      "        b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/288.py"
  },
  {
    "name": "chunk_ttt_linear_fwd_kernel_o",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "hb",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    eta,",
      "    h,",
      "    hb,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    eta += bos * H + i_h",
      "    o += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    hb += (i_tg * H + i_h) * V",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_eta = H",
      "",
      "    p_q = tl.make_block_ptr(q, (T, K), (stride_qk, 1), (i_t * BT, 0), (BT, BK), (1, 0))",
      "    p_k = tl.make_block_ptr(k, (K, T), (1, stride_qk), (0, i_t * BT), (BK, BT), (0, 1))",
      "    p_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))",
      "    p_h = tl.make_block_ptr(h, (K, V), (V, 1), (0, i_v * BV), (BK, BV), (1, 0))",
      "    p_hb = tl.make_block_ptr(hb, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    b_eta = tl.load(p_eta, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    b_hb = tl.load(p_hb, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    b_o = tl.dot(b_q, b_h, allow_tf32=False)",
      "",
      "    b_A = tl.dot(b_q, b_k, allow_tf32=False)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_A = tl.where(m_A, b_A, 0)",
      "    b_Ae = tl.where(m_A, b_eta[:, None], 0.0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "    b_o = (",
      "        b_o - tl.dot(b_eta[:, None] * b_A.to(b_v.dtype), b_v, allow_tf32=False)",
      "    ) * scale",
      "    b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v.dtype), b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/288.py"
  },
  {
    "name": "chunk_ttt_linear_bwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_bwd_kernel_h(",
      "    k,",
      "    v,",
      "    v_new,",
      "    eta,",
      "    w,",
      "    b,",
      "    eps,",
      "    h,",
      "    h0,",
      "    hb0,",
      "    x,",
      "    y,",
      "    r,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    offs = tl.arange(0, BV)",
      "    b_w = tl.load(w + i_h * V + offs, mask=offs < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + offs, mask=offs < V, other=0.0)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_eta_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((offs < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((offs < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((offs < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_v_new, b_v2.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "        b_eta_last = tl.load(p_eta_last)",
      "        b_h = b_h - tl.dot(b_eta_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_eta_last * b_v2.to(b_k.dtype), axis=0)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/288.py"
  },
  {
    "name": "chunk_ttt_linear_bwd_kernel_dv_local",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [4]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_bwd_kernel_dv_local(",
      "    q,",
      "    k,",
      "    eta,",
      "    do,",
      "    dv,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    eta += bos * H + i_h",
      "    do += (bos * H + i_h) * V",
      "    dv += (bos * H + i_h) * V",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_eta = H",
      "",
      "    b_A = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_k = tl.make_block_ptr(",
      "            k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q, (K, T), (1, stride_qk), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_A += tl.dot(b_k, b_q)",
      "",
      "    p_eta = tl.make_block_ptr(eta, (T,), (stride_eta,), (i_t * BT,), (BT,), (0,))",
      "    b_eta = tl.load(p_eta, boundary_check=(0,))",
      "    mask = tl.arange(0, BT)[:, None] <= tl.arange(0, BT)[None, :]",
      "    b_A = -tl.where(mask, b_A * scale * b_eta[None, :], 0).to(do.dtype.element_ty)",
      "    b_Ae = -tl.where(mask, b_eta[None, :], 0).to(do.dtype.element_ty)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dv = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/288.py"
  },
  {
    "name": "chunk_ttt_linear_bwd_kernel_norm",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'USE_FINAL_STATE_GRADIENT_B': lambda args: args['dhbt'] is not None, 'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['dhb0'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [2, 4, 8, 16]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dhbt",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "dhb0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dhb",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "dv_new",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_ttt_linear_bwd_kernel_norm(",
      "    q,",
      "    k,",
      "    v,",
      "    v_new,",
      "    x,",
      "    y,",
      "    r,",
      "    w,",
      "    b,",
      "    eta,",
      "    h,",
      "    dht,",
      "    dhbt,",
      "    dh0,",
      "    dhb0,",
      "    do,",
      "    dh,",
      "    dhb,",
      "    dv,",
      "    dv_new,",
      "    dk,",
      "    dw,",
      "    db,",
      "    cu_seqlens,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT_B: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_v, i_nh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "        boh = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "        boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_dhb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option=\"zero\")",
      "    if USE_FINAL_STATE_GRADIENT_B:",
      "        p_dhbt = tl.make_block_ptr(",
      "            dhbt + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    offs_v = tl.arange(0, BV)",
      "    offs_t = tl.arange(0, BT)",
      "    b_w = tl.load(w + i_h * V + offs_v, mask=offs_v < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + offs_v, mask=offs_v < V, other=0.0)",
      "    b_dw = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_w.dtype,",
      "    )",
      "    b_db = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_b.dtype,",
      "    )",
      "    p_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "    p_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (i_v * BV, i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dhb = tl.make_block_ptr(",
      "            dhb + ((boh + i_t) * H + i_h) * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dhb, b_dhb.to(p_dhb.dtype.element_ty), boundary_check=(0,))",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K,",
      "            (K, T),",
      "            (1, H * K),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_v_new = tl.make_block_ptr(",
      "            v_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv_new = tl.make_block_ptr(",
      "            dv_new + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_eta_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_dv_new = tl.load(p_dv_new, boundary_check=(0, 1), padding_option=\"zero\").to(",
      "            b_k.dtype",
      "        )",
      "        b_eta_last = tl.load(p_eta_last)",
      "        b_dv_new -= tl.dot(b_eta_last * b_k, b_dh.to(b_k.dtype))",
      "        b_dv_new -= b_eta_last * b_dhb.to(b_k.dtype)[None, :]",
      "",
      "        b_v_new = tl.load(p_v_new, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_x = tl.load(p_x, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_y = tl.load(p_y, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option=\"zero\").to(",
      "            tl.float32",
      "        )",
      "        b_dy = (",
      "            b_rstd",
      "            * (",
      "                b_dv_new * V",
      "                - tl.sum(b_dv_new, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dx = (",
      "            -b_rstd",
      "            * (",
      "                b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True)",
      "                + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_drstd = tl.sum(",
      "            b_dv_new.to(b_rstd.dtype) * b_v_new.to(b_rstd.dtype) / b_rstd,",
      "            axis=1,",
      "            keep_dims=True,",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_w = b_w.to(b_k.dtype)",
      "        b_b = b_b.to(b_k.dtype)",
      "        b_dv = -b_w * b_dy.to(b_k.dtype)",
      "        b_dk = b_w * b_dy.to(b_k.dtype)",
      "        b_dw += tl.sum(",
      "            2 * b_w * b_x * b_dy.to(b_k.dtype)",
      "            + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype),",
      "            axis=0,",
      "        ).to(b_dw.dtype)",
      "        b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)",
      "        b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_dkh = (",
      "            b_rstd",
      "            * (",
      "                V * b_dx",
      "                - tl.sum(b_dx, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V",
      "        b_dkh = tl.where(",
      "            (offs_v < V)[None, :] * (offs_t < T - i_t * BT)[:, None], b_dkh, 0.0",
      "        )",
      "        b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)",
      "        b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(",
      "            tl.trans(b_k).to(b_dkh.dtype), b_dkh",
      "        )",
      "        b_dhb += tl.sum(b_do + b_dkh, axis=0)",
      "        b_dh = tl.where((offs_v < V)[None, :], b_dh, 0.0)",
      "        b_dhb = tl.where((offs_v < V), b_dhb, 0.0)",
      "",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_INITIAL_STATE_B:",
      "        p_dhb0 = tl.make_block_ptr(",
      "            dhb0 + i_nh * V, (V,), (1,), (i_v * BV,), (BV,), (0,)",
      "        )",
      "        tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/288.py"
  },
  {
    "name": "chunk_bwd_kernel_dqke",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3]], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "e",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dhb",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "de",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_bwd_kernel_dqke(",
      "    q,",
      "    k,",
      "    v,",
      "    e,",
      "    h,",
      "    do,",
      "    dh,",
      "    dhb,",
      "    dq,",
      "    dk,",
      "    de,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    v += (bos * H + i_h) * V",
      "    do += (bos * H + i_h) * V",
      "    h += (i_tg * H + i_h) * K * V",
      "    dh += (i_tg * H + i_h) * K * V",
      "    dhb += (i_tg * H + i_h) * V",
      "    q += (bos * H + i_h) * K",
      "    k += (bos * H + i_h) * K",
      "    dq += (bos * H + i_h) * K",
      "    dk += (bos * H + i_h) * K",
      "    e += bos * H + i_h",
      "    de += bos * H + i_h",
      "    stride_qk = H * K",
      "    stride_vo = H * V",
      "    stride_e = H",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_de = tl.zeros(",
      "        [",
      "            BT,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    p_e_last = (",
      "        (e + (i_t * BT + BT - 1) * stride_e)",
      "        if (i_t * BT + BT) <= T",
      "        else (e + (T - 1) * stride_e)",
      "    )",
      "    i_last = (BT - 1) if (i_t * BT + BT) <= T else (T % BT - 1)",
      "    mask = tl.arange(0, BT) == i_last",
      "    b_e_last = tl.load(p_e_last)",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do, (T, V), (stride_vo, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh, (V, K), (1, V), (i_v * BV, i_k * BK), (BV, BK), (0, 1)",
      "        )",
      "        p_dhb = tl.make_block_ptr(dhb, (V,), (1,), (i_v * BV,), (BV,), (0,))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dhb = tl.load(p_dhb, boundary_check=(0,))",
      "",
      "        b_ds += tl.dot(b_do, tl.trans(b_v))",
      "",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "",
      "        b_dk -= b_e_last * tl.dot(b_v, b_dh.to(b_v.dtype))",
      "        b_de -= mask * tl.sum(tl.trans(b_dh) * tl.dot(tl.trans(b_k), b_v.to(b_k.dtype)))",
      "        b_de -= mask * tl.sum(b_dhb * tl.sum(b_v, axis=0).to(b_k.dtype))",
      "",
      "    o_i = tl.arange(0, BT)",
      "    p_q = tl.make_block_ptr(",
      "        q, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_e = tl.make_block_ptr(e, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_e = tl.load(p_e, boundary_check=(0,))",
      "",
      "    p_dq = tl.make_block_ptr(",
      "        dq, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk, (T, K), (stride_qk, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_de = tl.make_block_ptr(de, (T,), (stride_e,), (i_t * BT,), (BT,), (0,))",
      "",
      "    b_ds = tl.where(o_i[:, None] >= o_i[None, :], b_ds, 0)",
      "    b_ds = b_ds.to(b_k.dtype)",
      "    b_dq -= tl.dot(b_ds, b_k) * b_e[:, None]",
      "    b_dk -= tl.dot(tl.trans(b_ds), b_q * b_e[:, None]) * scale",
      "    b_de -= tl.sum(scale * tl.dot(b_ds, b_k) * b_q, axis=1)",
      "    b_de -= tl.sum(b_ds, axis=1)",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/288.py"
  },
  {
    "name": "fused_chunk_ttt_linear_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None, 'STORE_FINAL_STATE': lambda args: args['ht'] is not None, 'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "hbt",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_ttt_linear_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    eta,",
      "    w,",
      "    b,",
      "    o,",
      "    scale,",
      "    eps,",
      "    h0,",
      "    hb0,",
      "    ht,",
      "    hbt,",
      "    cu_seqlens,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "        NT = tl.cdiv(T, BT)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    v_i = tl.arange(0, BV)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_e = tl.make_block_ptr(",
      "            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_e_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")",
      "        b_q = (b_q * scale).to(b_k.dtype)",
      "",
      "        b_A = tl.dot(b_q, b_k, allow_tf32=False)",
      "        b_A = tl.where(m_A, b_A, 0)",
      "        b_Ae = tl.where(m_A, b_e[:, None], 0.0)",
      "",
      "        b_o = -tl.dot(b_e[:, None] * b_A.to(b_v2.dtype), b_v2, allow_tf32=False)",
      "        b_o += b_hb[None, :] - tl.dot(b_Ae.to(b_v2.dtype), b_v2, allow_tf32=False)",
      "        b_o += tl.dot(b_q, b_h.to(b_q.dtype), allow_tf32=False)",
      "        b_e_last = tl.load(p_e_last)",
      "        b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)",
      "        b_h = tl.where((v_i < V)[None, :], b_h, 0.0)",
      "        b_hb = tl.where((v_i < V), b_hb, 0.0)",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        p_hbt = tl.make_block_ptr(hbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_hbt, b_hb.to(p_hbt.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/289.py"
  },
  {
    "name": "fused_chunk_ttt_linear_bwd_kernel_h",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['h0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['hb0'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v2",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "hb0",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_ttt_linear_bwd_kernel_h(",
      "    k,",
      "    v,",
      "    v2,",
      "    x,",
      "    y,",
      "    r,",
      "    w,",
      "    b,",
      "    eta,",
      "    h0,",
      "    hb0,",
      "    h,",
      "    do,",
      "    dq,",
      "    scale,",
      "    eps,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    bos, _ = i_n * T, i_n * T + T",
      "    NT = tl.cdiv(T, BT)",
      "    boh = i_n * NT",
      "",
      "    o_i = tl.arange(0, BT)",
      "    v_i = tl.arange(0, BV)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_hb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1), padding_option=\"zero\").to(tl.float32)",
      "    if USE_INITIAL_STATE_B:",
      "        p_hb0 = tl.make_block_ptr(hb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        b_hb = tl.load(p_hb0, boundary_check=(0,), padding_option=\"zero\").to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (0, 0),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_v2 = tl.make_block_ptr(",
      "            v2 + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_e = tl.make_block_ptr(",
      "            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_dq = tl.make_block_ptr(",
      "            dq + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_e_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_kh = (",
      "            tl.dot(tl.trans(b_k), b_h.to(b_k.dtype), allow_tf32=False).to(tl.float32)",
      "            + b_hb[None, :]",
      "        )",
      "        b_kh = tl.where((v_i < V)[None, :], b_kh, 0.0)",
      "        mean = tl.sum(b_kh, axis=1, keep_dims=True) / V",
      "        xbar = tl.where((v_i < V)[None, :], b_kh - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=1, keep_dims=True) / V",
      "        rstd = 1 / tl.sqrt(var.to(tl.float32) + eps)",
      "        b_kh_hat = (b_kh - mean) * rstd",
      "",
      "        b_v = (",
      "            b_kh_hat.to(b_k.dtype) * b_w[None, :].to(b_k.dtype)",
      "            + b_b[None, :].to(b_k.dtype)",
      "            - b_v.to(b_k.dtype)",
      "            + tl.trans(b_k)",
      "        )",
      "        b_v = tl.where((v_i < V)[None, :], b_v * b_w[None, :].to(b_k.dtype), 0.0)",
      "        b_v2 = (",
      "            rstd",
      "            * (",
      "                V * b_v",
      "                - tl.sum(b_v, axis=1, keep_dims=True)",
      "                - b_kh_hat.to(b_k.dtype)",
      "                * tl.sum(b_v * b_kh_hat.to(b_k.dtype), axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        tl.store(p_x, b_kh_hat.to(p_x.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_y, b_v.to(p_y.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_r, rstd.to(p_r.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_v2, b_v2.to(p_v2.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")",
      "        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        b_v2 = tl.where((v_i < V)[None, :], b_v2, 0.0)",
      "        b_ds = tl.dot(b_do, tl.trans(b_v2).to(b_do.dtype))",
      "        b_ds = tl.where(m_A, b_ds, 0)",
      "        b_ds = b_ds.to(b_k.dtype)",
      "        b_dq = tl.dot(b_do, tl.trans(b_h).to(b_do.dtype))",
      "        b_dq -= tl.dot(b_ds, tl.trans(b_k)) * b_e[:, None]",
      "        b_dq *= scale",
      "",
      "        b_e_last = tl.load(p_e_last)",
      "        b_h = b_h - tl.dot(b_e_last * b_k, b_v2.to(b_k.dtype), allow_tf32=False)",
      "        b_hb = b_hb - tl.sum(b_e_last * b_v2.to(b_k.dtype), axis=0)",
      "        b_h = tl.where((v_i < V)[None, :], b_h, 0.0)",
      "        b_hb = tl.where((v_i < V), b_hb, 0.0)",
      "        tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/289.py"
  },
  {
    "name": "fused_chunk_ttt_linear_bwd_kernel_dh",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_INITIAL_STATE': lambda args: args['dh0'] is not None, 'USE_INITIAL_STATE_B': lambda args: args['dhb0'] is not None, 'USE_FINAL_STATE_GRADIENT': lambda args: args['dht'] is not None, 'USE_FINAL_STATE_GRADIENT_B': lambda args: args['dhbt'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in NUM_WARPS], key=['BT', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "v2",
        "annotation": null
      },
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "w",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "eta",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "dht",
        "annotation": null
      },
      {
        "name": "dhbt",
        "annotation": null
      },
      {
        "name": "dh0",
        "annotation": null
      },
      {
        "name": "dhb0",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "de",
        "annotation": null
      },
      {
        "name": "dw",
        "annotation": null
      },
      {
        "name": "db",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE_B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_FINAL_STATE_GRADIENT_B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_chunk_ttt_linear_bwd_kernel_dh(",
      "    q,",
      "    k,",
      "    v,",
      "    v2,",
      "    x,",
      "    y,",
      "    r,",
      "    w,",
      "    b,",
      "    eta,",
      "    h,",
      "    dht,",
      "    dhbt,",
      "    dh0,",
      "    dhb0,",
      "    do,",
      "    dk,",
      "    dv,",
      "    de,",
      "    dw,",
      "    db,",
      "    scale,",
      "    T,",
      "    H: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    USE_INITIAL_STATE_B: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT: tl.constexpr,",
      "    USE_FINAL_STATE_GRADIENT_B: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    bos, _ = i_n * T, i_n * T + T",
      "    NT = tl.cdiv(T, BT)",
      "    boh = i_n * NT",
      "",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    b_dhb = tl.zeros([BV], dtype=tl.float32)",
      "    if USE_FINAL_STATE_GRADIENT:",
      "        p_dht = tl.make_block_ptr(",
      "            dht + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        b_dh += tl.load(p_dht, boundary_check=(0, 1), padding_option=\"zero\")",
      "    if USE_FINAL_STATE_GRADIENT_B:",
      "        p_dhbt = tl.make_block_ptr(dhbt + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        b_dhb += tl.load(p_dhbt, boundary_check=(0,), padding_option=\"zero\")",
      "",
      "    o_i = tl.arange(0, BT)",
      "    v_i = tl.arange(0, BV)",
      "    m_A = o_i[:, None] >= o_i[None, :]",
      "    m_A_t = o_i[:, None] <= o_i[None, :]",
      "    b_w = tl.load(w + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_b = tl.load(b + i_h * V + v_i, mask=v_i < V, other=0.0)",
      "    b_dw = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_w.dtype,",
      "    )",
      "    b_db = tl.zeros(",
      "        [",
      "            BV,",
      "        ],",
      "        dtype=b_b.dtype,",
      "    )",
      "    p_dw = tl.make_block_ptr(dw + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "    p_db = tl.make_block_ptr(db + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_h = tl.make_block_ptr(",
      "            h + ((boh + i_t) * H + i_h) * K * V,",
      "            (V, K),",
      "            (1, V),",
      "            (0, 0),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos * H + i_h) * K, (K, T), (1, H * K), (0, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_t * BT, 0), (BT, BK), (1, 0)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_v2 = tl.make_block_ptr(",
      "            v2 + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_x = tl.make_block_ptr(",
      "            x + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_y = tl.make_block_ptr(",
      "            y + (bos * H + i_h) * V, (T, V), (H * V, 1), (i_t * BT, 0), (BT, BV), (1, 0)",
      "        )",
      "        p_r = tl.make_block_ptr(",
      "            r + bos * H + i_h, (T, 1), (H, 1), (i_t * BT, 0), (BT, 1), (1, 0)",
      "        )",
      "        p_e = tl.make_block_ptr(",
      "            eta + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dk = tl.make_block_ptr(",
      "            dk + (bos * H + i_h) * K,",
      "            (T, K),",
      "            (H * K, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos * H + i_h) * V,",
      "            (T, V),",
      "            (H * V, 1),",
      "            (i_t * BT, 0),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_de = tl.make_block_ptr(",
      "            de + (bos * H + i_h), (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_e_last = (",
      "            eta + bos * H + i_h + (T - 1) * H",
      "            if i_t == NT - 1",
      "            else eta + bos * H + i_h + (i_t * BT + BT - 1) * H",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_k = tl.load(p_k, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_e = tl.load(p_e, boundary_check=(0,), padding_option=\"zero\")",
      "        b_do = tl.load(p_do, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_e_last = tl.load(p_e_last)",
      "        b_A = tl.dot(b_k, b_q)",
      "        b_A = -tl.where(m_A_t, b_A * scale * b_e[None, :], 0).to(do.dtype.element_ty)",
      "        b_Ae = -tl.where(m_A_t, b_e[None, :], 0).to(do.dtype.element_ty)",
      "        b_dv_new = tl.dot(b_A.to(b_do.dtype), b_do) + tl.dot(b_Ae.to(b_do.dtype), b_do)",
      "        b_dv_new -= tl.dot(b_e_last * b_k, b_dh.to(b_k.dtype))",
      "        b_dv_new -= b_e_last * b_dhb.to(b_k.dtype)[None, :]",
      "",
      "        b_v2 = tl.load(p_v2, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_x = tl.load(p_x, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_y = tl.load(p_y, boundary_check=(0, 1), padding_option=\"zero\").to(b_k.dtype)",
      "        b_rstd = tl.load(p_r, boundary_check=(0, 1), padding_option=\"zero\").to(",
      "            tl.float32",
      "        )",
      "        b_dy = (",
      "            b_rstd",
      "            * (",
      "                b_dv_new * V",
      "                - tl.sum(b_dv_new, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dx = (",
      "            -b_rstd",
      "            * (",
      "                b_dv_new * tl.sum(b_x * b_y, axis=1, keep_dims=True)",
      "                + b_y * tl.sum(b_dv_new * b_x, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_drstd = tl.sum(",
      "            b_dv_new.to(b_rstd.dtype) * b_v2.to(b_rstd.dtype) / b_rstd,",
      "            axis=1,",
      "            keep_dims=True,",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_w = b_w.to(b_k.dtype)",
      "        b_b = b_b.to(b_k.dtype)",
      "        b_dv = -b_w * b_dy.to(b_k.dtype)",
      "        b_dk = b_w * b_dy.to(b_k.dtype)",
      "        b_dw += tl.sum(",
      "            2 * b_w * b_x * b_dy.to(b_k.dtype)",
      "            + (b_b - b_v.to(b_k.dtype) + b_k) * b_dy.to(b_k.dtype),",
      "            axis=0,",
      "        ).to(b_dw.dtype)",
      "        b_db += tl.sum(b_w * b_dy.to(b_k.dtype), axis=0).to(b_db.dtype)",
      "        b_dx = b_dx.to(b_k.dtype) + b_w * b_w * b_dy.to(b_k.dtype)",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1), padding_option=\"zero\")",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_dkh = (",
      "            b_rstd",
      "            * (",
      "                V * b_dx",
      "                - tl.sum(b_dx, axis=1, keep_dims=True)",
      "                - b_x * tl.sum(b_x * b_dx, axis=1, keep_dims=True)",
      "            )",
      "            / V",
      "        )",
      "        b_dkh -= b_rstd * b_rstd * b_drstd * b_x / V",
      "        b_dkh = tl.where((v_i < V)[None, :] * (o_i < T - i_t * BT)[:, None], b_dkh, 0.0)",
      "        b_dk += tl.dot(b_dkh, b_h.to(b_dkh.dtype)).to(b_k.dtype)",
      "",
      "        b_ds = tl.dot(b_do, tl.trans(b_v2))",
      "        b_ds = tl.where(m_A, b_ds, 0)",
      "        b_ds = b_ds.to(b_k.dtype)",
      "        i_last = (BT - 1) if (i_t * BT + BT) <= T else (T % BT - 1)",
      "        mask = o_i == i_last",
      "        b_dk -= b_e_last * tl.dot(b_v2, tl.trans(b_dh).to(b_v2.dtype))",
      "        b_dk -= tl.dot(tl.trans(b_ds), tl.trans(b_q) * b_e[:, None])",
      "        b_de = mask * tl.sum(-b_dh * tl.trans(tl.dot(tl.trans(b_v2), b_k))).to(",
      "            b_k.dtype",
      "        )",
      "        b_de -= mask * tl.sum(b_dhb * tl.sum(b_v2, axis=0)).to(b_k.dtype)",
      "        b_de -= tl.sum(tl.dot(b_ds, b_k) * tl.trans(b_q).to(b_k.dtype), axis=1)",
      "        b_de -= tl.sum(b_ds, axis=1)",
      "        b_dh += tl.dot(b_q, b_do.to(b_q.dtype)) + tl.dot(",
      "            tl.trans(b_k).to(b_dkh.dtype), b_dkh",
      "        )",
      "        b_dhb += tl.sum(b_do + b_dkh, axis=0)",
      "        b_dh = tl.where((v_i < V)[None, :], b_dh, 0.0)",
      "        b_dhb = tl.where((v_i < V), b_dhb, 0.0)",
      "",
      "        tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "        tl.store(p_de, b_de.to(p_de.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_dw, b_dw.to(p_dw.dtype.element_ty), boundary_check=(0,))",
      "    tl.store(p_db, b_db.to(p_db.dtype.element_ty), boundary_check=(0,))",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_dh0 = tl.make_block_ptr(",
      "            dh0 + i_nh * K * V, (K, V), (V, 1), (0, 0), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_dh0, b_dh.to(p_dh0.dtype.element_ty), boundary_check=(0, 1))",
      "    if USE_INITIAL_STATE_B:",
      "        p_dhb0 = tl.make_block_ptr(dhb0 + i_nh * V, (V,), (1,), (0,), (BV,), (0,))",
      "        tl.store(p_dhb0, b_dhb.to(p_dhb0.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/ttt/289.py"
  },
  {
    "name": "chunk_local_cumsum_scalar_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8]], key=['B', 'H', 'BT', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_local_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    if HEAD_FIRST:",
      "        p_s = tl.make_block_ptr(",
      "            s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "        )",
      "    else:",
      "        p_s = tl.make_block_ptr(s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "        p_o = tl.make_block_ptr(o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,))",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)",
      "    b_o = tl.cumsum(b_s, axis=0)",
      "    if REVERSE:",
      "        b_z = tl.sum(b_s, axis=0)",
      "        b_o = -b_o + b_z[None] + b_s",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/290.py"
  },
  {
    "name": "chunk_local_cumsum_vector_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BS': BS}, num_warps=num_warps) for BS in BS_LIST for num_warps in [2, 4, 8]], key=['B', 'H', 'S', 'BT', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_local_cumsum_vector_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    o_i = tl.arange(0, BT)",
      "    if REVERSE:",
      "        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "    else:",
      "        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "",
      "    if HEAD_FIRST:",
      "        p_s = tl.make_block_ptr(",
      "            s + (bos * H + i_h * T) * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (bos * H + i_h * T) * S,",
      "            (T, S),",
      "            (S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "    else:",
      "        p_s = tl.make_block_ptr(",
      "            s + (bos * H + i_h) * S,",
      "            (T, S),",
      "            (H * S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "        p_o = tl.make_block_ptr(",
      "            o + (bos * H + i_h) * S,",
      "            (T, S),",
      "            (H * S, 1),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "    b_o = tl.dot(m_s, b_s, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/290.py"
  },
  {
    "name": "chunk_global_cumsum_scalar_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [32, 64, 128, 256] for num_warps in [2, 4, 8] for num_stages in [1, 2, 3, 4]], key=['B', 'H', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    cu_seqlens,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_nh = tl.program_id(0)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    T = eos - bos",
      "",
      "    b_z = tl.zeros([], dtype=tl.float32)",
      "    NT = tl.cdiv(T, BT)",
      "    for i_c in range(NT):",
      "        i_t = NT - 1 - i_c if REVERSE else i_c",
      "        if HEAD_FIRST:",
      "            p_s = tl.make_block_ptr(",
      "                s + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "            p_o = tl.make_block_ptr(",
      "                o + bos * H + i_h * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "        else:",
      "            p_s = tl.make_block_ptr(",
      "                s + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "            p_o = tl.make_block_ptr(",
      "                o + bos * H + i_h, (T,), (H,), (i_t * BT,), (BT,), (0,)",
      "            )",
      "        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)",
      "        b_o = tl.cumsum(b_s, axis=0)",
      "        b_ss = tl.sum(b_s, 0)",
      "        if REVERSE:",
      "            b_o = -b_o + b_ss + b_s",
      "        b_o += b_z",
      "        if i_c >= 0:",
      "            b_z += b_ss",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/290.py"
  },
  {
    "name": "chunk_global_cumsum_vector_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps, num_stages=num_stages) for BT in [16, 32, 64, 128] for num_warps in [2, 4, 8] for num_stages in [1, 2, 3, 4]], key=['B', 'H', 'S', 'IS_VARLEN', 'REVERSE'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_FIRST",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_cumsum_vector_kernel(",
      "    s,",
      "    z,",
      "    cu_seqlens,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "    HEAD_FIRST: tl.constexpr,",
      "):",
      "    i_s, i_nh = tl.program_id(0), tl.program_id(1)",
      "    i_n, i_h = i_nh // H, i_nh % H",
      "    if IS_VARLEN:",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "    else:",
      "        bos, eos = i_n * T, i_n * T + T",
      "    T = eos - bos",
      "",
      "    o_i = tl.arange(0, BT)",
      "    if REVERSE:",
      "        m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "    else:",
      "        m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "",
      "    b_z = tl.zeros([BS], dtype=tl.float32)",
      "    NT = tl.cdiv(T, BT)",
      "    for i_c in range(NT):",
      "        i_t = NT - 1 - i_c if REVERSE else i_c",
      "        if HEAD_FIRST:",
      "            p_s = tl.make_block_ptr(",
      "                s + (bos * H + i_h * T) * S,",
      "                (T, S),",
      "                (S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            p_z = tl.make_block_ptr(",
      "                z + (bos * H + i_h * T) * S,",
      "                (T, S),",
      "                (S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "        else:",
      "            p_s = tl.make_block_ptr(",
      "                s + (bos * H + i_h) * S,",
      "                (T, S),",
      "                (H * S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "            p_z = tl.make_block_ptr(",
      "                z + (bos * H + i_h) * S,",
      "                (T, S),",
      "                (H * S, 1),",
      "                (i_t * BT, i_s * BS),",
      "                (BT, BS),",
      "                (1, 0),",
      "            )",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)",
      "        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_c >= 0:",
      "            b_z += tl.sum(b_s, 0)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/290.py"
  },
  {
    "name": "prepare_position_ids_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [4, 8, 16, 32]], key=['B'])"
    ],
    "args": [
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def prepare_position_ids_kernel(y, cu_seqlens, B: tl.constexpr):",
      "    i_n = tl.program_id(0)",
      "    bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(cu_seqlens + i_n + 1).to(",
      "        tl.int32",
      "    )",
      "    T = eos - bos",
      "",
      "    o = tl.arange(0, B)",
      "    for i in range(0, tl.cdiv(T, B) * B, B):",
      "        o_i = o + i",
      "        tl.store(y + bos + o_i, o_i, o_i < T)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/291.py"
  },
  {
    "name": "logcumsumexp_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': BT}, num_warps=num_warps) for BT in [16, 32, 64] for num_warps in [2, 4, 8]], key=['S'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logcumsumexp_fwd_kernel(s, z, T, S: tl.constexpr, BT: tl.constexpr):",
      "    i_bh = tl.program_id(0)",
      "    o_i = tl.arange(0, BT)",
      "    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "",
      "    b_mp = tl.full(",
      "        [",
      "            S,",
      "        ],",
      "        float(\"-inf\"),",
      "        dtype=tl.float32,",
      "    )",
      "    b_zp = tl.zeros(",
      "        [",
      "            S,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    for i_t in range(tl.cdiv(T, BT)):",
      "        p_s = tl.make_block_ptr(",
      "            s + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0)",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * T * S, (T, S), (S, 1), (i_t * BT, 0), (BT, S), (1, 0)",
      "        )",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "        b_mc = tl.max(b_s, 0)",
      "        b_mc = tl.maximum(b_mp, b_mc)",
      "        b_zp = b_zp * exp(b_mp - b_mc)",
      "",
      "        b_s = exp(b_s - b_mc)",
      "        b_z = tl.dot(m_s, b_s, allow_tf32=False) + b_zp",
      "",
      "        b_zc = tl.max(b_z, 0)",
      "        b_mp = b_mc",
      "        b_zp = b_zc",
      "",
      "        b_z = log(tl.where(b_z != 0, b_z, 1e-20)) + b_mc",
      "        tl.store(p_z, b_z.to(p_z.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/292.py"
  },
  {
    "name": "logsumexp_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SCALE': lambda args: args['scale'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SCALE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_fwd_kernel(",
      "    x, z, scale, D: tl.constexpr, B: tl.constexpr, HAS_SCALE: tl.constexpr",
      "):",
      "    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    o_d = i_d * B + tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))",
      "    if HAS_SCALE:",
      "        b_x = b_x * scale",
      "    b_m = tl.max(b_x, 0)",
      "    b_z = log(tl.sum(exp(b_x - b_m), 0)) + b_m",
      "    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/293.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_ALPHA': lambda args: args['alpha'] is not None, 'HAS_BETA': lambda args: args['beta'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BM': 128, 'BK': 64, 'BN': 256, 'G': 4}, num_stages=3, num_warps=8), triton.Config({'BM': 64, 'BK': 32, 'BN': 256, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 128, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 128, 'BK': 32, 'BN': 32, 'G': 4}, num_stages=4, num_warps=4), triton.Config({'BM': 64, 'BK': 32, 'BN': 32, 'G': 4}, num_stages=5, num_warps=2), triton.Config({'BM': 32, 'BK': 32, 'BN': 64, 'G': 4}, num_stages=5, num_warps=2)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a",
        "annotation": null
      },
      {
        "name": "b",
        "annotation": null
      },
      {
        "name": "c",
        "annotation": null
      },
      {
        "name": "input",
        "annotation": null
      },
      {
        "name": "alpha",
        "annotation": null
      },
      {
        "name": "beta",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_ab",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cb",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_INPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ALPHA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BETA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ALLOW_TF32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "X_DIM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a,",
      "    b,",
      "    c,",
      "    input,",
      "    alpha,",
      "    beta,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_ab,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cb,",
      "    stride_cm,",
      "    stride_cn,",
      "    BM: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BN: tl.constexpr,",
      "    G: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    HAS_INPUT: tl.constexpr,",
      "    HAS_ALPHA: tl.constexpr,",
      "    HAS_BETA: tl.constexpr,",
      "    ALLOW_TF32: tl.constexpr,",
      "    X_DIM: tl.constexpr = 1,",
      "):",
      "",
      "    i_b, i_m, i_n = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    NM, NN = tl.num_programs(1), tl.num_programs(2)",
      "    i_m, i_n = tl.swizzle2d(i_m, i_n, NM, NN, G)",
      "",
      "    a_batch_ptr = a + i_b * stride_ab",
      "    o_am = (i_m * BM + tl.arange(0, BM)) % M",
      "    o_bn = (i_n * BN + tl.arange(0, BN)) % N",
      "    o_k = tl.arange(0, BK)",
      "",
      "    p_a = a_batch_ptr + (o_am[:, None] * stride_am + o_k[None, :] * stride_ak)",
      "    p_b = b + (o_k[:, None] * stride_bk + o_bn[None, :] * stride_bn)",
      "",
      "    b_acc = tl.zeros((BM, BN), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BK)):",
      "",
      "        b_a = tl.load(p_a, mask=o_k[None, :] < K - k * BK, other=0.0)",
      "        b_b = tl.load(p_b, mask=o_k[:, None] < K - k * BK, other=0.0)",
      "",
      "        b_acc = tl.dot(b_a, b_b, acc=b_acc, allow_tf32=ALLOW_TF32)",
      "",
      "        p_a += BK * stride_ak",
      "        p_b += BK * stride_bk",
      "",
      "    o_cm = i_m * BM + tl.arange(0, BM)",
      "    o_cn = i_n * BN + tl.arange(0, BN)",
      "    mask = (o_cm[:, None] < M) & (o_cn[None, :] < N)",
      "",
      "    b_c = b_acc",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        b_c = leaky_relu(b_c)",
      "    elif ACTIVATION == \"relu\":",
      "        b_c = relu(b_c)",
      "    elif ACTIVATION == \"sigmoid\":",
      "        b_c = sigmoid(b_c)",
      "    elif ACTIVATION == \"tanh\":",
      "        b_c = tanh(b_c)",
      "",
      "    if HAS_ALPHA:",
      "        b_c *= tl.load(alpha)",
      "",
      "    if HAS_INPUT:",
      "        p_i = (",
      "            input",
      "            + (stride_cm * o_cm[:, None] if X_DIM == 2 else 0)",
      "            + stride_cn * o_cn[None, :]",
      "        )",
      "        mask_p = (o_cn[None, :] < N) if X_DIM == 1 else mask",
      "        b_i = tl.load(p_i, mask=mask_p, other=0.0).to(tl.float32)",
      "        if HAS_BETA:",
      "            b_i *= tl.load(beta)",
      "        b_c += b_i",
      "",
      "    c_batch_ptr = c + i_b * stride_cb",
      "    p_c = c_batch_ptr + stride_cm * o_cm[:, None] + stride_cn * o_cn[None, :]",
      "    tl.store(p_c, b_c.to(c.dtype.element_ty), mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/294.py"
  },
  {
    "name": "packunpack_sequence_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [4, 8, 16, 32]], key=['D', 'PADDING_SIDE', 'PACK'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "y",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDING_SIDE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PACK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def packunpack_sequence_kernel(",
      "    x,",
      "    y,",
      "    cu_seqlens,",
      "    S,",
      "    D,",
      "    BD: tl.constexpr,",
      "    PADDING_SIDE: tl.constexpr,",
      "    PACK: tl.constexpr,",
      "):",
      "    i_d, i_s, i_b = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    bos, eos = tl.load(cu_seqlens + i_b), tl.load(cu_seqlens + i_b + 1)",
      "",
      "    T = eos - bos",
      "    if PADDING_SIDE == \"left\":",
      "        NP = S - T",
      "        if i_s < NP:",
      "            return",
      "        i_t = bos + (i_s - NP)",
      "    else:",
      "        if i_s >= T:",
      "            return",
      "        i_t = bos + i_s",
      "",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    if PACK:",
      "        b_x = tl.load(x + (i_b * S + i_s) * D + o_d, mask=mask)",
      "        tl.store(y + i_t * D + o_d, b_x, mask=mask)",
      "    else:",
      "        b_x = tl.load(x + i_t * D + o_d, mask=mask)",
      "        tl.store(y + (i_b * S + i_s) * D + o_d, b_x, mask=mask)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/296.py"
  },
  {
    "name": "mean_pooling_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mean_pooling_fwd_kernel(",
      "    x,",
      "    o,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_x = tl.make_block_ptr(",
      "        x + (bos * H + i_h) * D,",
      "        (T, D),",
      "        (H * D, 1),",
      "        (i_t * BT, i_d * BD),",
      "        (BT, BD),",
      "        (1, 0),",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,)",
      "    )",
      "",
      "    b_x = tl.load(p_x, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    b_o = tl.sum(b_x, axis=0) / min(BT, T - i_t * BT)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/297.py"
  },
  {
    "name": "mean_pooling_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({'BD': BD}, num_warps=num_warps) for BD in [16, 32, 64, 128] for num_warps in [1, 2, 4, 8]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mean_pooling_bwd_kernel(",
      "    do,",
      "    dx,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_d, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_tg = i_t",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "        NT = tl.cdiv(T, BT)",
      "    else:",
      "        NT = tl.cdiv(T, BT)",
      "        i_tg = i_b * NT + i_t",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_dx = tl.make_block_ptr(",
      "        dx + (bos * H + i_h) * D,",
      "        (T, D),",
      "        (H * D, 1),",
      "        (i_t * BT, i_d * BD),",
      "        (BT, BD),",
      "        (1, 0),",
      "    )",
      "    p_do = tl.make_block_ptr(",
      "        do + (i_tg * H + i_h) * D, (D,), (1,), (i_d * BD,), (BD,), (0,)",
      "    )",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0,)).to(tl.float32)",
      "",
      "    b_dx = b_do / tl.full((BT,), min(BT, T - i_t * BT), dtype=tl.float32)[:, None]",
      "    tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/297.py"
  },
  {
    "name": "softmax_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "p",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_fwd_kernel(x, p, D: tl.constexpr, B: tl.constexpr):",
      "    i_n = tl.program_id(0)",
      "    o_d = tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))",
      "    b_m = tl.max(b_x, 0)",
      "    b_x = exp(b_x - b_m)",
      "    b_p = b_x / tl.sum(b_x, 0)",
      "",
      "    tl.store(p + i_n * D + o_d, b_p.to(p.dtype.element_ty), mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/298.py"
  },
  {
    "name": "softmax_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['D'])"
    ],
    "args": [
      {
        "name": "p",
        "annotation": null
      },
      {
        "name": "dp",
        "annotation": null
      },
      {
        "name": "ds",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_bwd_kernel(p, dp, ds, D: tl.constexpr, B: tl.constexpr):",
      "    i_n = tl.program_id(0)",
      "    o_d = tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_p = tl.load(p + i_n * D + o_d, mask=m_d, other=0.0)",
      "    b_dp = tl.load(dp + i_n * D + o_d, mask=m_d, other=0.0)",
      "    b_pp = tl.sum(b_p * b_dp, 0)",
      "    b_ds = b_p * b_dp - b_p * b_pp",
      "    tl.store(ds + i_n * D + o_d, b_ds.to(ds.dtype.element_ty), mask=m_d)"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/298.py"
  },
  {
    "name": "solve_tril_16x16_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4, 5]], key=['BT'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ad",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def solve_tril_16x16_kernel(",
      "    A,",
      "    Ad,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A = A + (bos * H + i_h) * BT",
      "    Ad = Ad + (bos * H + i_h) * 16",
      "",
      "    offset = (i_t * 16) % BT",
      "    p_A = tl.make_block_ptr(",
      "        A, (T, BT), (H * BT, 1), (i_t * 16, offset), (16, 16), (1, 0)",
      "    )",
      "    p_Ai = tl.make_block_ptr(Ad, (T, 16), (H * 16, 1), (i_t * 16, 0), (16, 16), (1, 0))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1)).to(tl.float32)",
      "    b_A = -tl.where(tl.arange(0, 16)[:, None] > tl.arange(0, 16)[None, :], b_A, 0)",
      "",
      "    o_i = tl.arange(0, 16)",
      "    for i in range(1, min(16, T - i_t * 16)):",
      "        b_a = -tl.load(A + (i_t * 16 + i) * H * BT + o_i + offset)",
      "        b_a = b_a + tl.sum(b_a[:, None] * b_A, 0)",
      "        mask = o_i == i",
      "        b_A = tl.where(mask[:, None], b_a, b_A)",
      "    b_A += o_i[:, None] == o_i[None, :]",
      "    tl.store(",
      "        p_Ai,",
      "        b_A.to(p_Ai.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/299.py"
  },
  {
    "name": "merge_16x16_to_32x32_inverse_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [1, 2, 4, 8] for num_stages in [2, 3, 4, 5]], key=['H', 'BT', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ad",
        "annotation": null
      },
      {
        "name": "Ai",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def merge_16x16_to_32x32_inverse_kernel(",
      "    A,",
      "    Ad,",
      "    Ai,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A += (bos * H + i_h) * 32",
      "    Ad += (bos * H + i_h) * 16",
      "    Ai += (bos * H + i_h) * 32",
      "",
      "    p_A_21 = tl.make_block_ptr(",
      "        A, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_11 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_22 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_11 = tl.make_block_ptr(",
      "        Ai, (T, 32), (H * 32, 1), (i_t * 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_22 = tl.make_block_ptr(",
      "        Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_21 = tl.make_block_ptr(",
      "        Ai, (T, 32), (H * 32, 1), (i_t * 32 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "",
      "    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_21 = -tl.dot(",
      "        tl.dot(Ai_22, A_21, input_precision=\"ieee\"), Ai_11, input_precision=\"ieee\"",
      "    )",
      "    tl.store(",
      "        p_Ai_11,",
      "        Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_22,",
      "        Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_21,",
      "        Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/299.py"
  },
  {
    "name": "merge_16x16_to_64x64_inverse_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_VARLEN': lambda args: args['cu_seqlens'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps, num_stages=num_stages) for num_warps in [2, 4, 8] for num_stages in [2, 3, 4, 5]], key=['H', 'BT', 'IS_VARLEN'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "Ad",
        "annotation": null
      },
      {
        "name": "Ai",
        "annotation": null
      },
      {
        "name": "cu_seqlens",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_VARLEN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def merge_16x16_to_64x64_inverse_kernel(",
      "    A,",
      "    Ad,",
      "    Ai,",
      "    cu_seqlens,",
      "    chunk_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    IS_VARLEN: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "    if IS_VARLEN:",
      "        i_n, i_t = tl.load(chunk_indices + i_t * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(cu_seqlens + i_n).to(tl.int32), tl.load(",
      "            cu_seqlens + i_n + 1",
      "        ).to(tl.int32)",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    A += (bos * H + i_h) * 64",
      "    Ad += (bos * H + i_h) * 16",
      "    Ai += (bos * H + i_h) * 64",
      "",
      "    p_A_21 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_A_32 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0)",
      "    )",
      "    p_A_31 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_A_43 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0)",
      "    )",
      "    p_A_42 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0)",
      "    )",
      "    p_A_41 = tl.make_block_ptr(",
      "        A, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_11 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_22 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_33 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ad_44 = tl.make_block_ptr(",
      "        Ad, (T, 16), (H * 16, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)",
      "    )",
      "",
      "    A_21 = tl.load(p_A_21, boundary_check=(0, 1)).to(tl.float32)",
      "    A_32 = tl.load(p_A_32, boundary_check=(0, 1)).to(tl.float32)",
      "    A_31 = tl.load(p_A_31, boundary_check=(0, 1)).to(tl.float32)",
      "    A_43 = tl.load(p_A_43, boundary_check=(0, 1)).to(tl.float32)",
      "    A_42 = tl.load(p_A_42, boundary_check=(0, 1)).to(tl.float32)",
      "    A_41 = tl.load(p_A_41, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    Ai_11 = tl.load(p_Ad_11, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_22 = tl.load(p_Ad_22, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_33 = tl.load(p_Ad_33, boundary_check=(0, 1)).to(tl.float32)",
      "    Ai_44 = tl.load(p_Ad_44, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    Ai_21 = -tl.dot(",
      "        tl.dot(Ai_22, A_21, input_precision=\"ieee\"), Ai_11, input_precision=\"ieee\"",
      "    )",
      "    Ai_32 = -tl.dot(",
      "        tl.dot(Ai_33, A_32, input_precision=\"ieee\"), Ai_22, input_precision=\"ieee\"",
      "    )",
      "    Ai_43 = -tl.dot(",
      "        tl.dot(Ai_44, A_43, input_precision=\"ieee\"), Ai_33, input_precision=\"ieee\"",
      "    )",
      "",
      "    Ai_31 = -tl.dot(",
      "        Ai_33,",
      "        tl.dot(A_31, Ai_11, input_precision=\"ieee\")",
      "        + tl.dot(A_32, Ai_21, input_precision=\"ieee\"),",
      "        input_precision=\"ieee\",",
      "    )",
      "    Ai_42 = -tl.dot(",
      "        Ai_44,",
      "        tl.dot(A_42, Ai_22, input_precision=\"ieee\")",
      "        + tl.dot(A_43, Ai_32, input_precision=\"ieee\"),",
      "        input_precision=\"ieee\",",
      "    )",
      "    Ai_41 = -tl.dot(",
      "        Ai_44,",
      "        tl.dot(A_41, Ai_11, input_precision=\"ieee\")",
      "        + tl.dot(A_42, Ai_21, input_precision=\"ieee\")",
      "        + tl.dot(A_43, Ai_31, input_precision=\"ieee\"),",
      "        input_precision=\"ieee\",",
      "    )",
      "",
      "    p_Ai_11 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_22 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_33 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 32), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_44 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 48), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_21 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 16, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_31 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_32 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 32, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_41 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 0), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_42 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 16), (16, 16), (1, 0)",
      "    )",
      "    p_Ai_43 = tl.make_block_ptr(",
      "        Ai, (T, 64), (H * 64, 1), (i_t * 64 + 48, 32), (16, 16), (1, 0)",
      "    )",
      "    tl.store(",
      "        p_Ai_11,",
      "        Ai_11.to(p_Ai_11.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_22,",
      "        Ai_22.to(p_Ai_22.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_33,",
      "        Ai_33.to(p_Ai_33.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_44,",
      "        Ai_44.to(p_Ai_44.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_21,",
      "        Ai_21.to(p_Ai_21.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_31,",
      "        Ai_31.to(p_Ai_31.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_32,",
      "        Ai_32.to(p_Ai_32.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_41,",
      "        Ai_41.to(p_Ai_41.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_42,",
      "        Ai_42.to(p_Ai_42.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )",
      "    tl.store(",
      "        p_Ai_43,",
      "        Ai_43.to(p_Ai_43.dtype.element_ty, fp_downcast_rounding=\"rtne\"),",
      "        boundary_check=(0, 1),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-linear-attention/fla/ops/utils/299.py"
  },
  {
    "name": "parallel_nsa_compression_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    offsets,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (TC, V),",
      "            (H * V, 1),",
      "            (i_c, i_v * BV),",
      "            (BC, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "        b_r = tl.exp(b_mp - b_m)",
      "",
      "        b_p = tl.exp(b_s - b_m[:, None])",
      "",
      "        b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "        b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "        b_mp = b_m",
      "    if NC == 0:",
      "        b_lse = tl.zeros([G], dtype=tl.float32)",
      "    else:",
      "        b_o = b_o / b_acc[:, None]",
      "        b_lse = b_m + tl.log(b_acc)",
      "",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    if i_v == 0:",
      "        tl.store(",
      "            lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G),",
      "            b_lse.to(lse.dtype.element_ty),",
      "        )"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    offsets,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i_c in range(0, NC, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + (boc * H + i_h) * V,",
      "            (V, TC),",
      "            (1, H * V),",
      "            (i_v * BV, i_c),",
      "            (BV, BC),",
      "            (0, 1),",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_p = tl.exp(b_s - b_lse[:, None])",
      "        b_p = tl.where((o_c < NC)[None, :], b_p, 0)",
      "",
      "        b_dp = tl.dot(b_do, b_v)",
      "        b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "        b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_compression_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_compression_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    offsets,",
      "    chunk_indices,",
      "    chunk_offsets,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_v, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_c = tl.load(chunk_indices + i_c * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_c * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (boc * H + i_h) * K, (TC, K), (H * K, 1), (i_c * BC, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + boc * H + i_h) * K,",
      "        (TC, K),",
      "        (H * K, 1),",
      "        (i_c * BC, 0),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (i_v * B * T * H + boc * H + i_h) * V,",
      "        (TC, V),",
      "        (H * V, 1),",
      "        (i_c * BC, i_v * BV),",
      "        (BC, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BC, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BC, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_c * BC * BS, T):",
      "        o_c = i_c * BC + tl.arange(0, BC)",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "        p_do = tl.make_block_ptr(",
      "            do + (bos + i) * HQ * V,",
      "            (HQ, V),",
      "            (V, 1),",
      "            (i_h * G, i_v * BV),",
      "            (G, BV),",
      "            (1, 0),",
      "        )",
      "        p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "        p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_lse = tl.load(p_lse)",
      "        b_delta = tl.load(p_delta)",
      "",
      "        b_s = tl.dot(b_k, tl.trans(b_q))",
      "        b_p = tl.exp(b_s - b_lse[None, :])",
      "        b_p = tl.where((i >= max(0, (o_c + 1) * BS - 1))[:, None], b_p, 0)",
      "",
      "        b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "        b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "        b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "        b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_kernel_topk",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "chunk_offsets",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_topk(",
      "    q,",
      "    k,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    offsets,",
      "    token_indices,",
      "    chunk_offsets,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_t, i_bh = tl.program_id(0), tl.program_id(1)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "        boc = tl.load(chunk_offsets + i_n).to(tl.int32)",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "        boc = i_b * tl.cdiv(T, BS)",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    TC = tl.cdiv(T, BS)",
      "",
      "    NC = (i_t + 1) // BS",
      "",
      "    if lse is not None:",
      "        b_lse = tl.load(lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G))",
      "    else:",
      "",
      "        b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "",
      "        b_acc = tl.zeros([G], dtype=tl.float32)",
      "        for i_c in range(0, NC, BC):",
      "            o_c = i_c + tl.arange(0, BC)",
      "",
      "            p_k = tl.make_block_ptr(",
      "                k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where((o_c < NC)[None, :], b_s, float(\"-inf\"))",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = tl.exp(b_mp - b_m)",
      "",
      "            b_p = tl.exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_mp = b_m",
      "        if NC == 0:",
      "            b_lse = tl.zeros([G], dtype=tl.float32)",
      "        else:",
      "            b_lse = b_m + tl.log(b_acc)",
      "",
      "    b_i = tl.full([BC], -1, dtype=tl.float32)",
      "    o_i = tl.zeros([BC], dtype=tl.int32)",
      "    m_i = tl.arange(0, BC) < BC // 2",
      "    for i_c in range(0, i_t // BS + 1, BC):",
      "        o_c = i_c + tl.arange(0, BC)",
      "",
      "        p_k = tl.make_block_ptr(",
      "            k + (boc * H + i_h) * K, (K, TC), (1, H * K), (0, i_c), (BK, BC), (0, 1)",
      "        )",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_s = tl.dot(b_q, b_k)",
      "        b_s = tl.where((i_t // BS > o_c)[None, :], b_s, float(\"-inf\"))",
      "",
      "        b_p = tl.where(",
      "            (i_t // BS == o_c)[None, :], float(1.0), tl.exp(b_s - b_lse[:, None])",
      "        )",
      "",
      "        b_i, b_ip = tl.sum(b_p, 0), b_i",
      "        o_i, o_ip = tl.where(o_c <= i_t // BS, o_c + 1, 0), o_i",
      "",
      "        n_dims: tl.constexpr = tl.standard._log2(b_i.shape[0])",
      "        for i in tl.static_range(1, n_dims):",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), i, 2, n_dims)",
      "",
      "        if i_c != 0:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, False, n_dims)",
      "            b_i_new = b_ip * m_i + b_i * (1 - m_i)",
      "            o_i_new = o_ip * m_i + o_i * (1 - m_i)",
      "            b_i, o_i = _bitonic_merge(",
      "                b_i_new, o_i_new.to(tl.int32), n_dims, True, n_dims",
      "            )",
      "        else:",
      "            b_i, o_i = _bitonic_merge(b_i, o_i.to(tl.int32), n_dims, True, n_dims)",
      "",
      "    m_top = tl.arange(0, BC // S) == 0",
      "    b_top = tl.sum(m_top[:, None] * tl.reshape(o_i - 1, [BC // S, S]), 0)",
      "",
      "    p_b = tl.make_block_ptr(",
      "        block_indices + (bos + i_t) * H * S, (H * S,), (1,), (i_h * S,), (S,), (0,)",
      "    )",
      "    tl.store(p_b, b_top.to(p_b.dtype.element_ty))"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_fwd_kernel(",
      "    q,",
      "    k,",
      "    v,",
      "    o,",
      "    lse,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    offsets,",
      "    token_indices,",
      "    T,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + (bos + i_t) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_o = tl.make_block_ptr(",
      "        o + (bos + i_t) * HQ * V, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0)",
      "    )",
      "    p_lse = lse + (bos + i_t) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "    b_o = tl.zeros([G, BV], dtype=tl.float32)",
      "",
      "    b_m = tl.full([G], float(\"-inf\"), dtype=tl.float32)",
      "    b_acc = tl.zeros([G], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (T, V), (H * V, 1), (i_s, i_v * BV), (BS, BV), (1, 0)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_s = tl.where(",
      "                (i_t >= (i_s + tl.arange(0, BS)))[None, :], b_s, float(\"-inf\")",
      "            )",
      "",
      "            b_m, b_mp = tl.maximum(b_m, tl.max(b_s, 1)), b_m",
      "            b_r = tl.exp(b_mp - b_m)",
      "",
      "            b_p = tl.exp(b_s - b_m[:, None])",
      "",
      "            b_acc = b_acc * b_r + tl.sum(b_p, 1)",
      "",
      "            b_o = b_o * b_r[:, None] + tl.dot(b_p.to(b_q.dtype), b_v)",
      "",
      "            b_mp = b_m",
      "    b_o = b_o / b_acc[:, None]",
      "    b_m += tl.log(b_acc)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_lse, b_m.to(p_lse.dtype.element_ty))"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_kernel_mask",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})"
    ],
    "args": [
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_kernel_mask(",
      "    block_indices,",
      "    block_counts,",
      "    block_mask,",
      "    T: tl.constexpr,",
      "    H: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    NS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_b, i_hs = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h, i_s = i_hs // S, i_hs % S",
      "",
      "    b_i = tl.load(block_indices + i_b * T * H * S + i_t * H * S + i_h * S + i_s)",
      "    if USE_BLOCK_COUNTS:",
      "        b_m = b_i * BS <= i_t and i_s < tl.load(",
      "            block_counts + i_b * T * H + i_t * H + i_h",
      "        )",
      "    else:",
      "        b_m = b_i * BS <= i_t",
      "",
      "    if b_i < NS and b_i >= 0:",
      "        tl.store(",
      "            block_mask + i_b * T * H * NS + i_t * H * NS + i_h * NS + b_i,",
      "            b_m.to(block_mask.dtype.element_ty),",
      "        )"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dq",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None, 'USE_BLOCK_COUNTS': lambda args: isinstance(args['block_counts'], torch.Tensor)})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "block_indices",
        "annotation": null
      },
      {
        "name": "block_counts",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "token_indices",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BLOCK_COUNTS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dq(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dq,",
      "    scale,",
      "    block_indices,",
      "    block_counts,",
      "    offsets,",
      "    token_indices,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "    USE_BLOCK_COUNTS: tl.constexpr,",
      "):",
      "    i_t, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_t = tl.load(token_indices + i_t * 2).to(tl.int32), tl.load(",
      "            token_indices + i_t * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    q += (bos + i_t) * HQ * K",
      "    do += (bos + i_t) * HQ * V",
      "    lse += (bos + i_t) * HQ",
      "    delta += (bos + i_t) * HQ",
      "    dq += (i_v * B * T + bos + i_t) * HQ * K",
      "    block_indices += (bos + i_t) * H * S + i_h * S",
      "",
      "    if USE_BLOCK_COUNTS:",
      "        NS = tl.load(block_counts + (bos + i_t) * H + i_h)",
      "    else:",
      "        NS = S",
      "",
      "    k += (bos * H + i_h) * K",
      "    v += (bos * H + i_h) * V",
      "",
      "    p_q = tl.make_block_ptr(q, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "    p_dq = tl.make_block_ptr(dq, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0))",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "    p_do = tl.make_block_ptr(do, (HQ, V), (V, 1), (i_h * G, i_v * BV), (G, BV), (1, 0))",
      "    p_lse = lse + i_h * G + tl.arange(0, G)",
      "    p_delta = delta + i_h * G + tl.arange(0, G)",
      "",
      "    b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "    b_lse = tl.load(p_lse)",
      "    b_delta = tl.load(p_delta)",
      "",
      "    b_dq = tl.zeros([G, BK], dtype=tl.float32)",
      "    for i in range(NS):",
      "        i_s = tl.load(block_indices + i).to(tl.int32) * BS",
      "        if i_s <= i_t and i_s >= 0:",
      "            p_k = tl.make_block_ptr(k, (K, T), (1, H * K), (0, i_s), (BK, BS), (0, 1))",
      "            p_v = tl.make_block_ptr(",
      "                v, (V, T), (1, H * V), (i_v * BV, i_s), (BV, BS), (0, 1)",
      "            )",
      "",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "",
      "            b_s = tl.dot(b_q, b_k)",
      "            b_p = tl.exp(b_s - b_lse[:, None])",
      "            b_p = tl.where((i_t >= (i_s + tl.arange(0, BS)))[None, :], b_p, 0)",
      "",
      "            b_dp = tl.dot(b_do, b_v)",
      "            b_ds = b_p * (b_dp.to(tl.float32) - b_delta[:, None])",
      "",
      "            b_dq += tl.dot(b_ds.to(b_k.dtype), tl.trans(b_k))",
      "    b_dq *= scale",
      "",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "parallel_nsa_bwd_kernel_dkv",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'USE_OFFSETS': lambda args: args['offsets'] is not None})",
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4]], key=['BS', 'BK', 'BV'])",
      "@triton.jit(do_not_specialize=['T'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "lse",
        "annotation": null
      },
      {
        "name": "delta",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "block_mask",
        "annotation": null
      },
      {
        "name": "offsets",
        "annotation": null
      },
      {
        "name": "chunk_indices",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "G",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_OFFSETS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def parallel_nsa_bwd_kernel_dkv(",
      "    q,",
      "    k,",
      "    v,",
      "    lse,",
      "    delta,",
      "    do,",
      "    dk,",
      "    dv,",
      "    block_mask,",
      "    offsets,",
      "    chunk_indices,",
      "    scale,",
      "    T,",
      "    B: tl.constexpr,",
      "    H: tl.constexpr,",
      "    HQ: tl.constexpr,",
      "    G: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    M: tl.constexpr,",
      "    BS: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    USE_OFFSETS: tl.constexpr,",
      "):",
      "    i_v, i_s, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_b, i_h = i_bh // H, i_bh % H",
      "",
      "    if USE_OFFSETS:",
      "        i_n, i_s = tl.load(chunk_indices + i_s * 2).to(tl.int32), tl.load(",
      "            chunk_indices + i_s * 2 + 1",
      "        ).to(tl.int32)",
      "        bos, eos = tl.load(offsets + i_n).to(tl.int32), tl.load(offsets + i_n + 1).to(",
      "            tl.int32",
      "        )",
      "        T = eos - bos",
      "    else:",
      "        bos, eos = i_b * T, i_b * T + T",
      "",
      "    p_k = tl.make_block_ptr(",
      "        k + (bos * H + i_h) * K, (T, K), (H * K, 1), (i_s * BS, 0), (BS, BK), (1, 0)",
      "    )",
      "    p_v = tl.make_block_ptr(",
      "        v + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + (i_v * B * T * H + bos * H + i_h) * K,",
      "        (T, K),",
      "        (H * K, 1),",
      "        (i_s * BS, 0),",
      "        (BS, BK),",
      "        (1, 0),",
      "    )",
      "    p_dv = tl.make_block_ptr(",
      "        dv + (bos * H + i_h) * V,",
      "        (T, V),",
      "        (H * V, 1),",
      "        (i_s * BS, i_v * BV),",
      "        (BS, BV),",
      "        (1, 0),",
      "    )",
      "",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_dk = tl.zeros([BS, BK], dtype=tl.float32)",
      "",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_dv = tl.zeros([BS, BV], dtype=tl.float32)",
      "",
      "    for i in range(i_s * BS, T):",
      "        b_m = tl.load(block_mask + (bos + i) * H * M + i_h * M + i_s)",
      "        if b_m:",
      "            p_q = tl.make_block_ptr(",
      "                q + (bos + i) * HQ * K, (HQ, K), (K, 1), (i_h * G, 0), (G, BK), (1, 0)",
      "            )",
      "",
      "            b_q = tl.load(p_q, boundary_check=(0, 1))",
      "            b_q = (b_q * scale).to(b_q.dtype)",
      "",
      "            p_do = tl.make_block_ptr(",
      "                do + (bos + i) * HQ * V,",
      "                (HQ, V),",
      "                (V, 1),",
      "                (i_h * G, i_v * BV),",
      "                (G, BV),",
      "                (1, 0),",
      "            )",
      "            p_lse = lse + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "            p_delta = delta + (bos + i) * HQ + i_h * G + tl.arange(0, G)",
      "",
      "            b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "            b_lse = tl.load(p_lse)",
      "            b_delta = tl.load(p_delta)",
      "",
      "            b_s = tl.dot(b_k, tl.trans(b_q))",
      "            b_p = tl.exp(b_s - b_lse[None, :])",
      "            b_p = tl.where((i >= (i_s * BS + tl.arange(0, BS)))[:, None], b_p, 0)",
      "",
      "            b_dv += tl.dot(b_p.to(b_do.dtype), b_do)",
      "",
      "            b_dp = tl.dot(b_v, tl.trans(b_do))",
      "",
      "            b_ds = b_p * (b_dp - b_delta[None, :])",
      "",
      "            b_dk += tl.dot(b_ds.to(b_q.dtype), b_q)",
      "",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_native-sparse-attention/native_sparse_attention/ops/301.py"
  },
  {
    "name": "act_func_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dropout",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def act_func_forward_kernel(",
      "    input_pointer,",
      "    output_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    dropout: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input = tl.load(input_pointer + offset, mask=mask)",
      "    tl.store(",
      "        output_pointer + offset,",
      "        apply_act_func(input, drop_p, seed, offset, param, act_func, dropout),",
      "        mask=mask,",
      "    )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/1.py"
  },
  {
    "name": "act_func_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dropout",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def act_func_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    input_grad_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    dropout: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad = tl.load(output_grad_pointer + offset, mask=mask)",
      "    input = tl.load(input_pointer + offset, mask=mask)",
      "",
      "    tl.store(",
      "        input_grad_pointer + offset,",
      "        apply_act_func_grad(",
      "            output_grad, input, drop_p, seed, offset, param, act_func, dropout",
      "        ),",
      "        mask=mask,",
      "    )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/1.py"
  },
  {
    "name": "batch_norm_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'spatial_dim'], restore_value=['running_mean_pointer', 'running_var_pointer'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args['batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "bias_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "pre_act_add_pointer",
        "annotation": null
      },
      {
        "name": "pre_act_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "running_mean_pointer",
        "annotation": null
      },
      {
        "name": "running_var_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "spatial_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_spatial_stride",
        "annotation": null
      },
      {
        "name": "pre_act_add_batch_stride",
        "annotation": null
      },
      {
        "name": "pre_act_add_feat_stride",
        "annotation": null
      },
      {
        "name": "pre_act_add_spatial_stride",
        "annotation": null
      },
      {
        "name": "pre_act_batch_stride",
        "annotation": null
      },
      {
        "name": "pre_act_feat_stride",
        "annotation": null
      },
      {
        "name": "pre_act_spatial_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "output_spatial_stride",
        "annotation": null
      },
      {
        "name": "momentum",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "affine",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "track_running_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "is_train",
        "annotation": "tl.constexpr"
      },
      {
        "name": "add_pre_act",
        "annotation": "tl.constexpr"
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_pre_act",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_SPATIAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def batch_norm_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    bias_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    pre_act_add_pointer,",
      "    pre_act_pointer,",
      "    output_pointer,",
      "    running_mean_pointer,",
      "    running_var_pointer,",
      "    batch_dim,",
      "    spatial_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_spatial_stride,",
      "    pre_act_add_batch_stride,",
      "    pre_act_add_feat_stride,",
      "    pre_act_add_spatial_stride,",
      "    pre_act_batch_stride,",
      "    pre_act_feat_stride,",
      "    pre_act_spatial_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    output_spatial_stride,",
      "    momentum,",
      "    eps,",
      "    param,",
      "    affine: tl.constexpr,",
      "    save_stats: tl.constexpr,",
      "    track_running_stats: tl.constexpr,",
      "    is_train: tl.constexpr,",
      "    add_pre_act: tl.constexpr,",
      "    act_func: tl.constexpr,",
      "    save_pre_act: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_SPATIAL: tl.constexpr,",
      "):",
      "",
      "    feat_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)",
      "    batch_mask = batch_offset < batch_dim",
      "",
      "    if is_train or not track_running_stats:",
      "        count = 0",
      "        mean = 0.0",
      "        var = 0.0",
      "",
      "        for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "            spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "                0, BLOCK_SIZE_SPATIAL",
      "            )",
      "            spatial_mask = spatial_offset < spatial_dim",
      "",
      "            curr_input_pointer = (",
      "                input_pointer",
      "                + input_feat_stride * feat_pid",
      "                + input_batch_stride * batch_offset[:, None]",
      "                + input_spatial_stride * spatial_offset[None, :]",
      "            )",
      "            curr_input = tl.load(",
      "                curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "            ).to(tl.float32)",
      "",
      "            spatial_count = min(",
      "                BLOCK_SIZE_SPATIAL, spatial_dim - block_ind * BLOCK_SIZE_SPATIAL",
      "            )",
      "            curr_count = spatial_count * batch_dim",
      "            count += curr_count",
      "",
      "            prev_mean = mean",
      "            mean += (tl.sum(curr_input) - curr_count * mean) / count",
      "            deltas = tl.where(",
      "                batch_mask[:, None] & spatial_mask[None, :],",
      "                (curr_input - mean) * (curr_input - prev_mean),",
      "                0.0,",
      "            )",
      "            var += tl.sum(deltas)",
      "",
      "        var /= count",
      "        inv_std = tl.rsqrt(var + eps)",
      "",
      "        if save_stats:",
      "            tl.store(feat_pid + mean_pointer, mean)",
      "            tl.store(feat_pid + inv_std_pointer, inv_std)",
      "",
      "        if track_running_stats:",
      "            running_mean_pointer += feat_pid",
      "            running_var_pointer += feat_pid",
      "",
      "            running_mean = tl.load(running_mean_pointer)",
      "            running_var = tl.load(running_var_pointer)",
      "",
      "            n = batch_dim * spatial_dim",
      "            tl.store(",
      "                running_mean_pointer, (1 - momentum) * running_mean + momentum * mean",
      "            )",
      "            tl.store(",
      "                running_var_pointer,",
      "                (1 - momentum) * running_var + momentum * var * n / (n - 1),",
      "            )",
      "",
      "    else:",
      "        mean = tl.load(feat_pid + running_mean_pointer)",
      "        inv_std = tl.rsqrt(tl.load(feat_pid + running_var_pointer) + eps)",
      "",
      "    if affine:",
      "        weight = tl.load(feat_pid + weight_pointer)",
      "        bias = tl.load(feat_pid + bias_pointer)",
      "",
      "    else:",
      "        weight = 1.0",
      "        bias = 0.0",
      "",
      "    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "            0, BLOCK_SIZE_SPATIAL",
      "        )",
      "        spatial_mask = spatial_offset < spatial_dim",
      "",
      "        curr_input_pointer = (",
      "            input_pointer",
      "            + input_feat_stride * feat_pid",
      "            + input_batch_stride * batch_offset[:, None]",
      "            + input_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_output_pointer = (",
      "            output_pointer",
      "            + output_feat_stride * feat_pid",
      "            + output_batch_stride * batch_offset[:, None]",
      "            + output_spatial_stride * spatial_offset[None, :]",
      "        )",
      "",
      "        curr_input = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        output = weight * (curr_input - mean) * inv_std + bias",
      "",
      "        if add_pre_act:",
      "            curr_pre_act_add_pointer = (",
      "                pre_act_add_pointer",
      "                + pre_act_add_feat_stride * feat_pid",
      "                + pre_act_add_batch_stride * batch_offset[:, None]",
      "                + pre_act_add_spatial_stride * spatial_offset[None, :]",
      "            )",
      "            curr_pre_act_add = tl.load(",
      "                curr_pre_act_add_pointer,",
      "                mask=batch_mask[:, None] & spatial_mask[None, :],",
      "            )",
      "            output += curr_pre_act_add",
      "",
      "        if act_func is not None:",
      "            if save_pre_act:",
      "                curr_pre_act_pointer = (",
      "                    pre_act_pointer",
      "                    + pre_act_feat_stride * feat_pid",
      "                    + pre_act_batch_stride * batch_offset[:, None]",
      "                    + pre_act_spatial_stride * spatial_offset[None, :]",
      "                )",
      "                tl.store(",
      "                    curr_pre_act_pointer,",
      "                    output,",
      "                    mask=batch_mask[:, None] & spatial_mask[None, :],",
      "                )",
      "",
      "            output = apply_act_func(output, None, None, None, param, act_func, False)",
      "",
      "        tl.store(",
      "            curr_output_pointer,",
      "            output,",
      "            mask=batch_mask[:, None] & spatial_mask[None, :],",
      "        )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/2.py"
  },
  {
    "name": "batch_norm_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'spatial_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': lambda args: next_power_of_2(args['batch_dim']), 'BLOCK_SIZE_SPATIAL': BLOCK_SIZE_SPATIAL_heuristic})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "weight_grad_pointer",
        "annotation": null
      },
      {
        "name": "bias_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "spatial_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "output_grad_spatial_stride",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_spatial_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_spatial_stride",
        "annotation": null
      },
      {
        "name": "affine",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_SPATIAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def batch_norm_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    weight_pointer,",
      "    input_grad_pointer,",
      "    weight_grad_pointer,",
      "    bias_grad_pointer,",
      "    batch_dim,",
      "    spatial_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    output_grad_spatial_stride,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_spatial_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    input_grad_spatial_stride,",
      "    affine: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_SPATIAL: tl.constexpr,",
      "):",
      "",
      "    feat_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = tl.arange(0, BLOCK_SIZE_BATCH)",
      "    batch_mask = batch_offset < batch_dim",
      "",
      "    mean = tl.load(feat_pid + mean_pointer)",
      "    inv_std = tl.load(feat_pid + inv_std_pointer)",
      "",
      "    term1 = 0.0",
      "    term2 = 0.0",
      "",
      "    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "            0, BLOCK_SIZE_SPATIAL",
      "        )",
      "        spatial_mask = spatial_offset < spatial_dim",
      "",
      "        curr_output_grad_pointer = (",
      "            output_grad_pointer",
      "            + output_grad_feat_stride * feat_pid",
      "            + output_grad_batch_stride * batch_offset[:, None]",
      "            + output_grad_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_input_pointer = (",
      "            input_pointer",
      "            + input_feat_stride * feat_pid",
      "            + input_batch_stride * batch_offset[:, None]",
      "            + input_spatial_stride * spatial_offset[None, :]",
      "        )",
      "",
      "        curr_input = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        curr_pre_lin = (curr_input - mean) * inv_std",
      "        curr_output_grad = tl.load(",
      "            curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "",
      "        term1 += tl.sum(curr_pre_lin * curr_output_grad)",
      "        term2 += tl.sum(curr_output_grad)",
      "",
      "    if affine:",
      "        weight = tl.load(feat_pid + weight_pointer)",
      "        weight_grad = 0.0",
      "        bias_grad = 0.0",
      "",
      "    else:",
      "        weight = 1.0",
      "",
      "    count = batch_dim * spatial_dim",
      "    term1 *= weight / count",
      "    term2 *= weight / count",
      "",
      "    for block_ind in range(0, tl.cdiv(spatial_dim, BLOCK_SIZE_SPATIAL)):",
      "        spatial_offset = block_ind * BLOCK_SIZE_SPATIAL + tl.arange(",
      "            0, BLOCK_SIZE_SPATIAL",
      "        )",
      "        spatial_mask = spatial_offset < spatial_dim",
      "",
      "        curr_output_grad_pointer = (",
      "            output_grad_pointer",
      "            + output_grad_feat_stride * feat_pid",
      "            + output_grad_batch_stride * batch_offset[:, None]",
      "            + output_grad_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_input_pointer = (",
      "            input_pointer",
      "            + input_feat_stride * feat_pid",
      "            + input_batch_stride * batch_offset[:, None]",
      "            + input_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        curr_input_grad_pointer = (",
      "            input_grad_pointer",
      "            + input_grad_feat_stride * feat_pid",
      "            + input_grad_batch_stride * batch_offset[:, None]",
      "            + input_grad_spatial_stride * spatial_offset[None, :]",
      "        )",
      "",
      "        curr_input = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        curr_pre_lin = (curr_input - mean) * inv_std",
      "        curr_output_grad = tl.load(",
      "            curr_output_grad_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        curr_input_grad = inv_std * (",
      "            weight * curr_output_grad - (term1 * curr_pre_lin + term2)",
      "        )",
      "        tl.store(",
      "            curr_input_grad_pointer,",
      "            curr_input_grad,",
      "            mask=batch_mask[:, None] & spatial_mask[None, :],",
      "        )",
      "",
      "        if affine:",
      "            weight_grad += tl.sum(curr_pre_lin * curr_output_grad)",
      "            bias_grad += tl.sum(curr_output_grad)",
      "",
      "    if affine:",
      "        tl.store(feat_pid + weight_grad_pointer, weight_grad)",
      "        tl.store(feat_pid + bias_grad_pointer, bias_grad)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/2.py"
  },
  {
    "name": "conv2d_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[conv2d_forward_config(128, 32, 128, n_warps=8, n_stages=2), conv2d_forward_config(256, 32, 64, n_warps=8, n_stages=2), conv2d_forward_config(256, 32, 32, n_warps=4, n_stages=4), conv2d_forward_config(256, 64, 32, n_warps=4, n_stages=4), conv2d_forward_config(256, 32, 16, n_warps=2, n_stages=4), conv2d_forward_config(64, 32, 128, n_warps=8, n_stages=4), conv2d_forward_config(128, 32, 64, n_warps=4, n_stages=4), conv2d_forward_config(64, 32, 64, n_warps=4, n_stages=4), conv2d_forward_config(128, 32, 16, n_warps=4, n_stages=4), conv2d_forward_config(128, 128, 128, n_warps=8, n_stages=3), conv2d_forward_config(256, 128, 64, n_warps=8, n_stages=3), conv2d_forward_config(256, 128, 32, n_warps=4, n_stages=4), conv2d_forward_config(64, 128, 128, n_warps=4, n_stages=4), conv2d_forward_config(128, 128, 64, n_warps=4, n_stages=4), conv2d_forward_config(128, 64, 32, n_warps=2, n_stages=4), conv2d_forward_config(64, 64, 64, n_warps=2, n_stages=4)], key=['batch_dim', 'in_feat_dim', 'in_height', 'in_width', 'out_feat_dim', 'out_height', 'out_width', 'kernel_height', 'kernel_width', 'stride_height', 'stride_width', 'padding_height', 'padding_width', 'groups', 'fp16'])",
      "@triton.heuristics({'tf32': lambda _: allow_tf32()})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "in_feat_dim",
        "annotation": null
      },
      {
        "name": "in_height",
        "annotation": null
      },
      {
        "name": "in_width",
        "annotation": null
      },
      {
        "name": "out_feat_dim",
        "annotation": null
      },
      {
        "name": "out_height",
        "annotation": null
      },
      {
        "name": "out_width",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_in_feat_stride",
        "annotation": null
      },
      {
        "name": "input_height_stride",
        "annotation": null
      },
      {
        "name": "input_width_stride",
        "annotation": null
      },
      {
        "name": "weight_out_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_in_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_height_stride",
        "annotation": null
      },
      {
        "name": "weight_width_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_out_feat_stride",
        "annotation": null
      },
      {
        "name": "output_height_stride",
        "annotation": null
      },
      {
        "name": "output_width_stride",
        "annotation": null
      },
      {
        "name": "kernel_height",
        "annotation": "tl.constexpr"
      },
      {
        "name": "kernel_width",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_height",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_width",
        "annotation": "tl.constexpr"
      },
      {
        "name": "padding_height",
        "annotation": "tl.constexpr"
      },
      {
        "name": "padding_width",
        "annotation": "tl.constexpr"
      },
      {
        "name": "groups",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp16",
        "annotation": "tl.constexpr"
      },
      {
        "name": "tf32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH_HEIGHT_WIDTH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_IN_FEAT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_OUT_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def conv2d_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    in_feat_dim,",
      "    in_height,",
      "    in_width,",
      "    out_feat_dim,",
      "    out_height,",
      "    out_width,",
      "    input_batch_stride,",
      "    input_in_feat_stride,",
      "    input_height_stride,",
      "    input_width_stride,",
      "    weight_out_feat_stride,",
      "    weight_in_feat_stride,",
      "    weight_height_stride,",
      "    weight_width_stride,",
      "    output_batch_stride,",
      "    output_out_feat_stride,",
      "    output_height_stride,",
      "    output_width_stride,",
      "    kernel_height: tl.constexpr,",
      "    kernel_width: tl.constexpr,",
      "    stride_height: tl.constexpr,",
      "    stride_width: tl.constexpr,",
      "    padding_height: tl.constexpr,",
      "    padding_width: tl.constexpr,",
      "    groups: tl.constexpr,",
      "    fp16: tl.constexpr,",
      "    tf32: tl.constexpr,",
      "    BLOCK_SIZE_BATCH_HEIGHT_WIDTH: tl.constexpr,",
      "    BLOCK_SIZE_IN_FEAT: tl.constexpr,",
      "    BLOCK_SIZE_OUT_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_height_width_pid = tl.program_id(0)",
      "    out_feat_pid = tl.program_id(1)",
      "    group_pid = tl.program_id(2)",
      "",
      "    in_group_dim = in_feat_dim // groups",
      "    out_group_dim = out_feat_dim // groups",
      "",
      "    batch_height_width_offset = (",
      "        batch_height_width_pid * BLOCK_SIZE_BATCH_HEIGHT_WIDTH",
      "        + tl.arange(0, BLOCK_SIZE_BATCH_HEIGHT_WIDTH)",
      "    )",
      "    batch_height_offset = batch_height_width_offset // out_width",
      "    batch_offset = batch_height_offset // out_height",
      "",
      "    output_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(",
      "        0, BLOCK_SIZE_OUT_FEAT",
      "    )",
      "    output_height_offset = batch_height_offset % out_height",
      "    output_width_offset = batch_height_width_offset % out_width",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset",
      "        + input_in_feat_stride * group_pid * in_group_dim",
      "    )[:, None]",
      "    weight_pointer += (",
      "        weight_out_feat_stride * output_feat_offset",
      "        + weight_out_feat_stride * group_pid * out_group_dim",
      "    )[None, :]",
      "",
      "    accum = tl.zeros(",
      "        (BLOCK_SIZE_BATCH_HEIGHT_WIDTH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32",
      "    )",
      "",
      "    for h in range(kernel_height):",
      "        for w in range(kernel_width):",
      "            for c in range(0, in_group_dim, BLOCK_SIZE_IN_FEAT):",
      "                input_feat_offset = c + tl.arange(0, BLOCK_SIZE_IN_FEAT)",
      "                input_height_offset = (",
      "                    h - padding_height + stride_height * output_height_offset",
      "                )",
      "                input_width_offset = (",
      "                    w - padding_width + stride_width * output_width_offset",
      "                )",
      "",
      "                curr_input_pointer = (",
      "                    input_pointer",
      "                    + (input_in_feat_stride * input_feat_offset)[None, :]",
      "                    + (input_height_stride * input_height_offset)[:, None]",
      "                    + (input_width_stride * input_width_offset)[:, None]",
      "                )",
      "                curr_weight_pointer = (",
      "                    weight_pointer",
      "                    + (weight_in_feat_stride * input_feat_offset)[:, None]",
      "                    + (weight_height_stride * h)",
      "                    + (weight_width_stride * w)",
      "                )",
      "",
      "                input_mask = (",
      "                    (batch_offset < batch_dim)[:, None]",
      "                    & (input_feat_offset < in_group_dim)[None, :]",
      "                    & (0 <= input_height_offset)[:, None]",
      "                    & (input_height_offset < in_height)[:, None]",
      "                    & (0 <= input_width_offset)[:, None]",
      "                    & (input_width_offset < in_width)[:, None]",
      "                )",
      "                weight_mask = (input_feat_offset < in_group_dim)[:, None] & (",
      "                    output_feat_offset < out_group_dim",
      "                )[None, :]",
      "",
      "                input_block = tl.load(curr_input_pointer, mask=input_mask)",
      "                weight_block = tl.load(curr_weight_pointer, mask=weight_mask)",
      "",
      "                if fp16:",
      "                    input_block = input_block.to(tl.float16)",
      "                    weight_block = weight_block.to(tl.float16)",
      "",
      "                accum += tl.dot(input_block, weight_block, allow_tf32=tf32)",
      "",
      "    output_pointer += (",
      "        (output_batch_stride * batch_offset)[:, None]",
      "        + (output_out_feat_stride * (group_pid * out_group_dim + output_feat_offset))[",
      "            None, :",
      "        ]",
      "        + (output_height_stride * output_height_offset)[:, None]",
      "        + (output_width_stride * output_width_offset)[:, None]",
      "    )",
      "    output_mask = (",
      "        (batch_offset < batch_dim)[:, None]",
      "        & (output_feat_offset < out_group_dim)[None, :]",
      "        & (output_height_offset < out_height)[:, None]",
      "        & (output_width_offset < out_width)[:, None]",
      "    )",
      "",
      "    tl.store(output_pointer, accum, mask=output_mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/3.py"
  },
  {
    "name": "cross_entropy_loss_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "sum_weights_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "weighted",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_loss_forward_kernel(",
      "    input_pointer,",
      "    target_pointer,",
      "    weight_pointer,",
      "    sum_weights_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    weighted: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    target = tl.load(target_pointer + batch_offset, mask=batch_mask)",
      "",
      "    pred_pointer = (",
      "        input_pointer + input_feat_stride * target + input_batch_stride * batch_offset",
      "    )",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(",
      "        input_pointer,",
      "        mask=batch_mask[:, None] & feat_mask[None, :],",
      "        other=-float(\"inf\"),",
      "    ).to(tl.float32)",
      "    pred = tl.load(pred_pointer, mask=batch_mask).to(tl.float32)",
      "    mx = tl.max(input, axis=1)",
      "    input -= mx[:, None]",
      "    loss = tl.log(tl.sum(tl.exp(input), axis=1)) - pred + mx",
      "",
      "    if weighted:",
      "        weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)",
      "        loss *= weight",
      "        tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))",
      "",
      "    else:",
      "        loss /= batch_dim",
      "",
      "    tl.store(output_pointer + batch_pid, tl.sum(loss))"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/4.py"
  },
  {
    "name": "cross_entropy_loss_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "sum_weights_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "weighted",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_loss_backward_kernel(",
      "    output_grad_pointer,",
      "    target_pointer,",
      "    input_pointer,",
      "    weight_pointer,",
      "    sum_weights_pointer,",
      "    input_grad_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    weighted: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_grad_pointer += (",
      "        input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(",
      "        input_pointer,",
      "        mask=batch_mask[:, None] & feat_mask[None, :],",
      "        other=-float(\"inf\"),",
      "    ).to(tl.float32)",
      "    input -= tl.max(input, axis=1)[:, None]",
      "    numerator = tl.exp(input)",
      "    softmax = numerator / tl.sum(numerator, axis=1)[:, None]",
      "",
      "    output_grad = tl.load(output_grad_pointer).to(tl.float32)",
      "    target = tl.load(target_pointer + batch_offset, mask=batch_mask)",
      "    broadcasted_feat_offset = tl.broadcast_to(",
      "        feat_offset[None, :], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT)",
      "    )",
      "    broadcasted_target = tl.broadcast_to(",
      "        target[:, None], (BLOCK_SIZE_BATCH, BLOCK_SIZE_FEAT)",
      "    )",
      "    input_grad = output_grad * (",
      "        softmax - (broadcasted_feat_offset == broadcasted_target)",
      "    )",
      "",
      "    if weighted:",
      "        weight = tl.load(weight_pointer + target, mask=batch_mask).to(tl.float32)",
      "        sum_weights = tl.load(sum_weights_pointer)",
      "        input_grad *= weight[:, None] / sum_weights",
      "",
      "    else:",
      "        input_grad /= batch_dim",
      "",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/4.py"
  },
  {
    "name": "dropout_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dropout_forward_kernel(",
      "    input_pointer,",
      "    output_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input = tl.load(input_pointer + offset, mask=mask)",
      "    output = apply_dropout(input, drop_p, seed, offset)",
      "    tl.store(output_pointer + offset, output, mask=mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/5.py"
  },
  {
    "name": "dropout_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "drop_p",
        "annotation": null
      },
      {
        "name": "seed",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dropout_backward_kernel(",
      "    output_grad_pointer,",
      "    input_grad_pointer,",
      "    size,",
      "    drop_p,",
      "    seed,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad = tl.load(output_grad_pointer + offset, mask=mask)",
      "    input_grad = apply_dropout_grad(output_grad, drop_p, seed, offset)",
      "    tl.store(input_grad_pointer + offset, input_grad, mask=mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/5.py"
  },
  {
    "name": "glu_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input1_pointer",
        "annotation": null
      },
      {
        "name": "input2_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def glu_forward_kernel(",
      "    input1_pointer,",
      "    input2_pointer,",
      "    output_pointer,",
      "    size,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input1 = tl.load(input1_pointer + offset, mask=mask)",
      "    input2 = tl.load(input2_pointer + offset, mask=mask)",
      "",
      "    output = input1 * apply_act_func(input2, None, None, None, param, act_func, False)",
      "    tl.store(output_pointer + offset, output, mask=mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/6.py"
  },
  {
    "name": "glu_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input1_pointer",
        "annotation": null
      },
      {
        "name": "input2_pointer",
        "annotation": null
      },
      {
        "name": "input1_grad_pointer",
        "annotation": null
      },
      {
        "name": "input2_grad_pointer",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def glu_backward_kernel(",
      "    output_grad_pointer,",
      "    input1_pointer,",
      "    input2_pointer,",
      "    input1_grad_pointer,",
      "    input2_grad_pointer,",
      "    size,",
      "    param,",
      "    act_func: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad = tl.load(output_grad_pointer + offset, mask=mask)",
      "    input1 = tl.load(input1_pointer + offset, mask=mask)",
      "    input2 = tl.load(input2_pointer + offset, mask=mask)",
      "",
      "    input1_grad = output_grad * apply_act_func(",
      "        input2, None, None, None, param, act_func, False",
      "    )",
      "    input2_grad = (",
      "        output_grad",
      "        * input1",
      "        * apply_act_func_grad(1, input2, None, None, None, param, act_func, False)",
      "    )",
      "",
      "    tl.store(input1_grad_pointer + offset, input1_grad, mask=mask)",
      "    tl.store(input2_grad_pointer + offset, input2_grad, mask=mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/6.py"
  },
  {
    "name": "layer_norm_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "bias_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "scale_by_weight",
        "annotation": "tl.constexpr"
      },
      {
        "name": "add_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    bias_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    eps,",
      "    scale_by_weight: tl.constexpr,",
      "    add_bias: tl.constexpr,",
      "    save_stats: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "    mean = tl.sum(input, axis=1) / feat_dim",
      "    diff = tl.where(feat_mask[None, :], input - mean[:, None], 0)",
      "    inv_std = tl.rsqrt(tl.sum(diff * diff, axis=1) / feat_dim + eps)",
      "",
      "    if save_stats:",
      "        tl.store(mean_pointer + batch_offset, mean, mask=batch_mask)",
      "        tl.store(inv_std_pointer + batch_offset, inv_std, mask=batch_mask)",
      "",
      "    output = diff * inv_std[:, None]",
      "    if scale_by_weight:",
      "        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)",
      "        output *= weight",
      "        if add_bias:",
      "            bias = tl.load(bias_pointer + feat_offset, mask=feat_mask)",
      "            output += bias",
      "",
      "    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/7.py"
  },
  {
    "name": "layer_norm_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "mean_pointer",
        "annotation": null
      },
      {
        "name": "inv_std_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "weight_grad_pointer",
        "annotation": null
      },
      {
        "name": "bias_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "weight_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "bias_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "bias_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "scale_by_weight",
        "annotation": "tl.constexpr"
      },
      {
        "name": "add_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def layer_norm_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    mean_pointer,",
      "    inv_std_pointer,",
      "    weight_pointer,",
      "    input_grad_pointer,",
      "    weight_grad_pointer,",
      "    bias_grad_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    weight_grad_batch_stride,",
      "    weight_grad_feat_stride,",
      "    bias_grad_batch_stride,",
      "    bias_grad_feat_stride,",
      "    scale_by_weight: tl.constexpr,",
      "    add_bias: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    output_grad_pointer += (",
      "        output_grad_batch_stride * batch_offset[:, None]",
      "        + output_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_grad_pointer += (",
      "        input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    output_grad = tl.load(",
      "        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    ).to(tl.float32)",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "    mean = tl.load(mean_pointer + batch_offset, mask=batch_mask)",
      "    inv_std = tl.load(inv_std_pointer + batch_offset, mask=batch_mask)",
      "    pre_lin = (input - mean[:, None]) * inv_std[:, None]",
      "",
      "    if scale_by_weight:",
      "        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)",
      "        weight_output_grad_prod = weight * output_grad",
      "",
      "    else:",
      "        weight_output_grad_prod = output_grad",
      "",
      "    term1 = tl.sum(pre_lin * weight_output_grad_prod, axis=1) / feat_dim",
      "    term1 = pre_lin * term1[:, None]",
      "    term2 = tl.sum(weight_output_grad_prod, axis=1) / feat_dim",
      "    input_grad = inv_std[:, None] * (weight_output_grad_prod - (term1 + term2[:, None]))",
      "",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    )",
      "",
      "    if scale_by_weight:",
      "        weight_grad_pointer += (",
      "            weight_grad_batch_stride * batch_pid + weight_grad_feat_stride * feat_offset",
      "        )",
      "        tl.store(",
      "            weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0), mask=feat_mask",
      "        )",
      "",
      "        if add_bias:",
      "            bias_grad_pointer += (",
      "                bias_grad_batch_stride * batch_pid + bias_grad_feat_stride * feat_offset",
      "            )",
      "            tl.store(bias_grad_pointer, tl.sum(output_grad, axis=0), mask=feat_mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/7.py"
  },
  {
    "name": "linear_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[linear_forward_config(32, 32, 32, n_warps=2, n_stages=2), linear_forward_config(64, 32, 32, n_warps=2, n_stages=5), linear_forward_config(64, 32, 128, n_warps=4, n_stages=4), linear_forward_config(64, 32, 256, n_warps=4, n_stages=4), linear_forward_config(128, 32, 32, n_warps=4, n_stages=4), linear_forward_config(128, 32, 64, n_warps=4, n_stages=4), linear_forward_config(128, 32, 128, n_warps=4, n_stages=4), linear_forward_config(128, 64, 256, n_warps=8, n_stages=3)], key=['batch_dim', 'in_feat_dim', 'out_feat_dim', 'fp16'])",
      "@triton.heuristics({'tf32': lambda _: allow_tf32()})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "bias_pointer",
        "annotation": null
      },
      {
        "name": "pre_act_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "in_feat_dim",
        "annotation": null
      },
      {
        "name": "out_feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_in_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_in_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_out_feat_stride",
        "annotation": null
      },
      {
        "name": "pre_act_batch_stride",
        "annotation": null
      },
      {
        "name": "pre_act_out_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_out_feat_stride",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "add_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "act_func",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_pre_act",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp16",
        "annotation": "tl.constexpr"
      },
      {
        "name": "tf32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_IN_FEAT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_OUT_FEAT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_BATCH",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    bias_pointer,",
      "    pre_act_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    in_feat_dim,",
      "    out_feat_dim,",
      "    input_batch_stride,",
      "    input_in_feat_stride,",
      "    weight_in_feat_stride,",
      "    weight_out_feat_stride,",
      "    pre_act_batch_stride,",
      "    pre_act_out_feat_stride,",
      "    output_batch_stride,",
      "    output_out_feat_stride,",
      "    param,",
      "    add_bias: tl.constexpr,",
      "    act_func: tl.constexpr,",
      "    save_pre_act: tl.constexpr,",
      "    fp16: tl.constexpr,",
      "    tf32: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_IN_FEAT: tl.constexpr,",
      "    BLOCK_SIZE_OUT_FEAT: tl.constexpr,",
      "    GROUP_SIZE_BATCH: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    n_batch_pids = tl.cdiv(batch_dim, BLOCK_SIZE_BATCH)",
      "    n_out_feat_pids = tl.cdiv(out_feat_dim, BLOCK_SIZE_OUT_FEAT)",
      "    pids_per_group = GROUP_SIZE_BATCH * n_out_feat_pids",
      "    group_id = pid // pids_per_group",
      "    first_batch_pid = group_id * GROUP_SIZE_BATCH",
      "    GROUP_SIZE_BATCH = min(n_batch_pids - first_batch_pid, GROUP_SIZE_BATCH)",
      "    batch_pid = first_batch_pid + (pid % GROUP_SIZE_BATCH)",
      "    out_feat_pid = (pid % pids_per_group) // GROUP_SIZE_BATCH",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    out_feat_offset = out_feat_pid * BLOCK_SIZE_OUT_FEAT + tl.arange(",
      "        0, BLOCK_SIZE_OUT_FEAT",
      "    )",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    out_feat_mask = out_feat_offset < out_feat_dim",
      "",
      "    input_pointer += input_batch_stride * batch_offset[:, None]",
      "    weight_pointer += weight_out_feat_stride * out_feat_offset[None, :]",
      "",
      "    accum = tl.zeros((BLOCK_SIZE_BATCH, BLOCK_SIZE_OUT_FEAT), dtype=tl.float32)",
      "",
      "    for block_ind in range(0, tl.cdiv(in_feat_dim, BLOCK_SIZE_IN_FEAT)):",
      "        in_feat_offset = block_ind * BLOCK_SIZE_IN_FEAT + tl.arange(",
      "            0, BLOCK_SIZE_IN_FEAT",
      "        )",
      "        in_feat_mask = in_feat_offset < in_feat_dim",
      "",
      "        curr_input_pointer = (",
      "            input_pointer + input_in_feat_stride * in_feat_offset[None, :]",
      "        )",
      "        curr_weight_pointer = (",
      "            weight_pointer + weight_in_feat_stride * in_feat_offset[:, None]",
      "        )",
      "",
      "        input_block = tl.load(",
      "            curr_input_pointer, mask=batch_mask[:, None] & in_feat_mask[None, :]",
      "        )",
      "        weight_block = tl.load(",
      "            curr_weight_pointer, mask=out_feat_mask[None, :] & in_feat_mask[:, None]",
      "        )",
      "",
      "        if fp16:",
      "            input_block = input_block.to(tl.float16)",
      "            weight_block = weight_block.to(tl.float16)",
      "",
      "        accum += tl.dot(input_block, weight_block, allow_tf32=tf32)",
      "",
      "    if add_bias:",
      "        bias = tl.load(bias_pointer + out_feat_offset, mask=out_feat_mask)",
      "",
      "        if fp16:",
      "            bias = bias.to(tl.float16)",
      "",
      "        accum += bias[None, :]",
      "",
      "    if act_func is not None:",
      "        if save_pre_act:",
      "            pre_act_pointer += (",
      "                pre_act_batch_stride * batch_offset[:, None]",
      "                + pre_act_out_feat_stride * out_feat_offset[None, :]",
      "            )",
      "            tl.store(",
      "                pre_act_pointer,",
      "                accum,",
      "                mask=batch_mask[:, None] & out_feat_mask[None, :],",
      "            )",
      "",
      "        accum = apply_act_func(accum, None, None, None, param, act_func, False)",
      "",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_out_feat_stride * out_feat_offset[None, :]",
      "    )",
      "    tl.store(output_pointer, accum, mask=batch_mask[:, None] & out_feat_mask[None, :])"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/8.py"
  },
  {
    "name": "nll_loss_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'spatial_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_SPATIAL': lambda args: next_power_of_2(args['spatial_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "sum_weights_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "spatial_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_spatial_stride",
        "annotation": null
      },
      {
        "name": "target_batch_stride",
        "annotation": null
      },
      {
        "name": "target_spatial_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_spatial_stride",
        "annotation": null
      },
      {
        "name": "reduction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "weighted",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_SPATIAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def nll_loss_forward_kernel(",
      "    input_pointer,",
      "    target_pointer,",
      "    weight_pointer,",
      "    sum_weights_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    spatial_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_spatial_stride,",
      "    target_batch_stride,",
      "    target_spatial_stride,",
      "    output_batch_stride,",
      "    output_spatial_stride,",
      "    reduction: tl.constexpr,",
      "    weighted: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_SPATIAL: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    spatial_mask = spatial_offset < spatial_dim",
      "",
      "    target_pointer += (",
      "        target_batch_stride * batch_offset[:, None]",
      "        + target_spatial_stride * spatial_offset[None, :]",
      "    )",
      "    target = tl.load(target_pointer, mask=batch_mask[:, None] & spatial_mask[None, :])",
      "",
      "    input_pointer += (",
      "        input_feat_stride * target",
      "        + input_batch_stride * batch_offset[:, None]",
      "        + input_spatial_stride * spatial_offset[None, :]",
      "    )",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & spatial_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "",
      "    output = -input",
      "    if weighted:",
      "        weight = tl.load(",
      "            weight_pointer + target, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        output *= weight",
      "",
      "    if reduction == \"none\":",
      "        output_pointer += (",
      "            output_batch_stride * batch_offset[:, None]",
      "            + output_spatial_stride * spatial_offset[None, :]",
      "        )",
      "        tl.store(",
      "            output_pointer, output, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        )",
      "",
      "    elif reduction == \"mean\":",
      "        if weighted:",
      "            tl.store(sum_weights_pointer + batch_pid, tl.sum(weight))",
      "            tl.store(output_pointer + batch_pid, tl.sum(output))",
      "",
      "        else:",
      "            tl.store(",
      "                output_pointer + batch_pid, tl.sum(output) / (batch_dim * spatial_dim)",
      "            )",
      "",
      "    elif reduction == \"sum\":",
      "        tl.store(output_pointer + batch_pid, tl.sum(output))"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/11.py"
  },
  {
    "name": "nll_loss_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'spatial_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_SPATIAL': lambda args: next_power_of_2(args['spatial_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "sum_weights_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "spatial_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "target_batch_stride",
        "annotation": null
      },
      {
        "name": "target_spatial_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_spatial_stride",
        "annotation": null
      },
      {
        "name": "reduction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "weighted",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_SPATIAL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def nll_loss_backward_kernel(",
      "    output_grad_pointer,",
      "    target_pointer,",
      "    weight_pointer,",
      "    sum_weights_pointer,",
      "    input_grad_pointer,",
      "    batch_dim,",
      "    spatial_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    target_batch_stride,",
      "    target_spatial_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    input_grad_spatial_stride,",
      "    reduction: tl.constexpr,",
      "    weighted: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_SPATIAL: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    spatial_offset = tl.arange(0, BLOCK_SIZE_SPATIAL)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    spatial_mask = spatial_offset < spatial_dim",
      "",
      "    output_grad_mask = None",
      "    if reduction == \"none\":",
      "        output_grad_pointer += (",
      "            output_grad_batch_stride * batch_offset[:, None]",
      "            + output_grad_feat_stride * spatial_offset[None, :]",
      "        )",
      "        output_grad_mask = batch_mask[:, None] & spatial_mask[None, :]",
      "",
      "    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask).to(tl.float32)",
      "    input_grad = -output_grad",
      "",
      "    target_pointer += (",
      "        target_batch_stride * batch_offset[:, None]",
      "        + target_spatial_stride * spatial_offset[None, :]",
      "    )",
      "    target = tl.load(target_pointer, mask=batch_mask[:, None] & spatial_mask[None, :])",
      "",
      "    if weighted:",
      "        weight = tl.load(",
      "            weight_pointer + target, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "        ).to(tl.float32)",
      "        input_grad *= weight",
      "",
      "        if reduction == \"mean\":",
      "            input_grad /= tl.load(sum_weights_pointer)",
      "",
      "    elif reduction == \"mean\":",
      "        input_grad /= batch_dim * spatial_dim",
      "",
      "    input_grad_pointer += (",
      "        input_grad_feat_stride * target",
      "        + input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_spatial_stride * spatial_offset[None, :]",
      "    )",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & spatial_mask[None, :]",
      "    )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/11.py"
  },
  {
    "name": "p_loss_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "p_loss",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def p_loss_forward_kernel(",
      "    input_pointer,",
      "    target_pointer,",
      "    output_pointer,",
      "    param,",
      "    size,",
      "    p_loss: tl.constexpr,",
      "    reduction: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    input = tl.load(input_pointer + offset, mask=mask).to(tl.float32)",
      "    target = tl.load(target_pointer + offset, mask=mask).to(tl.float32)",
      "    diff = input - target",
      "",
      "    if p_loss == 0:",
      "        error = tl.where(",
      "            diff < param, 0.5 * diff * diff / param, tl.abs(diff) - 0.5 * param",
      "        )",
      "",
      "    elif p_loss == 1:",
      "        error = tl.abs(diff)",
      "",
      "    elif p_loss == 2:",
      "        error = diff * diff",
      "",
      "    elif p_loss == 3:",
      "        error = tl.where(",
      "            diff < param, 0.5 * diff * diff, param * (tl.abs(diff) - 0.5 * param)",
      "        )",
      "",
      "    if reduction == \"none\":",
      "        tl.store(output_pointer + offset, error, mask=mask)",
      "",
      "    elif reduction == \"mean\":",
      "        tl.store(output_pointer + pid, tl.sum(error) / size)",
      "",
      "    elif reduction == \"sum\":",
      "        tl.store(output_pointer + pid, tl.sum(error))"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/12.py"
  },
  {
    "name": "p_loss_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=element_wise_kernel_configs(), key=['size'])"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "target_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "target_grad_pointer",
        "annotation": null
      },
      {
        "name": "param",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "p_loss",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def p_loss_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    target_pointer,",
      "    input_grad_pointer,",
      "    target_grad_pointer,",
      "    param,",
      "    size,",
      "    p_loss: tl.constexpr,",
      "    reduction: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    offset = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offset < size",
      "",
      "    output_grad_mask = None",
      "    if reduction == \"none\":",
      "        output_grad_pointer += offset",
      "        output_grad_mask = mask",
      "",
      "    input = tl.load(input_pointer + offset, mask=mask).to(tl.float32)",
      "    target = tl.load(target_pointer + offset, mask=mask).to(tl.float32)",
      "    diff = input - target",
      "    output_grad = tl.load(output_grad_pointer, mask=output_grad_mask).to(tl.float32)",
      "",
      "    if p_loss == 0:",
      "        input_grad = tl.where(diff < param, diff / param, tl.where(0 <= diff, 1, -1))",
      "",
      "    elif p_loss == 1:",
      "        input_grad = tl.where(0 <= diff, 1, -1)",
      "",
      "    elif p_loss == 2:",
      "        input_grad = 2 * diff",
      "",
      "    elif p_loss == 3:",
      "        input_grad = tl.where(diff < param, diff, param * tl.where(0 <= diff, 1, -1))",
      "",
      "    if reduction == \"mean\":",
      "        input_grad /= size",
      "",
      "    input_grad *= output_grad",
      "    tl.store(input_grad_pointer + offset, input_grad, mask=mask)",
      "    tl.store(target_grad_pointer + offset, -input_grad, mask=mask)"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/12.py"
  },
  {
    "name": "rms_norm_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "inv_rms_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "scale_by_weight",
        "annotation": "tl.constexpr"
      },
      {
        "name": "save_stats",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rms_norm_forward_kernel(",
      "    input_pointer,",
      "    weight_pointer,",
      "    inv_rms_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    eps,",
      "    scale_by_weight: tl.constexpr,",
      "    save_stats: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "    inv_rms = tl.rsqrt(tl.sum(input * input, axis=1) / feat_dim + eps)",
      "    output = input * inv_rms[:, None]",
      "",
      "    if save_stats:",
      "        tl.store(inv_rms_pointer + batch_offset, inv_rms, mask=batch_mask)",
      "",
      "    if scale_by_weight:",
      "        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)",
      "        output *= weight",
      "",
      "    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/13.py"
  },
  {
    "name": "rms_norm_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "inv_rms_pointer",
        "annotation": null
      },
      {
        "name": "weight_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "weight_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "weight_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "weight_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "scale_by_weight",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rms_norm_backward_kernel(",
      "    output_grad_pointer,",
      "    input_pointer,",
      "    inv_rms_pointer,",
      "    weight_pointer,",
      "    input_grad_pointer,",
      "    weight_grad_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    weight_grad_batch_stride,",
      "    weight_grad_feat_stride,",
      "    scale_by_weight: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    output_grad_pointer += (",
      "        output_grad_batch_stride * batch_offset[:, None]",
      "        + output_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_grad_pointer += (",
      "        input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    output_grad = tl.load(",
      "        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    ).to(tl.float32)",
      "    input = tl.load(input_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "    inv_rms = tl.load(inv_rms_pointer + batch_offset, mask=batch_mask)",
      "    pre_lin = input * inv_rms[:, None]",
      "",
      "    if scale_by_weight:",
      "        weight = tl.load(weight_pointer + feat_offset, mask=feat_mask)",
      "        weight_output_grad_prod = weight * output_grad",
      "",
      "    else:",
      "        weight_output_grad_prod = output_grad",
      "",
      "    term1 = input * tl.sum(input * weight_output_grad_prod, axis=1)",
      "    term2 = inv_rms[:, None] * inv_rms[:, None]",
      "    input_grad = inv_rms[:, None] * (weight_output_grad_prod - term1 * term2 / feat_dim)",
      "",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    )",
      "",
      "    if scale_by_weight:",
      "        weight_grad_pointer += (",
      "            weight_grad_batch_stride * batch_pid + weight_grad_feat_stride * feat_offset",
      "        )",
      "        tl.store(",
      "            weight_grad_pointer, tl.sum(output_grad * pre_lin, axis=0), mask=feat_mask",
      "        )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/13.py"
  },
  {
    "name": "softmax_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "input_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "input_batch_stride",
        "annotation": null
      },
      {
        "name": "input_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "log",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_forward_kernel(",
      "    input_pointer,",
      "    output_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    input_batch_stride,",
      "    input_feat_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    log: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    input_pointer += (",
      "        input_batch_stride * batch_offset[:, None]",
      "        + input_feat_stride * feat_offset[None, :]",
      "    )",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    input = tl.load(",
      "        input_pointer,",
      "        mask=batch_mask[:, None] & feat_mask[None, :],",
      "        other=-float(\"inf\"),",
      "    ).to(tl.float32)",
      "    input -= tl.max(input, axis=1)[:, None]",
      "    numerator = tl.exp(input)",
      "    denominator = tl.sum(numerator, axis=1)[:, None]",
      "",
      "    if log:",
      "        output = input - tl.log(denominator)",
      "",
      "    else:",
      "        output = numerator / denominator",
      "",
      "    tl.store(output_pointer, output, mask=batch_mask[:, None] & feat_mask[None, :])"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/14.py"
  },
  {
    "name": "softmax_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=warps_kernel_configs(), key=['batch_dim', 'feat_dim'])",
      "@triton.heuristics({'BLOCK_SIZE_BATCH': BLOCK_SIZE_BATCH_heuristic, 'BLOCK_SIZE_FEAT': lambda args: next_power_of_2(args['feat_dim'])})"
    ],
    "args": [
      {
        "name": "output_grad_pointer",
        "annotation": null
      },
      {
        "name": "output_pointer",
        "annotation": null
      },
      {
        "name": "input_grad_pointer",
        "annotation": null
      },
      {
        "name": "batch_dim",
        "annotation": null
      },
      {
        "name": "feat_dim",
        "annotation": null
      },
      {
        "name": "output_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "output_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "output_batch_stride",
        "annotation": null
      },
      {
        "name": "output_feat_stride",
        "annotation": null
      },
      {
        "name": "input_grad_batch_stride",
        "annotation": null
      },
      {
        "name": "input_grad_feat_stride",
        "annotation": null
      },
      {
        "name": "log",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_BATCH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_FEAT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_backward_kernel(",
      "    output_grad_pointer,",
      "    output_pointer,",
      "    input_grad_pointer,",
      "    batch_dim,",
      "    feat_dim,",
      "    output_grad_batch_stride,",
      "    output_grad_feat_stride,",
      "    output_batch_stride,",
      "    output_feat_stride,",
      "    input_grad_batch_stride,",
      "    input_grad_feat_stride,",
      "    log: tl.constexpr,",
      "    BLOCK_SIZE_BATCH: tl.constexpr,",
      "    BLOCK_SIZE_FEAT: tl.constexpr,",
      "):",
      "",
      "    batch_pid = tl.program_id(axis=0)",
      "",
      "    batch_offset = batch_pid * BLOCK_SIZE_BATCH + tl.arange(0, BLOCK_SIZE_BATCH)",
      "    feat_offset = tl.arange(0, BLOCK_SIZE_FEAT)",
      "",
      "    batch_mask = batch_offset < batch_dim",
      "    feat_mask = feat_offset < feat_dim",
      "",
      "    output_grad_pointer += (",
      "        output_grad_batch_stride * batch_offset[:, None]",
      "        + output_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "    output_pointer += (",
      "        output_batch_stride * batch_offset[:, None]",
      "        + output_feat_stride * feat_offset[None, :]",
      "    )",
      "    input_grad_pointer += (",
      "        input_grad_batch_stride * batch_offset[:, None]",
      "        + input_grad_feat_stride * feat_offset[None, :]",
      "    )",
      "",
      "    output_grad = tl.load(",
      "        output_grad_pointer, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    ).to(tl.float32)",
      "    output = tl.load(output_pointer, mask=batch_mask[:, None] & feat_mask[None, :]).to(",
      "        tl.float32",
      "    )",
      "",
      "    if log:",
      "        input_grad = output_grad - tl.exp(output) * tl.sum(output_grad, axis=1)[:, None]",
      "",
      "    else:",
      "        input_grad = output * (",
      "            output_grad - tl.sum(output_grad * output, axis=1)[:, None]",
      "        )",
      "",
      "    tl.store(",
      "        input_grad_pointer, input_grad, mask=batch_mask[:, None] & feat_mask[None, :]",
      "    )"
    ],
    "file": "triton_repos/BobMcDear_attorch/attorch/14.py"
  },
  {
    "name": "_paged_attn_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'num_warps': lambda args: get_num_warps(args['QUERY_GROUP_SIZE'], args['HEAD_SIZE'], args['KV_BLOCK_SIZE']), 'num_stages': lambda args: get_num_stages(args['QUERY_GROUP_SIZE'], args['KV_BLOCK_SIZE'])})"
    ],
    "args": [
      {
        "name": "m_i_ptr",
        "annotation": null
      },
      {
        "name": "l_i_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "q_ptr",
        "annotation": null
      },
      {
        "name": "k_cache_ptr",
        "annotation": null
      },
      {
        "name": "v_cache_ptr",
        "annotation": null
      },
      {
        "name": "context_lens_ptr",
        "annotation": null
      },
      {
        "name": "block_tables_ptr",
        "annotation": null
      },
      {
        "name": "attn_scale",
        "annotation": null
      },
      {
        "name": "stride_bt0",
        "annotation": null
      },
      {
        "name": "stride_bt1",
        "annotation": null
      },
      {
        "name": "stride_q0",
        "annotation": null
      },
      {
        "name": "stride_q1",
        "annotation": null
      },
      {
        "name": "stride_q2",
        "annotation": null
      },
      {
        "name": "stride_kv0",
        "annotation": null
      },
      {
        "name": "stride_kv1",
        "annotation": null
      },
      {
        "name": "stride_kv2",
        "annotation": null
      },
      {
        "name": "stride_kv3",
        "annotation": null
      },
      {
        "name": "stride_o0",
        "annotation": null
      },
      {
        "name": "stride_o1",
        "annotation": null
      },
      {
        "name": "stride_o2",
        "annotation": null
      },
      {
        "name": "stride_o3",
        "annotation": null
      },
      {
        "name": "stride_o4",
        "annotation": null
      },
      {
        "name": "HEAD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "QUERY_GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_QUERY_GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_KV_HEADS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "KV_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PARTITION_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _paged_attn_kernel(",
      "    m_i_ptr,",
      "    l_i_ptr,",
      "    out_ptr,",
      "    q_ptr,",
      "    k_cache_ptr,",
      "    v_cache_ptr,",
      "    context_lens_ptr,",
      "    block_tables_ptr,",
      "    attn_scale,",
      "    stride_bt0,",
      "    stride_bt1,",
      "    stride_q0,",
      "    stride_q1,",
      "    stride_q2,",
      "    stride_kv0,",
      "    stride_kv1,",
      "    stride_kv2,",
      "    stride_kv3,",
      "    stride_o0,",
      "    stride_o1,",
      "    stride_o2,",
      "    stride_o3,",
      "    stride_o4,",
      "    HEAD_SIZE: tl.constexpr,",
      "    QUERY_GROUP_SIZE: tl.constexpr,",
      "    PADDED_QUERY_GROUP_SIZE: tl.constexpr,",
      "    NUM_KV_HEADS: tl.constexpr,",
      "    KV_BLOCK_SIZE: tl.constexpr,",
      "    PARTITION_SIZE: tl.constexpr,",
      "):",
      "    seq_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "    part_idx = tl.program_id(2)",
      "    max_num_partitions = tl.num_programs(2)",
      "",
      "    log2e: tl.constexpr = 1.4426950408889634",
      "",
      "    USE_PARTITIONING = PARTITION_SIZE > 0",
      "    context_len = tl.load(context_lens_ptr + seq_idx)",
      "    if USE_PARTITIONING:",
      "        context_start_idx = part_idx * PARTITION_SIZE",
      "        if context_start_idx >= context_len:",
      "            return",
      "        context_end_idx = tl.minimum(context_start_idx + PARTITION_SIZE, context_len)",
      "        num_blocks = tl.cdiv(context_end_idx - context_start_idx, KV_BLOCK_SIZE)",
      "    else:",
      "        num_blocks = tl.cdiv(context_len, KV_BLOCK_SIZE)",
      "",
      "    block_offset = tl.arange(0, KV_BLOCK_SIZE)",
      "    head_offset = tl.arange(0, HEAD_SIZE)",
      "    padding_group_offset = tl.arange(0, PADDED_QUERY_GROUP_SIZE)",
      "",
      "    kv_offset = (",
      "        kv_head_idx * stride_kv1",
      "        + block_offset[:, None] * stride_kv2",
      "        + head_offset[None, :] * stride_kv3",
      "    )",
      "",
      "    q_offset = (",
      "        seq_idx * stride_q0",
      "        + (kv_head_idx * QUERY_GROUP_SIZE + padding_group_offset[:, None]) * stride_q1",
      "        + head_offset[None, :] * stride_q2",
      "    )",
      "    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE",
      "",
      "    q = tl.load(q_ptr + q_offset, mask=group_mask, other=0.0)",
      "",
      "    m_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([PADDED_QUERY_GROUP_SIZE], dtype=tl.float32)",
      "    acc = tl.zeros([PADDED_QUERY_GROUP_SIZE, HEAD_SIZE], dtype=tl.float32)",
      "",
      "    num_prev_blocks = part_idx * (PARTITION_SIZE // KV_BLOCK_SIZE)",
      "    for i in range(num_blocks):",
      "        block_idx = num_prev_blocks + i",
      "        block_number = tl.load(",
      "            block_tables_ptr + seq_idx * stride_bt0 + block_idx * stride_bt1",
      "        )",
      "",
      "        kv_block_offset = block_number * stride_kv0 + kv_offset",
      "        mask_offset = block_idx * KV_BLOCK_SIZE + block_offset",
      "        kv_mask = mask_offset[:, None] < context_len",
      "",
      "        k = tl.load(k_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)",
      "",
      "        if PADDED_QUERY_GROUP_SIZE == 1:",
      "            qk = tl.sum(q[:, None, :] * k[None, :, :], axis=2)",
      "        else:",
      "            qk = tl.dot(q, k.T, out_dtype=tl.float32)",
      "",
      "        qk *= attn_scale",
      "        qk = tl.where(mask_offset < context_len, qk, float(\"-inf\"))",
      "",
      "        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))",
      "",
      "        p = tl.math.exp2((qk - m_i_new[:, None]) * log2e)",
      "        alpha = tl.math.exp2((m_i - m_i_new) * log2e)",
      "        acc *= alpha[:, None]",
      "",
      "        v = tl.load(v_cache_ptr + kv_block_offset, mask=kv_mask, other=0.0)",
      "",
      "        if PADDED_QUERY_GROUP_SIZE == 1:",
      "            acc += tl.sum(p.T[:, :, None] * v[:, None, :], axis=0)",
      "        else:",
      "            p = p.to(v.dtype)",
      "            acc += tl.dot(p, v, out_dtype=tl.float32)",
      "",
      "        l_i = l_i * alpha + tl.sum(p, axis=1)",
      "        m_i = m_i_new",
      "    acc = acc / l_i[:, None]",
      "",
      "    if USE_PARTITIONING:",
      "        part_offset = (",
      "            (seq_idx * NUM_KV_HEADS + kv_head_idx)",
      "            * max_num_partitions",
      "            * QUERY_GROUP_SIZE",
      "            + part_idx * QUERY_GROUP_SIZE",
      "            + padding_group_offset",
      "        )",
      "        mask = padding_group_offset < QUERY_GROUP_SIZE",
      "        tl.store(m_i_ptr + part_offset, m_i, mask=mask)",
      "        tl.store(l_i_ptr + part_offset, l_i, mask=mask)",
      "",
      "    out_offset = seq_idx * stride_o0",
      "    if USE_PARTITIONING:",
      "        out_offset += kv_head_idx * stride_o1",
      "    else:",
      "        out_offset += kv_head_idx * QUERY_GROUP_SIZE * stride_o1",
      "    out_offset += (",
      "        part_idx * stride_o2",
      "        + padding_group_offset[:, None] * stride_o3",
      "        + head_offset[None, :] * stride_o4",
      "    )",
      "",
      "    group_mask = padding_group_offset[:, None] < QUERY_GROUP_SIZE",
      "    tl.store(out_ptr + out_offset, acc, mask=group_mask)"
    ],
    "file": "triton_repos/FlagOpen_FlagAttention/src/flag_attn/25.py"
  },
  {
    "name": "bmm_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "o_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_al",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bl",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_ol",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def bmm_kernel(",
      "    x_ptr,",
      "    y_ptr,",
      "    o_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_al,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bl,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_ol,",
      "    stride_om,",
      "    stride_on,",
      "    **meta,",
      "):",
      "    BLOCK_SIZE_M = meta[\"BLOCK_SIZE_M\"]",
      "    BLOCK_SIZE_N = meta[\"BLOCK_SIZE_N\"]",
      "    BLOCK_SIZE_K = meta[\"BLOCK_SIZE_K\"]",
      "    GROUP_SIZE_M = 8",
      "",
      "    pid_batch = tl.program_id(0)",
      "    pid = tl.program_id(1)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_am[:, None] * stride_am",
      "        + offs_k[None, :] * stride_ak",
      "        + pid_batch * stride_al",
      "    )",
      "    y_ptrs = y_ptr + (",
      "        offs_k[:, None] * stride_bk",
      "        + offs_bn[None, :] * stride_bn",
      "        + pid_batch * stride_bl",
      "    )",
      "",
      "    o = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, K, BLOCK_SIZE_K):",
      "        x = tl.load(x_ptrs)",
      "        y = tl.load(y_ptrs)",
      "        o += tl.dot(x, y)",
      "",
      "        x_ptrs += BLOCK_SIZE_K * stride_ak",
      "        y_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if exists(meta[\"ACTIVATION\"]):",
      "        o = meta[\"ACTIVATION\"](o)",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)",
      "",
      "    o_ptrs = (",
      "        o_ptr",
      "        + stride_om * offs_m[:, None]",
      "        + stride_on * offs_n[None, :]",
      "        + stride_ol * pid_batch",
      "    )",
      "    tl.store(o_ptrs, o, mask=mask)"
    ],
    "file": "triton_repos/lucidrains_triton-transformer/triton_transformer/422.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,))",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        input = tl.where(condition, input, float(\"-inf\"))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    output = tl.argmax(input, 1)",
      "    tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/353.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_boundary_check': lambda args: args['size_along_dim'] % args['block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "denominator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "numerator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x1_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x2_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "z_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "z_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "size_along_dim",
        "annotation": "tl.int32"
      },
      {
        "name": "output_y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "output_x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    denominator_ptr: tl.tensor,",
      "    numerator_ptr: tl.tensor,",
      "    x1_ptr: tl.tensor,",
      "    x2_ptr: tl.tensor,",
      "    z_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    z_stride: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    eps: tl.float32,",
      "    size_along_dim: tl.int32,",
      "    output_y_size: tl.int32,",
      "    output_x_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    block_size: tl.constexpr,",
      "    require_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "",
      "    num_output_y = pid // output_x_size",
      "    num_output_x = pid % output_x_size",
      "",
      "    x1_block_ptr = tl.make_block_ptr(",
      "        x1_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    x2_block_ptr = tl.make_block_ptr(",
      "        x2_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    denominator_block_ptr = tl.make_block_ptr(",
      "        denominator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    numerator_block_ptr = tl.make_block_ptr(",
      "        numerator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "",
      "    denominator_accumulation1 = tl.zeros((block_size, 1, 1), tl.float32)",
      "    denominator_accumulation2 = tl.zeros((block_size, 1, 1), tl.float32)",
      "    numerator_accumulation = tl.zeros((block_size, 1, 1), tl.float32)",
      "",
      "    for _ in range(0, size_along_dim, block_size):",
      "        if require_boundary_check:",
      "            x1 = tl.load(x1_block_ptr, boundary_check=(0,), padding_option=\"zero\")",
      "            x2 = tl.load(x2_block_ptr, boundary_check=(0,), padding_option=\"zero\")",
      "        else:",
      "            x1 = tl.load(x1_block_ptr)",
      "            x2 = tl.load(x2_block_ptr)",
      "",
      "        denominator_accumulation1 += x1 * x1",
      "        denominator_accumulation2 += x2 * x2",
      "        numerator_accumulation += x1 * x2",
      "",
      "        x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))",
      "        x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))",
      "",
      "    denominator1 = tl.sum(denominator_accumulation1, 0)",
      "    denominator2 = tl.sum(denominator_accumulation2, 0)",
      "    denominator = tl.sqrt(denominator1) * tl.sqrt(denominator2)",
      "",
      "    numerator = tl.sum(numerator_accumulation, 0)",
      "    output = numerator / tl.math.max(denominator, eps)",
      "",
      "    tl.store(output_block_ptr, output.to(dtype))",
      "    tl.store(denominator_block_ptr, denominator.to(dtype))",
      "    tl.store(numerator_block_ptr, numerator.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/356.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_boundary_check': lambda args: args['size_along_dim'] % args['block_size']})"
    ],
    "args": [
      {
        "name": "grad_x1_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_x2_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "denominator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "numerator_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x1_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x2_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "z_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "z_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "size_along_dim",
        "annotation": "tl.int32"
      },
      {
        "name": "output_y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "output_x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_x1_ptr: tl.tensor,",
      "    grad_x2_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    denominator_ptr: tl.tensor,",
      "    numerator_ptr: tl.tensor,",
      "    x1_ptr: tl.tensor,",
      "    x2_ptr: tl.tensor,",
      "    z_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    z_stride: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    size_along_dim: tl.int32,",
      "    output_y_size: tl.int32,",
      "    output_x_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    block_size: tl.constexpr,",
      "    require_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_output_y = pid // output_x_size",
      "    num_output_x = pid % output_x_size",
      "",
      "    grad_x1_block_ptr = tl.make_block_ptr(",
      "        grad_x1_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    grad_x2_block_ptr = tl.make_block_ptr(",
      "        grad_x2_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    x1_block_ptr = tl.make_block_ptr(",
      "        x1_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    x2_block_ptr = tl.make_block_ptr(",
      "        x2_ptr,",
      "        shape=(z_size, y_size, x_size),",
      "        strides=(z_stride, y_stride, x_stride),",
      "        offsets=(0, num_output_y, num_output_x),",
      "        block_shape=(block_size, 1, 1),",
      "        order=(2, 1, 0),",
      "    )",
      "    denominator_block_ptr = tl.make_block_ptr(",
      "        denominator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "    numerator_block_ptr = tl.make_block_ptr(",
      "        numerator_ptr,",
      "        shape=(output_y_size, output_x_size),",
      "        strides=(output_x_size, 1),",
      "        offsets=(num_output_y, num_output_x),",
      "        block_shape=(1, 1),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for _ in range(0, size_along_dim, block_size):",
      "        if require_boundary_check:",
      "            x1 = tl.load(",
      "                x1_block_ptr, boundary_check=(0,), padding_option=\"zero\"",
      "            ).to(tl.float32)",
      "            x2 = tl.load(",
      "                x2_block_ptr, boundary_check=(0,), padding_option=\"zero\"",
      "            ).to(tl.float32)",
      "        else:",
      "            x1 = tl.load(x1_block_ptr)",
      "            x2 = tl.load(x2_block_ptr)",
      "",
      "        denominator = tl.load(denominator_block_ptr)",
      "        numerator = tl.load(numerator_block_ptr)",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "        squared_x1 = x1 * x1",
      "        squared_x2 = x2 * x2",
      "        squared_x1_sum = tl.sum(squared_x1, 0)",
      "        squared_x2_sum = tl.sum(squared_x2, 0)",
      "",
      "        grad_denominator = (",
      "            grad_output * numerator * (-1 / (denominator * denominator))",
      "        )",
      "",
      "        grad_mul1 = grad_denominator * tl.sqrt(tl.sum(squared_x2, 0))",
      "        grad_mul2 = grad_denominator * tl.sqrt(tl.sum(squared_x1, 0))",
      "",
      "        grad_sqrt1 = grad_mul1 / (2 * tl.sqrt(squared_x1_sum))",
      "        grad_sqrt2 = grad_mul2 / (2 * tl.sqrt(squared_x2_sum))",
      "",
      "        grad_to_dot = grad_output / denominator",
      "",
      "        grad_x1 = (grad_sqrt1 * 2 * x1) + (grad_to_dot * x2)",
      "        grad_x2 = (grad_sqrt2 * 2 * x2) + (grad_to_dot * x1)",
      "",
      "        if require_boundary_check:",
      "            tl.store(grad_x1_block_ptr, grad_x1.to(dtype), boundary_check=(0,))",
      "            tl.store(grad_x2_block_ptr, grad_x2.to(dtype), boundary_check=(0,))",
      "        else:",
      "            tl.store(grad_x1_block_ptr, grad_x1.to(dtype))",
      "            tl.store(grad_x2_block_ptr, grad_x2.to(dtype))",
      "",
      "        x1_block_ptr = tl.advance(x1_block_ptr, (block_size, 0, 0))",
      "        x2_block_ptr = tl.advance(x2_block_ptr, (block_size, 0, 0))",
      "        grad_x1_block_ptr = tl.advance(grad_x1_block_ptr, (block_size, 0, 0))",
      "        grad_x2_block_ptr = tl.advance(grad_x2_block_ptr, (block_size, 0, 0))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/356.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "p",
        "annotation": "tl.float32"
      },
      {
        "name": "seed",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    x_size: tl.int32,",
      "    p: tl.float32,",
      "    seed: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    x_offset = pid * x_block_size",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(0,))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    condition = tl.rand(seed, tl.arange(0, x_block_size) + x_offset) > p",
      "    output = tl.where(condition, input / (1.0 - p + language.eps), 0.0)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/357.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "p",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    x_size: tl.int32,",
      "    p: tl.float32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    x_offset = pid * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,))",
      "        output = tl.load(output_block_ptr, boundary_check=(0,))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        output = tl.load(output_block_ptr)",
      "",
      "    condition = (p == 0.0) | (output > 0.0)",
      "    grad_input = tl.where(condition, grad_output * (1.0 - p + language.eps), 0.0)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/357.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'] == 0, 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size'] == 0, 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size'] == 0})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "state_gate_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_n_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    state_gate_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    x_size: tl.int32,",
      "    input_batch_stride: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    weight_n_stride: tl.int32,",
      "    weight_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_m_blocks = tl.cdiv(m_size, m_block_size)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    num_blocks = num_m_blocks * num_x_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    m_block = block // num_x_blocks",
      "    x_block = block % num_x_blocks",
      "    m_offset = m_block * m_block_size",
      "    x_offset = x_block * x_block_size",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr + batch * m_size * x_size,",
      "        shape=(m_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(m_offset, x_offset),",
      "        block_shape=(m_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    state_block_ptr = tl.make_block_ptr(",
      "        state_gate_ptr + batch * m_size * n_size,",
      "        shape=(m_size, n_size),",
      "        strides=(n_size, 1),",
      "        offsets=(m_offset, x_offset),",
      "        block_shape=(m_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    gate_block_ptr = tl.make_block_ptr(",
      "        state_gate_ptr + batch * m_size * n_size,",
      "        shape=(m_size, n_size),",
      "        strides=(n_size, 1),",
      "        offsets=(m_offset, x_offset + x_size),",
      "        block_shape=(m_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    state = language.Linear.forward(",
      "        input_ptr + batch * input_batch_stride,",
      "        weight_ptr,",
      "        bias_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        x_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        x_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_x_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "    gate = language.Linear.forward(",
      "        input_ptr + batch * input_batch_stride,",
      "        weight_ptr,",
      "        bias_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        x_offset + x_size,",
      "        use_accelerator,",
      "        m_block_size,",
      "        x_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_x_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "    output = state * language.math.GELU.forward(gate)",
      "",
      "    if require_m_boundary_check & require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype))",
      "        tl.store(state_block_ptr, state.to(dtype))",
      "        tl.store(gate_block_ptr, gate.to(dtype))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))",
      "        tl.store(state_block_ptr, state.to(dtype), boundary_check=(0, 1))",
      "        tl.store(gate_block_ptr, gate.to(dtype), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/358.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_group_boundary_check': lambda args: args['group_size'] % args['group_block_size'], 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "group_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "num_groups",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "group_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_group_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    group_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    num_groups: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    eps: tl.float32,",
      "    dtype: tl.constexpr,",
      "    group_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_group_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    batch = pid // num_groups",
      "    group = pid % num_groups",
      "    num_elements = group_size * x_size",
      "    batch_offset = batch * num_groups * num_elements",
      "    group_offset = batch_offset + group * num_elements",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        input = tl.load(",
      "            input_block_ptr, boundary_check=(0, 1), padding_option=\"zero\"",
      "        )",
      "        mean = tl.sum(",
      "            tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1",
      "        )",
      "        group_condition = tl.arange(0, group_block_size) < group_size",
      "        x_condition = tl.arange(0, x_block_size) < x_size",
      "        condition = group_condition[:, None] & x_condition[None, :]",
      "        centered_mean = tl.where(condition, input - mean, 0)",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        mean = tl.sum(",
      "            tl.view(input / num_elements, (1, group_block_size * x_block_size)), 1",
      "        )",
      "        centered_mean = input - mean",
      "",
      "    var = tl.sum(",
      "        tl.view(",
      "            centered_mean * centered_mean / num_elements,",
      "            (1, group_block_size * x_block_size),",
      "        ),",
      "        1,",
      "    )",
      "    rstd = tl.math.rsqrt(var + eps)",
      "    output = centered_mean * rstd",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(y_size, 1),",
      "            strides=(1, y_size),",
      "            offsets=(group * group_size, 0),",
      "            block_shape=(group_block_size, 1),",
      "            order=(0, 1),",
      "        )",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "        output *= weight",
      "",
      "    if bias_ptr is not None:",
      "        bias_block_ptr = tl.make_block_ptr(",
      "            bias_ptr,",
      "            shape=(y_size, 1),",
      "            strides=(1, y_size),",
      "            offsets=(group * group_size, 0),",
      "            block_shape=(group_block_size, 1),",
      "            order=(0, 1),",
      "        )",
      "        bias = tl.load(bias_block_ptr, boundary_check=(0,))",
      "        output += bias",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0, 1))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))",
      "",
      "    tl.store(rstd_block_ptr, rstd.to(dtype))",
      "    tl.store(mean_block_ptr, mean.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/360.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_group_boundary_check': lambda args: args['group_size'] % args['group_block_size'], 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_bias_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "group_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "num_groups",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "group_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_group_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_bias_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    group_size: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    num_groups: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    dtype: tl.constexpr,",
      "    group_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_group_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    batch = pid // num_groups",
      "    group = pid % num_groups",
      "    num_elements = group_size * x_size",
      "    batch_offset = batch * num_groups * num_elements",
      "    group_offset = batch_offset + group * num_elements",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr + group_offset,",
      "        shape=(group_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(0, 0),",
      "        block_shape=(group_block_size, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr + batch * num_groups,",
      "        shape=(group_size,),",
      "        strides=(1,),",
      "        offsets=(group,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    rstd = tl.load(rstd_block_ptr)",
      "    mean = tl.load(mean_block_ptr)",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0, 1))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(y_size, 1),",
      "            strides=(1, y_size),",
      "            offsets=(group * group_size, 0),",
      "            block_shape=(group_block_size, 1),",
      "            order=(0, 1),",
      "        )",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "        grad_norm = weight * grad_output",
      "    else:",
      "        grad_norm = grad_output",
      "",
      "    if require_group_boundary_check | require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(0, 1))",
      "        group_condition = tl.arange(0, group_block_size) < group_size",
      "        x_condition = tl.arange(0, x_block_size) < x_size",
      "        condition = group_condition[:, None] & x_condition[None, :]",
      "        centered_mean = tl.where(condition, input - mean, 0)",
      "        grad_std = tl.sum(",
      "            tl.view(",
      "                grad_norm * centered_mean, (1, group_block_size * x_block_size)",
      "            ),",
      "            1,",
      "        )",
      "        grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)",
      "        grad_distance = 2 * centered_mean * grad_var",
      "        grad_centered_mean = tl.where(",
      "            condition, grad_norm * rstd + grad_distance, 0",
      "        )",
      "        grad_mean = (",
      "            -tl.sum(",
      "                tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1",
      "            )",
      "            / num_elements",
      "        )",
      "        grad_input = grad_centered_mean + grad_mean",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0, 1))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        centered_mean = input - mean",
      "        grad_std = tl.sum(",
      "            tl.view(",
      "                grad_norm * centered_mean, (1, group_block_size * x_block_size)",
      "            ),",
      "            1,",
      "        )",
      "        grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / (x_size * group_size)",
      "        grad_distance = 2 * centered_mean * grad_var",
      "        grad_centered_mean = grad_norm * rstd + grad_distance",
      "        grad_mean = (",
      "            -tl.sum(",
      "                tl.view(grad_centered_mean, (1, group_block_size * x_block_size)), 1",
      "            )",
      "            / num_elements",
      "        )",
      "        grad_input = grad_centered_mean + grad_mean",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))",
      "",
      "    if grad_weight_staging_ptr is not None:",
      "        norm = centered_mean * rstd",
      "        grad_weight = tl.sum(norm * grad_output, 1)",
      "        offset = batch * y_size + group * group_size",
      "        grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "            grad_weight_staging_ptr + offset,",
      "            shape=(group_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(group_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_group_boundary_check:",
      "            tl.store(",
      "                grad_weight_staging_block_ptr,",
      "                grad_weight.to(dtype),",
      "                boundary_check=(0,),",
      "            )",
      "        else:",
      "            tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))",
      "",
      "    if grad_bias_staging_ptr is not None:",
      "        grad_bias = tl.sum(grad_output, 1)",
      "        offset = batch * y_size + group * group_size",
      "        grad_bias_staging_block_ptr = tl.make_block_ptr(",
      "            grad_bias_staging_ptr + offset,",
      "            shape=(group_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(group_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_group_boundary_check:",
      "            tl.store(",
      "                grad_bias_staging_block_ptr,",
      "                grad_bias.to(dtype),",
      "                boundary_check=(0,),",
      "            )",
      "        else:",
      "            tl.store(grad_bias_staging_block_ptr, grad_bias.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/360.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    eps: tl.float32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,), padding_option=\"zero\")",
      "        mean = tl.sum(input / x_size, 1)",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        centered_mean = tl.where(condition, input - mean, 0)",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        mean = tl.sum(input / x_size, 1)",
      "        centered_mean = input - mean",
      "",
      "    var = tl.sum(centered_mean * centered_mean / x_size, 1)",
      "    rstd = tl.math.rsqrt(var + eps)",
      "    output = centered_mean * rstd",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(x_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(x_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "        else:",
      "            weight = tl.load(weight_block_ptr)",
      "",
      "        output *= weight",
      "",
      "    if bias_ptr is not None:",
      "        bias_block_ptr = tl.make_block_ptr(",
      "            bias_ptr,",
      "            shape=(x_size,),",
      "            strides=(1,),",
      "            offsets=(0,),",
      "            block_shape=(x_block_size,),",
      "            order=(0,),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            bias = tl.load(bias_block_ptr, boundary_check=(0,))",
      "        else:",
      "            bias = tl.load(bias_block_ptr)",
      "",
      "        output += bias",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))",
      "",
      "    tl.store(rstd_block_ptr, rstd.to(dtype))",
      "    tl.store(mean_block_ptr, mean.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/362.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rstd_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    weight_ptr: tl.tensor,",
      "    rstd_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(x_size, 1),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rstd_block_ptr = tl.make_block_ptr(",
      "        rstd_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(",
      "            grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"",
      "        )",
      "        input = tl.load(input_block_ptr, boundary_check=(1,), padding_option=\"zero\")",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        input = tl.load(input_block_ptr)",
      "",
      "    rstd = tl.load(rstd_block_ptr)",
      "    mean = tl.load(mean_block_ptr)",
      "    centered_mean = input - mean",
      "",
      "    if weight_ptr is not None:",
      "        weight_block_ptr = tl.make_block_ptr(",
      "            weight_ptr,",
      "            shape=(1, x_size),",
      "            strides=(x_size, 1),",
      "            offsets=(0, 0),",
      "            block_shape=(1, x_block_size),",
      "            order=(1, 0),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            weight = tl.load(weight_block_ptr, boundary_check=(1,))",
      "        else:",
      "            weight = tl.load(weight_block_ptr)",
      "",
      "        grad_norm = weight * grad_output",
      "    else:",
      "        grad_norm = grad_output",
      "",
      "    grad_std = tl.sum(grad_norm * centered_mean, 1)",
      "    grad_var = grad_std * -(0.5 * rstd * rstd * rstd) / x_size",
      "    grad_distance = 2 * centered_mean * grad_var",
      "    grad_centered_mean = grad_norm * rstd + grad_distance",
      "    grad_mean = -tl.sum(grad_centered_mean, 1) / x_size",
      "    grad_input = grad_centered_mean + grad_mean",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))",
      "",
      "    if grad_weight_staging_ptr is not None:",
      "        grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "            grad_weight_staging_ptr,",
      "            shape=(y_size, x_size),",
      "            strides=(x_size, 1),",
      "            offsets=(y_offset, 0),",
      "            block_shape=(1, x_block_size),",
      "            order=(1, 0),",
      "        )",
      "",
      "        norm = centered_mean * rstd",
      "        grad_weight = norm * grad_output",
      "",
      "        if require_x_boundary_check:",
      "            tl.store(",
      "                grad_weight_staging_block_ptr,",
      "                grad_weight.to(dtype),",
      "                boundary_check=(1,),",
      "            )",
      "        else:",
      "            tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/362.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'], 'require_n_boundary_check': lambda args: args['n_size'] % args['n_block_size'], 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_n_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_n_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    input_batch_stride: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    weight_n_stride: tl.int32,",
      "    weight_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    n_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_n_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_m_blocks = tl.cdiv(m_size, m_block_size)",
      "    num_n_blocks = tl.cdiv(n_size, n_block_size)",
      "    num_blocks = num_m_blocks * num_n_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    m_block = block // num_n_blocks",
      "    n_block = block % num_n_blocks",
      "    m_offset = m_block * m_block_size",
      "    n_offset = n_block * n_block_size",
      "",
      "    output = language.Linear.forward(",
      "        input_ptr + batch * input_batch_stride,",
      "        weight_ptr,",
      "        bias_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        n_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        n_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_n_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr + batch * m_size * n_size,",
      "        shape=(m_size, n_size),",
      "        strides=(n_size, 1),",
      "        offsets=(m_offset, n_offset),",
      "        block_shape=(m_block_size, n_block_size),",
      "        order=(1, 0),",
      "    )",
      "    if require_m_boundary_check | require_n_boundary_check:",
      "        tl.store(output_block_ptr, output, boundary_check=(0, 1))",
      "    else:",
      "        tl.store(output_block_ptr, output)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/364.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'], 'require_n_boundary_check': lambda args: args['n_size'] % args['n_block_size'], 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_n_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "weight_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_n_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    weight_n_stride: tl.int32,",
      "    weight_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    n_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_n_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_m_blocks = tl.cdiv(m_size, m_block_size)",
      "    num_k_blocks = tl.cdiv(k_size, k_block_size)",
      "    num_blocks = num_m_blocks * num_k_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    m_block = block // num_k_blocks",
      "    k_block = block % num_k_blocks",
      "    m_offset = m_block * m_block_size",
      "    k_offset = k_block * k_block_size",
      "",
      "    grad_input = language.Linear.backward(",
      "        grad_output_ptr + batch * m_size * n_size,",
      "        weight_ptr,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        weight_n_stride,",
      "        weight_k_stride,",
      "        m_offset,",
      "        k_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        n_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_n_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr + batch * m_size * k_size,",
      "        shape=(m_size, k_size),",
      "        strides=(input_m_stride, input_k_stride),",
      "        offsets=(m_offset, k_offset),",
      "        block_shape=(m_block_size, k_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_m_boundary_check | require_k_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input, boundary_check=(0, 1))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/364.py"
  },
  {
    "name": "backward_weight",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size'], 'require_n_boundary_check': lambda args: args['n_size'] % args['n_block_size'], 'require_k_boundary_check': lambda args: args['k_size'] % args['k_block_size']})"
    ],
    "args": [
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "k_size",
        "annotation": "tl.int32"
      },
      {
        "name": "input_batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_m_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "input_k_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "use_accelerator",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "k_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_n_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_k_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_weight(",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    k_size: tl.int32,",
      "    input_batch_stride: tl.int32,",
      "    input_m_stride: tl.int32,",
      "    input_k_stride: tl.int32,",
      "    use_accelerator: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    n_block_size: tl.constexpr,",
      "    k_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "    require_n_boundary_check: tl.constexpr,",
      "    require_k_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_n_blocks = tl.cdiv(n_size, n_block_size)",
      "    num_k_blocks = tl.cdiv(k_size, k_block_size)",
      "    num_blocks = num_n_blocks * num_k_blocks",
      "    batch = pid // num_blocks",
      "    block = pid % num_blocks",
      "    n_block = block // num_k_blocks",
      "    k_block = block % num_k_blocks",
      "    n_offset = n_block * n_block_size",
      "    k_offset = k_block * k_block_size",
      "",
      "    grad_weight = language.Linear.backward_weight(",
      "        grad_output_ptr + batch * m_size * n_size,",
      "        input_ptr + batch * input_batch_stride,",
      "        m_size,",
      "        n_size,",
      "        k_size,",
      "        input_m_stride,",
      "        input_k_stride,",
      "        n_offset,",
      "        k_offset,",
      "        use_accelerator,",
      "        m_block_size,",
      "        n_block_size,",
      "        k_block_size,",
      "        require_m_boundary_check,",
      "        require_n_boundary_check,",
      "        require_k_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "        grad_weight_staging_ptr + batch * n_size * k_size,",
      "        shape=(n_size, k_size),",
      "        strides=(k_size, 1),",
      "        offsets=(n_offset, k_offset),",
      "        block_shape=(n_block_size, k_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_n_boundary_check | require_k_boundary_check:",
      "        tl.store(grad_weight_staging_block_ptr, grad_weight, boundary_check=(0, 1))",
      "    else:",
      "        tl.store(grad_weight_staging_block_ptr, grad_weight)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/364.py"
  },
  {
    "name": "backward_bias",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_m_boundary_check': lambda args: args['m_size'] % args['m_block_size']})"
    ],
    "args": [
      {
        "name": "grad_bias_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "m_size",
        "annotation": "tl.int32"
      },
      {
        "name": "n_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "m_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_m_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_bias(",
      "    grad_bias_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    m_size: tl.int32,",
      "    n_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    m_block_size: tl.constexpr,",
      "    require_m_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    batch = pid // n_size",
      "    n_offset = pid % n_size",
      "    grad_bias = language.Linear.backward_bias(",
      "        grad_output_ptr + batch * m_size * n_size,",
      "        m_size,",
      "        n_size,",
      "        n_offset,",
      "        m_block_size,",
      "        require_m_boundary_check,",
      "        dtype,",
      "    )",
      "",
      "    grad_bias_staging_block_ptr = tl.make_block_ptr(",
      "        grad_bias_staging_ptr + batch * n_size,",
      "        shape=(n_size,),",
      "        strides=(1,),",
      "        offsets=(n_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    tl.store(grad_bias_staging_block_ptr, grad_bias)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/364.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "mask_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    mask_ptr: tl.tensor,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    mask_block_ptr = tl.make_block_ptr(",
      "        mask_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,))",
      "        mask = tl.load(mask_block_ptr, boundary_check=(1,))",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        mask = tl.where(condition, mask, 1)",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        mask = tl.load(mask_block_ptr)",
      "",
      "    input = tl.where(mask > language.eps, float(\"-inf\"), input)",
      "    max = tl.max(input, 1)",
      "    numerator = tl.math.fast_expf(input - max)",
      "    output = numerator / tl.sum(numerator)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/365.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "delta_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    delta_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    delta_block_ptr = tl.make_block_ptr(",
      "        delta_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        output = tl.load(output_block_ptr, boundary_check=(1,))",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))",
      "    else:",
      "        output = tl.load(output_block_ptr)",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "    delta = tl.load(delta_block_ptr)",
      "    grad_input = output * (grad_output - delta)",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/365.py"
  },
  {
    "name": "backward_delta",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "delta_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_delta(",
      "    delta_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    delta_block_ptr = tl.make_block_ptr(",
      "        delta_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(",
      "            grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"",
      "        )",
      "        output = tl.load(output_block_ptr, boundary_check=(1,))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        output = tl.load(output_block_ptr)",
      "",
      "    delta = tl.sum(grad_output * output, 1)",
      "    tl.store(delta_block_ptr, delta.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/365.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    output = language.Mean.forward(",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "    tl.store(output_block_ptr, output)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/367.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    y_offset = pid // num_x_blocks",
      "    x = pid % num_x_blocks",
      "    x_offset = x * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, x_offset),",
      "        block_shape=(1, x_block_size),",
      "        order=(0, 1),",
      "    )",
      "",
      "    grad_input = language.Mean.backward(",
      "        grad_output_ptr, y_size, x_size, y_offset, dtype, x_block_size",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/367.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_y_boundary_check': lambda args: args['y_size'] % args['y_block_size'], 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "num_batches",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "y_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_y_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    num_batches: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    batch_stride: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    y_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_y_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_y_blocks = tl.cdiv(y_size, y_block_size)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    num_blocks = num_y_blocks * num_x_blocks",
      "    batch_offset = pid // num_blocks",
      "    block = pid % num_blocks",
      "    y_block = block // num_x_blocks",
      "    x_block = block % num_x_blocks",
      "    y_offset = y_block * y_block_size",
      "    x_offset = x_block * x_block_size",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(num_batches, y_size, x_size),",
      "        strides=(batch_stride, y_stride, x_stride),",
      "        offsets=(batch_offset, y_offset, x_offset),",
      "        block_shape=(1, y_block_size, x_block_size),",
      "        order=(2, 1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(num_batches, y_size, x_size),",
      "        strides=(batch_stride, y_stride, x_stride),",
      "        offsets=(batch_offset, y_offset, x_offset),",
      "        block_shape=(1, y_block_size, x_block_size),",
      "        order=(2, 1, 0),",
      "    )",
      "    weight_block_ptr = tl.make_block_ptr(",
      "        weight_ptr,",
      "        shape=(y_size, 1),",
      "        strides=(1, 0),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(y_block_size, 1),",
      "        order=(1, 0),",
      "    )",
      "",
      "    if require_y_boundary_check | require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1, 2))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    if require_y_boundary_check:",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "    else:",
      "        weight = tl.load(weight_block_ptr)",
      "",
      "    output = language.math.LeakyReLU.forward(input, weight)",
      "",
      "    if require_y_boundary_check | require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1, 2))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/368.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_y_boundary_check': lambda args: args['y_size'] % args['y_block_size'], 'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_weight_staging_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "num_batches",
        "annotation": "tl.int32"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "batch_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "y_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_y_boundary_check",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_weight_staging_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    weight_ptr: tl.tensor,",
      "    num_batches: tl.int32,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    batch_stride: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    y_block_size: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_y_boundary_check: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_y_blocks = tl.cdiv(y_size, y_block_size)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    num_blocks = num_y_blocks * num_x_blocks",
      "    batch_offset = pid // num_blocks",
      "    block = pid % num_blocks",
      "    y_block = block // num_x_blocks",
      "    x_block = block % num_x_blocks",
      "    y_offset = y_block * y_block_size",
      "    x_offset = x_block * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(num_batches, y_size, x_size),",
      "        strides=(batch_stride, y_stride, x_stride),",
      "        offsets=(batch_offset, y_offset, x_offset),",
      "        block_shape=(1, y_block_size, x_block_size),",
      "        order=(2, 1, 0),",
      "    )",
      "    grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "        grad_weight_staging_ptr,",
      "        shape=(num_batches, y_size, x_size),",
      "        strides=(batch_stride, y_stride, x_stride),",
      "        offsets=(batch_offset, y_offset, x_offset),",
      "        block_shape=(1, y_block_size, x_block_size),",
      "        order=(2, 1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(num_batches, y_size, x_size),",
      "        strides=(batch_stride, y_stride, x_stride),",
      "        offsets=(batch_offset, y_offset, x_offset),",
      "        block_shape=(1, y_block_size, x_block_size),",
      "        order=(2, 1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(num_batches, y_size, x_size),",
      "        strides=(batch_stride, y_stride, x_stride),",
      "        offsets=(batch_offset, y_offset, x_offset),",
      "        block_shape=(1, y_block_size, x_block_size),",
      "        order=(2, 1, 0),",
      "    )",
      "    weight_block_ptr = tl.make_block_ptr(",
      "        weight_ptr,",
      "        shape=(y_size, 1),",
      "        strides=(1, 0),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(y_block_size, 1),",
      "        order=(1, 0),",
      "    )",
      "    if require_y_boundary_check | require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1, 2))",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1, 2))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "",
      "    weight = tl.load(weight_block_ptr)",
      "    grad_input = language.math.LeakyReLU.backward(grad_output, input, weight)",
      "    grad_weight = grad_output * tl.where(input > 0, 0, input)",
      "",
      "    if require_y_boundary_check | require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1, 2))",
      "        tl.store(",
      "            grad_weight_staging_block_ptr,",
      "            grad_weight.to(dtype),",
      "            boundary_check=(1, 2),",
      "        )",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))",
      "        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/368.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "rms_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "partial_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "bias_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    rms_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    partial_size: tl.constexpr,",
      "    weight_ptr: tl.tensor,",
      "    bias_ptr: tl.tensor,",
      "    eps: tl.float32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rms_block_ptr = tl.make_block_ptr(",
      "        rms_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    weight_block_ptr = tl.make_block_ptr(",
      "        weight_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(0,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(1,))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    if x_block_size != partial_size:",
      "        condition = tl.arange(0, x_block_size) < partial_size",
      "        partial_input = tl.where(condition, input, 0)",
      "    else:",
      "        partial_input = input",
      "",
      "    rms = tl.math.sqrt(tl.sum(partial_input * partial_input / partial_size, 1))",
      "    norm = input / (rms + eps)",
      "",
      "    if require_x_boundary_check:",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "    else:",
      "        weight = tl.load(weight_block_ptr)",
      "",
      "    output = norm * weight",
      "",
      "    if bias_ptr is not None:",
      "        bias_block_ptr = tl.make_block_ptr(",
      "            bias_ptr,",
      "            shape=(1, x_size),",
      "            strides=(x_stride, 1),",
      "            offsets=(0, 0),",
      "            block_shape=(1, x_block_size),",
      "            order=(1, 0),",
      "        )",
      "",
      "        if require_x_boundary_check:",
      "            bias = tl.load(bias_block_ptr, boundary_check=(1,))",
      "        else:",
      "            bias = tl.load(bias_block_ptr)",
      "",
      "        output += bias",
      "",
      "    tl.store(rms_block_ptr, rms.to(dtype))",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/370.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_weight_staging",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "rms_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "partial_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "weight_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "eps",
        "annotation": "tl.float32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_weight_staging: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    rms_ptr: tl.tensor,",
      "    partial_size: tl.constexpr,",
      "    weight_ptr: tl.tensor,",
      "    eps: tl.float32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_weight_staging_block_ptr = tl.make_block_ptr(",
      "        grad_weight_staging,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    rms_block_ptr = tl.make_block_ptr(",
      "        rms_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    weight_block_ptr = tl.make_block_ptr(",
      "        weight_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(0,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))",
      "        input = tl.load(input_block_ptr, boundary_check=(1,))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        input = tl.load(input_block_ptr)",
      "",
      "    rms = tl.load(rms_block_ptr)",
      "",
      "    if require_x_boundary_check:",
      "        weight = tl.load(weight_block_ptr, boundary_check=(0,))",
      "    else:",
      "        weight = tl.load(weight_block_ptr)",
      "",
      "    grad_norm = grad_output * weight",
      "    norm = input / (rms + eps)",
      "    grad_weight = grad_output * norm",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(",
      "            grad_weight_staging_block_ptr,",
      "            grad_weight.to(dtype),",
      "            boundary_check=(1,),",
      "        )",
      "    else:",
      "        tl.store(grad_weight_staging_block_ptr, grad_weight.to(dtype))",
      "",
      "    grad_rms = grad_norm * -input / (rms * rms + eps)",
      "",
      "    if require_x_boundary_check:",
      "        condition = tl.arange(0, x_block_size) < x_size",
      "        grad_rms = tl.where(condition, grad_rms, 0.0)",
      "",
      "    grad_rms = tl.sum(grad_rms, 1)",
      "    grad_mean_square = grad_rms / (2 * rms)",
      "    grad_partial_input = 2 * input * grad_mean_square / partial_size",
      "",
      "    if x_block_size != partial_size:",
      "        condition = tl.arange(0, x_block_size) < partial_size",
      "        grad_partial_input = tl.where(condition, grad_partial_input, 0)",
      "",
      "    grad_input = (grad_norm / (rms + eps)) + grad_partial_input",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/370.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    x_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    x_offset = tl.program_id(0) * x_block_size",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        input = tl.load(input_block_ptr, boundary_check=(0,))",
      "    else:",
      "        input = tl.load(input_block_ptr)",
      "",
      "    sigma = 1 / (1 + tl.math.fast_expf(-input.to(tl.float32)))",
      "    output = input * sigma",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(output_block_ptr, output.to(dtype), boundary_check=(0,))",
      "    else:",
      "        tl.store(output_block_ptr, output.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/372.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    x_size: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    x_offset = tl.program_id(0) * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(x_size,),",
      "        strides=(1,),",
      "        offsets=(x_offset,),",
      "        block_shape=(x_block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        grad_output = tl.load(grad_output_block_ptr, boundary_check=(0,))",
      "        input = tl.load(input_block_ptr, boundary_check=(0,))",
      "    else:",
      "        grad_output = tl.load(grad_output_block_ptr)",
      "        input = tl.load(input_block_ptr)",
      "",
      "    sigma = 1 / (1 + tl.math.fast_expf(-input.to(tl.float32)))",
      "    grad_input = grad_output * (sigma + input * sigma * (1 - sigma))",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype), boundary_check=(0,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/372.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    max = tl.full((1, x_block_size), -float(\"inf\"), tl.float32)",
      "    sum = tl.zeros((1, x_block_size), tl.float32)",
      "",
      "    for x_offset in range(0, x_size, x_block_size):",
      "        if require_x_boundary_check:",
      "            input = tl.load(input_block_ptr, boundary_check=(1,))",
      "            condition = tl.arange(0, x_block_size) + x_offset < x_size",
      "            input = tl.where(condition, input, -float(\"inf\"))",
      "            peak = tl.where(condition, tl.maximum(max, input), 0)",
      "        else:",
      "            input = tl.load(input_block_ptr)",
      "            peak = tl.maximum(max, input)",
      "",
      "        sum = sum * tl.math.fast_expf(max - peak) + tl.math.fast_expf(input - peak)",
      "        max = peak",
      "        input_block_ptr = tl.advance(input_block_ptr, (0, x_block_size))",
      "",
      "    max, sum = tl.reduce((max, sum), 1, language.combine_softmax)",
      "",
      "    input_block_ptr = tl.make_block_ptr(",
      "        input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for x_offset in range(0, x_size, x_block_size):",
      "        if require_x_boundary_check:",
      "            input = tl.load(input_block_ptr, boundary_check=(1,))",
      "        else:",
      "            input = tl.load(input_block_ptr)",
      "",
      "        output = tl.math.fast_expf(input - max) / sum",
      "",
      "        if require_x_boundary_check:",
      "            tl.store(output_block_ptr, output.to(dtype), boundary_check=(1,))",
      "        else:",
      "            tl.store(output_block_ptr, output.to(dtype))",
      "",
      "        output_block_ptr = tl.advance(output_block_ptr, (0, x_block_size))",
      "        input_block_ptr = tl.advance(input_block_ptr, (0, x_block_size))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/373.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "delta_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    delta_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    delta_block_ptr = tl.make_block_ptr(",
      "        delta_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    for x_offset in range(0, x_size, x_block_size):",
      "        if require_x_boundary_check:",
      "            output = tl.load(output_block_ptr, boundary_check=(1,))",
      "            grad_output = tl.load(grad_output_block_ptr, boundary_check=(1,))",
      "        else:",
      "            output = tl.load(output_block_ptr)",
      "            grad_output = tl.load(grad_output_block_ptr)",
      "",
      "        delta = tl.load(delta_block_ptr)",
      "        grad_input = output * (grad_output - delta)",
      "",
      "        if require_x_boundary_check:",
      "            tl.store(",
      "                grad_input_block_ptr, grad_input.to(dtype), boundary_check=(1,)",
      "            )",
      "        else:",
      "            tl.store(grad_input_block_ptr, grad_input.to(dtype))",
      "",
      "        grad_input_block_ptr = tl.advance(grad_input_block_ptr, (0, x_block_size))",
      "        grad_output_block_ptr = tl.advance(grad_output_block_ptr, (0, x_block_size))",
      "        output_block_ptr = tl.advance(output_block_ptr, (0, x_block_size))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/373.py"
  },
  {
    "name": "backward_delta",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "delta_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward_delta(",
      "    delta_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    output_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    delta_block_ptr = tl.make_block_ptr(",
      "        delta_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    grad_output_block_ptr = tl.make_block_ptr(",
      "        grad_output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    delta = tl.zeros((1, x_block_size), dtype)",
      "",
      "    for _ in range(0, x_size, x_block_size):",
      "        if require_x_boundary_check:",
      "            grad_output = tl.load(",
      "                grad_output_block_ptr, boundary_check=(1,), padding_option=\"zero\"",
      "            )",
      "            output = tl.load(output_block_ptr, boundary_check=(1,))",
      "        else:",
      "            grad_output = tl.load(grad_output_block_ptr)",
      "            output = tl.load(output_block_ptr)",
      "",
      "        delta += grad_output * output",
      "        output_block_ptr = tl.advance(output_block_ptr, (0, x_block_size))",
      "        grad_output_block_ptr = tl.advance(grad_output_block_ptr, (0, x_block_size))",
      "",
      "    delta = tl.sum(delta, 1)",
      "    tl.store(delta_block_ptr, delta.to(dtype))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/373.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output = language.Sum.forward(",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    tl.store(output_block_ptr, output)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/374.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, 0),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    grad_input = language.Sum.backward(",
      "        grad_output_ptr, y_size, y_offset, x_block_size",
      "    )",
      "",
      "    for x_offset in range(0, x_size, x_block_size):",
      "        if require_x_boundary_check:",
      "            tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))",
      "        else:",
      "            tl.store(grad_input_block_ptr, grad_input)",
      "",
      "        grad_input_block_ptr = tl.advance(grad_input_block_ptr, (0, x_block_size))"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/374.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "correction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    correction: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    output, mean = language.VarMean.forward(",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        correction,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "    tl.store(output_block_ptr, output)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/375.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "correction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    correction: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    y_offset = pid // num_x_blocks",
      "    x = pid % num_x_blocks",
      "    x_offset = x * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, x_offset),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "",
      "    mean = language.Mean.forward(",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "    grad_input = language.Var.backward(",
      "        grad_output_ptr,",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        x_offset,",
      "        mean,",
      "        correction,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/375.py"
  },
  {
    "name": "forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "correction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def forward(",
      "    output_ptr: tl.tensor,",
      "    mean_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    correction: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    y_offset = tl.program_id(0)",
      "",
      "    output_block_ptr = tl.make_block_ptr(",
      "        output_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    output, mean = language.VarMean.forward(",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        correction,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "    tl.store(output_block_ptr, output)",
      "    tl.store(mean_block_ptr, mean)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/376.py"
  },
  {
    "name": "backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'require_x_boundary_check': lambda args: args['x_size'] % args['x_block_size']})"
    ],
    "args": [
      {
        "name": "grad_input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "grad_output_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "input_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "y_size",
        "annotation": "tl.int32"
      },
      {
        "name": "x_size",
        "annotation": "tl.int32"
      },
      {
        "name": "y_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "x_stride",
        "annotation": "tl.int32"
      },
      {
        "name": "mean_ptr",
        "annotation": "tl.tensor"
      },
      {
        "name": "correction",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "require_x_boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def backward(",
      "    grad_input_ptr: tl.tensor,",
      "    grad_output_ptr: tl.tensor,",
      "    input_ptr: tl.tensor,",
      "    y_size: tl.int32,",
      "    x_size: tl.int32,",
      "    y_stride: tl.int32,",
      "    x_stride: tl.int32,",
      "    mean_ptr: tl.tensor,",
      "    correction: tl.constexpr,",
      "    dtype: tl.constexpr,",
      "    x_block_size: tl.constexpr,",
      "    require_x_boundary_check: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_x_blocks = tl.cdiv(x_size, x_block_size)",
      "    y_offset = pid // num_x_blocks",
      "    x = pid % num_x_blocks",
      "    x_offset = x * x_block_size",
      "",
      "    grad_input_block_ptr = tl.make_block_ptr(",
      "        grad_input_ptr,",
      "        shape=(y_size, x_size),",
      "        strides=(y_stride, x_stride),",
      "        offsets=(y_offset, x_offset),",
      "        block_shape=(1, x_block_size),",
      "        order=(1, 0),",
      "    )",
      "    mean_block_ptr = tl.make_block_ptr(",
      "        mean_ptr,",
      "        shape=(y_size,),",
      "        strides=(1,),",
      "        offsets=(y_offset,),",
      "        block_shape=(1,),",
      "        order=(0,),",
      "    )",
      "",
      "    mean = tl.load(mean_block_ptr)",
      "    grad_input = language.Var.backward(",
      "        grad_output_ptr,",
      "        input_ptr,",
      "        y_size,",
      "        x_size,",
      "        y_stride,",
      "        x_stride,",
      "        y_offset,",
      "        x_offset,",
      "        mean,",
      "        correction,",
      "        dtype,",
      "        x_block_size,",
      "        require_x_boundary_check,",
      "    )",
      "",
      "    if require_x_boundary_check:",
      "        tl.store(grad_input_block_ptr, grad_input, boundary_check=(1,))",
      "    else:",
      "        tl.store(grad_input_block_ptr, grad_input)"
    ],
    "file": "triton_repos/kakaobrain_trident/trident/kernel/376.py"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune([triton.Config({'BLOCK_SIZE_Q': BLOCK_SIZE_Q, 'BLOCK_SIZE_KV': BLOCK_SIZE_KV}, num_stages=num_stages, num_warps=num_warps) for BLOCK_SIZE_Q in [64, 128] for BLOCK_SIZE_KV in [32, 64] for num_stages in [3, 4, 7] for num_warps in [2, 4]], key=['SEQ_LEN', 'HEAD_DIM'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "stride_Q_batch",
        "annotation": null
      },
      {
        "name": "stride_Q_head",
        "annotation": null
      },
      {
        "name": "stride_Q_seq",
        "annotation": null
      },
      {
        "name": "stride_Q_dim",
        "annotation": null
      },
      {
        "name": "stride_K_batch",
        "annotation": null
      },
      {
        "name": "stride_K_head",
        "annotation": null
      },
      {
        "name": "stride_K_seq",
        "annotation": null
      },
      {
        "name": "stride_K_dim",
        "annotation": null
      },
      {
        "name": "stride_V_batch",
        "annotation": null
      },
      {
        "name": "stride_V_head",
        "annotation": null
      },
      {
        "name": "stride_V_seq",
        "annotation": null
      },
      {
        "name": "stride_V_dim",
        "annotation": null
      },
      {
        "name": "stride_O_batch",
        "annotation": null
      },
      {
        "name": "stride_O_head",
        "annotation": null
      },
      {
        "name": "stride_O_seq",
        "annotation": null
      },
      {
        "name": "stride_O_dim",
        "annotation": null
      },
      {
        "name": "BATCH_SIZE",
        "annotation": null
      },
      {
        "name": "NUM_HEADS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEQ_LEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_Q",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_KV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    softmax_scale,",
      "    M,",
      "    O,",
      "    stride_Q_batch,",
      "    stride_Q_head,",
      "    stride_Q_seq,",
      "    stride_Q_dim,",
      "    stride_K_batch,",
      "    stride_K_head,",
      "    stride_K_seq,",
      "    stride_K_dim,",
      "    stride_V_batch,",
      "    stride_V_head,",
      "    stride_V_seq,",
      "    stride_V_dim,",
      "    stride_O_batch,",
      "    stride_O_head,",
      "    stride_O_seq,",
      "    stride_O_dim,",
      "    BATCH_SIZE,",
      "    NUM_HEADS: tl.constexpr,",
      "    SEQ_LEN: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    BLOCK_SIZE_Q: tl.constexpr,",
      "    BLOCK_SIZE_KV: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_SIZE_KV <= HEAD_DIM)",
      "",
      "    block_index_q = tl.program_id(0)",
      "",
      "    index_batch_head = tl.program_id(1)",
      "",
      "    index_batch = index_batch_head // NUM_HEADS",
      "",
      "    index_head = index_batch_head % NUM_HEADS",
      "",
      "    qvk_offset = (",
      "        index_batch.to(tl.int64) * stride_Q_batch",
      "        + index_head.to(tl.int64) * stride_Q_head",
      "    )",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(SEQ_LEN, HEAD_DIM),",
      "        strides=(stride_Q_seq, stride_Q_dim),",
      "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),",
      "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(SEQ_LEN, HEAD_DIM),",
      "        strides=(stride_V_seq, stride_V_dim),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_SIZE_KV, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(HEAD_DIM, SEQ_LEN),",
      "        strides=(",
      "            stride_K_dim,",
      "            stride_K_seq,",
      "        ),",
      "        offsets=(0, 0),",
      "        block_shape=(HEAD_DIM, BLOCK_SIZE_KV),",
      "        order=(0, 1),",
      "    )",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=O + qvk_offset,",
      "        shape=(SEQ_LEN, HEAD_DIM),",
      "        strides=(stride_O_seq, stride_O_dim),",
      "        offsets=(block_index_q * BLOCK_SIZE_Q, 0),",
      "        block_shape=(BLOCK_SIZE_Q, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_q = block_index_q * BLOCK_SIZE_Q + tl.arange(0, BLOCK_SIZE_Q)",
      "",
      "    offs_kv = tl.arange(0, BLOCK_SIZE_KV)",
      "",
      "    m_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) - float(\"inf\")",
      "",
      "    l_i = tl.zeros([BLOCK_SIZE_Q], dtype=tl.float32) + 1.0",
      "",
      "    O_block = tl.zeros([BLOCK_SIZE_Q, HEAD_DIM], dtype=tl.float32)",
      "",
      "    Q_block = tl.load(Q_block_ptr)",
      "",
      "    if STAGE == 1 or STAGE == 3:",
      "",
      "        O_block, l_i, m_i = _attn_fwd_inner(",
      "            O_block,",
      "            l_i,",
      "            m_i,",
      "            Q_block,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            block_index_q,",
      "            softmax_scale,",
      "            BLOCK_SIZE_Q,",
      "            BLOCK_SIZE_KV,",
      "            4 - STAGE,",
      "            offs_q,",
      "            offs_kv,",
      "            SEQ_LEN,",
      "        )",
      "",
      "    if STAGE == 3:",
      "",
      "        O_block, l_i, m_i = _attn_fwd_inner(",
      "            O_block,",
      "            l_i,",
      "            m_i,",
      "            Q_block,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            block_index_q,",
      "            softmax_scale,",
      "            BLOCK_SIZE_Q,",
      "            BLOCK_SIZE_KV,",
      "            2,",
      "            offs_q,",
      "            offs_kv,",
      "            SEQ_LEN,",
      "        )",
      "",
      "    m_i += tl.math.log(l_i)",
      "    O_block = O_block / l_i[:, None]",
      "    m_ptrs = M + index_batch_head * SEQ_LEN + offs_q",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, O_block.to(O.type.element_ty))"
    ],
    "file": "triton_repos/hkproj_triton-flash-attention/triton/336.py"
  },
  {
    "name": "_attn_fwd_base_opt",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(base_config, key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "profile_mem",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_base_opt(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    profile_mem,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    desc_q = None",
      "    desc_k = None",
      "    desc_v = None",
      "    desc_o = None",
      "",
      "    if ENABLE_TMA:",
      "        desc_k = tl.make_tensor_descriptor(",
      "            K,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_N, HEAD_DIM],",
      "        )",
      "        if V.dtype == torch.float8_e5m2:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * HEAD_DIM, N_CTX],",
      "                strides=[N_CTX, 1],",
      "                block_shape=[HEAD_DIM, BLOCK_N],",
      "            )",
      "        else:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * N_CTX, HEAD_DIM],",
      "                strides=[HEAD_DIM, 1],",
      "                block_shape=[BLOCK_N, HEAD_DIM],",
      "            )",
      "",
      "        desc_q = tl.make_tensor_descriptor(",
      "            Q,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "        desc_o = tl.make_tensor_descriptor(",
      "            Out,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "",
      "    _attn_fwd_compute(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        desc_q,",
      "        desc_k,",
      "        desc_v,",
      "        desc_o,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        off_hz,",
      "        pid,",
      "        Z,",
      "        H,",
      "        N_CTX,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        HEAD_DIM,",
      "        STAGE,",
      "        ENABLE_TMA,",
      "        LOOP_SCHEDULE,",
      "    )"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/kernels/433.py"
  },
  {
    "name": "_attn_fwd_ws",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsWS)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "desc_q",
        "annotation": null
      },
      {
        "name": "desc_k",
        "annotation": null
      },
      {
        "name": "desc_v",
        "annotation": null
      },
      {
        "name": "desc_o",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_ws(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    desc_q,",
      "    desc_k,",
      "    desc_v,",
      "    desc_o,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    _attn_fwd_compute_ws(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        desc_q,",
      "        desc_k,",
      "        desc_v,",
      "        desc_o,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        off_hz,",
      "        pid,",
      "        Z,",
      "        H,",
      "        N_CTX,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        HEAD_DIM,",
      "        STAGE,",
      "        ENABLE_TMA,",
      "        LOOP_SCHEDULE,",
      "    )"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/kernels/434.py"
  },
  {
    "name": "_attn_fwd_base_opt",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsOrig + configsOpt)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "desc_q",
        "annotation": null
      },
      {
        "name": "desc_k",
        "annotation": null
      },
      {
        "name": "desc_v",
        "annotation": null
      },
      {
        "name": "desc_o",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_base_opt(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    desc_q,",
      "    desc_k,",
      "    desc_v,",
      "    desc_o,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.assume(stride_qz >= 0)",
      "    tl.assume(stride_qh >= 0)",
      "    tl.assume(stride_qm >= 0)",
      "    tl.assume(stride_qk >= 0)",
      "    tl.assume(stride_kz >= 0)",
      "    tl.assume(stride_kh >= 0)",
      "    tl.assume(stride_kn >= 0)",
      "    tl.assume(stride_kk >= 0)",
      "    tl.assume(stride_vz >= 0)",
      "    tl.assume(stride_vh >= 0)",
      "    tl.assume(stride_vk >= 0)",
      "    tl.assume(stride_vn >= 0)",
      "    tl.assume(stride_oz >= 0)",
      "    tl.assume(stride_oh >= 0)",
      "    tl.assume(stride_om >= 0)",
      "    tl.assume(stride_on >= 0)",
      "    tl.assume(Z >= 0)",
      "    tl.assume(H >= 0)",
      "",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    _attn_fwd_compute(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        desc_q,",
      "        desc_k,",
      "        desc_v,",
      "        desc_o,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        off_hz,",
      "        pid,",
      "        Z,",
      "        H,",
      "        N_CTX,",
      "        BLOCK_M,",
      "        BLOCK_N,",
      "        HEAD_DIM,",
      "        STAGE,",
      "        ENABLE_TMA,",
      "        LOOP_SCHEDULE,",
      "    )"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/kernels/434.py"
  },
  {
    "name": "_attn_fwd_tma_unified",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsTma + configsTmaWS)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_tma_unified(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    pid = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    desc_q = None",
      "    desc_k = None",
      "    desc_v = None",
      "    desc_o = None",
      "",
      "    if ENABLE_TMA:",
      "        desc_k = tl.make_tensor_descriptor(",
      "            K,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_N, HEAD_DIM],",
      "        )",
      "        if V.dtype == torch.float8_e5m2:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * HEAD_DIM, N_CTX],",
      "                strides=[N_CTX, 1],",
      "                block_shape=[HEAD_DIM, BLOCK_N],",
      "            )",
      "        else:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * N_CTX, HEAD_DIM],",
      "                strides=[HEAD_DIM, 1],",
      "                block_shape=[BLOCK_N, HEAD_DIM],",
      "            )",
      "",
      "        desc_q = tl.make_tensor_descriptor(",
      "            Q,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "        desc_o = tl.make_tensor_descriptor(",
      "            Out,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "",
      "    if ENABLE_WS:",
      "        _attn_fwd_compute_ws(",
      "            Q,",
      "            K,",
      "            V,",
      "            sm_scale,",
      "            M,",
      "            Out,",
      "            desc_q,",
      "            desc_k,",
      "            desc_v,",
      "            desc_o,",
      "            stride_qz,",
      "            stride_qh,",
      "            stride_qm,",
      "            stride_qk,",
      "            stride_kz,",
      "            stride_kh,",
      "            stride_kn,",
      "            stride_kk,",
      "            stride_vz,",
      "            stride_vh,",
      "            stride_vk,",
      "            stride_vn,",
      "            stride_oz,",
      "            stride_oh,",
      "            stride_om,",
      "            stride_on,",
      "            off_hz,",
      "            pid,",
      "            Z,",
      "            H,",
      "            N_CTX,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            HEAD_DIM,",
      "            STAGE,",
      "            ENABLE_TMA,",
      "            LOOP_SCHEDULE,",
      "        )",
      "    else:",
      "        _attn_fwd_compute(",
      "            Q,",
      "            K,",
      "            V,",
      "            sm_scale,",
      "            M,",
      "            Out,",
      "            desc_q,",
      "            desc_k,",
      "            desc_v,",
      "            desc_o,",
      "            stride_qz,",
      "            stride_qh,",
      "            stride_qm,",
      "            stride_qk,",
      "            stride_kz,",
      "            stride_kh,",
      "            stride_kn,",
      "            stride_kk,",
      "            stride_vz,",
      "            stride_vh,",
      "            stride_vk,",
      "            stride_vn,",
      "            stride_oz,",
      "            stride_oh,",
      "            stride_om,",
      "            stride_on,",
      "            off_hz,",
      "            pid,",
      "            Z,",
      "            H,",
      "            N_CTX,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            HEAD_DIM,",
      "            STAGE,",
      "            ENABLE_TMA,",
      "            LOOP_SCHEDULE,",
      "        )"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/kernels/434.py"
  },
  {
    "name": "_attn_fwd_tma_ws_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configsTmaWSPersistent)), key=['N_CTX'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_TMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOOP_SCHEDULE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_WS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GRID_MULTIPLE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd_tma_ws_persistent(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HEAD_DIM: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    ENABLE_TMA: tl.constexpr,",
      "    LOOP_SCHEDULE: tl.constexpr,",
      "    ENABLE_WS: tl.constexpr,",
      "    GRID_MULTIPLE: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "",
      "    n_tile_num = tl.cdiv(N_CTX, BLOCK_M)",
      "    prog_id = tl.program_id(0)",
      "    num_progs = tl.num_programs(0)",
      "    total_tiles = n_tile_num * Z * H",
      "",
      "    tiles_per_sm = total_tiles // num_progs",
      "    if prog_id < total_tiles % num_progs:",
      "        tiles_per_sm += 1",
      "",
      "    tile_idx = prog_id",
      "",
      "    if ENABLE_TMA:",
      "        desc_k = tl.make_tensor_descriptor(",
      "            K,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_N, HEAD_DIM],",
      "        )",
      "        if V.dtype == torch.float8_e5m2:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * HEAD_DIM, N_CTX],",
      "                strides=[N_CTX, 1],",
      "                block_shape=[HEAD_DIM, BLOCK_N],",
      "            )",
      "        else:",
      "            desc_v = tl.make_tensor_descriptor(",
      "                V,",
      "                shape=[Z * H * N_CTX, HEAD_DIM],",
      "                strides=[HEAD_DIM, 1],",
      "                block_shape=[BLOCK_N, HEAD_DIM],",
      "            )",
      "",
      "        desc_q = tl.make_tensor_descriptor(",
      "            Q,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "        desc_o = tl.make_tensor_descriptor(",
      "            Out,",
      "            shape=[Z * H * N_CTX, HEAD_DIM],",
      "            strides=[HEAD_DIM, 1],",
      "            block_shape=[BLOCK_M, HEAD_DIM],",
      "        )",
      "",
      "    for _ in range(0, tiles_per_sm):",
      "",
      "        pid = tile_idx % n_tile_num",
      "        off_hz = tile_idx // n_tile_num",
      "        _attn_fwd_compute_ws(",
      "            Q,",
      "            K,",
      "            V,",
      "            sm_scale,",
      "            M,",
      "            Out,",
      "            desc_q,",
      "            desc_k,",
      "            desc_v,",
      "            desc_o,",
      "            stride_qz,",
      "            stride_qh,",
      "            stride_qm,",
      "            stride_qk,",
      "            stride_kz,",
      "            stride_kh,",
      "            stride_kn,",
      "            stride_kk,",
      "            stride_vz,",
      "            stride_vh,",
      "            stride_vk,",
      "            stride_vn,",
      "            stride_oz,",
      "            stride_oh,",
      "            stride_om,",
      "            stride_on,",
      "            off_hz,",
      "            pid,",
      "            Z,",
      "            H,",
      "            N_CTX,",
      "            BLOCK_M,",
      "            BLOCK_N,",
      "            HEAD_DIM,",
      "            STAGE,",
      "            ENABLE_TMA,",
      "            LOOP_SCHEDULE,",
      "        )",
      "        tile_idx += num_progs"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/kernels/434.py"
  },
  {
    "name": "bf16xbf16_matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bf16xbf16_matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/bf16xint16_gemm/435.py"
  },
  {
    "name": "bf16xint16_matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TRANSPOSE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bf16xint16_matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    TRANSPOSE: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0)",
      "        if TRANSPOSE:",
      "            tl.static_assert(a.dtype == tl.int16)",
      "            tl.static_assert(b.dtype == tl.bfloat16)",
      "            a_bf16 = a.to(tl.bfloat16)",
      "            b_bf16 = b",
      "        else:",
      "            tl.static_assert(a.dtype == tl.bfloat16)",
      "            tl.static_assert(b.dtype == tl.int16)",
      "            a_bf16 = a",
      "            b_bf16 = b.to(tl.bfloat16)",
      "",
      "        accumulator = tl.dot(a_bf16, b_bf16, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/bf16xint16_gemm/435.py"
  },
  {
    "name": "matmul_kernel_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            start_m = pid_m * BLOCK_SIZE_M",
      "            start_n = pid_n * BLOCK_SIZE_N",
      "            offs_am = tl.arange(0, BLOCK_SIZE_M)",
      "            offs_bn = tl.arange(0, BLOCK_SIZE_N)",
      "            offs_am = tl.where(offs_am < M - start_m, offs_am, 0)",
      "            offs_bn = tl.where(offs_bn < N - start_n, offs_bn, 0)",
      "            offs_am = tl.max_contiguous(",
      "                tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M",
      "            )",
      "            offs_bn = tl.max_contiguous(",
      "                tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N",
      "            )",
      "        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "        a = tl.load(",
      "            a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0",
      "        )",
      "        b = tl.load(",
      "            b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0",
      "        )",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "            if c_ptr.dtype == tl.float8e4nv:",
      "                c = accumulator.to(tl.float8e4nv)",
      "            else:",
      "                c = accumulator.to(tl.float16)",
      "            tl.store(c_ptrs, c, mask=c_mask)",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/fp8_gemm/436.py"
  },
  {
    "name": "matmul_kernel_tma_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc_ptr",
        "annotation": null
      },
      {
        "name": "b_desc_ptr",
        "annotation": null
      },
      {
        "name": "c_desc_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_tma_persistent(",
      "    a_desc_ptr,",
      "    b_desc_ptr,",
      "    c_desc_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    FP8_OUTPUT: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = tl._experimental_descriptor_load(",
      "            a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype",
      "        )",
      "        b = tl._experimental_descriptor_load(",
      "            b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype",
      "        )",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            c = accumulator.to(dtype)",
      "",
      "            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/fp8_gemm/436.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        accumulator = leaky_relu(accumulator)",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/fp8_gemm/437.py"
  },
  {
    "name": "triton_red_fused_mv_0",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 8}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 4}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 8, 'RBLOCK': 512}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 8, 'RBLOCK': 256}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 64, 'RBLOCK': 64}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr1",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_mv_0(",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr1,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0).to(tl.int64) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None].to(tl.int64)",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :].to(tl.int64)",
      "    x0 = xindex",
      "",
      "    tmp0 = tl.load(in_ptr0 + (x0 // rnumel), None, eviction_policy=\"evict_last\")",
      "    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp7 = tl.load(in_ptr2 + (r1), None, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp1 = tmp0 + 8",
      "        tmp2 = tmp0 < 0",
      "        tmp3 = tl.where(tmp2, tmp1, tmp0)",
      "",
      "        tmp4 = tl.load(",
      "            in_ptr1 + (r1 + (rnumel * (x0 % rnumel)) + (rnumel * rnumel * tmp3)),",
      "            None,",
      "            eviction_policy=\"evict_first\",",
      "        )",
      "        tmp5 = tmp4.to(tl.float32)",
      "        tmp6 = tmp5.to(tl.float32)",
      "        tmp8 = tmp7.to(tl.float32)",
      "        tmp9 = tmp6 * tmp8",
      "        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])",
      "        tmp12 = _tmp11 + tmp10",
      "        _tmp11 = tmp12",
      "    tmp11 = tl.sum(_tmp11, 1)[:, None]",
      "    tmp13 = tmp11.to(tl.float32)",
      "    tl.store(out_ptr1 + (x0), tmp13, None)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/gather_gemv/438.py"
  },
  {
    "name": "_matmul_partition_k",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_mm_configs(), key=['M', 'N', 'K', 'PK'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_buf_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "PK",
        "annotation": null
      },
      {
        "name": "PK_SIZE",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cb_m",
        "annotation": null
      },
      {
        "name": "stride_cb_n",
        "annotation": null
      },
      {
        "name": "stride_cb_k",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _matmul_partition_k(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_buf_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    PK,",
      "    PK_SIZE,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cb_m,",
      "    stride_cb_n,",
      "    stride_cb_k,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_pk = PK",
      "    num_pid_nk = num_pid_n * num_pid_pk",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_nk",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_nk = (pid % num_pid_in_group) // group_size_m",
      "    pid_n = pid_nk // num_pid_pk",
      "    pid_pk = pid_nk % num_pid_pk",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = (pid_pk * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)) % K",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(PK_SIZE, BLOCK_SIZE_K)):",
      "",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "        accumulator += tl.dot(a, b)",
      "        a_ptrs += PK_SIZE * stride_ak",
      "        b_ptrs += PK_SIZE * stride_bk",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_ck = pid_pk",
      "    c_buf_ptrs = (",
      "        c_buf_ptr",
      "        + stride_cb_m * offs_cm[:, None, None]",
      "        + stride_cb_n * offs_cn[None, :, None]",
      "        + stride_cb_k * offs_ck[None, None, :]",
      "    )",
      "    tl.store(c_buf_ptrs, accumulator[:, :, None])"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/gemm/439.py"
  },
  {
    "name": "matmul_kernel_persistent",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=tuning_configs, key=['M', 'N', 'K'])",
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_BUFFER_OPS_ASSUMES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,",
      "):",
      "    if ENABLE_BUFFER_OPS_ASSUMES:",
      "        tl.assume(M >= 0)",
      "        tl.assume(N >= 0)",
      "        tl.assume(K >= 0)",
      "        tl.assume(stride_am >= 0)",
      "        tl.assume(stride_ak >= 0)",
      "        tl.assume(stride_bn >= 0)",
      "        tl.assume(stride_bk >= 0)",
      "        tl.assume(stride_cm >= 0)",
      "        tl.assume(stride_cn >= 0)",
      "",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    offs_k_for_mask = tl.arange(0, BLOCK_K)",
      "",
      "    num_pid_in_group = GROUP_M * num_pid_n",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = tl.arange(0, BLOCK_M)",
      "    offs_bn = tl.arange(0, BLOCK_N)",
      "",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            start_m = pid_m * BLOCK_M",
      "            start_n = pid_n * BLOCK_N",
      "            offs_am = start_m + tl.arange(0, BLOCK_M)",
      "            offs_bn = start_n + tl.arange(0, BLOCK_N)",
      "            offs_am = tl.where(offs_am < M, offs_am, 0)",
      "            offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "            offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_M), BLOCK_M)",
      "            offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_N), BLOCK_N)",
      "        offs_k = ki * BLOCK_K + tl.arange(0, BLOCK_K)",
      "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "            offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "            if c_ptr.dtype == tl.float8e4nv:",
      "                c = accumulator.to(tl.float8e4nv)",
      "            else:",
      "                c = accumulator.to(tl.float16)",
      "            tl.store(c_ptrs, c, mask=c_mask)",
      "            accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/gemm/440.py"
  },
  {
    "name": "matmul_kernel_tma_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_desc_ptr",
        "annotation": null
      },
      {
        "name": "b_desc_ptr",
        "annotation": null
      },
      {
        "name": "c_desc_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "FP8_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_tma_persistent(",
      "    a_desc_ptr,",
      "    b_desc_ptr,",
      "    c_desc_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    FP8_OUTPUT: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    dtype = tl.float8e4nv if FP8_OUTPUT else tl.float16",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = 0",
      "    offs_bn = 0",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            offs_am = pid_m * BLOCK_SIZE_M",
      "            offs_bn = pid_n * BLOCK_SIZE_N",
      "",
      "        offs_k = ki * BLOCK_SIZE_K",
      "",
      "        a = tl._experimental_descriptor_load(",
      "            a_desc_ptr, [offs_am, offs_k], [BLOCK_SIZE_M, BLOCK_SIZE_K], dtype",
      "        )",
      "        b = tl._experimental_descriptor_load(",
      "            b_desc_ptr, [offs_bn, offs_k], [BLOCK_SIZE_N, BLOCK_SIZE_K], dtype",
      "        )",
      "        accumulator = tl.dot(a, b.T, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            c = accumulator.to(dtype)",
      "",
      "            tl._experimental_descriptor_store(c_desc_ptr, c, [offs_am, offs_bn])",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/gemm/440.py"
  },
  {
    "name": "streamk_gemm",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=tuning_configs, key=['M', 'N', 'K'])",
      "@triton.heuristics(values={'EVEN_M': lambda args: args['M'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['N'] % args['BLOCK_N'] == 0, 'EVEN_K': lambda args: args['K'] % args['BLOCK_K'] == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "bias_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_bias_m",
        "annotation": null
      },
      {
        "name": "stride_bias_n",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_XCDS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STREAMK_TILES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_BUFFER_OPS_ASSUMES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def streamk_gemm(",
      "    A,",
      "    B,",
      "    C,",
      "    bias_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_bias_m,",
      "    stride_bias_n,",
      "    stride_cm,",
      "    stride_cn,",
      "    HAS_BIAS: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "    NUM_XCDS: tl.constexpr,",
      "    STREAMK_TILES: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,",
      "):",
      "    if ENABLE_BUFFER_OPS_ASSUMES:",
      "        tl.assume(M >= 0)",
      "        tl.assume(N >= 0)",
      "        tl.assume(K >= 0)",
      "        tl.assume(stride_am >= 0)",
      "        tl.assume(stride_ak >= 0)",
      "        tl.assume(stride_bn >= 0)",
      "        tl.assume(stride_bk >= 0)",
      "        tl.assume(stride_cm >= 0)",
      "        tl.assume(stride_cn >= 0)",
      "    if stride_bias_m:",
      "        tl.assume(stride_bias_m >= 0)",
      "    if stride_bias_n:",
      "        tl.assume(stride_bias_n >= 0)",
      "",
      "    pid = tl.program_id(0)",
      "    if NUM_XCDS != 1:",
      "        pid = (pid % NUM_XCDS) * (NUM_SMS // NUM_XCDS) + (pid // NUM_XCDS)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    iters_per_tile = tl.cdiv(K, BLOCK_K)",
      "    total_tiles = num_pid_m * num_pid_n",
      "    total_full_tiles = total_tiles - STREAMK_TILES",
      "",
      "    acc_dtype = tl.float32 if C.type.element_ty != tl.int8 else tl.int32",
      "    for tile_id in range(pid, total_full_tiles, NUM_SMS):",
      "        num_pid_in_group = GROUP_M * num_pid_n",
      "        group_id = tile_id // num_pid_in_group",
      "        first_pid_m = group_id * GROUP_M",
      "        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)",
      "        pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "        rk = tl.arange(0, BLOCK_K)",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "        B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "",
      "        loop_k = tl.cdiv(K, BLOCK_K)",
      "        if not EVEN_K:",
      "            loop_k -= 1",
      "",
      "        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)",
      "",
      "        if HAS_BIAS:",
      "            mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "            bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m",
      "            bias = tl.load(",
      "                bias_ptr + (tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N))),",
      "                mask=mask,",
      "            ).to(acc.dtype)",
      "",
      "        for k in range(0, loop_k):",
      "            a = tl.load(tl.multiple_of(A_BASE, (1, 16)))",
      "            b = tl.load(tl.multiple_of(B_BASE, (16, 1)))",
      "            acc += tl.dot(a, b)",
      "            A_BASE += BLOCK_K * stride_ak",
      "            B_BASE += BLOCK_K * stride_bk",
      "",
      "        if not EVEN_K:",
      "            k = loop_k",
      "            rk = k * BLOCK_K + tl.arange(0, BLOCK_K)",
      "            A_BASE = A + rm[:, None] * stride_am + rk[None, :] * stride_ak",
      "            B_BASE = B + rk[:, None] * stride_bk + rn[None, :] * stride_bn",
      "            A_BASE = tl.multiple_of(A_BASE, (1, 16))",
      "            B_BASE = tl.multiple_of(B_BASE, (16, 1))",
      "            a = tl.load(A_BASE, mask=rk[None, :] < K, other=0.0)",
      "            b = tl.load(B_BASE, mask=rk[:, None] < K, other=0.0)",
      "            acc += tl.dot(a, b)",
      "",
      "        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn",
      "        mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "",
      "        if HAS_BIAS:",
      "            acc += bias",
      "",
      "        c = acc.to(C.type.element_ty)",
      "        tl.store(C_, c, mask=mask)",
      "",
      "    tl.assume(pid >= 0)",
      "    total_streamk_iters = STREAMK_TILES * iters_per_tile",
      "    streamk_iters_pcu = total_streamk_iters // NUM_SMS",
      "    streamk_remainder_iters = total_streamk_iters % NUM_SMS",
      "",
      "    start_iter = (",
      "        total_full_tiles * iters_per_tile",
      "        + pid * streamk_iters_pcu",
      "        + tl.minimum(pid, streamk_remainder_iters)",
      "    )",
      "",
      "    last_iter = (",
      "        total_full_tiles * iters_per_tile",
      "        + (pid + 1) * streamk_iters_pcu",
      "        + tl.minimum(pid + 1, streamk_remainder_iters)",
      "    )",
      "    while start_iter < last_iter:",
      "        remainder = start_iter % iters_per_tile",
      "        tile_iter_end = start_iter + (iters_per_tile - remainder)",
      "        tile_id = start_iter // iters_per_tile",
      "        end_iter = tl.minimum(tile_iter_end, last_iter)",
      "",
      "        num_pid_in_group = GROUP_M * num_pid_n",
      "        group_id = tile_id // num_pid_in_group",
      "        first_pid_m = group_id * GROUP_M",
      "        group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "        pid_m = first_pid_m + ((tile_id % num_pid_in_group) % group_size_m)",
      "        pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "        rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M) % M",
      "        rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N) % N",
      "        rk = tl.arange(0, BLOCK_K)",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        A_BASE = (",
      "            A",
      "            + rm[:, None] * stride_am",
      "            + rk[None, :] * stride_ak",
      "            + BLOCK_K * stride_ak * remainder",
      "        )",
      "        B_BASE = (",
      "            B",
      "            + rk[:, None] * stride_bk",
      "            + rn[None, :] * stride_bn",
      "            + BLOCK_K * stride_bk * remainder",
      "        )",
      "        A_BASE = tl.multiple_of(A_BASE, (1, 16))",
      "        B_BASE = tl.multiple_of(B_BASE, (16, 1))",
      "",
      "        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=acc_dtype)",
      "        if HAS_BIAS:",
      "            mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "            bias_ = rn[None, :] * stride_bias_n + rm[:, None] * stride_bias_m",
      "            bias = tl.load(",
      "                bias_ptr + (tl.broadcast_to(bias_, (BLOCK_M, BLOCK_N))),",
      "                mask=mask,",
      "            ).to(acc.dtype)",
      "        for current_iter in range(start_iter, end_iter):",
      "            if EVEN_K:",
      "                if EVEN_M:",
      "                    a = tl.load(A_BASE)",
      "                else:",
      "                    mask_a = (rm < M)[:, None]",
      "                    a = tl.load(A_BASE, mask=mask_a, other=0.0)",
      "                if EVEN_N:",
      "                    b = tl.load(B_BASE)",
      "                else:",
      "                    mask_b = (rn < N)[None, :]",
      "                    b = tl.load(B_BASE, mask=mask_b, other=0.0)",
      "            else:",
      "                global_k_offset = (current_iter % iters_per_tile) * BLOCK_K",
      "                k_mask = global_k_offset + rk < K",
      "                if EVEN_M:",
      "                    a = tl.load(A_BASE, mask=k_mask[None, :], other=0.0)",
      "                else:",
      "                    mask_a = (rm < M)[:, None]",
      "                    a = tl.load(A_BASE, mask=k_mask[None, :] & mask_a, other=0.0)",
      "                if EVEN_N:",
      "                    b = tl.load(B_BASE, mask=k_mask[:, None], other=0.0)",
      "                else:",
      "                    mask_b = (rn < N)[None, :]",
      "                    b = tl.load(B_BASE, mask=k_mask[:, None] & mask_b, other=0.0)",
      "            acc += tl.dot(a, b)",
      "            A_BASE += BLOCK_K * stride_ak",
      "            B_BASE += BLOCK_K * stride_bk",
      "",
      "        rm = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "        rn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "        rm = tl.max_contiguous(tl.multiple_of(rm, BLOCK_M), BLOCK_M)",
      "        rn = tl.max_contiguous(tl.multiple_of(rn, BLOCK_N), BLOCK_N)",
      "        C_ = C + rm[:, None] * stride_cm + rn[None, :] * stride_cn",
      "        mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "",
      "        if HAS_BIAS:",
      "            acc += bias",
      "",
      "        c = acc.to(C.type.element_ty)",
      "        tl.atomic_add(C_, c, mask=mask, sem=\"relaxed\")",
      "",
      "        start_iter = end_iter"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/gemm/441.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=tuning_configs, key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_BUFFER_OPS_ASSUMES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "    ENABLE_BUFFER_OPS_ASSUMES: tl.constexpr,",
      "):",
      "",
      "    if ENABLE_BUFFER_OPS_ASSUMES:",
      "        tl.assume(M >= 0)",
      "        tl.assume(N >= 0)",
      "        tl.assume(K >= 0)",
      "        tl.assume(stride_am >= 0)",
      "        tl.assume(stride_ak >= 0)",
      "        tl.assume(stride_bn >= 0)",
      "        tl.assume(stride_bk >= 0)",
      "        tl.assume(stride_cm >= 0)",
      "        tl.assume(stride_cn >= 0)",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_N)",
      "    num_pid_in_group = GROUP_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    tl.assume(pid_m >= 0)",
      "    tl.assume(pid_n >= 0)",
      "",
      "    offs_am = (pid_m * BLOCK_M + tl.arange(0, BLOCK_M)) % M",
      "    offs_bn = (pid_n * BLOCK_N + tl.arange(0, BLOCK_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_K)):",
      "",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_K, other=0.0)",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_K * stride_ak",
      "        b_ptrs += BLOCK_K * stride_bk",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        accumulator = leaky_relu(accumulator)",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_cn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/gemm/442.py"
  },
  {
    "name": "grouped_matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'NUM_SM': 84}), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'NUM_SM': 128}), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'NUM_SM': 84}), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'NUM_SM': 128})], key=['group_size'])"
    ],
    "args": [
      {
        "name": "group_a_ptrs",
        "annotation": null
      },
      {
        "name": "group_b_ptrs",
        "annotation": null
      },
      {
        "name": "group_c_ptrs",
        "annotation": null
      },
      {
        "name": "group_gemm_sizes",
        "annotation": null
      },
      {
        "name": "g_lds",
        "annotation": null
      },
      {
        "name": "group_size",
        "annotation": null
      },
      {
        "name": "NUM_SM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def grouped_matmul_kernel(",
      "    group_a_ptrs,",
      "    group_b_ptrs,",
      "    group_c_ptrs,",
      "    group_gemm_sizes,",
      "    g_lds,",
      "    group_size,",
      "    NUM_SM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    tile_idx = tl.program_id(0)",
      "    last_problem_end = 0",
      "    for g in range(group_size):",
      "",
      "        gm = tl.load(group_gemm_sizes + g * 3)",
      "        gn = tl.load(group_gemm_sizes + g * 3 + 1)",
      "        gk = tl.load(group_gemm_sizes + g * 3 + 2)",
      "        num_m_tiles = tl.cdiv(gm, BLOCK_SIZE_M)",
      "        num_n_tiles = tl.cdiv(gn, BLOCK_SIZE_N)",
      "        num_tiles = num_m_tiles * num_n_tiles",
      "",
      "        while tile_idx >= last_problem_end and tile_idx < last_problem_end + num_tiles:",
      "",
      "            k = gk",
      "            lda = tl.load(g_lds + g * 3)",
      "            ldb = tl.load(g_lds + g * 3 + 1)",
      "            ldc = tl.load(g_lds + g * 3 + 2)",
      "            a_ptr = tl.load(group_a_ptrs + g).to(tl.pointer_type(tl.float16))",
      "            b_ptr = tl.load(group_b_ptrs + g).to(tl.pointer_type(tl.float16))",
      "            c_ptr = tl.load(group_c_ptrs + g).to(tl.pointer_type(tl.float16))",
      "",
      "            tile_idx_in_gemm = tile_idx - last_problem_end",
      "            tile_m_idx = tile_idx_in_gemm // num_n_tiles",
      "            tile_n_idx = tile_idx_in_gemm % num_n_tiles",
      "",
      "            offs_am = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_bn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "            a_ptrs = a_ptr + offs_am[:, None] * lda + offs_k[None, :]",
      "            b_ptrs = b_ptr + offs_k[:, None] * ldb + offs_bn[None, :]",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "            for kk in range(0, tl.cdiv(k, BLOCK_SIZE_K)):",
      "",
      "                tl.multiple_of(a_ptrs, [16, 16])",
      "                tl.multiple_of(b_ptrs, [16, 16])",
      "",
      "                a = tl.load(a_ptrs)",
      "                b = tl.load(b_ptrs)",
      "                accumulator += tl.dot(a, b)",
      "                a_ptrs += BLOCK_SIZE_K",
      "                b_ptrs += BLOCK_SIZE_K * ldb",
      "            c = accumulator.to(tl.float16)",
      "",
      "            offs_cm = tile_m_idx * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_cn = tile_n_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            c_ptrs = c_ptr + ldc * offs_cm[:, None] + offs_cn[None, :]",
      "",
      "            tl.store(c_ptrs, c)",
      "",
      "            tile_idx += NUM_SM",
      "",
      "        last_problem_end = last_problem_end + num_tiles"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/grouped_gemm/443.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=AUTOTUNE_CONFIGS, key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    tl.device_assert(K % BLOCK_SIZE_K == 0)",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_ak = tl.arange(0, BLOCK_SIZE_K)",
      "    offs_bk = tl.arange(0, BLOCK_SIZE_K // 2)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_ak[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_bk[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_ak[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs)",
      "        tl.static_assert(b.dtype == tl.int8)",
      "",
      "        _4_i8 = tl.full((1,), 4, dtype=tl.int8)",
      "        b_lo = (b << _4_i8) >> _4_i8",
      "        b_hi = b >> _4_i8",
      "",
      "        b_f16 = (",
      "            tl.join(b_lo.to(tl.bfloat16), b_hi.to(tl.bfloat16))",
      "            .permute(0, 2, 1)",
      "            .reshape(BLOCK_SIZE_K, BLOCK_SIZE_N)",
      "        )",
      "",
      "        accumulator += tl.dot(a, b_f16)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk // 2",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/int4_gemm/444.py"
  },
  {
    "name": "triton_jagged_mean_kernel_simple_fused_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_simple_fused_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_mean/445.py"
  },
  {
    "name": "triton_jagged_mean_kernel_simple_fused_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_simple_fused_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_mean/445.py"
  },
  {
    "name": "triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_variable_length_loop_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_mean/445.py"
  },
  {
    "name": "triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_mean_kernel_variable_length_loop_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "    ragged_len = ragged_end - ragged_start",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    buffer_view_mean = buffer_view * (1 / ragged_len)",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view_mean, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_mean/445.py"
  },
  {
    "name": "triton_jagged_softmax_kernel_simple_fused_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_softmax_kernel_simple_fused_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "",
      "    buffer_max_all = tl.full(",
      "        (BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float(\"-inf\"), dtype=tl.float32",
      "    )",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer_max_all = tl.maximum(buffer_max_all, input)",
      "",
      "    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer += tl.exp(input - buffer_max)",
      "",
      "    buffer_exp_sum = tl.sum(buffer, axis=0)",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)",
      "",
      "        tl.store(output_ptr + idxs, output, mask=mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_softmax/446.py"
  },
  {
    "name": "triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES_RAGGED, BLOCK_SIZES_M, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_softmax_kernel_variable_length_loop_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "",
      "    buffer_max_all = tl.full(",
      "        (BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), value=float(\"-inf\"), dtype=tl.float32",
      "    )",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer_max_all = tl.maximum(buffer_max_all, input)",
      "",
      "    buffer_max = tl.max(buffer_max_all, axis=0, keep_dims=True)",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        buffer += tl.exp(input - buffer_max)",
      "",
      "    buffer_exp_sum = tl.sum(buffer, axis=0)",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=float(\"-inf\"))",
      "        output = tl.fdiv(tl.exp(input - buffer_max), buffer_exp_sum)",
      "",
      "        tl.store(output_ptr + idxs, output, mask=mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_softmax/446.py"
  },
  {
    "name": "triton_jagged_sum_kernel_simple_fused_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_simple_fused_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_sum/447.py"
  },
  {
    "name": "triton_jagged_sum_kernel_simple_fused_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "MAX_SEQLEN",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_simple_fused_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    MAX_SEQLEN,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "",
      "    for block_pos in range(0, MAX_SEQLEN, BLOCK_SIZE_RAGGED):",
      "        block_start_ragged = ragged_start + block_pos",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_sum/447.py"
  },
  {
    "name": "triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_variable_length_loop_sum_then_buffer(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_b = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_b),",
      "        tl.load(input_ptr_offsets + (pid_b + 1)),",
      "    )",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        input = tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_b * M)",
      "    output_mask = output_offsets < (M * (pid_b + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_sum/447.py"
  },
  {
    "name": "triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_RAGGED': b_r, 'BLOCK_SIZE_M': b_m}, num_warps=w, num_stages=s) for b_r, b_m, w, s in itertools.product(BLOCK_SIZES, BLOCK_SIZES, NUM_WARPS, NUM_STAGES)], key=['M'])"
    ],
    "args": [
      {
        "name": "input_ptr_values",
        "annotation": null
      },
      {
        "name": "input_ptr_offsets",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_RAGGED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_jagged_sum_kernel_variable_length_loop_buffer_then_sum(",
      "    input_ptr_values,",
      "    input_ptr_offsets,",
      "    output_ptr,",
      "    M,",
      "    BLOCK_SIZE_RAGGED: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    pid_ragged = pid // tl.cdiv(M, BLOCK_SIZE_M)",
      "    pid_m = pid % tl.cdiv(M, BLOCK_SIZE_M)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_RAGGED, BLOCK_SIZE_M), dtype=tl.float32)",
      "",
      "    block_start_m = pid_m * BLOCK_SIZE_M",
      "    offsets_m = block_start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    mask_m = offsets_m < M",
      "",
      "    ragged_start, ragged_end = (",
      "        tl.load(input_ptr_offsets + pid_ragged),",
      "        tl.load(input_ptr_offsets + (pid_ragged + 1)),",
      "    )",
      "",
      "    for block_start_ragged in range(ragged_start, ragged_end, BLOCK_SIZE_RAGGED):",
      "        offsets_ragged = block_start_ragged + tl.arange(0, BLOCK_SIZE_RAGGED)",
      "        mask_ragged = offsets_ragged < ragged_end",
      "",
      "        idxs = (offsets_ragged[:, None] * M) + offsets_m",
      "        mask = mask_ragged[:, None] & mask_m",
      "",
      "        buffer += tl.load(input_ptr_values + idxs, mask=mask, other=0)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=0)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_M,),",
      "    )",
      "",
      "    output_offsets = offsets_m + (pid_ragged * M)",
      "    output_mask = output_offsets < (M * (pid_ragged + 1))",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=output_mask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/jagged_sum/447.py"
  },
  {
    "name": "quantize_2d_bf16_to_int2",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 32, 'BLOCK_N': 32}, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32}, num_warps=8), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64}, num_warps=16)], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "bf16_ptr",
        "annotation": null
      },
      {
        "name": "int8_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_im",
        "annotation": null
      },
      {
        "name": "stride_in",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def quantize_2d_bf16_to_int2(",
      "    bf16_ptr,",
      "    int8_ptr,",
      "    M,",
      "    N,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_im,",
      "    stride_in,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_n = tl.program_id(1)",
      "",
      "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)",
      "",
      "    mask_m = offs_m < M",
      "    mask_n = offs_n < (N // 4)",
      "",
      "    packed_output = tl.zeros((BLOCK_M, BLOCK_N // 4), dtype=tl.int32)",
      "",
      "    for i in range(4):",
      "",
      "        column_idx = offs_n * 4 + i",
      "        bf16_vals = tl.load(",
      "            bf16_ptr + offs_m[:, None] * stride_bm + column_idx[None, :] * stride_bn,",
      "            mask=mask_m[:, None] & (column_idx[None, :] < N),",
      "            other=0.0,",
      "        )",
      "",
      "        int2_vals = tl.where(",
      "            bf16_vals == -2.0,",
      "            0,",
      "            tl.where(",
      "                bf16_vals == -1.0,",
      "                1,",
      "                tl.where(bf16_vals == 0.0, 2, 3),",
      "            ),",
      "        )",
      "",
      "        packed_output = packed_output | (int2_vals << (i * 2))",
      "",
      "    tl.store(",
      "        int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in,",
      "        packed_output.to(tl.int8),",
      "        mask=mask_m[:, None] & mask_n[None, :],",
      "    )"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/mixed_gemm/451.py"
  },
  {
    "name": "dequantize_2d_int2_to_bf16",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128}, num_warps=8), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256}, num_warps=16)], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "int8_ptr",
        "annotation": null
      },
      {
        "name": "bf16_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_im",
        "annotation": null
      },
      {
        "name": "stride_in",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dequantize_2d_int2_to_bf16(",
      "    int8_ptr,",
      "    bf16_ptr,",
      "    M,",
      "    N,",
      "    stride_im,",
      "    stride_in,",
      "    stride_bm,",
      "    stride_bn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_n = tl.program_id(1)",
      "",
      "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = pid_n * (BLOCK_N // 4) + tl.arange(0, BLOCK_N // 4)",
      "",
      "    mask_m = offs_m < M",
      "    mask_n = offs_n < (N // 4)",
      "",
      "    packed_vals = tl.load(",
      "        int8_ptr + offs_m[:, None] * stride_im + offs_n[None, :] * stride_in,",
      "        mask=mask_m[:, None] & mask_n[None, :],",
      "        other=0,",
      "    ).to(tl.int32)",
      "",
      "    for i in range(4):",
      "        shift = i * 2",
      "        mask = 0b11 << shift",
      "        int2_vals = (packed_vals & mask) >> shift",
      "",
      "        bf16_vals = tl.where(",
      "            int2_vals == 0b00,",
      "            tl.full(int2_vals.shape, -2.0, dtype=tl.float32),",
      "            tl.where(",
      "                int2_vals == 0b01,",
      "                tl.full(int2_vals.shape, -1.0, dtype=tl.float32),",
      "                tl.where(",
      "                    int2_vals == 0b10,",
      "                    tl.full(int2_vals.shape, 0.0, dtype=tl.float32),",
      "                    tl.full(int2_vals.shape, 1.0, dtype=tl.float32),",
      "                ),",
      "            ),",
      "        )",
      "",
      "        output_idx = offs_n * 4 + i",
      "",
      "        tl.store(",
      "            bf16_ptr + offs_m[:, None] * stride_bm + output_idx[None, :] * stride_bn,",
      "            bf16_vals.to(tl.bfloat16),",
      "            mask=mask_m[:, None] & (output_idx[None, :] < N),",
      "        )"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/mixed_gemm/451.py"
  },
  {
    "name": "triton_sum_kernel_1D_result_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_NON_REDUCE_DIM': b_nr, 'BLOCK_SIZE_REDUCE_DIM': b_r}, num_warps=w) for b_nr, b_r, w in itertools.product([2, 4, 8, 16], [2, 4, 8, 16], [2, 4, 8])], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_NON_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dim",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_1D_result_sum_then_buffer(",
      "    input_ptr,",
      "    output_ptr,",
      "    M,",
      "    N,",
      "    BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr,",
      "    BLOCK_SIZE_REDUCE_DIM: tl.constexpr,",
      "    dim: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    reduce_dim_len = M if dim == 0 else N",
      "    non_reduce_dim_len = N if dim == 0 else M",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32)",
      "",
      "    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM",
      "    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(",
      "        0, BLOCK_SIZE_NON_REDUCE_DIM",
      "    )",
      "    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len",
      "",
      "    for block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):",
      "        offsets_reduce_dim = block_start_reduce_dim + tl.arange(",
      "            0, BLOCK_SIZE_REDUCE_DIM",
      "        )",
      "        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len",
      "",
      "        idxs, mask = None, None",
      "        if dim == 0:",
      "            idxs = (",
      "                offsets_reduce_dim[:, None] * non_reduce_dim_len",
      "            ) + offsets_non_reduce_dim",
      "            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim",
      "        elif dim == 1:",
      "            idxs = (",
      "                offsets_non_reduce_dim[:, None] * reduce_dim_len",
      "            ) + offsets_reduce_dim",
      "            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim",
      "",
      "        input = tl.load(input_ptr + idxs, mask=mask, other=mask)",
      "",
      "        buffer += tl.sum(input, axis=dim)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_NON_REDUCE_DIM,),",
      "    )",
      "",
      "    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/sum/453.py"
  },
  {
    "name": "triton_sum_kernel_1D_result_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_NON_REDUCE_DIM': b, 'BLOCK_SIZE_REDUCE_DIM': b}, num_warps=w) for b, w in itertools.product([2, 4, 8, 16], [2, 4, 8])], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_NON_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_REDUCE_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dim",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_1D_result_buffer_then_sum(",
      "    input_ptr,",
      "    output_ptr,",
      "    M,",
      "    N,",
      "    BLOCK_SIZE_NON_REDUCE_DIM: tl.constexpr,",
      "    BLOCK_SIZE_REDUCE_DIM: tl.constexpr,",
      "    dim: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    reduce_dim_len = M if dim == 0 else N",
      "    non_reduce_dim_len = N if dim == 0 else M",
      "",
      "    buffer = tl.zeros(",
      "        (BLOCK_SIZE_REDUCE_DIM, BLOCK_SIZE_NON_REDUCE_DIM), dtype=tl.float32",
      "    )",
      "",
      "    block_start_non_reduce_dim = pid * BLOCK_SIZE_NON_REDUCE_DIM",
      "    offsets_non_reduce_dim = block_start_non_reduce_dim + tl.arange(",
      "        0, BLOCK_SIZE_NON_REDUCE_DIM",
      "    )",
      "    mask_non_reduce_dim = offsets_non_reduce_dim < non_reduce_dim_len",
      "",
      "    for block_start_reduce_dim in range(0, reduce_dim_len, BLOCK_SIZE_REDUCE_DIM):",
      "        offsets_reduce_dim = block_start_reduce_dim + tl.arange(",
      "            0, BLOCK_SIZE_REDUCE_DIM",
      "        )",
      "        mask_reduce_dim = offsets_reduce_dim < reduce_dim_len",
      "",
      "        idxs, mask = None, None",
      "        if dim == 0:",
      "            idxs = (",
      "                offsets_reduce_dim[:, None] * non_reduce_dim_len",
      "            ) + offsets_non_reduce_dim",
      "            mask = mask_reduce_dim[:, None] & mask_non_reduce_dim",
      "        elif dim == 1:",
      "            idxs = (",
      "                offsets_non_reduce_dim[:, None] * reduce_dim_len",
      "            ) + offsets_reduce_dim",
      "            mask = mask_non_reduce_dim[:, None] & mask_reduce_dim",
      "",
      "        buffer += tl.load(input_ptr + idxs, mask=mask, other=mask)",
      "",
      "    buffer_sum = tl.sum(buffer, axis=dim)",
      "",
      "    buffer_view = buffer_sum.reshape(",
      "        (BLOCK_SIZE_NON_REDUCE_DIM,),",
      "    )",
      "",
      "    tl.store(output_ptr + offsets_non_reduce_dim, buffer_view, mask=mask_non_reduce_dim)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/sum/453.py"
  },
  {
    "name": "triton_sum_kernel_2D_result_dim_1",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_K': b}, num_warps=w) for b, w in itertools.product([2, 4, 16, 32, 128, 256], [2, 4, 8])], key=['N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_2D_result_dim_1(",
      "    input_ptr,",
      "    output_ptr,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)",
      "    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    block_start_n = 0",
      "    block_start_k = pid_k * BLOCK_SIZE_K",
      "",
      "    offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)",
      "    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    mask_n = offsets_n < N",
      "    mask_k = offsets_k < K",
      "",
      "    idxs_base = (offsets_n[:, None] * K) + offsets_k",
      "    idxs = idxs_base + (pid_m * N * K)",
      "",
      "    mask = mask_n[:, None] & mask_k",
      "",
      "    input = tl.load(input_ptr + idxs, mask=mask, other=0)",
      "",
      "    output = tl.sum(input, axis=0)",
      "",
      "    output_offsets = (pid_m * K) + offsets_k",
      "",
      "    tl.store(output_ptr + output_offsets, output, mask=mask_k)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/sum/453.py"
  },
  {
    "name": "triton_sum_kernel_2D_result_dim_1_sum_then_buffer",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': b_n, 'BLOCK_SIZE_K': b_k}, num_warps=w) for b_n, b_k, w in itertools.product([4 ** n for n in range(6)], [4 ** n for n in range(4)], [2, 4, 8])], key=['N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_2D_result_dim_1_sum_then_buffer(",
      "    input_ptr,",
      "    output_ptr,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)",
      "    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    buffer = tl.zeros((1, BLOCK_SIZE_K), dtype=tl.float32)",
      "",
      "    block_start_k = pid_k * BLOCK_SIZE_K",
      "    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)",
      "    mask_k = offsets_k < K",
      "",
      "    for block_start_n in range(0, N, BLOCK_SIZE_N):",
      "        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)",
      "        mask_n = offsets_n < N",
      "",
      "        idxs_base = (offsets_n[:, None] * K) + offsets_k",
      "        idxs = idxs_base + (pid_m * N * K)",
      "",
      "        mask = mask_n[:, None] & mask_k",
      "",
      "        input = tl.load(input_ptr + idxs, mask=mask, other=0)",
      "",
      "        buffer += tl.sum(input, axis=0)",
      "",
      "    buffer_view = buffer.reshape(",
      "        (BLOCK_SIZE_K,),",
      "    )",
      "",
      "    output_offsets = (pid_m * K) + offsets_k",
      "",
      "    tl.store(output_ptr + output_offsets, buffer_view, mask=mask_k)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/sum/453.py"
  },
  {
    "name": "triton_sum_kernel_2D_result_dim_1_buffer_then_sum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': b_n, 'BLOCK_SIZE_K': b_k}, num_warps=w) for b_n, b_k, w in itertools.product([4 ** n for n in range(7)], [4 ** n for n in range(4)], [2, 4, 8])], key=['N'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_sum_kernel_2D_result_dim_1_buffer_then_sum(",
      "    input_ptr,",
      "    output_ptr,",
      "    M: tl.constexpr,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    pid_m = pid // tl.cdiv(K, BLOCK_SIZE_K)",
      "    pid_k = pid % tl.cdiv(K, BLOCK_SIZE_K)",
      "",
      "    buffer = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)",
      "",
      "    block_start_k = pid_k * BLOCK_SIZE_K",
      "    offsets_k = block_start_k + tl.arange(0, BLOCK_SIZE_K)",
      "    mask_k = offsets_k < K",
      "",
      "    for block_start_n in range(0, N, BLOCK_SIZE_N):",
      "        offsets_n = block_start_n + tl.arange(0, BLOCK_SIZE_N)",
      "        mask_n = offsets_n < N",
      "",
      "        idxs_base = (offsets_n[:, None] * K) + offsets_k",
      "        idxs = idxs_base + (pid_m * N * K)",
      "",
      "        mask = mask_n[:, None] & mask_k",
      "",
      "        input = tl.load(input_ptr + idxs, mask=mask, other=0)",
      "",
      "        buffer += input",
      "",
      "    output = tl.sum(buffer, axis=0)",
      "",
      "    output_offsets = (pid_m * K) + offsets_k",
      "",
      "    tl.store(output_ptr + output_offsets, output, mask=mask_k)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/sum/453.py"
  },
  {
    "name": "triton_tem_fused_no_exp2",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_DMODEL': 64}, num_stages=3, num_warps=4)], key=['num_queries'])"
    ],
    "args": [
      {
        "name": "arg_Q",
        "annotation": null
      },
      {
        "name": "arg_K",
        "annotation": null
      },
      {
        "name": "arg_V",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "num_queries",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_tem_fused_no_exp2(",
      "    arg_Q,",
      "    arg_K,",
      "    arg_V,",
      "    out_ptr0,",
      "    num_queries: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "):",
      "    Q = arg_Q",
      "    K = arg_K",
      "    V = arg_V",
      "",
      "    stride_qz = 4194304",
      "    stride_qh = 262144",
      "    stride_qm = 64",
      "    stride_qk = 1",
      "",
      "    stride_kz = 4194304",
      "    stride_kh = 262144",
      "    stride_kn = 64",
      "    stride_kk = 1",
      "",
      "    stride_vz = 4194304",
      "    stride_vh = 262144",
      "    stride_vk = 64",
      "    stride_vn = 1",
      "",
      "    Z = 16",
      "    H = 16",
      "    N_CTX = 4096",
      "",
      "    qk_scale = 1.0",
      "    MATMUL_PRECISION = tl.float16",
      "",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    qkv_offset = off_hz * stride_qh",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qkv_offset,",
      "        shape=(BLOCK_DMODEL, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "    q = tl.load(Q_block_ptr)",
      "    q = (q * qk_scale).to(MATMUL_PRECISION)",
      "",
      "    lo = 0",
      "    hi = N_CTX",
      "    for start_n in range(lo, hi, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        k = tl.load(K_block_ptr)",
      "        v = tl.load(V_block_ptr)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k.to(MATMUL_PRECISION))",
      "",
      "        tmp0 = tl.full([1], 1024, tl.int64)",
      "        tmp1 = (offs_m[:, None]) <= tmp0",
      "        tmp2 = (start_n + offs_n[None, :]) <= tmp0",
      "        tmp3 = tmp1 & tmp2",
      "        tmp4 = (offs_m[:, None]) >= (start_n + offs_n[None, :])",
      "        tmp5 = tmp3 | tmp4",
      "        tmp6 = float(\"-inf\")",
      "        tmp7 = tmp6.to(tl.float32)",
      "        tmp8 = tl.where(tmp5, (qk), tmp7)",
      "        qk = tmp8",
      "",
      "        row_max = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, row_max)",
      "        masked_out_rows = m_i_new == float(\"-inf\")",
      "",
      "        alpha = tl.math.exp(m_i - m_i_new)",
      "        alpha = tl.where(masked_out_rows, 0, alpha)",
      "        p = tl.math.exp(qk - m_i_new[:, None])",
      "        p = tl.where(masked_out_rows[:, None], 0, p)",
      "",
      "        acc_scale = l_i * 0 + alpha",
      "        acc *= acc_scale[:, None]",
      "        acc += tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION))",
      "",
      "        l_i = l_i * alpha + tl.sum(p, 1)",
      "        m_i = m_i_new",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))",
      "",
      "    acc = acc / l_i[:, None]",
      "",
      "    idx_z = tl.program_id(1) // H",
      "    idx_h = tl.program_id(1) % H",
      "    idx_m = offs_m[:, None]",
      "    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]",
      "",
      "    mask = (idx_m != -1) & (idx_d != -1)",
      "    xindex = idx_d + (64 * idx_m) + (262144 * idx_h) + (4194304 * idx_z)",
      "    tl.store(out_ptr0 + (xindex), acc, None)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/template_attention/454.py"
  },
  {
    "name": "triton_tem_fused_with_exp2",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_DMODEL': 64}, num_stages=3, num_warps=4)], key=['num_queries'])"
    ],
    "args": [
      {
        "name": "arg_Q",
        "annotation": null
      },
      {
        "name": "arg_K",
        "annotation": null
      },
      {
        "name": "arg_V",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "num_queries",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_tem_fused_with_exp2(",
      "    arg_Q,",
      "    arg_K,",
      "    arg_V,",
      "    out_ptr0,",
      "    num_queries: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "):",
      "",
      "    SCORE_MOD_IS_LINEAR: tl.constexpr = False",
      "    ROWS_GUARANTEED_SAFE: tl.constexpr = False",
      "    Q = arg_Q",
      "    K = arg_K",
      "    V = arg_V",
      "",
      "    stride_qz = 4194304",
      "    stride_qh = 262144",
      "    stride_qm = 64",
      "    stride_qk = 1",
      "",
      "    stride_kz = 4194304",
      "    stride_kh = 262144",
      "    stride_kn = 64",
      "    stride_kk = 1",
      "",
      "    stride_vz = 4194304",
      "    stride_vh = 262144",
      "    stride_vk = 64",
      "    stride_vn = 1",
      "",
      "    Z = 16",
      "    H = 16",
      "    N_CTX = 4096",
      "",
      "    qk_scale = 1.0",
      "    MATMUL_PRECISION = Q.dtype.element_ty",
      "",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "",
      "    qkv_offset = off_hz * stride_qh",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qkv_offset,",
      "        shape=(BLOCK_DMODEL, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qkv_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "    q = tl.load(Q_block_ptr)",
      "    if SCORE_MOD_IS_LINEAR:",
      "        qk_scale *= 1.44269504",
      "    q = (q * qk_scale).to(MATMUL_PRECISION)",
      "",
      "    lo = 0",
      "    hi = N_CTX",
      "    for start_n in range(lo, hi, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        k = tl.load(K_block_ptr)",
      "        v = tl.load(V_block_ptr)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk = tl.dot(q, k.to(MATMUL_PRECISION), acc=qk)",
      "",
      "        tmp0 = tl.full([1], 1024, tl.int64)",
      "        tmp1 = (offs_m[:, None]) <= tmp0",
      "        tmp2 = (start_n + offs_n[None, :]) <= tmp0",
      "        tmp3 = tmp1 & tmp2",
      "        tmp4 = (offs_m[:, None]) >= (start_n + offs_n[None, :])",
      "        tmp5 = tmp3 | tmp4",
      "        tmp6 = float(\"-inf\")",
      "        tmp7 = tmp6.to(tl.float32)",
      "        tmp8 = tl.where(tmp5, (qk), tmp7)",
      "        qk = tmp8",
      "",
      "        if not SCORE_MOD_IS_LINEAR:",
      "            qk *= 1.44269504",
      "",
      "        row_max = tl.max(qk, 1)",
      "        m_i_new = tl.maximum(m_i, row_max)",
      "        masked_out_rows = m_i_new == float(\"-inf\")",
      "",
      "        alpha = tl.math.exp2(m_i - m_i_new)",
      "        p = tl.math.exp2(qk - m_i_new[:, None])",
      "        if not ROWS_GUARANTEED_SAFE:",
      "            alpha = tl.where(masked_out_rows, 0, alpha)",
      "            p = tl.where(masked_out_rows[:, None], 0, p)",
      "",
      "        acc_scale = l_i * 0 + alpha",
      "        acc *= acc_scale[:, None]",
      "        acc = tl.dot(p.to(MATMUL_PRECISION), v.to(MATMUL_PRECISION), acc)",
      "",
      "        l_i = l_i * alpha + tl.sum(p, 1)",
      "        m_i = m_i_new",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))",
      "",
      "    acc = acc / l_i[:, None]",
      "",
      "    idx_z = tl.program_id(1) // H",
      "    idx_h = tl.program_id(1) % H",
      "    idx_m = offs_m[:, None]",
      "    idx_d = tl.arange(0, BLOCK_DMODEL)[None, :]",
      "",
      "    mask = (idx_m != -1) & (idx_d != -1)",
      "    xindex = idx_d + (64 * idx_m) + (262144 * idx_h) + (4194304 * idx_z)",
      "    tl.store(out_ptr0 + (xindex), acc, None)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/template_attention/454.py"
  },
  {
    "name": "triton_red_fused_native_layer_norm_0",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_out_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "out_ptr1",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_native_layer_norm_0(",
      "    in_out_ptr0,",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr0,",
      "    out_ptr1,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :]",
      "    x0 = xindex",
      "    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp0 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp1 = tmp0.to(tl.float32)",
      "        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])",
      "        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(",
      "            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0",
      "        )",
      "        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)",
      "        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)",
      "        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)",
      "    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(",
      "        tmp3_mean, tmp3_m2, tmp3_weight, 1",
      "    )",
      "    tmp3 = tmp3_tmp[:, None]",
      "    tmp4 = tmp4_tmp[:, None]",
      "    tmp5 = tmp5_tmp[:, None]",
      "    tl.store(out_ptr0 + (x0), tmp3, None)",
      "    tmp6 = rnumel",
      "    tmp7 = tmp4 / tmp6",
      "    tmp8 = 1e-05",
      "    tmp9 = tmp7 + tmp8",
      "    tmp10 = libdevice.rsqrt(tmp9)",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr0 + (x0), tmp10, None)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp11 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"",
      "        ).to(tl.float32)",
      "        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp12 = tmp11.to(tl.float32)",
      "        tmp13 = tmp12 - tmp3",
      "        tmp14 = tmp13 * tmp10",
      "        tmp16 = tmp15.to(tl.float32)",
      "        tmp17 = tmp14 * tmp16",
      "        tmp19 = tmp18.to(tl.float32)",
      "        tmp20 = tmp17 + tmp19",
      "        tmp21 = tmp20.to(tl.float32)",
      "        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/welford/457.py"
  },
  {
    "name": "triton_red_fused_native_layer_norm_no_welford",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_out_ptr0",
        "annotation": null
      },
      {
        "name": "in_out_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_native_layer_norm_no_welford(",
      "    in_out_ptr0,",
      "    in_out_ptr1,",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr0,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :]",
      "    x0 = xindex",
      "    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp0 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp1 = tmp0.to(tl.float32)",
      "        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])",
      "        tmp4 = _tmp3 + tmp2",
      "        _tmp3 = tmp4",
      "    tmp3 = tl.sum(_tmp3, 1)[:, None]",
      "    tmp5 = rnumel",
      "    tmp6 = tmp3 / tmp5",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr0 + (x0), tmp6, None)",
      "    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp7 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp8 = tmp7.to(tl.float32)",
      "        tmp9 = tmp8 - tmp6",
      "        tmp10 = tmp9 * tmp9",
      "        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])",
      "        tmp13 = _tmp12 + tmp11",
      "        _tmp12 = tmp13",
      "    tmp12 = tl.sum(_tmp12, 1)[:, None]",
      "    tmp14 = rnumel",
      "    tmp15 = tmp12 / tmp14",
      "    tmp16 = 1e-05",
      "    tmp17 = tmp15 + tmp16",
      "    tmp18 = libdevice.rsqrt(tmp17)",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr1 + (x0), tmp18, None)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp19 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"",
      "        ).to(tl.float32)",
      "        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp20 = tmp19.to(tl.float32)",
      "        tmp21 = tmp20 - tmp6",
      "        tmp22 = tmp21 * tmp18",
      "        tmp24 = tmp23.to(tl.float32)",
      "        tmp25 = tmp22 * tmp24",
      "        tmp27 = tmp26.to(tl.float32)",
      "        tmp28 = tmp25 + tmp27",
      "        tmp29 = tmp28.to(tl.float32)",
      "        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)"
    ],
    "file": "triton_repos/pytorch-labs_tritonbench/tritonbench/operators/welford/457.py"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configs)), key=['N_CTX', 'HEAD_DIM'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "mask",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "stride_mask_z",
        "annotation": null
      },
      {
        "name": "stride_mask_h",
        "annotation": null
      },
      {
        "name": "stride_mask_m",
        "annotation": null
      },
      {
        "name": "stride_mask_n",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_MASK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    mask,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    stride_mask_z,",
      "    stride_mask_h,",
      "    stride_mask_m,",
      "    stride_mask_n,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    HEAD_DIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    STAGE: tl.constexpr,",
      "    USE_MASK: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "",
      "    if USE_MASK:",
      "        mask_offset = (",
      "            off_z.to(tl.int64) * stride_mask_z + off_h.to(tl.int64) * stride_mask_h",
      "        )",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "    v_order: tl.constexpr = (0, 1) if V.dtype.element_ty == tl.float8e5 else (1, 0)",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, HEAD_DIM),",
      "        order=v_order,",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(HEAD_DIM, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(HEAD_DIM, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    mask_block_ptr = (",
      "        None",
      "        if not USE_MASK",
      "        else tl.make_block_ptr(",
      "            base=mask + mask_offset,",
      "            shape=(N_CTX, N_CTX),",
      "            strides=(stride_mask_m, stride_mask_n),",
      "            offsets=(start_m * BLOCK_M, 0),",
      "            block_shape=(BLOCK_M, BLOCK_N),",
      "            order=(0, 1),",
      "        )",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0",
      "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)",
      "",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.44269504",
      "",
      "    q = tl.load(Q_block_ptr)",
      "",
      "    if USE_MASK:",
      "        acc, l_i, m_i = _attn_fwd_inner(",
      "            acc,",
      "            l_i,",
      "            m_i,",
      "            q,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            mask_block_ptr,",
      "            start_m,",
      "            qk_scale,",
      "            BLOCK_M,",
      "            HEAD_DIM,",
      "            BLOCK_N,",
      "            4 - STAGE,",
      "            offs_m,",
      "            offs_n,",
      "            N_CTX,",
      "            V.dtype.element_ty == tl.float8e5,",
      "            USE_MASK,",
      "        )",
      "    else:",
      "        acc, l_i, m_i = _attn_fwd_inner(",
      "            acc,",
      "            l_i,",
      "            m_i,",
      "            q,",
      "            K_block_ptr,",
      "            V_block_ptr,",
      "            None,",
      "            start_m,",
      "            qk_scale,",
      "            BLOCK_M,",
      "            HEAD_DIM,",
      "            BLOCK_N,",
      "            2,",
      "            offs_m,",
      "            offs_n,",
      "            N_CTX,",
      "            V.dtype.element_ty == tl.float8e5,",
      "            USE_MASK,",
      "        )",
      "",
      "    m_i += tl.math.log2(l_i)",
      "    acc = acc / l_i[:, None]",
      "    m_ptrs = M + off_hz * N_CTX + offs_m",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))"
    ],
    "file": "triton_repos/alexzhang13_flashattention2-custom-mask/fa2_custom_mask/157.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['loss_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    loss_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.atomic_add(loss_ptr, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/72.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_prologue",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512})], key=['V', 'N', 'H'], reset_to_zero=['sz_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_prologue(",
      "    sz_ptr,",
      "    x_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    lse = tl.load(lse_global_ptr + offsets)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    tl.store(sz_block_ptr, softmax_z.to(tl.float16))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/72.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_x_grad_N",
        "annotation": null
      },
      {
        "name": "stride_x_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    sz_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_x_grad_N,",
      "    stride_x_grad_H,",
      "    stride_A_grad_H,",
      "    stride_A_grad_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_H = tl.program_id(axis=0)",
      "    idx_N = tl.program_id(axis=1)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_grad_N, stride_x_grad_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_offsets)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "        A_v = tl.load(A_t_block_ptr).trans()",
      "        sz = tl.load(sz_block_ptr)",
      "",
      "        x_grad_acc = tl.dot(sz, A_v, x_grad_acc)",
      "        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        sz_block_ptr = tl.advance(sz_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    tl.store(x_grad_block_ptr, x_grad_acc / N)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/72.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_x_grad_N",
        "annotation": null
      },
      {
        "name": "stride_x_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    sz_ptr,",
      "    x_ptr,",
      "    y_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_x_grad_N,",
      "    stride_x_grad_H,",
      "    stride_A_grad_H,",
      "    stride_A_grad_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_H = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1) - (N // N_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_grad_H, stride_A_grad_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + N_offsets)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "        x_chunk = tl.load(x_block_ptr)",
      "        sz = tl.load(sz_block_ptr).trans()",
      "",
      "        A_grad_acc = tl.dot(sz, x_chunk, A_grad_acc)",
      "        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        sz_block_ptr = tl.advance(sz_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_offsets += N_BLOCK_SIZE",
      "",
      "    tl.store(A_t_grad_block_ptr, A_grad_acc.trans() / N)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/72.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    V_TILES: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N, V // 64),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = (",
      "        losses_ptr",
      "        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb",
      "        + idx_V_group * stride_loss_B",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/73.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/73.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/74.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "        x_block_ptr = tl.make_block_ptr(",
      "            base=x_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(local_A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            A_grad_block_ptr = tl.make_block_ptr(",
      "                base=A_grad_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "",
      "            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)",
      "            A_v = tl.load(local_A_block_ptr).to(tl.float32)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N + tl.load(A_grad_block_ptr)",
      "            tl.store(A_grad_block_ptr, temp_AgradT, boundary_check=(0, 1))",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/74.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(0, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(local_A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "        local_x_block_ptr = x_block_ptr",
      "        local_A_block_ptr = A_block_ptr",
      "        local_x_grad_block_ptr = x_grad_block_ptr",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "",
      "            x_chunk = tl.load(local_x_block_ptr).to(tl.float32)",
      "            A_v = tl.load(local_A_block_ptr).to(tl.float32)",
      "",
      "            temp_xgrad = tl.dot(softmax_z, A_v.trans()) / N",
      "            temp_xgrad -= (",
      "                tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1) / N",
      "            )",
      "",
      "            temp_xgrad += tl.load(local_x_grad_block_ptr)",
      "            tl.store(local_x_grad_block_ptr, temp_xgrad, boundary_check=(0, 1))",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            local_x_grad_block_ptr = tl.advance(",
      "                local_x_grad_block_ptr, [0, H_BLOCK_SIZE]",
      "            )",
      "            local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        V_offsets += V_BLOCK_SIZE"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/74.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/75.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N",
      "            tl.store(",
      "                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)",
      "            )",
      "",
      "            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/75.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0 * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "            x_block_ptr = tl.make_block_ptr(",
      "                base=x_ptr,",
      "                shape=(N, H),",
      "                strides=(stride_x_N, stride_x_H),",
      "                offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),",
      "                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_block_ptr = tl.make_block_ptr(",
      "                base=A_t_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_grad_block_ptr = tl.make_block_ptr(",
      "                base=x_grad_ptr,",
      "                shape=(N, H),",
      "                strides=(stride_x_N, stride_x_H),",
      "                offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_block_ptr = tl.make_block_ptr(",
      "                base=A_t_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_v = tl.load(A_block_ptr).trans()",
      "",
      "            temp_xgrad = tl.dot(softmax_z, A_v) / N",
      "            temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N",
      "            tl.store(",
      "                x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16)",
      "            )",
      "",
      "        V_offsets += V_BLOCK_SIZE"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/75.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/76.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N",
      "            tl.store(",
      "                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)",
      "            )",
      "",
      "            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/76.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_GROUP_SIZE: tl.constexpr = 4,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H_group = tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    H_GROUPS: tl.constexpr = H // (H_GROUP_SIZE * H_BLOCK_SIZE)",
      "    tl.static_print(H, V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE, H_GROUPS)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "    H_group_offsets = tl.arange(0, H_GROUP_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE, H_GROUP_SIZE), dtype=tl.float16)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "            x_block_ptr = tl.make_block_ptr(",
      "                base=x_ptr,",
      "                shape=(N, H),",
      "                strides=(stride_x_N, stride_x_H),",
      "                offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),",
      "                block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            A_block_ptr = tl.make_block_ptr(",
      "                base=A_t_ptr,",
      "                shape=(H, V),",
      "                strides=(stride_A_H, stride_A_V),",
      "                offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "                block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "                order=(1, 0),",
      "            )",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=((H_GROUP_SIZE * H_BLOCK_SIZE) * idx_H_group, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        for idx_H_in_group in range(H_GROUP_SIZE):",
      "",
      "            A_v = tl.load(A_block_ptr).trans()",
      "",
      "            x_grad_block = tl.dot(softmax_z, A_v) / N",
      "            x_grad_block -= tl.sum(tl.where(mask, A_v[None, :, :], 0), axis=1) / N",
      "            x_grad_slice = x_grad_block[:, :, None].to(tl.float16)",
      "",
      "            accum_mask = (idx_H_in_group == H_group_offsets)[None, None, :]",
      "            x_grad_acc += tl.where(accum_mask, x_grad_slice, 0)",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_H_group * H_GROUP_SIZE * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(",
      "        x_grad_block_ptr, x_grad_acc.reshape(N_BLOCK_SIZE, H_GROUP_SIZE * H_BLOCK_SIZE)",
      "    )"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/76.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/77.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "locks_N_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    locks_N_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V, idx_N = tl.program_id(axis=0), tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "    x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "",
      "    mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "    for idx_H in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "",
      "        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "        temp_AgradT = (temp_Agrad.trans() / N).to(tl.float16)",
      "",
      "        while tl.atomic_cas(locks_N_ptr + idx_V, 0, 1) == 1:",
      "            pass",
      "        tl.store(A_grad_block_ptr, temp_AgradT + tl.load(A_grad_block_ptr))",
      "        tl.atomic_xchg(locks_N_ptr + idx_V, 0)",
      "",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/77.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "locks_V_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    locks_V_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V, idx_N = tl.program_id(axis=0), tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "        x_block_ptr = tl.make_block_ptr(",
      "            base=x_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, idx_H_1 * H_BLOCK_SIZE),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H_1 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "    mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "    for idx_H in range(H // H_BLOCK_SIZE):",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        A_block_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        A_v = tl.load(A_block_ptr).trans()",
      "",
      "        temp_xgrad = tl.dot(softmax_z, A_v) / N",
      "        temp_xgrad -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N",
      "",
      "        while tl.atomic_cas(locks_V_ptr + idx_N, 0, 1) == 1:",
      "            pass",
      "        tl.store(",
      "            x_grad_block_ptr, tl.load(x_grad_block_ptr) + temp_xgrad.to(tl.float16)",
      "        )",
      "        tl.atomic_xchg(locks_V_ptr + idx_N, 0)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/77.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/78.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        for idx_H in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "",
      "            temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "            temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "            temp_AgradT = temp_Agrad.trans() / N",
      "            tl.store(",
      "                A_grad_block_ptr, temp_AgradT.to(tl.float16) + tl.load(A_grad_block_ptr)",
      "            )",
      "",
      "            A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/78.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 4})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 1,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "    x_grad_slice = tl.zeros((N_BLOCK_SIZE, H), tl.float16)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "",
      "        x_block_ptr = tl.make_block_ptr(",
      "            base=x_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "            block_shape=(N_BLOCK_SIZE, H),",
      "            order=(1, 0),",
      "        )",
      "        A_slice_ptr = tl.make_block_ptr(",
      "            base=A_t_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(0, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H, V_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v_full = tl.load(A_slice_ptr)",
      "",
      "        z_j_to_k = tl.sum(x_chunk[:, :, None] * A_v_full[None, :, :], axis=1).to(",
      "            tl.float32",
      "        )",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        temp_xgrad = (",
      "            tl.sum(softmax_z[:, :, None] * A_v_full.trans()[None, :, :], axis=1) / N",
      "        )",
      "        temp_xgrad -= (",
      "            tl.sum(tl.where(mask, A_v_full.trans()[None, :, :], 0.0), axis=1) / N",
      "        )",
      "        temp_xgrad = temp_xgrad.to(tl.float16)",
      "",
      "        x_grad_slice += temp_xgrad",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(x_grad_block_ptr, x_grad_slice)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/78.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 2}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 4}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=2)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 32,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/79.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4)], key=['V', 'N', 'H'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/79.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3)], key=['V', 'N', 'H'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(N_group // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + idx_N_group * N_group + N_range)",
      "        lse = tl.load(lse_ptr + N_range)",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    A_grad_T_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "    if idx_N_group > 0:",
      "        tl.store(",
      "            A_grad_T_block_ptr,",
      "            tl.load(A_grad_T_block_ptr)",
      "            + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "        )",
      "    else:",
      "        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/79.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "        A_v = tl.load(A_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, mask=N_range < N, other=ignore_index)",
      "",
      "    reduction = tl.load(reduction_ptr)",
      "    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction",
      "",
      "    tl.store(",
      "        z_block_ptr,",
      "        (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty),",
      "        boundary_check=(0, 1),",
      "    )",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))",
      "    lse = tl.where(",
      "        y != ignore_index,",
      "        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,",
      "        zero_lse_constant,",
      "    )",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None], boundary_check=(0,))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/81.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == V_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            A_v.type.element_ty",
      "        )",
      "",
      "        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/81.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            x_ptr.type.element_ty",
      "        )",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/81.py"
  },
  {
    "name": "logsumexp_reduction_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'N_BLOCK_SIZE': 2}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 4}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=16, num_stages=5), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=1)], key=['N_group', 'V', 'V_BLOCK_SIZE'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "lse_local_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "lse_sum_ptr",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_reduction_kernel(",
      "    lse_local_ptr,",
      "    lse_global_ptr,",
      "    lse_sum_ptr,",
      "    reduction_ptr,",
      "    z_regularization: tl.constexpr,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr = 32,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_local_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_local = tl.load(lse_row_ptr)",
      "    m = tl.max(lse_local, 1)",
      "    lse = tl.log(tl.sum(tl.exp((lse_local - m[:, None])), axis=1)) + m",
      "",
      "    lse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(",
      "        reduction_ptr",
      "    )",
      "",
      "    tl.atomic_add(lse_sum_ptr, lse_reduction)",
      "    tl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/81.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 2}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 4}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=2)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/83.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (V // V_BLOCK_SIZE), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/83.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE",
      "    idx_H = tl.program_id(axis=1)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(N_group // N_BLOCK_SIZE):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    A_grad_T_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "    if idx_N_group > 0:",
      "        tl.store(",
      "            A_grad_T_block_ptr,",
      "            tl.load(A_grad_T_block_ptr)",
      "            + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "        )",
      "    else:",
      "        tl.store(A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/83.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 2}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 4}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 8}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'GROUP_SIZE': 32}, num_warps=16, num_stages=2)], key=['V', 'N', 'H'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/84.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=2)",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (V // V_BLOCK_SIZE), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(",
      "            x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/84.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "):",
      "    idx_V = tl.program_id(axis=0) - N_group // N_BLOCK_SIZE",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = tl.program_id(axis=2)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(N, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr)",
      "                + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(",
      "                A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "            )",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(",
      "            A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/84.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    s = tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    mask = y[:, None] == V_range[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/85.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), x_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(A_t_ptr.type.element_ty)",
      "",
      "        x_grad_acc = tl.dot(",
      "            z_grad, A_v.trans(), x_grad_acc, out_dtype=x_grad_ptr.type.element_ty",
      "        )",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(",
      "            x_grad_simple_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/85.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), A_grad_ptr.type.element_ty)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(x_ptr.type.element_ty)",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), z_grad, A_grad_acc, out_dtype=A_grad_ptr.type.element_ty",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr)",
      "                + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(",
      "                A_grad_T_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "            )",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(",
      "            A_grad_T_simple_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty)",
      "        )"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/85.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE, GROUP_SIZE)",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    reduction = tl.load(reduction_ptr)",
      "    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / reduction",
      "",
      "    tl.store(",
      "        z_block_ptr, (z_j_to_k + tl.log(1 / reduction)).to(z_nv_ptr.type.element_ty)",
      "    )",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))",
      "    lse = tl.where(",
      "        y != ignore_index,",
      "        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,",
      "        zero_lse_constant,",
      "    )",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/86.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == V_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0.0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            A_v.type.element_ty",
      "        )",
      "",
      "        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/86.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    reduction = tl.load(reduction_ptr)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        if z_regularization > 0:",
      "            softmax_z += 2.0 * z_regularization * lse[:, None] * softmax_z",
      "        z_grad = softmax_z - tl.where(mask, 1 / reduction, 0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            x_ptr.type.element_ty",
      "        )",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/86.py"
  },
  {
    "name": "logsumexp_reduction_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'N_BLOCK_SIZE': 2}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 4}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=2, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=4, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=8, num_stages=4), triton.Config({'N_BLOCK_SIZE': 8}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=2, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'N_BLOCK_SIZE': 32}, num_warps=16, num_stages=5), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=2), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=4, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=8, num_stages=1), triton.Config({'N_BLOCK_SIZE': 16}, num_warps=16, num_stages=1)], key=['N_group', 'V', 'V_BLOCK_SIZE'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "lse_local_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "lse_sum_ptr",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_reduction_kernel(",
      "    lse_local_ptr,",
      "    lse_global_ptr,",
      "    lse_sum_ptr,",
      "    reduction_ptr,",
      "    z_regularization: tl.constexpr,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    N_group,",
      "    V: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr = 32,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_local_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V // V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_local = tl.load(lse_row_ptr)",
      "    m = tl.max(lse_local, 1)",
      "    lse = tl.log(tl.sum(tl.exp((lse_local - m[:, None])), axis=1)) + m",
      "",
      "    lse_reduction = (tl.sum(lse) + z_regularization * tl.sum(lse * lse)) / tl.load(",
      "        reduction_ptr",
      "    )",
      "",
      "    tl.atomic_add(lse_sum_ptr, lse_reduction)",
      "    tl.store(lse_global_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE), lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/86.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {
          "warmup": 100,
          "rep": 500
        }
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fwd_configs, key=['V', 'N', 'H'], prune_configs_by={'early_config_prune': early_config_prune}, warmup=100, rep=500)"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    y = tl.load(",
      "        y_ptr",
      "        + idx_N_group * N_group",
      "        + idx_N * N_BLOCK_SIZE",
      "        + tl.arange(0, N_BLOCK_SIZE)",
      "    )",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    mask = y[:, None] == tl.where(V_range != ignore_index, V_range, -1)[None, :]",
      "    loss = -tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "    tl.store(z_block_ptr, (z_j_to_k + tl.log(1 / N)).to(z_nv_ptr.type.element_ty))",
      "",
      "    m = tl.max(z_j_to_k, 1)",
      "    zero_lse_constant: tl.constexpr = tl.log(1 / tl.cdiv(V, V_BLOCK_SIZE))",
      "    lse = tl.where(",
      "        y != ignore_index,",
      "        tl.log(tl.sum(tl.exp((z_j_to_k - m[:, None])), axis=1)) + m,",
      "        zero_lse_constant,",
      "    )",
      "",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    tl.store(loss_val_ptr, tl.load(loss_val_ptr) + loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/87.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0) // SPLIT_V",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V_tile = tl.program_id(axis=0) % SPLIT_V",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0) - (",
      "        triton.cdiv(V, V_BLOCK_SIZE) * SPLIT_N",
      "    ), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(",
      "        idx_N, idx_H, num_idx_N // SPLIT_V, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    V_split_offset = idx_V_tile * tl.cdiv(V, SPLIT_V)",
      "",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, V_split_offset),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = V_split_offset + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range, eviction_policy=\"evict_last\")",
      "    lse = tl.load(",
      "        lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE),",
      "        eviction_policy=\"evict_last\",",
      "    )",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else x_grad_ptr.type.element_ty",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(V, V_BLOCK_SIZE * SPLIT_V)):",
      "        mask = y[:, None] == V_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "        z_grad = softmax_z - tl.where(mask, 1 / N, 0.0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            A_v.type.element_ty",
      "        )",
      "",
      "        x_grad_acc = tl.dot(valid_z_grad, A_v.trans(), x_grad_acc, out_dtype=acc_dtype)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range += V_BLOCK_SIZE",
      "",
      "    if SPLIT_V == 1:",
      "        x_grad_block_ptr = tl.make_block_ptr(",
      "            base=x_grad_ptr,",
      "            shape=(N, H),",
      "            strides=(stride_x_N, stride_x_H),",
      "            offsets=(",
      "                idx_N_group * N_group + idx_N * N_BLOCK_SIZE,",
      "                idx_H * H_BLOCK_SIZE,",
      "            ),",
      "            block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "            order=(1, 0),",
      "        )",
      "        tl.store(x_grad_block_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))",
      "    else:",
      "        row_n = (",
      "            idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "        )",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        x_grad_simple_ptr = (",
      "            x_grad_ptr + row_n[:, None] * stride_x_N + row_h[None, :] * stride_x_H",
      "        )",
      "        tl.atomic_add(x_grad_simple_ptr, x_grad_acc.to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/87.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "z_regularization",
        "annotation": "tl.constexpr"
      },
      {
        "name": "fp32_grad_accumulators",
        "annotation": "tl.constexpr"
      },
      {
        "name": "reduction_ptr",
        "annotation": null
      },
      {
        "name": "ignore_index",
        "annotation": "tl.constexpr"
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_V",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    z_regularization: tl.constexpr,",
      "    fp32_grad_accumulators: tl.constexpr,",
      "    reduction_ptr,",
      "    ignore_index: tl.constexpr,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    GROUP_SIZE: tl.constexpr,",
      "    SPLIT_N: tl.constexpr,",
      "    SPLIT_V: tl.constexpr,",
      "):",
      "    idx_V = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) // SPLIT_N",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N_tile = (tl.program_id(axis=0) - N_group // N_BLOCK_SIZE * SPLIT_V) % SPLIT_N",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0) - (",
      "        N_group // N_BLOCK_SIZE * SPLIT_V",
      "    ), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(",
      "        idx_V, idx_H, num_idx_V // SPLIT_N, num_idx_H, GROUP_SIZE",
      "    )",
      "",
      "    N_split_offset = idx_N_tile * tl.cdiv(N_group, SPLIT_N)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + N_split_offset, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(N_split_offset, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = N_split_offset + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    acc_dtype = tl.float32 if fp32_grad_accumulators else A_grad_ptr.type.element_ty",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), acc_dtype)",
      "    for _ in range(0, tl.cdiv(N_group, N_BLOCK_SIZE * SPLIT_N)):",
      "        y = tl.load(",
      "            y_ptr + idx_N_group * N_group + N_range, eviction_policy=\"evict_last\"",
      "        )",
      "        lse = tl.load(lse_ptr + N_range, eviction_policy=\"evict_last\")",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr, eviction_policy=\"evict_first\")",
      "        z_j_to_k = tl.load(z_block_ptr, eviction_policy=\"evict_last\")",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "        z_grad = softmax_z - tl.where(mask, 1 / N, 0)",
      "        valid_z_grad = tl.where((y == ignore_index)[:, None], 0.0, z_grad).to(",
      "            x_ptr.type.element_ty",
      "        )",
      "",
      "        A_grad_acc = tl.dot(",
      "            x_chunk.trans(), valid_z_grad, A_grad_acc, out_dtype=acc_dtype",
      "        )",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "",
      "    if SPLIT_N == 1:",
      "        A_grad_T_block_ptr = tl.make_block_ptr(",
      "            base=A_grad_ptr,",
      "            shape=(H, V),",
      "            strides=(stride_A_H, stride_A_V),",
      "            offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "            block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "            order=(0, 1),",
      "        )",
      "        if idx_N_group > 0:",
      "            tl.store(",
      "                A_grad_T_block_ptr,",
      "                tl.load(A_grad_T_block_ptr) + A_grad_acc.to(A_grad_ptr.type.element_ty),",
      "            )",
      "        else:",
      "            tl.store(A_grad_T_block_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))",
      "    else:",
      "        row_h = idx_H * H_BLOCK_SIZE + tl.arange(0, H_BLOCK_SIZE)",
      "        row_v = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "        A_grad_T_simple_ptr = (",
      "            A_grad_ptr + row_h[:, None] * stride_A_H + row_v[None, :] * stride_A_V",
      "        )",
      "        tl.atomic_add(A_grad_T_simple_ptr, A_grad_acc.to(A_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/87.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['loss_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    loss_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.atomic_add(loss_ptr, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/88.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_prologue",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512})], key=['V', 'N', 'H'], reset_to_zero=['sz_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_prologue(",
      "    sz_ptr,",
      "    x_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    lse = tl.load(lse_global_ptr + offsets)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr)",
      "        A_v = tl.load(A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    tl.store(sz_block_ptr, softmax_z.to(tl.float16))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/88.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    sz_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_H = tl.program_id(axis=0)",
      "    idx_N = tl.program_id(axis=1)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_offsets)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "        A_v = tl.load(A_t_block_ptr).trans()",
      "        sz = tl.load(sz_block_ptr)",
      "",
      "        x_grad_acc = tl.dot(sz, A_v, x_grad_acc)",
      "        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        sz_block_ptr = tl.advance(sz_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_offsets += V_BLOCK_SIZE",
      "",
      "    tl.store(x_grad_block_ptr, x_grad_acc / N)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/88.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "sz_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_sz_N",
        "annotation": null
      },
      {
        "name": "stride_sz_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    sz_ptr,",
      "    x_ptr,",
      "    y_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_sz_N,",
      "    stride_sz_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_H = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1) - (N // N_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    sz_block_ptr = tl.make_block_ptr(",
      "        base=sz_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_sz_N, stride_sz_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + N_offsets)",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "        x_chunk = tl.load(x_block_ptr)",
      "        sz = tl.load(sz_block_ptr).trans()",
      "",
      "        A_grad_acc = tl.dot(sz, x_chunk, A_grad_acc)",
      "        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        sz_block_ptr = tl.advance(sz_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_offsets += N_BLOCK_SIZE",
      "",
      "    tl.store(A_t_grad_block_ptr, A_grad_acc.trans() / N)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/88.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['loss_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    loss_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.atomic_add(loss_ptr, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/89.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr', 'x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "locks_N_ptr",
        "annotation": null
      },
      {
        "name": "locks_V_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_x_grad_N",
        "annotation": null
      },
      {
        "name": "stride_x_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_H",
        "annotation": null
      },
      {
        "name": "stride_A_grad_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    x_grad_ptr,",
      "    locks_N_ptr,",
      "    locks_V_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_x_grad_N,",
      "    stride_x_grad_H,",
      "    stride_A_grad_H,",
      "    stride_A_grad_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + offsets)",
      "    lse = tl.load(lse_global_ptr + offsets)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_grad_N, stride_x_grad_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_grad_H, stride_A_grad_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "",
      "    local_x_block_ptr = x_block_ptr",
      "    local_A_block_ptr = A_block_ptr",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(local_x_block_ptr)",
      "        A_v = tl.load(local_A_block_ptr)",
      "",
      "        z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "        local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "        local_A_block_ptr = tl.advance(local_A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "    mask = (y[:, None] == v_range[None, :])[:, :, None]",
      "",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "",
      "    for _ in range(H // H_BLOCK_SIZE):",
      "        x_chunk = tl.load(x_block_ptr).to(tl.float32)",
      "        A_v = tl.load(A_block_ptr).to(tl.float32)",
      "",
      "        temp_xgrad = tl.dot(softmax_z, A_v.trans())",
      "        temp_xgrad -= tl.sum(tl.where(mask, A_v.trans()[None, :, :], 0.0), axis=1)",
      "",
      "        while tl.atomic_cas(locks_V_ptr + idx_N, 0, 1) == 1:",
      "            pass",
      "        temp_xgrad = temp_xgrad / N + tl.load(x_grad_block_ptr)",
      "        tl.store(x_grad_block_ptr, temp_xgrad)",
      "        tl.atomic_xchg(locks_V_ptr + idx_N, 0)",
      "",
      "        temp_Agrad = tl.dot(softmax_z.trans(), x_chunk)",
      "        temp_Agrad -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "        temp_Agrad = temp_Agrad.trans()",
      "",
      "        while tl.atomic_cas(locks_N_ptr + idx_V, 0, 1) == 1:",
      "            pass",
      "        temp_Agrad = temp_Agrad / N + tl.load(A_grad_block_ptr)",
      "",
      "        tl.store(A_grad_block_ptr, temp_Agrad)",
      "        tl.atomic_xchg(locks_N_ptr + idx_V, 0)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "        x_grad_block_ptr = tl.advance(x_grad_block_ptr, [0, H_BLOCK_SIZE])",
      "",
      "        A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "        A_grad_block_ptr = tl.advance(A_grad_block_ptr, [H_BLOCK_SIZE, 0])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/89.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128}, num_warps=8)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx_N + idx_N_group * N_group // N_BLOCK_SIZE, loss)",
      "    tl.store(lse_ptr + N_range, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/90.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'], restore_value=['z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/90.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "N_offset",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    N_offset,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offsets = N_offset + idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/91.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16})], key=['V', 'N'], reset_to_zero=['z_grad_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "z_grad_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "N_offset",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    z_grad_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    N_offset,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_grad_block_ptr = tl.make_block_ptr(",
      "        base=z_grad_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = N_offset + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_grad_block_ptr, z_grad.to(tl.float16))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/91.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "N_offset",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    N_offset,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(N_offset + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = N_offset + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        mask = (y[:, None] == v_range[None, :])[:, :, None]",
      "        A_v = tl.load(A_t_block_ptr).trans()",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        x_grad_acc = tl.dot(softmax_z, A_v, x_grad_acc)",
      "        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(tl.float16))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/91.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx_N + idx_N_group * N_group // N_BLOCK_SIZE, loss)",
      "    tl.store(lse_ptr + N_range, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/92.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        mask = (y[:, None] == v_range[None, :])[:, :, None]",
      "        A_v = tl.load(A_t_block_ptr).trans()",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        x_grad_acc = tl.dot(softmax_z, A_v, x_grad_acc)",
      "        x_grad_acc -= tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/92.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_N = 0",
      "",
      "    tl.static_print(V_BLOCK_SIZE, N_BLOCK_SIZE, H_BLOCK_SIZE)",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(V, H),",
      "        strides=(stride_A_V, stride_A_H),",
      "        offsets=(idx_V * V_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(V_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((V_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + N_range)",
      "        lse = tl.load(lse_ptr + N_range)",
      "        mask = (y[:, None] == V_range[None, :])[:, :, None]",
      "",
      "        x_chunk = tl.load(x_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        A_grad_acc = tl.dot(softmax_z.trans(), x_chunk, A_grad_acc)",
      "        A_grad_acc -= tl.sum(tl.where(mask, x_chunk[:, None, :], 0.0), axis=0)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "    tl.store(A_grad_block_ptr, (A_grad_acc / N).to(A_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/92.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    V_TILES: tl.constexpr = 4,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N, V // 16),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = (",
      "        losses_ptr",
      "        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb",
      "        + idx_V_group * stride_loss_B",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/93.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'], restore_value=['z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/93.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=6)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    V_TILES: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 16),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/94.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'], restore_value=['z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/94.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=32), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=5), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=6), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 1, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 2, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'V_TILES': 4, 'GROUP_SIZE': 32}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 2, 'GROUP_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 4, 'GROUP_SIZE': 64}, num_warps=16)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    V_TILES: tl.constexpr = 1,",
      "    GROUP_SIZE: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_V_group = tl.swizzle2d(",
      "        idx_N, idx_V_group, num_idx_N, num_idx_V_group, GROUP_SIZE",
      "    )",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 128),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/95.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3)], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dx(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    x_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "    idx_V = 0",
      "",
      "    num_idx_N, num_idx_H = tl.num_programs(0), tl.num_programs(1)",
      "    idx_N, idx_H = tl.swizzle2d(idx_N, idx_H, num_idx_N, num_idx_H, GROUP_SIZE)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_t_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = 0 + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE))",
      "",
      "    x_grad_acc = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float32)",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "        mask = y[:, None] == v_range[None, :]",
      "        A_v = tl.load(A_t_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(tl.float16)",
      "",
      "        x_grad_acc = tl.dot(z_grad, A_v.trans(), x_grad_acc)",
      "",
      "        A_t_block_ptr = tl.advance(A_t_block_ptr, [0, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        v_range += V_BLOCK_SIZE",
      "",
      "    tl.store(x_grad_block_ptr, (x_grad_acc / N).to(x_grad_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/95.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_epilogue_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 64}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 256, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=4, num_stages=3), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128, 'GROUP_SIZE': 32}, num_warps=8, num_stages=3)], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_epilogue_dA(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    x_ptr,",
      "    A_grad_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    GROUP_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    idx_H = tl.program_id(axis=1)",
      "",
      "    num_idx_V, num_idx_H = tl.num_programs(0), tl.num_programs(1)",
      "    idx_V, idx_H = tl.swizzle2d(idx_V, idx_H, num_idx_V, num_idx_H, GROUP_SIZE)",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group, idx_H * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_T_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(0, 1),",
      "    )",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    N_range = tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_grad_acc = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), tl.float32)",
      "    for _ in range(N_group // N_BLOCK_SIZE):",
      "        y = tl.load(y_ptr + idx_N_group * N_group + N_range)",
      "        lse = tl.load(lse_ptr + N_range)",
      "        mask = y[:, None] == V_range[None, :]",
      "",
      "        x_chunk = tl.load(x_block_ptr)",
      "        z_j_to_k = tl.load(z_block_ptr)",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "        z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)).to(tl.float16)",
      "",
      "        A_grad_acc = tl.dot(x_chunk.trans(), z_grad, A_grad_acc)",
      "",
      "        x_block_ptr = tl.advance(x_block_ptr, [N_BLOCK_SIZE, 0])",
      "        z_block_ptr = tl.advance(z_block_ptr, [N_BLOCK_SIZE, 0])",
      "        N_range += N_BLOCK_SIZE",
      "    tl.store(",
      "        A_grad_T_block_ptr,",
      "        tl.load(A_grad_T_block_ptr) + (A_grad_acc / N).to(A_grad_ptr.type.element_ty),",
      "    )"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/95.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "    V_TILES: tl.constexpr = 1,",
      "    GROUP_SIZE: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "    num_idx_N, num_idx_V_group = tl.num_programs(0), tl.num_programs(1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N_group, V // 16),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = losses_ptr + idx_N * stride_loss_Nb + idx_V_group * stride_loss_B",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/96.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'], restore_value=['z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/96.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=4), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=3), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8, num_stages=4), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 128, 'V_TILES': 1}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=16)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "stride_lse_N",
        "annotation": null
      },
      {
        "name": "stride_lse_B",
        "annotation": null
      },
      {
        "name": "stride_loss_Nb",
        "annotation": null
      },
      {
        "name": "stride_loss_B",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    stride_lse_N,",
      "    stride_lse_B,",
      "    stride_loss_Nb,",
      "    stride_loss_B,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    V_TILES: tl.constexpr = 1,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    lse_row_ptr = tl.make_block_ptr(",
      "        base=lse_ptr,",
      "        shape=(N, V // 64),",
      "        strides=(stride_lse_N, stride_lse_B),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, idx_V_group),",
      "        block_shape=(N_BLOCK_SIZE, 1),",
      "        order=(1, 0),",
      "    )",
      "    loss_val_ptr = (",
      "        losses_ptr",
      "        + (idx_N + idx_N_group * N_group // N_BLOCK_SIZE) * stride_loss_Nb",
      "        + idx_V_group * stride_loss_B",
      "    )",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "",
      "    tl.store(loss_val_ptr, loss)",
      "    tl.store(lse_row_ptr, lse[:, None])"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/97.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 1024, 'N_BLOCK_SIZE': 16}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64}, num_warps=16), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=16)], key=['V', 'N'], restore_value=['z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    lse_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "    lse = tl.load(lse_ptr + N_range)",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/97.py"
  },
  {
    "name": "linear_xent_fwd_prep_bwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64, 'V_TILES': 1}, num_warps=8)], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'sumexp_ptr', 'z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "sumexp_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_TILES",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_prep_bwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    z_nv_ptr,",
      "    losses_ptr,",
      "    sumexp_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "    V_TILES: tl.constexpr = 4,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V_group = tl.program_id(axis=1)",
      "",
      "    V_GROUP_SIZE: tl.constexpr = V_TILES * V_BLOCK_SIZE",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N_group * N_group + idx_N * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V_group * V_GROUP_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    sumexp_row_ptr = sumexp_ptr + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "",
      "    N_range = idx_N_group * N_group + idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_range = idx_V_group * V_GROUP_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + N_range)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e6)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V_TILES):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp((z_j_to_k - m_new[:, None])), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == V_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        tl.store(z_block_ptr, z_j_to_k.to(z_nv_ptr.type.element_ty))",
      "",
      "        m = m_new",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        A_block_ptr = tl.advance(A_block_ptr, [-H, V_BLOCK_SIZE])",
      "        z_block_ptr = tl.advance(z_block_ptr, [0, V_BLOCK_SIZE])",
      "        V_range = V_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    sum_exp = tl.exp(lse).to(sumexp_ptr.type.element_ty)",
      "",
      "    tl.atomic_add(losses_ptr + idx_N, loss)",
      "    tl.atomic_add(sumexp_row_ptr, sum_exp)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/98.py"
  },
  {
    "name": "linear_xent_mini_bwd_prologue_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 128}, num_warps=8)], key=['V', 'N'], restore_value=['z_nv_ptr'])"
    ],
    "args": [
      {
        "name": "z_nv_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "sumexp_ptr",
        "annotation": null
      },
      {
        "name": "stride_z_N",
        "annotation": null
      },
      {
        "name": "stride_z_V",
        "annotation": null
      },
      {
        "name": "idx_N_group",
        "annotation": null
      },
      {
        "name": "N_group",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_mini_bwd_prologue_kernel(",
      "    z_nv_ptr,",
      "    y_ptr,",
      "    sumexp_ptr,",
      "    stride_z_N,",
      "    stride_z_V,",
      "    idx_N_group,",
      "    N_group: tl.constexpr,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_V = tl.program_id(axis=1)",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        base=z_nv_ptr,",
      "        shape=(N_group, V),",
      "        strides=(stride_z_N, stride_z_V),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    N_range = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + idx_N_group * N_group + N_range)",
      "    lse = tl.log(tl.load(sumexp_ptr + N_range))",
      "    z_j_to_k = tl.load(z_block_ptr)",
      "",
      "    mask = y[:, None] == v_range[None, :]",
      "    softmax_z = (z_j_to_k - lse[:, None]).exp()",
      "    z_grad = (softmax_z - tl.where(mask, 1.0, 0.0)) / N",
      "",
      "    tl.store(z_block_ptr, z_grad.to(z_nv_ptr.type.element_ty))"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/98.py"
  },
  {
    "name": "linear_xent_fwd_kernel_matmul_t",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['losses_ptr', 'lse_ptr'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "losses_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_fwd_kernel_matmul_t(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    losses_ptr,",
      "    lse_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr,",
      "    N_BLOCK_SIZE: tl.constexpr,",
      "    H_BLOCK_SIZE: tl.constexpr,",
      "):",
      "    idx = tl.program_id(axis=0)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, 0),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    offsets = idx * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    v_range = tl.arange(0, V_BLOCK_SIZE)",
      "    y = tl.load(y_ptr + offsets)",
      "",
      "    m = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32) - float(10e5)",
      "    s = tl.zeros((N_BLOCK_SIZE,), dtype=tl.float32)",
      "    loss = 0.0",
      "",
      "    for _ in range(V // V_BLOCK_SIZE):",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        local_x_block_ptr = x_block_ptr",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(local_x_block_ptr)",
      "            A_v = tl.load(A_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            local_x_block_ptr = tl.advance(local_x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_block_ptr = tl.advance(A_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        m_new = tl.maximum(m, tl.max(z_j_to_k, 1))",
      "",
      "        s_update = tl.sum(tl.exp(z_j_to_k - m_new[:, None]), axis=1)",
      "        s = s * tl.exp(m - m_new) + s_update",
      "",
      "        mask = y[:, None] == v_range[None, :]",
      "        loss -= tl.sum(tl.where(mask, z_j_to_k, float(0.0))) / N",
      "",
      "        m = m_new",
      "        A_block_ptr = tl.advance(",
      "            A_block_ptr, [-H_BLOCK_SIZE * (H // H_BLOCK_SIZE), V_BLOCK_SIZE]",
      "        )",
      "        v_range = v_range + V_BLOCK_SIZE",
      "",
      "    lse = m + tl.log(s)",
      "    loss += tl.sum(lse) / N",
      "    tl.store(losses_ptr + idx, loss)",
      "    tl.store(lse_ptr + offsets, lse)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/100.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dA",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 128, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 256, 'H_BLOCK_SIZE': 128})], key=['V', 'N', 'H'], reset_to_zero=['A_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "A_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dA(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    A_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_V = tl.program_id(axis=0)",
      "    idx_H_grad = tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = idx_V * V_BLOCK_SIZE + tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    A_fwd_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_grad_block_ptr = tl.make_block_ptr(",
      "        base=A_grad_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H_grad * H_BLOCK_SIZE, idx_V * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_fwd_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, 0),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    x_bwd_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(0 * N_BLOCK_SIZE, idx_H_grad * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    AgradT = tl.zeros((H_BLOCK_SIZE, V_BLOCK_SIZE), tl.float16)",
      "",
      "    for idx_N in range(N // N_BLOCK_SIZE):",
      "",
      "        y = tl.load(y_ptr + N_offsets)",
      "        lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for _ in range(H // H_BLOCK_SIZE):",
      "            x_chunk = tl.load(x_fwd_block_ptr)",
      "            A_v = tl.load(A_fwd_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v, z_j_to_k)",
      "",
      "            x_fwd_block_ptr = tl.advance(x_fwd_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        x_chunk_bwd = tl.load(x_bwd_block_ptr)",
      "",
      "        AgradT += (tl.dot(x_chunk_bwd.trans(), softmax_z) / N).to(tl.float16)",
      "        AgradT -= (",
      "            tl.sum(tl.where(mask, x_chunk_bwd[:, None, :], 0.0), axis=0).trans() / N",
      "        ).to(tl.float16)",
      "",
      "        x_bwd_block_ptr = tl.advance(x_bwd_block_ptr, [N_BLOCK_SIZE, 0])",
      "        x_fwd_block_ptr = tl.advance(x_fwd_block_ptr, [N_BLOCK_SIZE, -H])",
      "        A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [-H, 0])",
      "        N_offsets += N_BLOCK_SIZE",
      "",
      "    tl.store(A_grad_block_ptr, AgradT)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/100.py"
  },
  {
    "name": "linear_xent_bwd_kernel_matmul_t_dx",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 32}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 32, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 32, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 64, 'N_BLOCK_SIZE': 64, 'H_BLOCK_SIZE': 64}), triton.Config({'V_BLOCK_SIZE': 512, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 256}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 16, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 512}), triton.Config({'V_BLOCK_SIZE': 128, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 128}), triton.Config({'V_BLOCK_SIZE': 256, 'N_BLOCK_SIZE': 16, 'H_BLOCK_SIZE': 16})], key=['V', 'N', 'H'], reset_to_zero=['x_grad_ptr'])",
      "@triton.jit"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "A_t_ptr",
        "annotation": null
      },
      {
        "name": "lse_global_ptr",
        "annotation": null
      },
      {
        "name": "x_grad_ptr",
        "annotation": null
      },
      {
        "name": "stride_x_N",
        "annotation": null
      },
      {
        "name": "stride_x_H",
        "annotation": null
      },
      {
        "name": "stride_A_H",
        "annotation": null
      },
      {
        "name": "stride_A_V",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "H_BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_xent_bwd_kernel_matmul_t_dx(",
      "    x_ptr,",
      "    y_ptr,",
      "    A_t_ptr,",
      "    lse_global_ptr,",
      "    x_grad_ptr,",
      "    stride_x_N,",
      "    stride_x_H,",
      "    stride_A_H,",
      "    stride_A_V,",
      "    V: tl.constexpr,",
      "    N: tl.constexpr,",
      "    H: tl.constexpr,",
      "    V_BLOCK_SIZE: tl.constexpr = 16,",
      "    N_BLOCK_SIZE: tl.constexpr = 16,",
      "    H_BLOCK_SIZE: tl.constexpr = 16,",
      "):",
      "    idx_N = tl.program_id(axis=0)",
      "    idx_H_grad = tl.program_id(axis=1)",
      "",
      "    tl.static_assert(N % N_BLOCK_SIZE == 0)",
      "    tl.static_assert(V % V_BLOCK_SIZE == 0)",
      "    tl.static_assert(H % H_BLOCK_SIZE == 0)",
      "",
      "    N_offsets = idx_N * N_BLOCK_SIZE + tl.arange(0, N_BLOCK_SIZE)",
      "    V_offsets = tl.arange(0, V_BLOCK_SIZE)",
      "",
      "    y = tl.load(y_ptr + N_offsets)",
      "    lse = tl.load(lse_global_ptr + N_offsets)",
      "",
      "    x_grad_block_ptr = tl.make_block_ptr(",
      "        base=x_grad_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, idx_H_grad * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        base=x_ptr,",
      "        shape=(N, H),",
      "        strides=(stride_x_N, stride_x_H),",
      "        offsets=(idx_N * N_BLOCK_SIZE, 0 * H_BLOCK_SIZE),",
      "        block_shape=(N_BLOCK_SIZE, H_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_fwd_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(0 * H_BLOCK_SIZE, 0 * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "    A_bwd_block_ptr = tl.make_block_ptr(",
      "        base=A_t_ptr,",
      "        shape=(H, V),",
      "        strides=(stride_A_H, stride_A_V),",
      "        offsets=(idx_H_grad * H_BLOCK_SIZE, 0 * V_BLOCK_SIZE),",
      "        block_shape=(H_BLOCK_SIZE, V_BLOCK_SIZE),",
      "        order=(1, 0),",
      "    )",
      "",
      "    x_grad = tl.zeros((N_BLOCK_SIZE, H_BLOCK_SIZE), tl.float16)",
      "",
      "    for idx_V in range(V // V_BLOCK_SIZE):",
      "        z_j_to_k = tl.zeros((N_BLOCK_SIZE, V_BLOCK_SIZE), dtype=tl.float32)",
      "        for idx_H_1 in range(H // H_BLOCK_SIZE):",
      "",
      "            x_chunk = tl.load(x_block_ptr)",
      "            A_v_fwd = tl.load(A_fwd_block_ptr)",
      "",
      "            z_j_to_k = tl.dot(x_chunk, A_v_fwd, z_j_to_k)",
      "",
      "            x_block_ptr = tl.advance(x_block_ptr, [0, H_BLOCK_SIZE])",
      "            A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [H_BLOCK_SIZE, 0])",
      "",
      "        mask = (y[:, None] == V_offsets[None, :])[:, :, None]",
      "",
      "        softmax_z = (z_j_to_k - lse[:, None]).exp().to(tl.float16)",
      "",
      "        A_v = tl.load(A_bwd_block_ptr).trans()",
      "",
      "        x_grad += (tl.dot(softmax_z, A_v) / N).to(tl.float16)",
      "        x_grad -= (tl.sum(tl.where(mask, A_v[None, :, :], 0.0), axis=1) / N).to(",
      "            tl.float16",
      "        )",
      "",
      "        A_bwd_block_ptr = tl.advance(A_bwd_block_ptr, [0, V_BLOCK_SIZE])",
      "        A_fwd_block_ptr = tl.advance(A_fwd_block_ptr, [-H, V_BLOCK_SIZE])",
      "        x_block_ptr = tl.advance(x_block_ptr, [0, -H])",
      "        V_offsets += V_BLOCK_SIZE",
      "    tl.store(x_grad_block_ptr, x_grad)"
    ],
    "file": "triton_repos/JonasGeiping_linear_cross_entropy_loss/variants/100.py"
  },
  {
    "name": "_swiglu_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_fwd_kernel(",
      "    X,",
      "    Y,",
      "    OUT,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_out_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    OUT += row * stride_out_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    out = x * tl.sigmoid(x) * y",
      "    tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/30.py"
  },
  {
    "name": "_swiglu_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DOUT",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dout_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_bwd_kernel(",
      "    X,",
      "    Y,",
      "    DOUT,",
      "    OUT,",
      "    DX,",
      "    DY,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dout_row,",
      "    stride_out_row,",
      "    stride_dx_row,",
      "    stride_dy_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    DOUT += row * stride_dout_row",
      "    if RECOMPUTE_OUTPUT:",
      "        OUT += row * stride_out_row",
      "    DX += row * stride_dx_row",
      "    DY += row * stride_dy_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    x_sigmoid = tl.sigmoid(x)",
      "    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout",
      "    dy = x * x_sigmoid * dout",
      "    tl.store(DX + cols, dx, mask=cols < ncols)",
      "    tl.store(DY + cols, dy, mask=cols < ncols)",
      "    if RECOMPUTE_OUTPUT:",
      "        out = x * x_sigmoid * y",
      "        tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/30.py"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])",
      "@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})",
      "@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "X1",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "B1",
        "annotation": null
      },
      {
        "name": "Y1",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "DROPOUT_MASK",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "stride_x1_row",
        "annotation": null
      },
      {
        "name": "stride_y1_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DROPOUT_MASK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_X1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_W1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    X1,",
      "    W1,",
      "    B1,",
      "    Y1,",
      "    RESIDUAL_OUT,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    DROPOUT_MASK,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    stride_x1_row,",
      "    stride_y1_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    STORE_DROPOUT_MASK: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_X1: tl.constexpr,",
      "    HAS_W1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "    if HAS_X1:",
      "        X1 += row * stride_x1_row",
      "    if HAS_W1:",
      "        Y1 += row * stride_y1_row",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_ROWSCALE:",
      "        rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "        x *= rowscale",
      "    if HAS_DROPOUT:",
      "",
      "        keep_mask = (",
      "            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p",
      "        )",
      "        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)",
      "        if STORE_DROPOUT_MASK:",
      "            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)",
      "    if HAS_X1:",
      "        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)",
      "            x1 *= rowscale",
      "        if HAS_DROPOUT:",
      "",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)",
      "            if STORE_DROPOUT_MASK:",
      "                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)",
      "        x += x1",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "",
      "    tl.store(Y + cols, y, mask=mask)",
      "    if HAS_W1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "        if HAS_B1:",
      "            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)",
      "        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1",
      "        tl.store(Y1 + cols, y1, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/31.py"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])",
      "@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})",
      "@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})",
      "@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "DY1",
        "annotation": null
      },
      {
        "name": "DX1",
        "annotation": null
      },
      {
        "name": "DW1",
        "annotation": null
      },
      {
        "name": "DB1",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dy1_row",
        "annotation": null
      },
      {
        "name": "stride_dx1_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DY1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DX1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    W1,",
      "    DY1,",
      "    DX1,",
      "    DW1,",
      "    DB1,",
      "    DRESIDUAL_IN,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dy1_row,",
      "    stride_dx1_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_DY1: tl.constexpr,",
      "    HAS_DX1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if HAS_DY1:",
      "        DY1 += row_start * stride_dy1_row",
      "    if HAS_DX1:",
      "        DX1 += row_start * stride_dx1_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_DY1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_DY1:",
      "        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "        if HAS_B1:",
      "            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if HAS_DY1:",
      "            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_DY1:",
      "            wdy += w1 * dy1",
      "            dw1 += dy1 * xhat",
      "            if HAS_B1:",
      "                db1 += dy1",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        if HAS_DX1:",
      "            if HAS_DROPOUT:",
      "                keep_mask = (",
      "                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                    > dropout_p",
      "                )",
      "                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "            else:",
      "                dx1 = dx",
      "            tl.store(DX1 + cols, dx1, mask=mask)",
      "        if HAS_DROPOUT:",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "            dx *= rowscale",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "        if HAS_DY1:",
      "            DY1 += stride_dy1_row",
      "        if HAS_DX1:",
      "            DX1 += stride_dx1_row",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)",
      "    if HAS_DY1:",
      "        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)",
      "        if HAS_B1:",
      "            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/31.py"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    Z,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/32.py"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DZ",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dz_row",
        "annotation": null
      },
      {
        "name": "stride_dw_row",
        "annotation": null
      },
      {
        "name": "stride_db_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Z,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DZ,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_z_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dz_row,",
      "    stride_dw_row,",
      "    stride_db_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row + group * N",
      "    if HAS_Z:",
      "        Z += row_start * stride_z_row + group * N",
      "        DZ += row_start * stride_dz_row + group * N",
      "    DY += row_start * stride_dy_row + group * N",
      "    DX += row_start * stride_dx_row + group * N",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:",
      "        B += group * N",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            x_og = x",
      "            x = x_og * z * tl.sigmoid(z)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if HAS_Z and NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            z_sigmoid = tl.sigmoid(z)",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            if RECOMPUTE_OUTPUT:",
      "                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)",
      "            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dy *= z * z_sigmoid",
      "        else:",
      "            if RECOMPUTE_OUTPUT:",
      "                y = xhat * w + b if HAS_BIAS else xhat * w",
      "                tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        c1 = tl.sum(xhat * wdy, axis=0) / N",
      "        if not IS_RMS_NORM:",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            dx = (wdy - xhat * c1) * rstd",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z_sigmoid = tl.sigmoid(z)",
      "            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dx *= z * z_sigmoid",
      "",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_Z:",
      "            Z += stride_z_row",
      "            DZ += stride_dz_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/32.py"
  },
  {
    "name": "_selective_scan_update_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not None})",
      "@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})",
      "@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.next_power_of_2(args['dstate'])})"
    ],
    "args": [
      {
        "name": "state_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_state_batch",
        "annotation": null
      },
      {
        "name": "stride_state_head",
        "annotation": null
      },
      {
        "name": "stride_state_dim",
        "annotation": null
      },
      {
        "name": "stride_state_dstate",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_dim",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_A_dim",
        "annotation": null
      },
      {
        "name": "stride_A_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_group",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_group",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_D_dim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TIE_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _selective_scan_update_kernel(",
      "    state_ptr,",
      "    x_ptr,",
      "    dt_ptr,",
      "    dt_bias_ptr,",
      "    A_ptr,",
      "    B_ptr,",
      "    C_ptr,",
      "    D_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    batch,",
      "    nheads,",
      "    dim,",
      "    dstate,",
      "    nheads_ngroups_ratio,",
      "    stride_state_batch,",
      "    stride_state_head,",
      "    stride_state_dim,",
      "    stride_state_dstate,",
      "    stride_x_batch,",
      "    stride_x_head,",
      "    stride_x_dim,",
      "    stride_dt_batch,",
      "    stride_dt_head,",
      "    stride_dt_dim,",
      "    stride_dt_bias_head,",
      "    stride_dt_bias_dim,",
      "    stride_A_head,",
      "    stride_A_dim,",
      "    stride_A_dstate,",
      "    stride_B_batch,",
      "    stride_B_group,",
      "    stride_B_dstate,",
      "    stride_C_batch,",
      "    stride_C_group,",
      "    stride_C_dstate,",
      "    stride_D_head,",
      "    stride_D_dim,",
      "    stride_z_batch,",
      "    stride_z_head,",
      "    stride_z_dim,",
      "    stride_out_batch,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    TIE_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head",
      "    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptr += pid_h * stride_dt_bias_head",
      "    A_ptr += pid_h * stride_A_head",
      "    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group",
      "    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group",
      "    if HAS_Z:",
      "        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "    state_ptrs = state_ptr + (",
      "        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate",
      "    )",
      "    x_ptrs = x_ptr + offs_m * stride_x_dim",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_dim",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim",
      "    if HAS_D:",
      "        D_ptr += pid_h * stride_D_head",
      "    A_ptrs = A_ptr + (",
      "        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate",
      "    )",
      "    B_ptrs = B_ptr + offs_n * stride_B_dstate",
      "    C_ptrs = C_ptr + offs_n * stride_C_dstate",
      "    if HAS_D:",
      "        D_ptrs = D_ptr + offs_m * stride_D_dim",
      "    if HAS_Z:",
      "        z_ptrs = z_ptr + offs_m * stride_z_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "",
      "    state = tl.load(",
      "        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "    )",
      "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if not TIE_HDIM:",
      "        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(",
      "            A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "        ).to(tl.float32)",
      "        dA = tl.exp(A * dt[:, None])",
      "    else:",
      "        dt = tl.load(dt_ptr).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptr).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(A_ptr).to(tl.float32)",
      "        dA = tl.exp(A * dt)",
      "",
      "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    if HAS_D:",
      "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if HAS_Z:",
      "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "",
      "    if not TIE_HDIM:",
      "        dB = B[None, :] * dt[:, None]",
      "    else:",
      "        dB = B * dt",
      "    state = state * dA + dB * x[:, None]",
      "    tl.store(",
      "        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)",
      "    )",
      "    out = tl.sum(state * C[None, :], axis=1)",
      "    if HAS_D:",
      "        out += x * D",
      "    if HAS_Z:",
      "        out *= z * tl.sigmoid(z)",
      "    tl.store(out_ptrs, out, mask=offs_m < dim)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/33.py"
  },
  {
    "name": "_bmm_chunk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_outm",
        "annotation": null
      },
      {
        "name": "stride_outn",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_fwd_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    out_ptr,",
      "    seq_idx_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_bk,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_outm,",
      "    stride_outn,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    IS_CAUSAL: tl.constexpr,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    if IS_CAUSAL:",
      "        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "            return",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + pid_h * stride_b_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit)",
      "            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)",
      "            & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    out = acc.to(out_ptr.dtype.element_ty)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)",
      "    tl.store(",
      "        out_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/35.py"
  },
  {
    "name": "_bmm_chunk_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "res_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_n",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_head",
        "annotation": null
      },
      {
        "name": "stride_db_k",
        "annotation": null
      },
      {
        "name": "stride_res_batch",
        "annotation": null
      },
      {
        "name": "stride_res_seqlen",
        "annotation": null
      },
      {
        "name": "stride_res_head",
        "annotation": null
      },
      {
        "name": "stride_res_k",
        "annotation": null
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_bwd_kernel(",
      "    a_ptr,",
      "    dout_ptr,",
      "    db_ptr,",
      "    res_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_csize_m,",
      "    stride_dout_csize_n,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_head,",
      "    stride_db_k,",
      "    stride_res_batch,",
      "    stride_res_seqlen,",
      "    stride_res_head,",
      "    stride_res_k,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_CS: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_cs = tl.arange(0, BLOCK_SIZE_CS)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m",
      "    )",
      "    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)",
      "            & (offs_n[None, :] < K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(dout, a)",
      "        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m",
      "        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_RESIDUAL:",
      "        res_ptr += (",
      "            pid_b * stride_res_batch",
      "            + pid_c * chunk_size * stride_res_seqlen",
      "            + pid_h * stride_res_head",
      "        )",
      "        res_ptrs = res_ptr + (",
      "            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k",
      "        )",
      "        res = tl.load(",
      "            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "        ).to(tl.float32)",
      "        acc += res",
      "    db = acc.to(db_ptr.dtype.element_ty)",
      "",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_h * stride_db_head",
      "    )",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k",
      "    )",
      "    tl.store(",
      "        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/35.py"
  },
  {
    "name": "_chunk_scan_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_f_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_b_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    C_ptr,",
      "    prev_states_f_ptr,",
      "    prev_states_b_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_states_f_batch,",
      "    stride_states_f_chunk,",
      "    stride_states_f_head,",
      "    stride_states_f_hdim,",
      "    stride_states_f_dstate,",
      "    stride_states_b_batch,",
      "    stride_states_b_chunk,",
      "    stride_states_b_head,",
      "    stride_states_b_hdim,",
      "    stride_states_b_dstate,",
      "    stride_D_head,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    prev_states_f_ptr += (",
      "        pid_b * stride_states_f_batch",
      "        + pid_c * stride_states_f_chunk",
      "        + pid_h * stride_states_f_head",
      "    )",
      "    prev_states_b_ptr += (",
      "        pid_b * stride_states_b_batch",
      "        + pid_c * stride_states_b_chunk",
      "        + pid_h * stride_states_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dA_cs_f_m = tl.load(",
      "        dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "        mask=offs_m < chunk_size,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dA_cs_b_m = tl.load(",
      "        dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "        mask=offs_m < chunk_size,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    if IS_TRITON_22 or pid_c > -1:",
      "",
      "        offs_k_dstate = tl.arange(",
      "            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "        )",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "        )",
      "        prev_states_f_ptrs = prev_states_f_ptr + (",
      "            offs_n[None, :] * stride_states_f_hdim",
      "            + offs_k_dstate[:, None] * stride_states_f_dstate",
      "        )",
      "        prev_states_b_ptrs = prev_states_b_ptr + (",
      "            offs_n[None, :] * stride_states_b_hdim",
      "            + offs_k_dstate[:, None] * stride_states_b_dstate",
      "        )",
      "        scale_f_m = tl.exp(dA_cs_f_m)",
      "        scale_b_m = tl.exp(dA_cs_b_m)",
      "        if BLOCK_SIZE_DSTATE <= 128:",
      "            C = tl.load(",
      "                C_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k_dstate[None, :] < dstate),",
      "                other=0.0,",
      "            )",
      "            prev_states_f = tl.load(",
      "                prev_states_f_ptrs,",
      "                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)",
      "            acc = tl.dot(C, prev_states_f) * scale_f_m[:, None]",
      "",
      "            prev_states_b = tl.load(",
      "                prev_states_b_ptrs,",
      "                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)",
      "            acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]",
      "        else:",
      "            for k in range(0, dstate, BLOCK_SIZE_K):",
      "                C = tl.load(",
      "                    C_ptrs,",
      "                    mask=(offs_m[:, None] < chunk_size_limit)",
      "                    & (offs_k_dstate[None, :] < dstate - k),",
      "                    other=0.0,",
      "                )",
      "",
      "                prev_states_f = tl.load(",
      "                    prev_states_f_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate - k)",
      "                    & (offs_n[None, :] < hdim),",
      "                    other=0.0,",
      "                )",
      "                prev_states_f = prev_states_f.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states_f) * scale_f_m[:, None]",
      "",
      "                prev_states_b = tl.load(",
      "                    prev_states_f_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                    other=0.0,",
      "                )",
      "                prev_states_b = prev_states_b.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states_b) * scale_b_m[:, None]",
      "                C_ptrs += BLOCK_SIZE_K",
      "                prev_states_f_ptrs += BLOCK_SIZE_K",
      "                prev_states_b_ptrs += BLOCK_SIZE_K",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "    K_F_MAX = min((pid_m) * BLOCK_SIZE_M, chunk_size_limit)",
      "    K_F_MAX_BEG = K_F_MAX - BLOCK_SIZE_K",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if k <= K_F_MAX and k >= K_F_MAX_BEG:",
      "",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_f = offs_m[:, None] >= (k + offs_k[None, :])",
      "            scale_f = tl.where(",
      "                mask_f, tl.exp((dA_cs_f_m[:, None] - dA_cs_f_k[None, :])), 0.0",
      "            )",
      "",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_b = offs_m[:, None] <= (k + offs_k[None, :])",
      "            scale_b = tl.where(",
      "                mask_b, tl.exp((dA_cs_b_m[:, None] - dA_cs_b_k[None, :])), 0.0",
      "            )",
      "",
      "            cb = cb * (scale_f + scale_b)",
      "",
      "        elif k < K_F_MAX:",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] >= k + offs_k[None, :]",
      "            cb *= tl.where(mask, tl.exp((dA_cs_f_m[:, None] - dA_cs_f_k[None, :])), 0.0)",
      "        elif k > K_F_MAX:",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] <= k + offs_k[None, :]",
      "            cb *= tl.where(mask, tl.exp((dA_cs_b_m[:, None] - dA_cs_b_k[None, :])), 0.0)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)",
      "        cb *= dt_k",
      "        cb = cb.to(x_ptr.dtype.element_ty)",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        acc += tl.dot(cb, x)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "",
      "    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    if HAS_D:",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        x_residual = tl.load(",
      "            x_ptr",
      "            + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim),",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc += x_residual * D",
      "",
      "    if HAS_Z:",
      "        out_x_ptr += (",
      "            pid_b * stride_out_batch",
      "            + pid_c * chunk_size * stride_out_seqlen",
      "            + pid_h * stride_out_head",
      "        )",
      "        out_x_ptrs = out_x_ptr + (",
      "            stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :]",
      "        )",
      "        tl.store(",
      "            out_x_ptrs,",
      "            acc,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "        )",
      "",
      "        z_ptr += (",
      "            pid_b * stride_z_batch",
      "            + pid_c * chunk_size * stride_z_seqlen",
      "            + pid_h * stride_z_head",
      "        )",
      "        z_ptrs = z_ptr + (",
      "            stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :]",
      "        )",
      "        z = tl.load(",
      "            z_ptrs,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= z * tl.sigmoid(z)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim",
      "    )",
      "    tl.store(",
      "        out_ptrs,",
      "        acc,",
      "        mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_fwd_kernel_wip",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_seqlen",
        "annotation": null
      },
      {
        "name": "stride_B_head",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel_wip(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    B_ptr,",
      "    prev_states_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_B_batch,",
      "    stride_B_seqlen,",
      "    stride_B_head,",
      "    stride_B_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_D_head,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_n = tl.program_id(axis=0)",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    B_ptr += (",
      "        pid_b * stride_B_batch",
      "        + pid_c * chunk_size * stride_B_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_B_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "",
      "    offs_m = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "    )",
      "    B_ptrs = B_ptr + (",
      "        offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_states_hdim",
      "        + offs_k_dstate[:, None] * stride_states_dstate",
      "    )",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)",
      "        dA_cs_m = tl.load(",
      "            dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize,",
      "            mask=offs_m < chunk_size - start_m,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_prev = tl.load(",
      "                seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "            )",
      "            seq_idx_m = tl.load(",
      "                seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen,",
      "                mask=offs_m < chunk_size_limit - start_m,",
      "                other=-1,",
      "            )",
      "        if not HAS_SEQ_IDX:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        else:",
      "            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        C = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_k_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]",
      "",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(",
      "            tl.float32",
      "        )",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        if HAS_D:",
      "            if D_HAS_HDIM:",
      "                D = tl.load(",
      "                    D_ptr + pid_h * stride_D_head + offs_n,",
      "                    mask=offs_n < hdim,",
      "                    other=0.0,",
      "                ).to(tl.float32)",
      "            else:",
      "                D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "            acc += x.to(tl.float32) * D",
      "",
      "        tl.store(",
      "            out_ptrs,",
      "            acc,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "        )",
      "",
      "        if start_m + BLOCK_SIZE_M < chunk_size_limit:",
      "",
      "            B = tl.load(",
      "                B_ptrs,",
      "                mask=(offs_m[None, :] < chunk_size_limit - start_m)",
      "                & (offs_k_dstate[:, None] < dstate),",
      "                other=0.0,",
      "            )",
      "            dA_cs_last = tl.load(",
      "                dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize",
      "            ).to(tl.float32)",
      "",
      "            scale = tl.exp((dA_cs_last - dA_cs_m)) * dt_m",
      "",
      "            B = B.to(x_ptr.dtype.element_ty)",
      "            tmp = tl.dot(B, x)",
      "            prev_states += tmp.to(prev_states.dtype)",
      "",
      "        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen",
      "        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen",
      "        cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize",
      "        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_dz_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "outz_ptr",
        "annotation": null
      },
      {
        "name": "dz_ptr",
        "annotation": null
      },
      {
        "name": "dout_x_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_outz_batch",
        "annotation": null
      },
      {
        "name": "stride_outz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_outz_head",
        "annotation": null
      },
      {
        "name": "stride_outz_hdim",
        "annotation": null
      },
      {
        "name": "stride_dz_batch",
        "annotation": null
      },
      {
        "name": "stride_dz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dz_head",
        "annotation": null
      },
      {
        "name": "stride_dz_hdim",
        "annotation": null
      },
      {
        "name": "stride_doutx_batch",
        "annotation": null
      },
      {
        "name": "stride_doutx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_doutx_head",
        "annotation": null
      },
      {
        "name": "stride_doutx_hdim",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DDACS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dz_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    z_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    outz_ptr,",
      "    dz_ptr,",
      "    dout_x_ptr,",
      "    dD_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_outz_batch,",
      "    stride_outz_seqlen,",
      "    stride_outz_head,",
      "    stride_outz_hdim,",
      "    stride_dz_batch,",
      "    stride_dz_seqlen,",
      "    stride_dz_head,",
      "    stride_dz_hdim,",
      "    stride_doutx_batch,",
      "    stride_doutx_seqlen,",
      "    stride_doutx_head,",
      "    stride_doutx_hdim,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_DDACS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dout_x_ptr += (",
      "        pid_b * stride_doutx_batch",
      "        + pid_c * chunk_size * stride_doutx_seqlen",
      "        + pid_h * stride_doutx_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    z_ptr += (",
      "        pid_b * stride_z_batch",
      "        + pid_c * chunk_size * stride_z_seqlen",
      "        + pid_h * stride_z_head",
      "    )",
      "    dz_ptr += (",
      "        pid_b * stride_dz_batch",
      "        + pid_c * chunk_size * stride_dz_seqlen",
      "        + pid_h * stride_dz_head",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptr += (",
      "            pid_b * stride_outz_batch",
      "            + pid_c * chunk_size * stride_outz_seqlen",
      "            + pid_h * stride_outz_head",
      "        )",
      "    if HAS_DDACS:",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + pid_h * stride_ddA_cs_head",
      "        )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dout_x_ptrs = dout_x_ptr + (",
      "        offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    z_ptrs = z_ptr + (",
      "        offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim",
      "    )",
      "    dz_ptrs = dz_ptr + (",
      "        offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptrs = outz_ptr + (",
      "            offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim",
      "        )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z = tl.load(",
      "        z_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z_sigmoid = tl.sigmoid(z)",
      "    if RECOMPUTE_OUTPUT:",
      "        outz = out * z * z_sigmoid",
      "        tl.store(",
      "            outz_ptrs,",
      "            outz,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        )",
      "    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "    tl.store(",
      "        dz_ptrs,",
      "        dz,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    dout *= z * z_sigmoid",
      "    tl.store(",
      "        dout_x_ptrs,",
      "        dout,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    if HAS_DDACS:",
      "        ddA_cs = tl.sum(dout * out, axis=1)",
      "        tl.store(",
      "            ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize,",
      "            ddA_cs,",
      "            mask=offs_m < chunk_size,",
      "        )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_dstates_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "dprev_states_f_ptr",
        "annotation": null
      },
      {
        "name": "dprev_states_b_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_c_batch",
        "annotation": null
      },
      {
        "name": "stride_c_seqlen",
        "annotation": null
      },
      {
        "name": "stride_c_head",
        "annotation": null
      },
      {
        "name": "stride_c_dstate",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dstates_kernel(",
      "    dout_ptr,",
      "    c_ptr,",
      "    dprev_states_f_ptr,",
      "    dprev_states_b_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_c_batch,",
      "    stride_c_seqlen,",
      "    stride_c_head,",
      "    stride_c_dstate,",
      "    stride_dprev_states_f_batch,",
      "    stride_dprev_states_f_chunk,",
      "    stride_dprev_states_f_head,",
      "    stride_dprev_states_f_hdim,",
      "    stride_dprev_states_f_dstate,",
      "    stride_dprev_states_b_batch,",
      "    stride_dprev_states_b_chunk,",
      "    stride_dprev_states_b_head,",
      "    stride_dprev_states_b_hdim,",
      "    stride_dprev_states_b_dstate,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    c_ptr += (",
      "        pid_b * stride_c_batch",
      "        + pid_c * chunk_size * stride_c_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_c_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen",
      "    )",
      "    c_ptrs = c_ptr + (",
      "        offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen",
      "    )",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    acc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_f_k = tl.load(",
      "            dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "        ).to(tl.float32)",
      "        dA_cs_b_k = tl.load(",
      "            dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "        ).to(tl.float32)",
      "        scale_f_k = tl.exp(dA_cs_f_k)",
      "        scale_b_k = tl.exp(dA_cs_b_k)",
      "        dout_f = (dout * scale_f_k).to(dout_ptr.dtype.element_ty)",
      "        dout_b = (dout * scale_b_k).to(dout_ptr.dtype.element_ty)",
      "        c = tl.load(",
      "            c_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc_f += tl.dot(dout_f, c)",
      "        acc_b += tl.dot(dout_b, c)",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "    out_f = acc_f.to(dprev_states_f_ptr.dtype.element_ty)",
      "    out_b = acc_b.to(dprev_states_b_ptr.dtype.element_ty)",
      "",
      "    dprev_states_f_ptr += (",
      "        pid_b * stride_dprev_states_f_batch",
      "        + pid_c * stride_dprev_states_f_chunk",
      "        + pid_h * stride_dprev_states_f_head",
      "    )",
      "    dprev_states_b_ptr += (",
      "        pid_b * stride_dprev_states_b_batch",
      "        + pid_c * stride_dprev_states_b_chunk",
      "        + pid_h * stride_dprev_states_b_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dprev_states_f_ptrs = dprev_states_f_ptr + (",
      "        offs_m[:, None] * stride_dprev_states_f_hdim",
      "        + offs_n[None, :] * stride_dprev_states_f_dstate",
      "    )",
      "    dprev_states_b_ptrs = dprev_states_b_ptr + (",
      "        offs_m[:, None] * stride_dprev_states_b_hdim",
      "        + offs_n[None, :] * stride_dprev_states_b_dstate",
      "    )",
      "    tl.store(",
      "        dprev_states_f_ptrs,",
      "        out_f,",
      "        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "    )",
      "    tl.store(",
      "        dprev_states_b_ptrs,",
      "        out_b,",
      "        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_dc_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_f_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_b_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "dc_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_dc_batch",
        "annotation": null
      },
      {
        "name": "stride_dc_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dc_split",
        "annotation": null
      },
      {
        "name": "stride_dc_group",
        "annotation": null
      },
      {
        "name": "stride_dc_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dc_kernel(",
      "    dout_ptr,",
      "    prev_states_f_ptr,",
      "    prev_states_b_ptr,",
      "    C_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    dc_ptr,",
      "    ddA_cumsum_f_ptr,",
      "    ddA_cumsum_b_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_f_batch,",
      "    stride_prev_states_f_chunk,",
      "    stride_prev_states_f_head,",
      "    stride_prev_states_f_hdim,",
      "    stride_prev_states_f_dstate,",
      "    stride_prev_states_b_batch,",
      "    stride_prev_states_b_chunk,",
      "    stride_prev_states_b_head,",
      "    stride_prev_states_b_hdim,",
      "    stride_prev_states_b_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_dc_batch,",
      "    stride_dc_seqlen,",
      "    stride_dc_split,",
      "    stride_dc_group,",
      "    stride_dc_dstate,",
      "    stride_ddA_cs_f_batch,",
      "    stride_ddA_cs_f_chunk,",
      "    stride_ddA_cs_f_head,",
      "    stride_ddA_cs_f_csize,",
      "    stride_ddA_cs_b_batch,",
      "    stride_ddA_cs_b_chunk,",
      "    stride_ddA_cs_b_head,",
      "    stride_ddA_cs_b_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dc_ptr += (",
      "        pid_b * stride_dc_batch",
      "        + pid_c * chunk_size * stride_dc_seqlen",
      "        + pid_g * stride_dc_group",
      "        + pid_s * stride_dc_split",
      "    )",
      "    prev_states_f_ptr += (",
      "        pid_b * stride_prev_states_f_batch",
      "        + pid_c * stride_prev_states_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_prev_states_f_head",
      "    )",
      "    prev_states_b_ptr += (",
      "        pid_b * stride_prev_states_b_batch",
      "        + pid_c * stride_prev_states_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_prev_states_b_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_b_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        C_ptr += (",
      "            pid_b * stride_C_batch",
      "            + pid_c * chunk_size * stride_C_seqlen",
      "            + pid_g * stride_C_head",
      "        )",
      "        ddA_cumsum_f_ptr += (",
      "            pid_b * stride_ddA_cs_f_batch",
      "            + pid_c * stride_ddA_cs_f_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_f_head",
      "        )",
      "        ddA_cumsum_b_ptr += (",
      "            pid_b * stride_ddA_cs_b_batch",
      "            + pid_c * stride_ddA_cs_b_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_b_head",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_f_ptrs = prev_states_f_ptr + (",
      "        offs_n[None, :] * stride_prev_states_f_dstate",
      "        + offs_k[:, None] * stride_prev_states_f_hdim",
      "    )",
      "    prev_states_b_ptrs = prev_states_b_ptr + (",
      "        offs_n[None, :] * stride_prev_states_b_dstate",
      "        + offs_k[:, None] * stride_prev_states_b_hdim",
      "    )",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize",
      "    if HAS_DDA_CS:",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "        )",
      "        ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize",
      "        ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        c = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        prev_states_f = tl.load(",
      "            prev_states_f_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        prev_states_f = prev_states_f.to(dout_ptrs.dtype.element_ty)",
      "        dc_f = tl.dot(dout, prev_states_f)",
      "        dA_cs_f_m = tl.load(",
      "            dA_cumsum_f_ptrs, mask=offs_m < chunk_size_limit, other=0.0",
      "        ).to(tl.float32)",
      "        scale_f = tl.exp(dA_cs_f_m)",
      "        dc_f *= scale_f[:, None]",
      "        if HAS_DDA_CS:",
      "            ddA_cs_f = tl.sum(dc_f * c, axis=1)",
      "            tl.atomic_add(ddA_cumsum_f_ptrs, ddA_cs_f, mask=offs_m < chunk_size)",
      "",
      "        prev_states_b = tl.load(",
      "            prev_states_b_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        prev_states_b = prev_states_b.to(dout_ptrs.dtype.element_ty)",
      "        dc_b = tl.dot(dout, prev_states_b)",
      "        dA_cs_b_m = tl.load(",
      "            dA_cumsum_b_ptrs, mask=offs_m < chunk_size_limit, other=0.0",
      "        ).to(tl.float32)",
      "        scale_b = tl.exp(dA_cs_b_m)",
      "        dc_b *= scale_b[:, None]",
      "        if HAS_DDA_CS:",
      "            ddA_cs_b = tl.sum(dc_b * c, axis=1)",
      "            tl.atomic_add(ddA_cumsum_b_ptrs, ddA_cs_b, mask=offs_m < chunk_size)",
      "",
      "        acc += dc_f + dc_b",
      "        dout_ptrs += stride_dout_head",
      "        prev_states_f_ptrs += stride_prev_states_f_head",
      "        prev_states_b_ptrs += stride_prev_states_b_head",
      "        dA_cumsum_f_ptrs += stride_dA_cs_f_head",
      "        dA_cumsum_b_ptrs += stride_dA_cs_b_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_f_ptrs += stride_ddA_cs_f_head",
      "            ddA_cumsum_b_ptrs += stride_ddA_cs_b_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dc_ptrs = dc_ptr + (",
      "        offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate",
      "    )",
      "    tl.store(",
      "        dc_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    D_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_D_head,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    K_MAX = chunk_size_limit",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])",
      "",
      "        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)",
      "        cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_dcb_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "dcb_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_dcb_batch",
        "annotation": null
      },
      {
        "name": "stride_dcb_chunk",
        "annotation": null
      },
      {
        "name": "stride_dcb_split",
        "annotation": null
      },
      {
        "name": "stride_dcb_group",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dcb_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    dcb_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_dcb_batch,",
      "    stride_dcb_chunk,",
      "    stride_dcb_split,",
      "    stride_dcb_group,",
      "    stride_dcb_csize_m,",
      "    stride_dcb_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        )",
      "        dcb = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "        dcb *= dt_n",
      "        if (pid_n * BLOCK_SIZE_N < (pid_m + 1) * BLOCK_SIZE_M) and (",
      "            (pid_n + 1) * BLOCK_SIZE_N > (pid_m) * BLOCK_SIZE_M",
      "        ):",
      "",
      "            dA_cs_f_m = tl.load(",
      "                dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_f_n = tl.load(",
      "                dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] >= offs_n[None, :]",
      "            A_f = tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)",
      "            dA_cs_b_m = tl.load(",
      "                dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_b_n = tl.load(",
      "                dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] <= offs_n[None, :]",
      "            A_b = tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)",
      "            dcb *= A_f + A_b",
      "        elif pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "",
      "            dA_cs_b_m = tl.load(",
      "                dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_b_n = tl.load(",
      "                dA_cumsum_b_ptr + offs_n * stride_dA_cs_b_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] <= offs_n[None, :]",
      "            dcb *= tl.where(mask, tl.exp(dA_cs_b_m[:, None] - dA_cs_b_n[None, :]), 0.0)",
      "        else:",
      "",
      "            dA_cs_f_m = tl.load(",
      "                dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "                mask=offs_m < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            dA_cs_f_n = tl.load(",
      "                dA_cumsum_f_ptr + offs_n * stride_dA_cs_f_csize,",
      "                mask=offs_n < chunk_size_limit,",
      "                other=0.0,",
      "            ).to(tl.float32)",
      "            mask = offs_m[:, None] >= offs_n[None, :]",
      "            dcb *= tl.where(mask, tl.exp(dA_cs_f_m[:, None] - dA_cs_f_n[None, :]), 0.0)",
      "        acc += dcb",
      "        dout_ptrs += stride_dout_head",
      "        x_ptrs += stride_x_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_f_ptr += stride_dA_cs_f_head",
      "        dA_cumsum_b_ptr += stride_dA_cs_b_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dcb_ptr += (",
      "        pid_b * stride_dcb_batch",
      "        + pid_c * stride_dcb_chunk",
      "        + pid_g * stride_dcb_group",
      "        + pid_s * stride_dcb_split",
      "    )",
      "    dcb_ptrs = dcb_ptr + (",
      "        offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n",
      "    )",
      "    tl.store(",
      "        dcb_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_unstable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SUBTRACT_DDTDT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_unstable_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dt_ptr,",
      "    ddt_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    ddA_cumsum_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    SUBTRACT_DDTDT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    ddA_cs = tl.sum(dout * out, axis=1)",
      "    if SUBTRACT_DDTDT:",
      "        dt = tl.load(",
      "            dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddt = tl.load(",
      "            ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddA_cs -= dt * ddt",
      "    tl.store(",
      "        ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_kernel_old",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 16}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 16}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddAcs_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_head",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_kernel_old(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddAcs_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddAcs_batch,",
      "    stride_ddAcs_chunk,",
      "    stride_ddAcs_head,",
      "    stride_ddAcs_csize_m,",
      "    stride_ddAcs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)",
      "",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),",
      "        other=0.0,",
      "    )",
      "    acc = tl.dot(dout, x)",
      "    cb = tl.load(",
      "        cb_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    acc *= cb",
      "    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= dt_n",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_n = tl.load(",
      "        dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "    mask = offs_m[:, None] >= offs_n[None, :] + 1",
      "    acc = tl.where(mask, acc, 0.0)",
      "    acc = tl.cumsum(acc, axis=1)",
      "    acc = tl.where(mask, acc, 0.0)",
      "    ddA_cs = tl.sum(acc, axis=0)",
      "    ddAcs_ptr += (",
      "        pid_b * stride_ddAcs_batch",
      "        + pid_c * stride_ddAcs_chunk",
      "        + pid_h * stride_ddAcs_head",
      "        + pid_m * stride_ddAcs_csize_m",
      "    )",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n",
      "    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)",
      "    tl.store(ddAcs_ptr, 0.0)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_bwd_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    start = chunk_size - BLOCK_SIZE_N",
      "",
      "    offs_m = (chunk_size - (pid_m + 1) * BLOCK_SIZE_M) + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        (start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m",
      "        + (start + offs_n[None, :]) * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit)",
      "        & (offs_m[:, None] >= 0)",
      "        & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m >= 0, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M",
      "",
      "    for start_n in range(lo, hi, BLOCK_SIZE_N):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit + start_n - start)",
      "            & (offs_n >= start_n - start),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n >= start_n - start, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] >= 0) & (offs_n[None, :] >= start_n - start),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + (start - start_n + offs_n) * stride_dA_cs_csize,",
      "            mask=(offs_n >= start_n - start),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] <= start - start_n + offs_n[None, :] - 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=(offs_n >= 1 + start_n - start),",
      "        )",
      "        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n >= 1 + start_n - start,",
      "        )",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_bwd_slow_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    start = (chunk_size - 1 // BLOCK_SIZE_N) * BLOCK_SIZE_N",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        (start + offs_n[None, :]) * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + (start + offs_n) * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m",
      "        + (start + offs_n[None, :]) * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + (start + offs_n) * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = start, ((pid_m * BLOCK_SIZE_M) // BLOCK_SIZE_N) * BLOCK_SIZE_N - 1",
      "",
      "    for start_n in range(lo, hi, -BLOCK_SIZE_N):",
      "        tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - start_n),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_n[None, :] < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize,",
      "            mask=(offs_n < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] <= start_n + offs_n[None, :] - 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=(offs_n < chunk_size - start_n) & (offs_n >= 1 - start_n),",
      "        )",
      "        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, -1, -BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n >= 1 - start_n,",
      "        )",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_bwd_old_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_bwd_old_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    end_offset = chunk_size - BLOCK_SIZE_N",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        (end_offset + offs_n[None, :]) * stride_x_seqlen",
      "        + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + (end_offset + offs_n) * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m",
      "        + (end_offset + offs_n[None, :]) * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + (end_offset + offs_n) * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize_n, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    for start_n in range(0, chunk_size - pid_m * BLOCK_SIZE_M, BLOCK_SIZE_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "        offset = (chunk_size - BLOCK_SIZE_N) - start_n",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - offset)",
      "            & (offs_n[None, :] >= -offset),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n >= (-offset), other=0.0).to(tl.float32)",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] >= (-offset)),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + offset + offs_n * stride_dA_cs_csize,",
      "            mask=offs_n >= (-offset),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] <= offset + offs_n[None, :] - 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1, reverse=True)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "        tl.store(ddAcs_ptrs - stride_ddA_cs_csize_n, ddA_cs, mask=offs_n >= 1 - offset)",
      "        x_ptrs -= BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs -= BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs -= BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(pid_m * BLOCK_SIZE_M, 0, -BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs - stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n >= 1 - start_n,",
      "        )",
      "        ddAcs_ptrs -= BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_fwd_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M",
      "",
      "    for start_n in range(lo, hi, BLOCK_SIZE_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - start_n),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_n[None, :] < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + start_n + offs_n * stride_dA_cs_csize,",
      "            mask=offs_n < chunk_size - start_n,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_prev_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_prev_kernel(",
      "    dout_ptr,",
      "    prev_states_ptr,",
      "    C_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_batch,",
      "    stride_prev_states_chunk,",
      "    stride_prev_states_head,",
      "    stride_prev_states_hdim,",
      "    stride_prev_states_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_prev_states_batch",
      "        + pid_c * stride_prev_states_chunk",
      "        + pid_h * stride_prev_states_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_prev_states_dstate",
      "        + offs_k[:, None] * stride_prev_states_hdim",
      "    )",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    )",
      "    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)",
      "    acc = tl.dot(dout, prev_states)",
      "    c = tl.load(",
      "        C_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddA_cs = tl.sum(acc * c, axis=1)",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_m)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "    ddA_cs *= scale",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/36.py"
  },
  {
    "name": "_chunk_cumsum_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}), triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64})], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "dt_out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_out_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_fwd_kernel(",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    dt_out_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_dt_out_batch,",
      "    stride_dt_out_chunk,",
      "    stride_dt_out_head,",
      "    stride_dt_out_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk",
      "    dA_cumsum_f_ptr += pid_b * stride_dA_cs_f_batch + pid_c * stride_dA_cs_f_chunk",
      "    dA_cumsum_b_ptr += pid_b * stride_dA_cs_b_batch + pid_c * stride_dA_cs_b_chunk",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    dt_out_ptrs = dt_out_ptr + (",
      "        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize",
      "    )",
      "    dA_cs_f_ptrs = dA_cumsum_f_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_f_head + offs_c[None, :] * stride_dA_cs_f_csize",
      "    )",
      "    dA_cs_b_ptrs = dA_cumsum_b_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_b_head + offs_c[None, :] * stride_dA_cs_b_csize",
      "    )",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt = softplus(dt)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    tl.store(",
      "        dt_out_ptrs,",
      "        dt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    dA = dt * A[:, None]",
      "    dA_cs_f = tl.cumsum(dA, axis=1)",
      "    tl.store(",
      "        dA_cs_f_ptrs,",
      "        dA_cs_f,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "",
      "    dA_cs_b = tl.flip(tl.cumsum(tl.flip(dA, dim=1), axis=1))",
      "",
      "    tl.store(",
      "        dA_cs_b_ptrs,",
      "        dA_cs_b,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_cumsum_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 2}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 4}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 8}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 16}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 32}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 64}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr']))], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "ddA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_bias_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_ddA_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_dA_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_bias_head",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_bwd_kernel(",
      "    ddA_ptr,",
      "    ddt_out_ptr,",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    ddt_ptr,",
      "    dA_ptr,",
      "    ddt_bias_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_ddA_batch,",
      "    stride_ddA_chunk,",
      "    stride_ddA_head,",
      "    stride_ddA_csize,",
      "    stride_ddt_out_batch,",
      "    stride_ddt_out_chunk,",
      "    stride_ddt_out_head,",
      "    stride_ddt_out_csize,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_ddt_batch,",
      "    stride_ddt_seqlen,",
      "    stride_ddt_head,",
      "    stride_dA_head,",
      "    stride_ddt_bias_head,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk",
      "    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    ddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    ddt_out_ptrs = ddt_out_ptr + (",
      "        offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize",
      "    )",
      "    ddA_ptrs = ddA_ptr + (",
      "        offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize",
      "    )",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    ddt_ptrs = ddt_ptr + (",
      "        offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    ddA = tl.load(",
      "        ddA_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt_out = tl.load(",
      "        ddt_out_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    ddt = ddA * A[:, None] + ddt_out",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt_presoftplus = dt",
      "        dt = softplus(dt)",
      "    clamp_mask = (dt < dt_min) | (dt > dt_max)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    ddt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0",
      "    )",
      "    ddt = tl.where(clamp_mask, 0.0, ddt)",
      "    if DT_SOFTPLUS:",
      "        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)",
      "    tl.store(",
      "        ddt_ptrs,",
      "        ddt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "    )",
      "    dA = tl.sum(ddA * dt, axis=1)",
      "    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)",
      "    if HAS_DT_BIAS:",
      "        ddt_bias = tl.sum(ddt, axis=1)",
      "        tl.atomic_add(",
      "            ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads",
      "        )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_state_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "states_f_ptr",
        "annotation": null
      },
      {
        "name": "states_b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_f_batch",
        "annotation": null
      },
      {
        "name": "stride_states_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_f_head",
        "annotation": null
      },
      {
        "name": "stride_states_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_b_batch",
        "annotation": null
      },
      {
        "name": "stride_states_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_b_head",
        "annotation": null
      },
      {
        "name": "stride_states_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_fwd_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    states_f_ptr,",
      "    states_b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_states_f_batch,",
      "    stride_states_f_chunk,",
      "    stride_states_f_head,",
      "    stride_states_f_hdim,",
      "    stride_states_f_dstate,",
      "    stride_states_b_batch,",
      "    stride_states_b_chunk,",
      "    stride_states_b_head,",
      "    stride_states_b_hdim,",
      "    stride_states_b_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_f_last = tl.load(",
      "        dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize",
      "    ).to(tl.float32)",
      "    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc_f = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    acc_b = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_f_k = tl.load(",
      "            dA_cumsum_f_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dA_cs_b_k = tl.load(",
      "            dA_cumsum_b_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale_f = tl.exp((dA_cs_f_last - dA_cs_f_k)) * dt_k",
      "        scale_b = tl.exp((dA_cs_b_last - dA_cs_b_k)) * dt_k",
      "        b_f = b * scale_f[:, None]",
      "        b_b = b * scale_b[:, None]",
      "        b_f = b_f.to(x_ptr.dtype.element_ty)",
      "        b_b = b_b.to(x_ptr.dtype.element_ty)",
      "        acc_f += tl.dot(x, b_f)",
      "        acc_b += tl.dot(x, b_b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "    states_f = acc_f.to(states_f_ptr.dtype.element_ty)",
      "    states_b = acc_b.to(states_b_ptr.dtype.element_ty)",
      "",
      "    states_f_ptr += (",
      "        pid_b * stride_states_f_batch",
      "        + pid_c * stride_states_f_chunk",
      "        + pid_h * stride_states_f_head",
      "    )",
      "    states_b_ptr += (",
      "        pid_b * stride_states_b_batch",
      "        + pid_c * stride_states_b_chunk",
      "        + pid_h * stride_states_b_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_f_ptrs = states_f_ptr + (",
      "        offs_m[:, None] * stride_states_f_hdim",
      "        + offs_n[None, :] * stride_states_f_dstate",
      "    )",
      "    states_b_ptrs = states_b_ptr + (",
      "        offs_m[:, None] * stride_states_b_hdim",
      "        + offs_n[None, :] * stride_states_b_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_f_ptrs, states_f, mask=c_mask)",
      "    tl.store(states_b_ptrs, states_b, mask=c_mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)",
      "    ddA_cs = -(ddt * dt_m)",
      "    ddA_cs_last = -tl.sum(ddA_cs)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)",
      "    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)",
      "",
      "    dx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_state_bwd_db_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_f_ptr', 'ddA_cumsum_b_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dstates_f_ptr",
        "annotation": null
      },
      {
        "name": "dstates_b_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_split",
        "annotation": null
      },
      {
        "name": "stride_db_group",
        "annotation": null
      },
      {
        "name": "stride_db_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_db_kernel(",
      "    x_ptr,",
      "    dstates_f_ptr,",
      "    dstates_b_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    db_ptr,",
      "    ddA_cumsum_f_ptr,",
      "    ddA_cumsum_b_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dstates_f_batch,",
      "    stride_dstates_f_chunk,",
      "    stride_dstates_f_head,",
      "    stride_dstates_f_hdim,",
      "    stride_dstates_f_dstate,",
      "    stride_dstates_b_batch,",
      "    stride_dstates_b_chunk,",
      "    stride_dstates_b_head,",
      "    stride_dstates_b_hdim,",
      "    stride_dstates_b_dstate,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_split,",
      "    stride_db_group,",
      "    stride_db_dstate,",
      "    stride_ddA_cs_f_batch,",
      "    stride_ddA_cs_f_chunk,",
      "    stride_ddA_cs_f_head,",
      "    stride_ddA_cs_f_csize,",
      "    stride_ddA_cs_b_batch,",
      "    stride_ddA_cs_b_chunk,",
      "    stride_ddA_cs_b_head,",
      "    stride_ddA_cs_b_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_g * stride_db_group",
      "        + pid_s * stride_db_split",
      "    )",
      "    dstates_f_ptr += (",
      "        pid_b * stride_dstates_f_batch",
      "        + pid_c * stride_dstates_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dstates_f_head",
      "    )",
      "    dstates_b_ptr += (",
      "        pid_b * stride_dstates_b_batch",
      "        + pid_c * stride_dstates_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dstates_b_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_dA_cs_b_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        b_ptr += (",
      "            pid_b * stride_b_batch",
      "            + pid_c * chunk_size * stride_b_seqlen",
      "            + pid_g * stride_b_head",
      "        )",
      "        ddA_cumsum_f_ptr += (",
      "            pid_b * stride_ddA_cs_f_batch",
      "            + pid_c * stride_ddA_cs_f_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_f_head",
      "        )",
      "        ddA_cumsum_b_ptr += (",
      "            pid_b * stride_ddA_cs_b_batch",
      "            + pid_c * stride_ddA_cs_b_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_b_head",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim",
      "    )",
      "    dstates_f_ptrs = dstates_f_ptr + (",
      "        offs_n[None, :] * stride_dstates_f_dstate",
      "        + offs_k[:, None] * stride_dstates_f_hdim",
      "    )",
      "    dstates_b_ptrs = dstates_b_ptr + (",
      "        offs_n[None, :] * stride_dstates_b_dstate",
      "        + offs_k[:, None] * stride_dstates_b_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize",
      "    if HAS_DDA_CS:",
      "        b_ptrs = b_ptr + (",
      "            offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate",
      "        )",
      "        ddA_cumsum_f_ptrs = ddA_cumsum_f_ptr + offs_m * stride_ddA_cs_f_csize",
      "        ddA_cumsum_b_ptrs = ddA_cumsum_b_ptr + offs_m * stride_ddA_cs_b_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        dstates_f = tl.load(",
      "            dstates_f_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates_f = dstates_f.to(x_ptrs.dtype.element_ty)",
      "        db_f = tl.dot(x, dstates_f)",
      "        dA_cs_f_last = tl.load(",
      "            dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize",
      "        ).to(tl.float32)",
      "        dA_cs_f_m = tl.load(dA_cumsum_f_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "        scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)",
      "        db_f *= (scale_f * dt_m)[:, None]",
      "        if HAS_DDA_CS:",
      "",
      "            ddA_cs_f = tl.sum(db_f * b, axis=1)",
      "",
      "            tl.atomic_add(",
      "                ddA_cumsum_f_ptrs + stride_ddA_cs_f_csize,",
      "                ddA_cs_f,",
      "                mask=offs_m < chunk_size - 1,",
      "            )",
      "",
      "        dstates_b = tl.load(",
      "            dstates_b_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates_b = dstates_b.to(x_ptrs.dtype.element_ty)",
      "        db_b = tl.dot(x, dstates_b)",
      "        dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)",
      "        dA_cs_b_m = tl.load(dA_cumsum_b_ptrs, mask=(offs_m < chunk_size), other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)",
      "        db_b *= (scale_b * dt_m)[:, None]",
      "        if HAS_DDA_CS:",
      "",
      "            ddA_cs_b = tl.sum(db_b * b, axis=1)",
      "",
      "            tl.atomic_add(",
      "                ddA_cumsum_b_ptrs - stride_ddA_cs_b_csize,",
      "                ddA_cs_b,",
      "                mask=(offs_m >= 1) & (offs_m < chunk_size),",
      "            )",
      "",
      "        acc += db_f + db_b",
      "        x_ptrs += stride_x_head",
      "        dstates_f_ptrs += stride_dstates_f_head",
      "        dstates_b_ptrs += stride_dstates_b_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_f_ptr += stride_dA_cs_f_head",
      "        dA_cumsum_f_ptrs += stride_dA_cs_f_head",
      "        dA_cumsum_b_ptr += stride_dA_cs_b_head",
      "        dA_cumsum_b_ptrs += stride_dA_cs_b_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_f_ptrs += stride_ddA_cs_f_head",
      "            ddA_cumsum_b_ptrs += stride_ddA_cs_b_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate",
      "    )",
      "    tl.store(",
      "        db_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_state_bwd_ddAcs_stable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_ddAcs_stable_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_last - dA_cs_m)",
      "    else:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)",
      "    acc *= scale[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "",
      "    ddA_cs = ddt * dt_m",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    tl.atomic_add(",
      "        ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_state_varlen_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_states_ptr",
        "annotation": null
      },
      {
        "name": "cu_seqlens_ptr",
        "annotation": null
      },
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_head",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_varlen_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    chunk_states_ptr,",
      "    cu_seqlens_ptr,",
      "    states_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_chunk_states_chunk,",
      "    stride_chunk_states_head,",
      "    stride_chunk_states_hdim,",
      "    stride_chunk_states_dstate,",
      "    stride_states_batch,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)",
      "    pid_c = (end_idx - 1) // chunk_size",
      "    b_ptr += (",
      "        pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    chunk_states_ptr += (",
      "        pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_last = tl.load(",
      "        dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize",
      "    ).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = end_idx - pid_c * chunk_size",
      "    start_idx = tl.load(cu_seqlens_ptr + pid_b)",
      "    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim)",
      "            & (offs_k[None, :] < chunk_size_limit - k)",
      "            & (offs_k[None, :] >= start_idx_cur - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k)",
      "            & (offs_n[None, :] < dstate)",
      "            & (offs_k[:, None] >= start_idx_cur - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(",
      "            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale = tl.where(",
      "            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),",
      "            tl.exp((dA_cs_last - dA_cs_k)) * dt_k,",
      "            0.0,",
      "        )",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    if start_idx < pid_c * chunk_size:",
      "        chunk_states_ptrs = chunk_states_ptr + (",
      "            offs_m[:, None] * stride_chunk_states_hdim",
      "            + offs_n[None, :] * stride_chunk_states_dstate",
      "        )",
      "        chunk_states = tl.load(",
      "            chunk_states_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "",
      "        scale = tl.exp(dA_cs_last)",
      "        acc += chunk_states * scale",
      "",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (",
      "        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/37.py"
  },
  {
    "name": "_chunk_scan_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_f_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_b_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_f_ptr",
        "annotation": null
      },
      {
        "name": "dstates_b_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_f_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_b_csize",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_f_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_f_ptr,",
      "    dA_cumsum_b_ptr,",
      "    D_ptr,",
      "    b_ptr,",
      "    dstates_f_ptr,",
      "    dstates_b_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_f_batch,",
      "    stride_dA_cs_f_chunk,",
      "    stride_dA_cs_f_head,",
      "    stride_dA_cs_f_csize,",
      "    stride_dA_cs_b_batch,",
      "    stride_dA_cs_b_chunk,",
      "    stride_dA_cs_b_head,",
      "    stride_dA_cs_b_csize,",
      "    stride_D_head,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_f_batch,",
      "    stride_dstates_f_chunk,",
      "    stride_dstates_f_head,",
      "    stride_dstates_f_hdim,",
      "    stride_dstates_f_dstate,",
      "    stride_dstates_b_batch,",
      "    stride_dstates_b_chunk,",
      "    stride_dstates_b_head,",
      "    stride_dstates_b_hdim,",
      "    stride_dstates_b_dstate,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_f_ptr += (",
      "        pid_b * stride_dA_cs_f_batch",
      "        + pid_c * stride_dA_cs_f_chunk",
      "        + pid_h * stride_dA_cs_f_head",
      "    )",
      "    dA_cumsum_b_ptr += (",
      "        pid_b * stride_dA_cs_b_batch",
      "        + pid_c * stride_dA_cs_b_chunk",
      "        + pid_h * stride_dA_cs_b_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_f_ptr += (",
      "        pid_b * stride_dstates_f_batch",
      "        + pid_c * stride_dstates_f_chunk",
      "        + pid_h * stride_dstates_f_head",
      "    )",
      "    dstates_b_ptr += (",
      "        pid_b * stride_dstates_b_batch",
      "        + pid_c * stride_dstates_b_chunk",
      "        + pid_h * stride_dstates_b_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    dA_cs_f_m = tl.load(",
      "        dA_cumsum_f_ptr + offs_m * stride_dA_cs_f_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dA_cs_b_m = tl.load(",
      "        dA_cumsum_b_ptr + offs_m * stride_dA_cs_b_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    dA_cs_f_last = tl.load(",
      "        dA_cumsum_f_ptr + (chunk_size - 1) * stride_dA_cs_f_csize",
      "    ).to(tl.float32)",
      "    dA_cs_b_last = tl.load(dA_cumsum_b_ptr).to(tl.float32)",
      "    scale_f = tl.exp(dA_cs_f_last - dA_cs_f_m)",
      "    scale_b = tl.exp(dA_cs_b_last - dA_cs_b_m)",
      "",
      "    offs_dstate = tl.arange(",
      "        0,",
      "        (",
      "            BLOCK_SIZE_DSTATE",
      "            if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128",
      "            else BLOCK_SIZE_K",
      "        ),",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate",
      "    )",
      "    dstates_f_ptrs = dstates_f_ptr + (",
      "        offs_n[None, :] * stride_dstates_f_hdim",
      "        + offs_dstate[:, None] * stride_dstates_f_dstate",
      "    )",
      "    dstates_b_ptrs = dstates_b_ptr + (",
      "        offs_n[None, :] * stride_dstates_b_hdim",
      "        + offs_dstate[:, None] * stride_dstates_b_dstate",
      "    )",
      "    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates_f = tl.load(",
      "            dstates_f_ptrs,",
      "            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates_f = dstates_f.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates_f) * scale_f[:, None]",
      "        dstates_b = tl.load(",
      "            dstates_b_ptrs,",
      "            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates_b = dstates_b.to(b_ptr.dtype.element_ty)",
      "        acc += tl.dot(b, dstates_b) * scale_b[:, None]",
      "    else:",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_dstate[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates_f = tl.load(",
      "                dstates_f_ptrs,",
      "                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates_f = dstates_f.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates_f) * scale_f[:, None]",
      "            dstates_b = tl.load(",
      "                dstates_b_ptrs,",
      "                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates_b = dstates_b.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates_b) * scale_b[:, None]",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_f_ptrs += BLOCK_SIZE_K * stride_dstates_f_dstate",
      "            dstates_b_ptrs += BLOCK_SIZE_K * stride_dstates_b_dstate",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_f_ptrs = dA_cumsum_f_ptr + offs_k * stride_dA_cs_f_csize",
      "    dA_cumsum_b_ptrs = dA_cumsum_b_ptr + offs_k * stride_dA_cs_b_csize",
      "    K_MAX = chunk_size_limit",
      "",
      "    K_F_MAX = min((pid_m) * BLOCK_SIZE_M, chunk_size_limit)",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "        k = tl.multiple_of(k, BLOCK_SIZE_K)",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        if (k <= K_F_MAX + BLOCK_SIZE_M) or (k + BLOCK_SIZE_K >= K_F_MAX):",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (",
      "                k + offs_k[None, :] < K_MAX",
      "            )",
      "            a_f = tl.where(",
      "                mask_f, tl.exp((dA_cs_f_k[None, :] - dA_cs_f_m[:, None])), 0.0",
      "            )",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_b = k + offs_k[None, :] <= offs_m[:, None]",
      "            cb *= a_f + tl.where(",
      "                mask_b, tl.exp((dA_cs_b_k[None, :] - dA_cs_b_m[:, None])), 0.0",
      "            )",
      "",
      "        elif k < K_F_MAX:",
      "            dA_cs_b_k = tl.load(",
      "                dA_cumsum_b_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_b = k + offs_k[None, :] <= offs_m[:, None]",
      "            cb *= tl.where(",
      "                mask_b, tl.exp((dA_cs_b_k[None, :] - dA_cs_b_m[:, None])), 0.0",
      "            )",
      "        else:",
      "            dA_cs_f_k = tl.load(",
      "                dA_cumsum_f_ptrs, mask=offs_k < chunk_size - k, other=0.0",
      "            ).to(tl.float32)",
      "            mask_f = (k + offs_k[None, :] >= offs_m[:, None]) & (",
      "                k + offs_k[None, :] < K_MAX",
      "            )",
      "            cb *= tl.where(",
      "                mask_f, tl.exp((dA_cs_f_k[None, :] - dA_cs_f_m[:, None])), 0.0",
      "            )",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_f_ptrs += BLOCK_SIZE_K * stride_dA_cs_f_csize",
      "        dA_cumsum_b_ptrs += BLOCK_SIZE_K * stride_dA_cs_b_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "            dD = tl.sum(dout_res * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "        else:",
      "            dD = tl.sum(dout_res * x)",
      "            tl.store(dD_ptr, dD)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/38.py"
  },
  {
    "name": "_state_passing_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "final_states_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_final_states_batch",
        "annotation": null
      },
      {
        "name": "stride_final_states_head",
        "annotation": null
      },
      {
        "name": "stride_final_states_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_fwd_kernel(",
      "    states_ptr,",
      "    out_ptr,",
      "    final_states_ptr,",
      "    dA_cs_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_final_states_batch,",
      "    stride_final_states_head,",
      "    stride_final_states_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    REVERSE: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    final_states_ptr += (",
      "        pid_b * stride_final_states_batch + pid_h * stride_final_states_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    states_ptrs = states_ptr + offs_m * stride_states_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim",
      "",
      "    if not REVERSE:",
      "",
      "        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "        tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        out_ptrs += stride_out_chunk",
      "        for c in range(nchunks):",
      "            new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(",
      "                tl.float32",
      "            )",
      "            dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "            scale = tl.exp(dA_cs)",
      "            states = scale * states + new_states",
      "            if c < nchunks - 1:",
      "                tl.store(out_ptrs, states, mask=offs_m < dim)",
      "            else:",
      "                tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "            states_ptrs += stride_states_chunk",
      "            dA_cs_ptr += stride_dA_cs_chunk",
      "            out_ptrs += stride_out_chunk",
      "    else:",
      "",
      "        states_ptrs += (nchunks - 1) * stride_states_chunk",
      "        dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk",
      "        out_ptrs += (nchunks - 1) * stride_out_chunk",
      "",
      "        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "        tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        out_ptrs -= stride_out_chunk",
      "        for c in range(nchunks - 1, -1, -1):",
      "            new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(",
      "                tl.float32",
      "            )",
      "            dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "            scale = tl.exp(dA_cs)",
      "            states = scale * states + new_states",
      "            if c > 0:",
      "                tl.store(out_ptrs, states, mask=offs_m < dim)",
      "            else:",
      "                tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "            states_ptrs -= stride_states_chunk",
      "            dA_cs_ptr -= stride_dA_cs_chunk",
      "            out_ptrs -= stride_out_chunk"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/39.py"
  },
  {
    "name": "_state_passing_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cs_ptr",
        "annotation": null
      },
      {
        "name": "states_converted_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_dim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "CONVERT_STATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_bwd_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dA_cs_ptr,",
      "    dstates_ptr,",
      "    ddA_cs_ptr,",
      "    states_converted_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_dstates_head,",
      "    stride_dstates_dim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    CONVERT_STATES: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    dstates_ptr += pid_b * stride_dstates_batch + pid_h * stride_dstates_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head",
      "    ddA_cs_ptr += pid_b * stride_ddA_cs_batch + pid_h * stride_ddA_cs_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    dout_ptr += pid_b * stride_dout_batch + pid_h * stride_dout_head",
      "    if not REVERSE:",
      "        dstates_ptr += (nchunks - 1) * stride_dstates_chunk",
      "        dA_cs_ptr += (nchunks - 1) * stride_dA_cs_chunk",
      "        ddA_cs_ptr += (nchunks - 1) * stride_ddA_cs_chunk",
      "        out_ptr += (nchunks - 1) * stride_out_chunk",
      "        dout_ptr += (nchunks - 1) * stride_dout_chunk",
      "",
      "    if CONVERT_STATES:",
      "        states_converted_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "        if not REVERSE:",
      "            states_converted_ptr += (nchunks - 1) * stride_out_chunk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    dout_ptrs = dout_ptr + offs_m * stride_dout_dim",
      "    if CONVERT_STATES:",
      "        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim",
      "",
      "    dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "    if not REVERSE:",
      "        dstates_ptrs -= stride_dstates_chunk",
      "    else:",
      "        dstates_ptrs += stride_dstates_chunk",
      "    for c in range(nchunks - 1):",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if CONVERT_STATES:",
      "            tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "        ddA = tl.sum(out * dstates) * scale",
      "        tl.store(ddA_cs_ptr, ddA)",
      "        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dstates = scale * dstates + dout",
      "        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "        if not REVERSE:",
      "            dout_ptrs -= stride_dout_chunk",
      "            dstates_ptrs -= stride_dstates_chunk",
      "            dA_cs_ptr -= stride_dA_cs_chunk",
      "            ddA_cs_ptr -= stride_ddA_cs_chunk",
      "            out_ptrs -= stride_out_chunk",
      "            if CONVERT_STATES:",
      "                states_converted_ptrs -= stride_out_chunk",
      "        else:",
      "            dout_ptrs += stride_dout_chunk",
      "            dstates_ptrs += stride_dstates_chunk",
      "            dA_cs_ptr += stride_dA_cs_chunk",
      "            ddA_cs_ptr += stride_ddA_cs_chunk",
      "            out_ptrs += stride_out_chunk",
      "            if CONVERT_STATES:",
      "                states_converted_ptrs += stride_out_chunk",
      "    if CONVERT_STATES:",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "    tl.store(ddA_cs_ptr, 0.0)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/bi/39.py"
  },
  {
    "name": "_swiglu_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_fwd_kernel(",
      "    X,",
      "    Y,",
      "    OUT,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_out_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    OUT += row * stride_out_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    out = x * tl.sigmoid(x) * y",
      "    tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/40.py"
  },
  {
    "name": "_swiglu_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DOUT",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dout_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_bwd_kernel(",
      "    X,",
      "    Y,",
      "    DOUT,",
      "    OUT,",
      "    DX,",
      "    DY,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dout_row,",
      "    stride_out_row,",
      "    stride_dx_row,",
      "    stride_dy_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    DOUT += row * stride_dout_row",
      "    if RECOMPUTE_OUTPUT:",
      "        OUT += row * stride_out_row",
      "    DX += row * stride_dx_row",
      "    DY += row * stride_dy_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    x_sigmoid = tl.sigmoid(x)",
      "    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout",
      "    dy = x * x_sigmoid * dout",
      "    tl.store(DX + cols, dx, mask=cols < ncols)",
      "    tl.store(DY + cols, dy, mask=cols < ncols)",
      "    if RECOMPUTE_OUTPUT:",
      "        out = x * x_sigmoid * y",
      "        tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/40.py"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])",
      "@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})",
      "@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "X1",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "B1",
        "annotation": null
      },
      {
        "name": "Y1",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "DROPOUT_MASK",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "stride_x1_row",
        "annotation": null
      },
      {
        "name": "stride_y1_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DROPOUT_MASK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_X1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_W1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    X1,",
      "    W1,",
      "    B1,",
      "    Y1,",
      "    RESIDUAL_OUT,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    DROPOUT_MASK,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    stride_x1_row,",
      "    stride_y1_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    STORE_DROPOUT_MASK: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_X1: tl.constexpr,",
      "    HAS_W1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "    if HAS_X1:",
      "        X1 += row * stride_x1_row",
      "    if HAS_W1:",
      "        Y1 += row * stride_y1_row",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_ROWSCALE:",
      "        rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "        x *= rowscale",
      "    if HAS_DROPOUT:",
      "",
      "        keep_mask = (",
      "            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p",
      "        )",
      "        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)",
      "        if STORE_DROPOUT_MASK:",
      "            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)",
      "    if HAS_X1:",
      "        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)",
      "            x1 *= rowscale",
      "        if HAS_DROPOUT:",
      "",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)",
      "            if STORE_DROPOUT_MASK:",
      "                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)",
      "        x += x1",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "",
      "    tl.store(Y + cols, y, mask=mask)",
      "    if HAS_W1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "        if HAS_B1:",
      "            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)",
      "        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1",
      "        tl.store(Y1 + cols, y1, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/41.py"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=pruned_configs_autotune, key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS', 'HAS_DROPOUT'])",
      "@triton.heuristics({'HAS_ROWSCALE': lambda args: args['ROWSCALE'] is not None})",
      "@triton.heuristics({'HAS_DY1': lambda args: args['DY1'] is not None})",
      "@triton.heuristics({'HAS_DX1': lambda args: args['DX1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['DB1'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "DY1",
        "annotation": null
      },
      {
        "name": "DX1",
        "annotation": null
      },
      {
        "name": "DW1",
        "annotation": null
      },
      {
        "name": "DB1",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dy1_row",
        "annotation": null
      },
      {
        "name": "stride_dx1_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DY1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DX1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    W1,",
      "    DY1,",
      "    DX1,",
      "    DW1,",
      "    DB1,",
      "    DRESIDUAL_IN,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dy1_row,",
      "    stride_dx1_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_DY1: tl.constexpr,",
      "    HAS_DX1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if HAS_DY1:",
      "        DY1 += row_start * stride_dy1_row",
      "    if HAS_DX1:",
      "        DX1 += row_start * stride_dx1_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    if HAS_DY1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_DY1:",
      "        dw1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "        if HAS_B1:",
      "            db1 = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if HAS_DY1:",
      "            dy1 = tl.load(DY1 + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_DY1:",
      "            wdy += w1 * dy1",
      "            dw1 += dy1 * xhat",
      "            if HAS_B1:",
      "                db1 += dy1",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        if HAS_DX1:",
      "            if HAS_DROPOUT:",
      "                keep_mask = (",
      "                    tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                    > dropout_p",
      "                )",
      "                dx1 = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "            else:",
      "                dx1 = dx",
      "            tl.store(DX1 + cols, dx1, mask=mask)",
      "        if HAS_DROPOUT:",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            dx = tl.where(keep_mask, dx / (1.0 - dropout_p), 0.0)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "            dx *= rowscale",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "        if HAS_DY1:",
      "            DY1 += stride_dy1_row",
      "        if HAS_DX1:",
      "            DX1 += stride_dx1_row",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)",
      "    if HAS_DY1:",
      "        tl.store(DW1 + row_block_id * N + cols, dw1, mask=mask)",
      "        if HAS_B1:",
      "            tl.store(DB1 + row_block_id * N + cols, db1, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/41.py"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    Z,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_z_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    X += row * stride_x_row + group * N",
      "    Y += row * stride_y_row + group * N",
      "    if HAS_Z:",
      "        Z += row * stride_z_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    if HAS_BIAS:",
      "        B += group * N",
      "",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_Z and not NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=cols < N).to(tl.float32)",
      "        x *= z * tl.sigmoid(z)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    if HAS_Z and NORM_BEFORE_GATE:",
      "        z = tl.load(Z + cols, mask=mask).to(tl.float32)",
      "        y *= z * tl.sigmoid(z)",
      "",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/42.py"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_BIAS': lambda args: args['B'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['Z'] is not None})",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DZ",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_z_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dz_row",
        "annotation": null
      },
      {
        "name": "stride_dw_row",
        "annotation": null
      },
      {
        "name": "stride_db_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "NORM_BEFORE_GATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Z,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DZ,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_z_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dz_row,",
      "    stride_dw_row,",
      "    stride_db_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    NORM_BEFORE_GATE: tl.constexpr,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    row_block_id = tl.program_id(0)",
      "    group = tl.program_id(1)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row + group * N",
      "    if HAS_Z:",
      "        Z += row_start * stride_z_row + group * N",
      "        DZ += row_start * stride_dz_row + group * N",
      "    DY += row_start * stride_dy_row + group * N",
      "    DX += row_start * stride_dx_row + group * N",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row + group * N",
      "    if not IS_RMS_NORM:",
      "        Mean += group * M",
      "    Rstd += group * M",
      "    W += group * N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if (RECOMPUTE_OUTPUT or HAS_Z) and HAS_BIAS:",
      "        B += group * N",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row)",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            x_og = x",
      "            x = x_og * z * tl.sigmoid(z)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if HAS_Z and NORM_BEFORE_GATE:",
      "            z = tl.load(Z + cols, mask=mask, other=0.0).to(tl.float32)",
      "            z_sigmoid = tl.sigmoid(z)",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            if RECOMPUTE_OUTPUT:",
      "                tl.store(Y + cols, y * z * z_sigmoid, mask=mask)",
      "            dz = dy * y * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dy *= z * z_sigmoid",
      "        else:",
      "            if RECOMPUTE_OUTPUT:",
      "                y = xhat * w + b if HAS_BIAS else xhat * w",
      "                tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        c1 = tl.sum(xhat * wdy, axis=0) / N",
      "        if not IS_RMS_NORM:",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            dx = (wdy - xhat * c1) * rstd",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if HAS_Z and not NORM_BEFORE_GATE:",
      "            z_sigmoid = tl.sigmoid(z)",
      "            dz = dx * x_og * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "            tl.store(DZ + cols, dz, mask=mask)",
      "            dx *= z * z_sigmoid",
      "",
      "        tl.store(DX + cols, dx, mask=mask)",
      "",
      "        X += stride_x_row",
      "        if HAS_Z:",
      "            Z += stride_z_row",
      "            DZ += stride_dz_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * stride_dw_row + group * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * stride_db_row + group * N + cols, db, mask=mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/42.py"
  },
  {
    "name": "_selective_scan_update_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_DT_BIAS': lambda args: args['dt_bias_ptr'] is not None})",
      "@triton.heuristics({'HAS_D': lambda args: args['D_ptr'] is not None})",
      "@triton.heuristics({'HAS_Z': lambda args: args['z_ptr'] is not None})",
      "@triton.heuristics({'BLOCK_SIZE_DSTATE': lambda args: triton.next_power_of_2(args['dstate'])})"
    ],
    "args": [
      {
        "name": "state_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_state_batch",
        "annotation": null
      },
      {
        "name": "stride_state_head",
        "annotation": null
      },
      {
        "name": "stride_state_dim",
        "annotation": null
      },
      {
        "name": "stride_state_dstate",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_dim",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_dim",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_A_dim",
        "annotation": null
      },
      {
        "name": "stride_A_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_group",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_group",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_D_dim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TIE_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _selective_scan_update_kernel(",
      "    state_ptr,",
      "    x_ptr,",
      "    dt_ptr,",
      "    dt_bias_ptr,",
      "    A_ptr,",
      "    B_ptr,",
      "    C_ptr,",
      "    D_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    batch,",
      "    nheads,",
      "    dim,",
      "    dstate,",
      "    nheads_ngroups_ratio,",
      "    stride_state_batch,",
      "    stride_state_head,",
      "    stride_state_dim,",
      "    stride_state_dstate,",
      "    stride_x_batch,",
      "    stride_x_head,",
      "    stride_x_dim,",
      "    stride_dt_batch,",
      "    stride_dt_head,",
      "    stride_dt_dim,",
      "    stride_dt_bias_head,",
      "    stride_dt_bias_dim,",
      "    stride_A_head,",
      "    stride_A_dim,",
      "    stride_A_dstate,",
      "    stride_B_batch,",
      "    stride_B_group,",
      "    stride_B_dstate,",
      "    stride_C_batch,",
      "    stride_C_group,",
      "    stride_C_dstate,",
      "    stride_D_head,",
      "    stride_D_dim,",
      "    stride_z_batch,",
      "    stride_z_head,",
      "    stride_z_dim,",
      "    stride_out_batch,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    TIE_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    state_ptr += pid_b * stride_state_batch + pid_h * stride_state_head",
      "    x_ptr += pid_b * stride_x_batch + pid_h * stride_x_head",
      "    dt_ptr += pid_b * stride_dt_batch + pid_h * stride_dt_head",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptr += pid_h * stride_dt_bias_head",
      "    A_ptr += pid_h * stride_A_head",
      "    B_ptr += pid_b * stride_B_batch + (pid_h // nheads_ngroups_ratio) * stride_B_group",
      "    C_ptr += pid_b * stride_C_batch + (pid_h // nheads_ngroups_ratio) * stride_C_group",
      "    if HAS_Z:",
      "        z_ptr += pid_b * stride_z_batch + pid_h * stride_z_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "    state_ptrs = state_ptr + (",
      "        offs_m[:, None] * stride_state_dim + offs_n[None, :] * stride_state_dstate",
      "    )",
      "    x_ptrs = x_ptr + offs_m * stride_x_dim",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_dim",
      "    if HAS_DT_BIAS:",
      "        dt_bias_ptrs = dt_bias_ptr + offs_m * stride_dt_bias_dim",
      "    if HAS_D:",
      "        D_ptr += pid_h * stride_D_head",
      "    A_ptrs = A_ptr + (",
      "        offs_m[:, None] * stride_A_dim + offs_n[None, :] * stride_A_dstate",
      "    )",
      "    B_ptrs = B_ptr + offs_n * stride_B_dstate",
      "    C_ptrs = C_ptr + offs_n * stride_C_dstate",
      "    if HAS_D:",
      "        D_ptrs = D_ptr + offs_m * stride_D_dim",
      "    if HAS_Z:",
      "        z_ptrs = z_ptr + offs_m * stride_z_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "",
      "    state = tl.load(",
      "        state_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "    )",
      "    x = tl.load(x_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if not TIE_HDIM:",
      "        dt = tl.load(dt_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(",
      "            A_ptrs, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate), other=0.0",
      "        ).to(tl.float32)",
      "        dA = tl.exp(A * dt[:, None])",
      "    else:",
      "        dt = tl.load(dt_ptr).to(tl.float32)",
      "        if HAS_DT_BIAS:",
      "            dt += tl.load(dt_bias_ptr).to(tl.float32)",
      "        if DT_SOFTPLUS:",
      "            dt = softplus(dt)",
      "        A = tl.load(A_ptr).to(tl.float32)",
      "        dA = tl.exp(A * dt)",
      "",
      "    B = tl.load(B_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    C = tl.load(C_ptrs, mask=offs_n < dstate, other=0.0).to(tl.float32)",
      "    if HAS_D:",
      "        D = tl.load(D_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    if HAS_Z:",
      "        z = tl.load(z_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "",
      "    if not TIE_HDIM:",
      "        dB = B[None, :] * dt[:, None]",
      "    else:",
      "        dB = B * dt",
      "    state = state * dA + dB * x[:, None]",
      "    tl.store(",
      "        state_ptrs, state, mask=(offs_m[:, None] < dim) & (offs_n[None, :] < dstate)",
      "    )",
      "    out = tl.sum(state * C[None, :], axis=1)",
      "    if HAS_D:",
      "        out += x * D",
      "    if HAS_Z:",
      "        out *= z * tl.sigmoid(z)",
      "    tl.store(out_ptrs, out, mask=offs_m < dim)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/43.py"
  },
  {
    "name": "_bmm_chunk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_outm",
        "annotation": null
      },
      {
        "name": "stride_outn",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_fwd_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    out_ptr,",
      "    seq_idx_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_bk,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_outm,",
      "    stride_outn,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    IS_CAUSAL: tl.constexpr,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    if IS_CAUSAL:",
      "        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "            return",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + pid_h * stride_b_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit)",
      "            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)",
      "            & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    out = acc.to(out_ptr.dtype.element_ty)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)",
      "    tl.store(",
      "        out_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/45.py"
  },
  {
    "name": "_bmm_chunk_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "res_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_n",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_head",
        "annotation": null
      },
      {
        "name": "stride_db_k",
        "annotation": null
      },
      {
        "name": "stride_res_batch",
        "annotation": null
      },
      {
        "name": "stride_res_seqlen",
        "annotation": null
      },
      {
        "name": "stride_res_head",
        "annotation": null
      },
      {
        "name": "stride_res_k",
        "annotation": null
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_bwd_kernel(",
      "    a_ptr,",
      "    dout_ptr,",
      "    db_ptr,",
      "    res_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_csize_m,",
      "    stride_dout_csize_n,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_head,",
      "    stride_db_k,",
      "    stride_res_batch,",
      "    stride_res_seqlen,",
      "    stride_res_head,",
      "    stride_res_k,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_CS: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_cs = tl.arange(0, BLOCK_SIZE_CS)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m",
      "    )",
      "    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)",
      "            & (offs_n[None, :] < K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(dout, a)",
      "        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m",
      "        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_RESIDUAL:",
      "        res_ptr += (",
      "            pid_b * stride_res_batch",
      "            + pid_c * chunk_size * stride_res_seqlen",
      "            + pid_h * stride_res_head",
      "        )",
      "        res_ptrs = res_ptr + (",
      "            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k",
      "        )",
      "        res = tl.load(",
      "            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "        ).to(tl.float32)",
      "        acc += res",
      "    db = acc.to(db_ptr.dtype.element_ty)",
      "",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_h * stride_db_head",
      "    )",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k",
      "    )",
      "    tl.store(",
      "        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/45.py"
  },
  {
    "name": "_chunk_scan_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'hdim', 'dstate', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    prev_states_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_D_head,",
      "    IS_CAUSAL: tl.constexpr,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    if IS_TRITON_22 or pid_c > -1:",
      "",
      "        offs_k_dstate = tl.arange(",
      "            0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "        )",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "        )",
      "        prev_states_ptrs = prev_states_ptr + (",
      "            offs_n[None, :] * stride_states_hdim",
      "            + offs_k_dstate[:, None] * stride_states_dstate",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        else:",
      "            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        if BLOCK_SIZE_DSTATE <= 128:",
      "            C = tl.load(",
      "                C_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k_dstate[None, :] < dstate),",
      "                other=0.0,",
      "            )",
      "            prev_states = tl.load(",
      "                prev_states_ptrs,",
      "                mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            prev_states = prev_states.to(C_ptr.dtype.element_ty)",
      "            acc = tl.dot(C, prev_states) * scale_m[:, None]",
      "        else:",
      "            for k in range(0, dstate, BLOCK_SIZE_K):",
      "                C = tl.load(",
      "                    C_ptrs,",
      "                    mask=(offs_m[:, None] < chunk_size_limit)",
      "                    & (offs_k_dstate[None, :] < dstate - k),",
      "                    other=0.0,",
      "                )",
      "",
      "                prev_states = tl.load(",
      "                    prev_states_ptrs,",
      "                    mask=(offs_k_dstate[:, None] < dstate - k)",
      "                    & (offs_n[None, :] < hdim),",
      "                    other=0.0,",
      "                )",
      "                prev_states = prev_states.to(C_ptr.dtype.element_ty)",
      "                acc += tl.dot(C, prev_states)",
      "                C_ptrs += BLOCK_SIZE_K",
      "                prev_states_ptrs += BLOCK_SIZE_K",
      "            acc *= scale_m[:, None]",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_k[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    K_MAX = (",
      "        chunk_size_limit",
      "        if not IS_CAUSAL",
      "        else min((pid_m + 1) * BLOCK_SIZE_M, chunk_size_limit)",
      "    )",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < chunk_size - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "",
      "        cb *= tl.exp((dA_cs_m[:, None] - dA_cs_k[None, :]))",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(tl.float32)",
      "        cb *= dt_k",
      "        if IS_CAUSAL:",
      "            mask = offs_m[:, None] >= k + offs_k[None, :]",
      "            cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(x_ptr.dtype.element_ty)",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        acc += tl.dot(cb, x)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_out_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_out_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    if HAS_D:",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        x_residual = tl.load(",
      "            x_ptr",
      "            + (offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim),",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc += x_residual * D",
      "",
      "    if HAS_Z:",
      "        out_x_ptr += (",
      "            pid_b * stride_out_batch",
      "            + pid_c * chunk_size * stride_out_seqlen",
      "            + pid_h * stride_out_head",
      "        )",
      "        out_x_ptrs = out_x_ptr + (",
      "            stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :]",
      "        )",
      "        tl.store(",
      "            out_x_ptrs,",
      "            acc,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "        )",
      "",
      "        z_ptr += (",
      "            pid_b * stride_z_batch",
      "            + pid_c * chunk_size * stride_z_seqlen",
      "            + pid_h * stride_z_head",
      "        )",
      "        z_ptrs = z_ptr + (",
      "            stride_z_seqlen * offs_out_m[:, None] + stride_z_hdim * offs_out_n[None, :]",
      "        )",
      "        z = tl.load(",
      "            z_ptrs,",
      "            mask=(offs_out_m[:, None] < chunk_size_limit)",
      "            & (offs_out_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= z * tl.sigmoid(z)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        stride_out_seqlen * offs_out_m[:, None] + offs_out_n[None, :] * stride_out_hdim",
      "    )",
      "    tl.store(",
      "        out_ptrs,",
      "        acc,",
      "        mask=(offs_out_m[:, None] < chunk_size_limit) & (offs_out_n[None, :] < hdim),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_fwd_kernel_wip",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_N': 32}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_x_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "B_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_B_batch",
        "annotation": null
      },
      {
        "name": "stride_B_seqlen",
        "annotation": null
      },
      {
        "name": "stride_B_head",
        "annotation": null
      },
      {
        "name": "stride_B_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_Z",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_fwd_kernel_wip(",
      "    cb_ptr,",
      "    x_ptr,",
      "    z_ptr,",
      "    out_ptr,",
      "    out_x_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    C_ptr,",
      "    B_ptr,",
      "    prev_states_ptr,",
      "    D_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_B_batch,",
      "    stride_B_seqlen,",
      "    stride_B_head,",
      "    stride_B_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_D_head,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_Z: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_n = tl.program_id(axis=0)",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    B_ptr += (",
      "        pid_b * stride_B_batch",
      "        + pid_c * chunk_size * stride_B_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_B_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "",
      "    offs_m = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k_dstate = tl.arange(0, BLOCK_SIZE_DSTATE)",
      "",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_k_dstate[None, :] * stride_C_dstate",
      "    )",
      "    B_ptrs = B_ptr + (",
      "        offs_m[None, :] * stride_B_seqlen + offs_k_dstate[:, None] * stride_B_dstate",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_states_hdim",
      "        + offs_k_dstate[:, None] * stride_states_dstate",
      "    )",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_m[None, :] * stride_cb_csize_k",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    for start_m in range(0, chunk_size_limit, BLOCK_SIZE_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_SIZE_M)",
      "        dA_cs_m = tl.load(",
      "            dA_cumsum_ptr + (start_m + offs_m) * stride_dA_cs_csize,",
      "            mask=offs_m < chunk_size - start_m,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_prev = tl.load(",
      "                seq_idx_ptr + start_m - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "            )",
      "            seq_idx_m = tl.load(",
      "                seq_idx_ptr + (start_m + offs_m) * stride_seq_idx_seqlen,",
      "                mask=offs_m < chunk_size_limit - start_m,",
      "                other=-1,",
      "            )",
      "        if not HAS_SEQ_IDX:",
      "            scale_m = tl.exp(dA_cs_m)",
      "        else:",
      "            scale_m = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        C = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_k_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(C, prev_states.to(C_ptr.dtype.element_ty)) * scale_m[:, None]",
      "",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size - start_m, other=0.0).to(",
      "            tl.float32",
      "        )",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "",
      "        if HAS_D:",
      "            if D_HAS_HDIM:",
      "                D = tl.load(",
      "                    D_ptr + pid_h * stride_D_head + offs_n,",
      "                    mask=offs_n < hdim,",
      "                    other=0.0,",
      "                ).to(tl.float32)",
      "            else:",
      "                D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "            acc += x.to(tl.float32) * D",
      "",
      "        tl.store(",
      "            out_ptrs,",
      "            acc,",
      "            mask=(offs_m[:, None] < chunk_size_limit - start_m)",
      "            & (offs_n[None, :] < hdim),",
      "        )",
      "",
      "        if start_m + BLOCK_SIZE_M < chunk_size_limit:",
      "",
      "            B = tl.load(",
      "                B_ptrs,",
      "                mask=(offs_m[None, :] < chunk_size_limit - start_m)",
      "                & (offs_k_dstate[:, None] < dstate),",
      "                other=0.0,",
      "            )",
      "            dA_cs_last = tl.load(",
      "                dA_cumsum_ptr + (start_m + BLOCK_SIZE_M) * stride_dA_cs_csize",
      "            ).to(tl.float32)",
      "",
      "            scale = tl.exp((dA_cs_last - dA_cs_m)) * dt_m",
      "",
      "            B = B.to(x_ptr.dtype.element_ty)",
      "            tmp = tl.dot(B, x)",
      "            prev_states += tmp.to(prev_states.dtype)",
      "",
      "        C_ptrs += BLOCK_SIZE_M * stride_C_seqlen",
      "        B_ptrs += BLOCK_SIZE_M * stride_B_seqlen",
      "        cb_ptrs += BLOCK_SIZE_M * stride_cb_csize_m + BLOCK_SIZE_M * stride_cb_csize_k",
      "        x_ptrs += BLOCK_SIZE_M * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_M * stride_dt_csize",
      "        out_ptrs += BLOCK_SIZE_M * stride_out_seqlen"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_dz_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "outz_ptr",
        "annotation": null
      },
      {
        "name": "dz_ptr",
        "annotation": null
      },
      {
        "name": "dout_x_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_z_batch",
        "annotation": null
      },
      {
        "name": "stride_z_seqlen",
        "annotation": null
      },
      {
        "name": "stride_z_head",
        "annotation": null
      },
      {
        "name": "stride_z_hdim",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_outz_batch",
        "annotation": null
      },
      {
        "name": "stride_outz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_outz_head",
        "annotation": null
      },
      {
        "name": "stride_outz_hdim",
        "annotation": null
      },
      {
        "name": "stride_dz_batch",
        "annotation": null
      },
      {
        "name": "stride_dz_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dz_head",
        "annotation": null
      },
      {
        "name": "stride_dz_hdim",
        "annotation": null
      },
      {
        "name": "stride_doutx_batch",
        "annotation": null
      },
      {
        "name": "stride_doutx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_doutx_head",
        "annotation": null
      },
      {
        "name": "stride_doutx_hdim",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DDACS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dz_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    z_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    outz_ptr,",
      "    dz_ptr,",
      "    dout_x_ptr,",
      "    dD_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_z_batch,",
      "    stride_z_seqlen,",
      "    stride_z_head,",
      "    stride_z_hdim,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_outz_batch,",
      "    stride_outz_seqlen,",
      "    stride_outz_head,",
      "    stride_outz_hdim,",
      "    stride_dz_batch,",
      "    stride_dz_seqlen,",
      "    stride_dz_head,",
      "    stride_dz_hdim,",
      "    stride_doutx_batch,",
      "    stride_doutx_seqlen,",
      "    stride_doutx_head,",
      "    stride_doutx_hdim,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_DDACS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dout_x_ptr += (",
      "        pid_b * stride_doutx_batch",
      "        + pid_c * chunk_size * stride_doutx_seqlen",
      "        + pid_h * stride_doutx_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    z_ptr += (",
      "        pid_b * stride_z_batch",
      "        + pid_c * chunk_size * stride_z_seqlen",
      "        + pid_h * stride_z_head",
      "    )",
      "    dz_ptr += (",
      "        pid_b * stride_dz_batch",
      "        + pid_c * chunk_size * stride_dz_seqlen",
      "        + pid_h * stride_dz_head",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptr += (",
      "            pid_b * stride_outz_batch",
      "            + pid_c * chunk_size * stride_outz_seqlen",
      "            + pid_h * stride_outz_head",
      "        )",
      "    if HAS_DDACS:",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + pid_h * stride_ddA_cs_head",
      "        )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dout_x_ptrs = dout_x_ptr + (",
      "        offs_m[:, None] * stride_doutx_seqlen + offs_n[None, :] * stride_doutx_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    z_ptrs = z_ptr + (",
      "        offs_m[:, None] * stride_z_seqlen + offs_n[None, :] * stride_z_hdim",
      "    )",
      "    dz_ptrs = dz_ptr + (",
      "        offs_m[:, None] * stride_dz_seqlen + offs_n[None, :] * stride_dz_hdim",
      "    )",
      "    if RECOMPUTE_OUTPUT:",
      "        outz_ptrs = outz_ptr + (",
      "            offs_m[:, None] * stride_outz_seqlen + offs_n[None, :] * stride_outz_hdim",
      "        )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z = tl.load(",
      "        z_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    z_sigmoid = tl.sigmoid(z)",
      "    if RECOMPUTE_OUTPUT:",
      "        outz = out * z * z_sigmoid",
      "        tl.store(",
      "            outz_ptrs,",
      "            outz,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        )",
      "    dz = dout * out * z_sigmoid * (1 + z * (1 - z_sigmoid))",
      "    tl.store(",
      "        dz_ptrs,",
      "        dz,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    dout *= z * z_sigmoid",
      "    tl.store(",
      "        dout_x_ptrs,",
      "        dout,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    if HAS_DDACS:",
      "        ddA_cs = tl.sum(dout * out, axis=1)",
      "        tl.store(",
      "            ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize,",
      "            ddA_cs,",
      "            mask=offs_m < chunk_size,",
      "        )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_dstates_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "dprev_states_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_c_batch",
        "annotation": null
      },
      {
        "name": "stride_c_seqlen",
        "annotation": null
      },
      {
        "name": "stride_c_head",
        "annotation": null
      },
      {
        "name": "stride_c_dstate",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_head",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_dprev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dstates_kernel(",
      "    dout_ptr,",
      "    c_ptr,",
      "    dprev_states_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_c_batch,",
      "    stride_c_seqlen,",
      "    stride_c_head,",
      "    stride_c_dstate,",
      "    stride_dprev_states_batch,",
      "    stride_dprev_states_chunk,",
      "    stride_dprev_states_head,",
      "    stride_dprev_states_hdim,",
      "    stride_dprev_states_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    c_ptr += (",
      "        pid_b * stride_c_batch",
      "        + pid_c * chunk_size * stride_c_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_c_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_hdim + offs_k[None, :] * stride_dout_seqlen",
      "    )",
      "    c_ptrs = c_ptr + (",
      "        offs_n[None, :] * stride_c_dstate + offs_k[:, None] * stride_c_seqlen",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < chunk_size - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale_k = tl.exp(dA_cs_k)",
      "        else:",
      "            seq_idx_k = tl.load(",
      "                seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1",
      "            )",
      "            scale_k = tl.where(seq_idx_k == seq_idx_prev, tl.exp(dA_cs_k), 0.0)",
      "        dout = (dout * scale_k).to(dout_ptr.dtype.element_ty)",
      "        c = tl.load(",
      "            c_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        acc += tl.dot(dout, c)",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        c_ptrs += BLOCK_SIZE_K * stride_c_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen",
      "    out = acc.to(dprev_states_ptr.dtype.element_ty)",
      "",
      "    dprev_states_ptr += (",
      "        pid_b * stride_dprev_states_batch",
      "        + pid_c * stride_dprev_states_chunk",
      "        + pid_h * stride_dprev_states_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dprev_states_ptrs = dprev_states_ptr + (",
      "        offs_m[:, None] * stride_dprev_states_hdim",
      "        + offs_n[None, :] * stride_dprev_states_dstate",
      "    )",
      "    tl.store(",
      "        dprev_states_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_dc_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dc_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dc_batch",
        "annotation": null
      },
      {
        "name": "stride_dc_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dc_split",
        "annotation": null
      },
      {
        "name": "stride_dc_group",
        "annotation": null
      },
      {
        "name": "stride_dc_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dc_kernel(",
      "    dout_ptr,",
      "    prev_states_ptr,",
      "    C_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    dc_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_batch,",
      "    stride_prev_states_chunk,",
      "    stride_prev_states_head,",
      "    stride_prev_states_hdim,",
      "    stride_prev_states_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_dc_batch,",
      "    stride_dc_seqlen,",
      "    stride_dc_split,",
      "    stride_dc_group,",
      "    stride_dc_dstate,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dc_ptr += (",
      "        pid_b * stride_dc_batch",
      "        + pid_c * chunk_size * stride_dc_seqlen",
      "        + pid_g * stride_dc_group",
      "        + pid_s * stride_dc_split",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_prev_states_batch",
      "        + pid_c * stride_prev_states_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_prev_states_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        C_ptr += (",
      "            pid_b * stride_C_batch",
      "            + pid_c * chunk_size * stride_C_seqlen",
      "            + pid_g * stride_C_head",
      "        )",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_prev_states_dstate",
      "        + offs_k[:, None] * stride_prev_states_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    if HAS_DDA_CS:",
      "        C_ptrs = C_ptr + (",
      "            offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "        )",
      "        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        c = tl.load(",
      "            C_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        prev_states = tl.load(",
      "            prev_states_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        prev_states = prev_states.to(dout_ptrs.dtype.element_ty)",
      "        dc = tl.dot(dout, prev_states)",
      "        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp(dA_cs_m)",
      "        else:",
      "            scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "        dc *= scale[:, None]",
      "        if HAS_DDA_CS:",
      "            ddA_cs = tl.sum(dc * c, axis=1)",
      "            tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)",
      "        acc += dc",
      "        dout_ptrs += stride_dout_head",
      "        prev_states_ptrs += stride_prev_states_head",
      "        dA_cumsum_ptrs += stride_dA_cs_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_ptrs += stride_ddA_cs_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dc_ptrs = dc_ptr + (",
      "        offs_m[:, None] * stride_dc_seqlen + offs_n[None, :] * stride_dc_dstate",
      "    )",
      "    tl.store(",
      "        dc_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    D_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_D_head,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    K_MAX = chunk_size_limit",
      "    for k in range(0, K_MAX, BLOCK_SIZE_K):",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])",
      "",
      "        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)",
      "        cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_dcb_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dcb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dcb_batch",
        "annotation": null
      },
      {
        "name": "stride_dcb_chunk",
        "annotation": null
      },
      {
        "name": "stride_dcb_split",
        "annotation": null
      },
      {
        "name": "stride_dcb_group",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dcb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_dcb_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    cb_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    dcb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_dcb_batch,",
      "    stride_dcb_chunk,",
      "    stride_dcb_split,",
      "    stride_dcb_group,",
      "    stride_dcb_csize_m,",
      "    stride_dcb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dout_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        cb_ptr += (",
      "            pid_b * stride_cb_batch + pid_c * stride_cb_chunk + pid_g * stride_cb_head",
      "        )",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_head",
      "            + pid_m * stride_ddA_cs_csize_m",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    if HAS_DDA_CS:",
      "        cb_ptrs = cb_ptr + (",
      "            offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "        )",
      "        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n",
      "",
      "    if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "        dcb_ptr += (",
      "            pid_b * stride_dcb_batch",
      "            + pid_c * stride_dcb_chunk",
      "            + pid_g * stride_dcb_group",
      "            + pid_s * stride_dcb_split",
      "        )",
      "        dcb_ptrs = dcb_ptr + (",
      "            offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n",
      "        )",
      "        tl.store(",
      "            dcb_ptrs,",
      "            tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=dcb_ptr.dtype.element_ty),",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "        )",
      "        return",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),",
      "            other=0.0,",
      "        )",
      "        dcb = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "        dcb *= dt_n",
      "        dA_cs_m = tl.load(",
      "            dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + offs_n * stride_dA_cs_csize,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dcb *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        if HAS_DDA_CS:",
      "            tl.static_assert(",
      "                not HAS_SEQ_IDX, \"HAS_SEQ_IDX not supported with HAS_DDA_CS yet\"",
      "            )",
      "            ddA_cs = dcb * cb",
      "            mask = offs_m[:, None] >= offs_n[None, :] + 1",
      "            ddA_cs = tl.where(mask, ddA_cs, 0.0)",
      "            ddA_cs = tl.cumsum(ddA_cs, axis=1)",
      "            ddA_cs = tl.where(mask, ddA_cs, 0.0)",
      "            ddA_cs = tl.sum(ddA_cs, axis=0)",
      "            tl.store(",
      "                ddA_cumsum_ptrs + stride_ddA_cs_csize_n,",
      "                ddA_cs,",
      "                mask=offs_n < chunk_size - 1,",
      "            )",
      "            tl.store(ddA_cumsum_ptr, 0.0)",
      "        acc += dcb",
      "        dout_ptrs += stride_dout_head",
      "        x_ptrs += stride_x_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_ptr += stride_dA_cs_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_ptr += stride_ddA_cs_head",
      "            ddA_cumsum_ptrs += stride_ddA_cs_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    mask = offs_m[:, None] >= offs_n[None, :]",
      "    acc = tl.where(mask, acc, 0.0)",
      "    dcb_ptr += (",
      "        pid_b * stride_dcb_batch",
      "        + pid_c * stride_dcb_chunk",
      "        + pid_g * stride_dcb_group",
      "        + pid_s * stride_dcb_split",
      "    )",
      "    dcb_ptrs = dcb_ptr + (",
      "        offs_m[:, None] * stride_dcb_csize_m + offs_n[None, :] * stride_dcb_csize_n",
      "    )",
      "    tl.store(",
      "        dcb_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_unstable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32}), triton.Config({'BLOCK_SIZE_M': 64}), triton.Config({'BLOCK_SIZE_M': 128}), triton.Config({'BLOCK_SIZE_M': 256})], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_seqlen",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SUBTRACT_DDTDT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_unstable_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dt_ptr,",
      "    ddt_ptr,",
      "    x_ptr,",
      "    D_ptr,",
      "    ddA_cumsum_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_out_batch,",
      "    stride_out_seqlen,",
      "    stride_out_head,",
      "    stride_out_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_D_head,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    SUBTRACT_DDTDT: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_c * chunk_size * stride_out_seqlen",
      "        + pid_h * stride_out_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    if HAS_D:",
      "        x_ptr += (",
      "            pid_b * stride_x_batch",
      "            + pid_c * chunk_size * stride_x_seqlen",
      "            + pid_h * stride_x_head",
      "        )",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    out_ptrs = out_ptr + (",
      "        offs_m[:, None] * stride_out_seqlen + offs_n[None, :] * stride_out_hdim",
      "    )",
      "    if HAS_D:",
      "        x_ptrs = x_ptr + (",
      "            offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    out = tl.load(",
      "        out_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            dD = tl.sum(dout * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            dD = tl.sum(dout * x)",
      "            tl.store(dD_ptr, dD)",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        out -= x * D",
      "    ddA_cs = tl.sum(dout * out, axis=1)",
      "    if SUBTRACT_DDTDT:",
      "        dt = tl.load(",
      "            dt_ptr + offs_m * stride_dt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddt = tl.load(",
      "            ddt_ptr + offs_m * stride_ddt_csize, mask=offs_m < chunk_size, other=0.0",
      "        ).to(tl.float32)",
      "        ddA_cs -= dt * ddt",
      "    tl.store(",
      "        ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_kernel_old",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 16}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 16}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 32}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64}, num_stages=4, num_warps=8), triton.Config({'BLOCK_SIZE_M': 128}, num_stages=4, num_warps=8)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddAcs_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_head",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddAcs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_kernel_old(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddAcs_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddAcs_batch,",
      "    stride_ddAcs_chunk,",
      "    stride_ddAcs_head,",
      "    stride_ddAcs_csize_m,",
      "    stride_ddAcs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    chunk_size_limit_n = min(chunk_size_limit, (pid_m + 1) * BLOCK_SIZE_M)",
      "",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < chunk_size_limit_n),",
      "        other=0.0,",
      "    )",
      "    acc = tl.dot(dout, x)",
      "    cb = tl.load(",
      "        cb_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    acc *= cb",
      "    dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= dt_n",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_n = tl.load(",
      "        dA_cumsum_ptr + offs_n * stride_dA_cs_csize, mask=offs_n < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "    mask = offs_m[:, None] >= offs_n[None, :] + 1",
      "    acc = tl.where(mask, acc, 0.0)",
      "    acc = tl.cumsum(acc, axis=1)",
      "    acc = tl.where(mask, acc, 0.0)",
      "    ddA_cs = tl.sum(acc, axis=0)",
      "    ddAcs_ptr += (",
      "        pid_b * stride_ddAcs_batch",
      "        + pid_c * stride_ddAcs_chunk",
      "        + pid_h * stride_ddAcs_head",
      "        + pid_m * stride_ddAcs_csize_m",
      "    )",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddAcs_ptrs = ddAcs_ptr + offs_n * stride_ddAcs_csize_n",
      "    tl.store(ddAcs_ptrs + stride_ddAcs_csize_n, ddA_cs, mask=offs_n < chunk_size - 1)",
      "    tl.store(ddAcs_ptr, 0.0)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_stable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4)], key=['chunk_size', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_n",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_m",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize_n",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_stable_kernel(",
      "    x_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    cb_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_n,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize_m,",
      "    stride_ddA_cs_csize_n,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "        + pid_m * stride_ddA_cs_csize_m",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    x_ptrs = x_ptr + (",
      "        offs_n[None, :] * stride_x_seqlen + offs_k[:, None] * stride_x_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_n * stride_dt_csize",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_n[None, :] * stride_cb_csize_n",
      "    )",
      "    ddAcs_ptrs = ddA_cumsum_ptr + offs_n * stride_ddA_cs_csize_n",
      "    tl.store(ddA_cumsum_ptr, 0.0)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    rowsum = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "",
      "    lo, hi = 0, (pid_m + 1) * BLOCK_SIZE_M",
      "",
      "    for start_n in range(lo, hi, BLOCK_SIZE_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_SIZE_N)",
      "",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_k[:, None] < hdim)",
      "            & (offs_n[None, :] < chunk_size_limit - start_n),",
      "            other=0.0,",
      "        )",
      "        acc = tl.dot(dout, x)",
      "        dt_n = tl.load(dt_ptrs, mask=offs_n < chunk_size - start_n, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        acc *= dt_n",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_n[None, :] < chunk_size - start_n),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= cb",
      "        dA_cs_n = tl.load(",
      "            dA_cumsum_ptr + (start_n + offs_n) * stride_dA_cs_csize,",
      "            mask=offs_n < chunk_size - start_n,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        acc *= tl.exp(dA_cs_m[:, None] - dA_cs_n[None, :])",
      "        mask = offs_m[:, None] >= start_n + offs_n[None, :] + 1",
      "        acc = tl.where(mask, acc, 0.0)",
      "        rowsum_new = rowsum + tl.sum(acc, axis=1)",
      "        acc = rowsum[:, None] + tl.cumsum(acc, axis=1)",
      "        rowsum = rowsum_new",
      "        acc = tl.where(mask, acc, 0.0)",
      "",
      "        ddA_cs = tl.sum(acc, axis=0)",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            ddA_cs,",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        x_ptrs += BLOCK_SIZE_N * stride_x_seqlen",
      "        dt_ptrs += BLOCK_SIZE_N * stride_dt_csize",
      "        cb_ptrs += BLOCK_SIZE_N * stride_cb_csize_n",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n",
      "",
      "    for start_n in range(hi, chunk_size, BLOCK_SIZE_N):",
      "        tl.store(",
      "            ddAcs_ptrs + stride_ddA_cs_csize_n,",
      "            tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32),",
      "            mask=offs_n < chunk_size - start_n - 1,",
      "        )",
      "        ddAcs_ptrs += BLOCK_SIZE_N * stride_ddA_cs_csize_n"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_scan_bwd_ddAcs_prev_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "prev_states_ptr",
        "annotation": null
      },
      {
        "name": "C_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_batch",
        "annotation": null
      },
      {
        "name": "stride_prev_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_prev_states_head",
        "annotation": null
      },
      {
        "name": "stride_prev_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_prev_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_C_batch",
        "annotation": null
      },
      {
        "name": "stride_C_seqlen",
        "annotation": null
      },
      {
        "name": "stride_C_head",
        "annotation": null
      },
      {
        "name": "stride_C_dstate",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_bwd_ddAcs_prev_kernel(",
      "    dout_ptr,",
      "    prev_states_ptr,",
      "    C_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nchunks,",
      "    nheads_ngroups_ratio,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_prev_states_batch,",
      "    stride_prev_states_chunk,",
      "    stride_prev_states_head,",
      "    stride_prev_states_hdim,",
      "    stride_prev_states_dstate,",
      "    stride_C_batch,",
      "    stride_C_seqlen,",
      "    stride_C_head,",
      "    stride_C_dstate,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    prev_states_ptr += (",
      "        pid_b * stride_prev_states_batch",
      "        + pid_c * stride_prev_states_chunk",
      "        + pid_h * stride_prev_states_head",
      "    )",
      "    C_ptr += (",
      "        pid_b * stride_C_batch",
      "        + pid_c * chunk_size * stride_C_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_C_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_seqlen + offs_k[None, :] * stride_dout_hdim",
      "    )",
      "    prev_states_ptrs = prev_states_ptr + (",
      "        offs_n[None, :] * stride_prev_states_dstate",
      "        + offs_k[:, None] * stride_prev_states_hdim",
      "    )",
      "    C_ptrs = C_ptr + (",
      "        offs_m[:, None] * stride_C_seqlen + offs_n[None, :] * stride_C_dstate",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    dout = tl.load(",
      "        dout_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "        other=0.0,",
      "    )",
      "    prev_states = tl.load(",
      "        prev_states_ptrs,",
      "        mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    )",
      "    prev_states = prev_states.to(dout_ptrs.dtype.element_ty)",
      "    acc = tl.dot(dout, prev_states)",
      "    c = tl.load(",
      "        C_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddA_cs = tl.sum(acc * c, axis=1)",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_m)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_prev = tl.load(",
      "            seq_idx_ptr - stride_seq_idx_seqlen, mask=pid_c >= 1, other=0",
      "        )",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_prev, tl.exp(dA_cs_m), 0.0)",
      "    ddA_cs *= scale",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/46.py"
  },
  {
    "name": "_chunk_cumsum_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}), triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64})], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "dt_out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_out_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_fwd_kernel(",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    dt_out_ptr,",
      "    dA_cumsum_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_dt_out_batch,",
      "    stride_dt_out_chunk,",
      "    stride_dt_out_head,",
      "    stride_dt_out_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk",
      "    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    dt_out_ptrs = dt_out_ptr + (",
      "        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize",
      "    )",
      "    dA_cs_ptrs = dA_cumsum_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize",
      "    )",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt = softplus(dt)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    tl.store(",
      "        dt_out_ptrs,",
      "        dt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    dA = dt * A[:, None]",
      "    dA_cs = tl.cumsum(dA, axis=1)",
      "    tl.store(",
      "        dA_cs_ptrs,",
      "        dA_cs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_cumsum_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 2}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 4}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 8}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 16}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 32}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr'])), triton.Config({'BLOCK_SIZE_H': 64}, pre_hook=init_to_zero(['dA_ptr', 'ddt_bias_ptr']))], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "ddA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_out_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dA_ptr",
        "annotation": null
      },
      {
        "name": "ddt_bias_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_ddA_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_csize",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_dA_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_bias_head",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_bwd_kernel(",
      "    ddA_ptr,",
      "    ddt_out_ptr,",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    ddt_ptr,",
      "    dA_ptr,",
      "    ddt_bias_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_ddA_batch,",
      "    stride_ddA_chunk,",
      "    stride_ddA_head,",
      "    stride_ddA_csize,",
      "    stride_ddt_out_batch,",
      "    stride_ddt_out_chunk,",
      "    stride_ddt_out_head,",
      "    stride_ddt_out_csize,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_ddt_batch,",
      "    stride_ddt_seqlen,",
      "    stride_ddt_head,",
      "    stride_dA_head,",
      "    stride_ddt_bias_head,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    ddt_out_ptr += pid_b * stride_ddt_out_batch + pid_c * stride_ddt_out_chunk",
      "    ddA_ptr += pid_b * stride_ddA_batch + pid_c * stride_ddA_chunk",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    ddt_ptr += pid_b * stride_ddt_batch + pid_c * chunk_size * stride_ddt_seqlen",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    ddt_out_ptrs = ddt_out_ptr + (",
      "        offs_h[:, None] * stride_ddt_out_head + offs_c[None, :] * stride_ddt_out_csize",
      "    )",
      "    ddA_ptrs = ddA_ptr + (",
      "        offs_h[:, None] * stride_ddA_head + offs_c[None, :] * stride_ddA_csize",
      "    )",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    ddt_ptrs = ddt_ptr + (",
      "        offs_h[:, None] * stride_ddt_head + offs_c[None, :] * stride_ddt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    ddA = tl.load(",
      "        ddA_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt_out = tl.load(",
      "        ddt_out_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    ddt = ddA * A[:, None] + ddt_out",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt_presoftplus = dt",
      "        dt = softplus(dt)",
      "    clamp_mask = (dt < dt_min) | (dt > dt_max)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    ddt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), ddt, 0.0",
      "    )",
      "    ddt = tl.where(clamp_mask, 0.0, ddt)",
      "    if DT_SOFTPLUS:",
      "        ddt = tl.where(dt_presoftplus <= 20.0, ddt * tl.sigmoid(dt_presoftplus), ddt)",
      "    tl.store(",
      "        ddt_ptrs,",
      "        ddt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "    )",
      "    dA = tl.sum(ddA * dt, axis=1)",
      "    tl.atomic_add(dA_ptr + offs_h * stride_dA_head, dA, mask=offs_h < nheads)",
      "    if HAS_DT_BIAS:",
      "        ddt_bias = tl.sum(ddt, axis=1)",
      "        tl.atomic_add(",
      "            ddt_bias_ptr + offs_h * stride_ddt_bias_head, ddt_bias, mask=offs_h < nheads",
      "        )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_state_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "REVERSE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_fwd_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    states_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    REVERSE: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    if not REVERSE:",
      "        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "            tl.float32",
      "        )",
      "    else:",
      "        dA_cs_last = tl.load(dA_cumsum_ptr).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptrs = seq_idx_ptr + offs_k * stride_seq_idx_seqlen",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_k[None, :] < chunk_size_limit - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(",
      "            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_k = tl.load(",
      "                seq_idx_ptrs, mask=offs_k < chunk_size_limit - k, other=-1",
      "            )",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp((dA_cs_last - dA_cs_k)) * dt_k",
      "        else:",
      "            scale = tl.where(",
      "                seq_idx_k == seq_idx_last, tl.exp((dA_cs_last - dA_cs_k)) * dt_k, 0.0",
      "            )",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_ptrs += BLOCK_SIZE_K * stride_seq_idx_seqlen",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += (",
      "        pid_b * stride_states_batch",
      "        + pid_c * stride_states_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (",
      "        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr', 'ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "        tl.float32",
      "    )",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    acc *= tl.exp(dA_cs_last - dA_cs_m)[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)",
      "    ddA_cs = -(ddt * dt_m)",
      "    ddA_cs_last = -tl.sum(ddA_cs)",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "    tl.atomic_add(ddA_cumsum_ptrs, ddA_cs, mask=offs_m < chunk_size)",
      "    tl.atomic_add(ddA_cumsum_ptr + (chunk_size - 1) * stride_ddA_cs_csize, ddA_cs_last)",
      "",
      "    dx = (acc * dt_m[:, None]).to(dx_ptr.dtype.element_ty)",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_state_bwd_db_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'dstate', 'hdim'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "nheads_per_program",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_split",
        "annotation": null
      },
      {
        "name": "stride_db_group",
        "annotation": null
      },
      {
        "name": "stride_db_dstate",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_DDA_CS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_db_kernel(",
      "    x_ptr,",
      "    dstates_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    db_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    dstate,",
      "    hdim,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    nheads_per_program,",
      "    ngroups,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_split,",
      "    stride_db_group,",
      "    stride_db_dstate,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_DDA_CS: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_sg = tl.program_id(axis=2)",
      "    pid_s = pid_sg // ngroups",
      "    pid_g = pid_sg - pid_s * ngroups",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_x_head",
      "    )",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_g * stride_db_group",
      "        + pid_s * stride_db_split",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "        * stride_states_head",
      "    )",
      "    dt_ptr += (",
      "        pid_b * stride_dt_batch",
      "        + pid_c * stride_dt_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program) * stride_dA_cs_head",
      "    )",
      "    if HAS_DDA_CS:",
      "        b_ptr += (",
      "            pid_b * stride_b_batch",
      "            + pid_c * chunk_size * stride_b_seqlen",
      "            + pid_g * stride_b_head",
      "        )",
      "        ddA_cumsum_ptr += (",
      "            pid_b * stride_ddA_cs_batch",
      "            + pid_c * stride_ddA_cs_chunk",
      "            + (pid_g * (nheads // ngroups) + pid_s * nheads_per_program)",
      "            * stride_ddA_cs_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_k[None, :] * stride_x_hdim",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_dstate + offs_k[:, None] * stride_states_hdim",
      "    )",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_m * stride_dA_cs_csize",
      "    if HAS_DDA_CS:",
      "        b_ptrs = b_ptr + (",
      "            offs_m[:, None] * stride_b_seqlen + offs_n[None, :] * stride_b_dstate",
      "        )",
      "        ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    if HAS_DDA_CS:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "    nheads_iter = min(",
      "        nheads_per_program, nheads // ngroups - pid_s * nheads_per_program",
      "    )",
      "    for h in range(nheads_iter):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(x_ptrs.dtype.element_ty)",
      "        db = tl.dot(x, dstates)",
      "        dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "            tl.float32",
      "        )",
      "        dA_cs_m = tl.load(dA_cumsum_ptrs, mask=offs_m < chunk_size, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "        if not HAS_SEQ_IDX:",
      "            scale = tl.exp(dA_cs_last - dA_cs_m)",
      "        else:",
      "            scale = tl.where(",
      "                seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0",
      "            )",
      "        db *= (scale * dt_m)[:, None]",
      "        if HAS_DDA_CS:",
      "",
      "            ddA_cs = tl.sum(db * b, axis=1)",
      "            tl.atomic_add(",
      "                ddA_cumsum_ptrs + stride_ddA_cs_csize,",
      "                ddA_cs,",
      "                mask=offs_m < chunk_size - 1,",
      "            )",
      "        acc += db",
      "        x_ptrs += stride_x_head",
      "        dstates_ptrs += stride_states_head",
      "        dt_ptrs += stride_dt_head",
      "        dA_cumsum_ptr += stride_dA_cs_head",
      "        dA_cumsum_ptrs += stride_dA_cs_head",
      "        if HAS_DDA_CS:",
      "            ddA_cumsum_ptrs += stride_ddA_cs_head",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_dstate",
      "    )",
      "    tl.store(",
      "        db_ptrs,",
      "        acc,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < dstate),",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_state_bwd_ddAcs_stable_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=3, num_warps=4, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr'])), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=8, pre_hook=init_to_zero(['ddA_cumsum_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_csize",
        "annotation": null
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_bwd_ddAcs_stable_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    ddA_cumsum_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_ddA_cs_csize,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_states_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddA_cumsum_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_c * stride_ddA_cs_chunk",
      "        + pid_h * stride_ddA_cs_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    offs_k = tl.arange(",
      "        0, BLOCK_SIZE_DSTATE if BLOCK_SIZE_DSTATE <= 128 else BLOCK_SIZE_K",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_k[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_states_hdim + offs_k[:, None] * stride_states_dstate",
      "    )",
      "    if BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_k[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_k[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates)",
      "    else:",
      "        acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_k[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_k[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_states_dstate",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize, mask=offs_m < chunk_size, other=0.0",
      "    ).to(tl.float32)",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_last - dA_cs_m)",
      "    else:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)",
      "    acc *= scale[:, None]",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size, other=0.0).to(tl.float32)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "",
      "    ddA_cs = ddt * dt_m",
      "    ddA_cumsum_ptrs = ddA_cumsum_ptr + offs_m * stride_ddA_cs_csize",
      "",
      "    tl.atomic_add(",
      "        ddA_cumsum_ptrs + stride_ddA_cs_csize, ddA_cs, mask=offs_m < chunk_size - 1",
      "    )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_state_varlen_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['hdim', 'dstate', 'chunk_size'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "chunk_states_ptr",
        "annotation": null
      },
      {
        "name": "cu_seqlens_ptr",
        "annotation": null
      },
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_head",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_chunk_states_dstate",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_hdim",
        "annotation": null
      },
      {
        "name": "stride_states_dstate",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_state_varlen_kernel(",
      "    x_ptr,",
      "    b_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    chunk_states_ptr,",
      "    cu_seqlens_ptr,",
      "    states_ptr,",
      "    hdim,",
      "    dstate,",
      "    chunk_size,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_chunk_states_chunk,",
      "    stride_chunk_states_head,",
      "    stride_chunk_states_hdim,",
      "    stride_chunk_states_dstate,",
      "    stride_states_batch,",
      "    stride_states_head,",
      "    stride_states_hdim,",
      "    stride_states_dstate,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(dstate, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    end_idx = tl.load(cu_seqlens_ptr + pid_b + 1)",
      "    pid_c = (end_idx - 1) // chunk_size",
      "    b_ptr += (",
      "        pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    x_ptr += pid_c * chunk_size * stride_x_seqlen + pid_h * stride_x_head",
      "    dt_ptr += pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    dA_cumsum_ptr += pid_c * stride_dA_cs_chunk + pid_h * stride_dA_cs_head",
      "    chunk_states_ptr += (",
      "        pid_c * stride_chunk_states_chunk + pid_h * stride_chunk_states_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_hdim + offs_k[None, :] * stride_x_seqlen",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_n[None, :] * stride_b_dstate + offs_k[:, None] * stride_b_seqlen",
      "    )",
      "    dt_ptrs = dt_ptr + offs_k * stride_dt_csize",
      "    dA_cs_last = tl.load(",
      "        dA_cumsum_ptr + (end_idx - pid_c * chunk_size - 1) * stride_dA_cs_csize",
      "    ).to(tl.float32)",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "",
      "    chunk_size_limit = end_idx - pid_c * chunk_size",
      "    start_idx = tl.load(cu_seqlens_ptr + pid_b)",
      "    start_idx_cur = tl.maximum(start_idx - pid_c * chunk_size, 0)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, chunk_size_limit, BLOCK_SIZE_K):",
      "        x = tl.load(",
      "            x_ptrs,",
      "            mask=(offs_m[:, None] < hdim)",
      "            & (offs_k[None, :] < chunk_size_limit - k)",
      "            & (offs_k[None, :] >= start_idx_cur - k),",
      "            other=0.0,",
      "        )",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < chunk_size_limit - k)",
      "            & (offs_n[None, :] < dstate)",
      "            & (offs_k[:, None] >= start_idx_cur - k),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        dA_cs_k = tl.load(",
      "            dA_cumsum_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0",
      "        ).to(tl.float32)",
      "        dt_k = tl.load(dt_ptrs, mask=offs_k < chunk_size_limit - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        scale = tl.where(",
      "            (offs_k >= start_idx_cur - k) & (offs_k < chunk_size_limit - k),",
      "            tl.exp((dA_cs_last - dA_cs_k)) * dt_k,",
      "            0.0,",
      "        )",
      "        b *= scale[:, None]",
      "        b = b.to(x_ptr.dtype.element_ty)",
      "        acc += tl.dot(x, b)",
      "        x_ptrs += BLOCK_SIZE_K * stride_x_seqlen",
      "        b_ptrs += BLOCK_SIZE_K * stride_b_seqlen",
      "        dt_ptrs += BLOCK_SIZE_K * stride_dt_csize",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    if start_idx < pid_c * chunk_size:",
      "        chunk_states_ptrs = chunk_states_ptr + (",
      "            offs_m[:, None] * stride_chunk_states_hdim",
      "            + offs_n[None, :] * stride_chunk_states_dstate",
      "        )",
      "        chunk_states = tl.load(",
      "            chunk_states_ptrs,",
      "            mask=(offs_m[:, None] < hdim) & (offs_n[None, :] < dstate),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "",
      "        scale = tl.exp(dA_cs_last)",
      "        acc += chunk_states * scale",
      "",
      "    states = acc.to(states_ptr.dtype.element_ty)",
      "",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    states_ptrs = states_ptr + (",
      "        offs_m[:, None] * stride_states_hdim + offs_n[None, :] * stride_states_dstate",
      "    )",
      "    c_mask = (offs_m[:, None] < hdim) & (offs_n[None, :] < dstate)",
      "    tl.store(states_ptrs, states, mask=c_mask)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/47.py"
  },
  {
    "name": "_chunk_scan_chunk_state_bwd_dx_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=4, pre_hook=init_to_zero(['ddt_ptr'])), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4, pre_hook=init_to_zero(['ddt_ptr']))], key=['chunk_size', 'hdim', 'dstate'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "cb_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "D_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "dx_ptr",
        "annotation": null
      },
      {
        "name": "ddt_ptr",
        "annotation": null
      },
      {
        "name": "dD_ptr",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "hdim",
        "annotation": null
      },
      {
        "name": "dstate",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads_ngroups_ratio",
        "annotation": null
      },
      {
        "name": "stride_x_batch",
        "annotation": null
      },
      {
        "name": "stride_x_seqlen",
        "annotation": null
      },
      {
        "name": "stride_x_head",
        "annotation": null
      },
      {
        "name": "stride_x_hdim",
        "annotation": null
      },
      {
        "name": "stride_cb_batch",
        "annotation": null
      },
      {
        "name": "stride_cb_chunk",
        "annotation": null
      },
      {
        "name": "stride_cb_head",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_m",
        "annotation": null
      },
      {
        "name": "stride_cb_csize_k",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_hdim",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_dt_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_D_head",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_b_dstate",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_hdim",
        "annotation": null
      },
      {
        "name": "stride_dstates_dstate",
        "annotation": null
      },
      {
        "name": "stride_dx_batch",
        "annotation": null
      },
      {
        "name": "stride_dx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dx_head",
        "annotation": null
      },
      {
        "name": "stride_dx_hdim",
        "annotation": null
      },
      {
        "name": "stride_ddt_batch",
        "annotation": null
      },
      {
        "name": "stride_ddt_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddt_head",
        "annotation": null
      },
      {
        "name": "stride_ddt_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_batch",
        "annotation": null
      },
      {
        "name": "stride_dD_chunk",
        "annotation": null
      },
      {
        "name": "stride_dD_head",
        "annotation": null
      },
      {
        "name": "stride_dD_csize",
        "annotation": null
      },
      {
        "name": "stride_dD_hdim",
        "annotation": null
      },
      {
        "name": "HAS_D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D_HAS_HDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_DSTATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_TRITON_22",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_scan_chunk_state_bwd_dx_kernel(",
      "    x_ptr,",
      "    cb_ptr,",
      "    dout_ptr,",
      "    dt_ptr,",
      "    dA_cumsum_ptr,",
      "    seq_idx_ptr,",
      "    D_ptr,",
      "    b_ptr,",
      "    dstates_ptr,",
      "    dx_ptr,",
      "    ddt_ptr,",
      "    dD_ptr,",
      "    chunk_size,",
      "    hdim,",
      "    dstate,",
      "    batch,",
      "    seqlen,",
      "    nheads_ngroups_ratio,",
      "    stride_x_batch,",
      "    stride_x_seqlen,",
      "    stride_x_head,",
      "    stride_x_hdim,",
      "    stride_cb_batch,",
      "    stride_cb_chunk,",
      "    stride_cb_head,",
      "    stride_cb_csize_m,",
      "    stride_cb_csize_k,",
      "    stride_dout_batch,",
      "    stride_dout_seqlen,",
      "    stride_dout_head,",
      "    stride_dout_hdim,",
      "    stride_dt_batch,",
      "    stride_dt_chunk,",
      "    stride_dt_head,",
      "    stride_dt_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_D_head,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_b_dstate,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_dstates_head,",
      "    stride_dstates_hdim,",
      "    stride_dstates_dstate,",
      "    stride_dx_batch,",
      "    stride_dx_seqlen,",
      "    stride_dx_head,",
      "    stride_dx_hdim,",
      "    stride_ddt_batch,",
      "    stride_ddt_chunk,",
      "    stride_ddt_head,",
      "    stride_ddt_csize,",
      "    stride_dD_batch,",
      "    stride_dD_chunk,",
      "    stride_dD_head,",
      "    stride_dD_csize,",
      "    stride_dD_hdim,",
      "    HAS_D: tl.constexpr,",
      "    D_HAS_HDIM: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_DSTATE: tl.constexpr,",
      "    IS_TRITON_22: tl.constexpr,",
      "):",
      "    pid_bc = tl.program_id(axis=1)",
      "    pid_c = pid_bc // batch",
      "    pid_b = pid_bc - pid_c * batch",
      "    pid_h = tl.program_id(axis=2)",
      "    num_pid_n = tl.cdiv(hdim, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    x_ptr += (",
      "        pid_b * stride_x_batch",
      "        + pid_c * chunk_size * stride_x_seqlen",
      "        + pid_h * stride_x_head",
      "    )",
      "    cb_ptr += (",
      "        pid_b * stride_cb_batch",
      "        + pid_c * stride_cb_chunk",
      "        + (pid_h // nheads_ngroups_ratio) * stride_cb_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_c * chunk_size * stride_dout_seqlen",
      "        + pid_h * stride_dout_head",
      "    )",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * stride_dt_chunk + pid_h * stride_dt_head",
      "    ddt_ptr += (",
      "        pid_b * stride_ddt_batch + pid_c * stride_ddt_chunk + pid_h * stride_ddt_head",
      "    )",
      "    dA_cumsum_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_c * stride_dA_cs_chunk",
      "        + pid_h * stride_dA_cs_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + (pid_h // nheads_ngroups_ratio) * stride_b_head",
      "    )",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_c * stride_dstates_chunk",
      "        + pid_h * stride_dstates_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    dA_cs_m = tl.load(",
      "        dA_cumsum_ptr + offs_m * stride_dA_cs_csize,",
      "        mask=offs_m < chunk_size_limit,",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "",
      "    dA_cs_last = tl.load(dA_cumsum_ptr + (chunk_size - 1) * stride_dA_cs_csize).to(",
      "        tl.float32",
      "    )",
      "    if not HAS_SEQ_IDX:",
      "        scale = tl.exp(dA_cs_last - dA_cs_m)",
      "    else:",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_last = tl.load(",
      "            seq_idx_ptr + (chunk_size_limit - 1) * stride_seq_idx_seqlen",
      "        )",
      "        scale = tl.where(seq_idx_m == seq_idx_last, tl.exp(dA_cs_last - dA_cs_m), 0.0)",
      "",
      "    offs_dstate = tl.arange(",
      "        0,",
      "        (",
      "            BLOCK_SIZE_DSTATE",
      "            if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128",
      "            else BLOCK_SIZE_K",
      "        ),",
      "    )",
      "    b_ptrs = b_ptr + (",
      "        offs_m[:, None] * stride_b_seqlen + offs_dstate[None, :] * stride_b_dstate",
      "    )",
      "    dstates_ptrs = dstates_ptr + (",
      "        offs_n[None, :] * stride_dstates_hdim",
      "        + offs_dstate[:, None] * stride_dstates_dstate",
      "    )",
      "    if IS_TRITON_22 and BLOCK_SIZE_DSTATE <= 128:",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_dstate[None, :] < dstate),",
      "            other=0.0,",
      "        )",
      "        dstates = tl.load(",
      "            dstates_ptrs,",
      "            mask=(offs_dstate[:, None] < dstate) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dstates = dstates.to(b_ptr.dtype.element_ty)",
      "        acc = tl.dot(b, dstates) * scale[:, None]",
      "    else:",
      "        for k in range(0, dstate, BLOCK_SIZE_K):",
      "            b = tl.load(",
      "                b_ptrs,",
      "                mask=(offs_m[:, None] < chunk_size_limit)",
      "                & (offs_dstate[None, :] < dstate - k),",
      "                other=0.0,",
      "            )",
      "            dstates = tl.load(",
      "                dstates_ptrs,",
      "                mask=(offs_dstate[:, None] < dstate - k) & (offs_n[None, :] < hdim),",
      "                other=0.0,",
      "            )",
      "            dstates = dstates.to(b_ptr.dtype.element_ty)",
      "            acc += tl.dot(b, dstates)",
      "            b_ptrs += BLOCK_SIZE_K * stride_b_dstate",
      "            dstates_ptrs += BLOCK_SIZE_K * stride_dstates_dstate",
      "        acc *= scale[:, None]",
      "",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    cb_ptrs = cb_ptr + (",
      "        offs_m[:, None] * stride_cb_csize_m + offs_k[None, :] * stride_cb_csize_k",
      "    )",
      "    dout_ptrs = dout_ptr + (",
      "        offs_k[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "    )",
      "    dA_cumsum_ptrs = dA_cumsum_ptr + offs_k * stride_dA_cs_csize",
      "    K_MAX = chunk_size_limit",
      "    K_MIN = pid_m * BLOCK_SIZE_M",
      "    cb_ptrs += K_MIN * stride_cb_csize_k",
      "    dout_ptrs += K_MIN * stride_dout_seqlen",
      "    dA_cumsum_ptrs += K_MIN * stride_dA_cs_csize",
      "    for k in range(K_MIN, K_MAX, BLOCK_SIZE_K):",
      "        k = tl.multiple_of(k, BLOCK_SIZE_K)",
      "",
      "        cb = tl.load(",
      "            cb_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size) & (offs_k[None, :] < K_MAX - k),",
      "            other=0.0,",
      "        )",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_k[:, None] < K_MAX - k) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        )",
      "        dA_cs_k = tl.load(dA_cumsum_ptrs, mask=offs_k < K_MAX - k, other=0.0).to(",
      "            tl.float32",
      "        )",
      "        cb *= tl.exp(dA_cs_k[None, :] - dA_cs_m[:, None])",
      "",
      "        mask = (k + offs_k[None, :] >= offs_m[:, None]) & (k + offs_k[None, :] < K_MAX)",
      "        cb = tl.where(mask, cb, 0.0)",
      "        cb = cb.to(dout_ptr.dtype.element_ty)",
      "        acc += tl.dot(cb, dout)",
      "        cb_ptrs += BLOCK_SIZE_K * stride_cb_csize_k",
      "        dout_ptrs += BLOCK_SIZE_K * stride_dout_seqlen",
      "        dA_cumsum_ptrs += BLOCK_SIZE_K * stride_dA_cs_csize",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    dt_ptrs = dt_ptr + offs_m * stride_dt_csize",
      "    dt_m = tl.load(dt_ptrs, mask=offs_m < chunk_size_limit, other=0.0).to(tl.float32)",
      "    dx = acc * dt_m[:, None]",
      "    dx_ptr += (",
      "        pid_b * stride_dx_batch",
      "        + pid_c * chunk_size * stride_dx_seqlen",
      "        + pid_h * stride_dx_head",
      "    )",
      "    dx_ptrs = dx_ptr + (",
      "        offs_m[:, None] * stride_dx_seqlen + offs_n[None, :] * stride_dx_hdim",
      "    )",
      "    if HAS_D:",
      "        dout_res_ptrs = dout_ptr + (",
      "            offs_m[:, None] * stride_dout_seqlen + offs_n[None, :] * stride_dout_hdim",
      "        )",
      "        dout_res = tl.load(",
      "            dout_res_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "        if D_HAS_HDIM:",
      "            D = tl.load(",
      "                D_ptr + pid_h * stride_D_head + offs_n, mask=offs_n < hdim, other=0.0",
      "            ).to(tl.float32)",
      "        else:",
      "            D = tl.load(D_ptr + pid_h * stride_D_head).to(tl.float32)",
      "        dx += dout_res * D",
      "    tl.store(",
      "        dx_ptrs,",
      "        dx,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "    )",
      "",
      "    x_ptrs = x_ptr + (",
      "        offs_m[:, None] * stride_x_seqlen + offs_n[None, :] * stride_x_hdim",
      "    )",
      "    x = tl.load(",
      "        x_ptrs,",
      "        mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < hdim),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_D:",
      "        dD_ptr += (",
      "            pid_b * stride_dD_batch",
      "            + pid_c * stride_dD_chunk",
      "            + pid_h * stride_dD_head",
      "            + pid_m * stride_dD_csize",
      "        )",
      "        if D_HAS_HDIM:",
      "            dD_ptrs = dD_ptr + offs_n * stride_dD_hdim",
      "            dD = tl.sum(dout_res * x, axis=0)",
      "            tl.store(dD_ptrs, dD, mask=offs_n < hdim)",
      "        else:",
      "            dD = tl.sum(dout_res * x)",
      "            tl.store(dD_ptr, dD)",
      "    ddt = tl.sum(acc * x, axis=1)",
      "    ddt_ptrs = ddt_ptr + offs_m * stride_ddt_csize",
      "    tl.atomic_add(ddt_ptrs, ddt, mask=offs_m < chunk_size)"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/48.py"
  },
  {
    "name": "_state_passing_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "states_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "final_states_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "initstates_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_states_batch",
        "annotation": null
      },
      {
        "name": "stride_states_chunk",
        "annotation": null
      },
      {
        "name": "stride_states_head",
        "annotation": null
      },
      {
        "name": "stride_states_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_final_states_batch",
        "annotation": null
      },
      {
        "name": "stride_final_states_head",
        "annotation": null
      },
      {
        "name": "stride_final_states_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_initstates_batch",
        "annotation": null
      },
      {
        "name": "stride_initstates_head",
        "annotation": null
      },
      {
        "name": "stride_initstates_dim",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "HAS_INITSTATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_fwd_kernel(",
      "    states_ptr,",
      "    out_ptr,",
      "    final_states_ptr,",
      "    dA_cs_ptr,",
      "    initstates_ptr,",
      "    seq_idx_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_states_batch,",
      "    stride_states_chunk,",
      "    stride_states_head,",
      "    stride_states_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_final_states_batch,",
      "    stride_final_states_head,",
      "    stride_final_states_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_initstates_batch,",
      "    stride_initstates_head,",
      "    stride_initstates_dim,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    HAS_INITSTATES: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    states_ptr += pid_b * stride_states_batch + pid_h * stride_states_head",
      "    dA_cs_ptr += pid_b * stride_dA_cs_batch + pid_h * stride_dA_cs_head",
      "    out_ptr += pid_b * stride_out_batch + pid_h * stride_out_head",
      "    final_states_ptr += (",
      "        pid_b * stride_final_states_batch + pid_h * stride_final_states_head",
      "    )",
      "    if HAS_INITSTATES:",
      "        initstates_ptr += (",
      "            pid_b * stride_initstates_batch + pid_h * stride_initstates_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    states_ptrs = states_ptr + offs_m * stride_states_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    final_states_ptrs = final_states_ptr + offs_m * stride_final_states_dim",
      "",
      "    if not HAS_INITSTATES:",
      "        states = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "    else:",
      "        initstates_ptrs = initstates_ptr + offs_m * stride_initstates_dim",
      "        states = tl.load(initstates_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "    tl.store(out_ptrs, states, mask=offs_m < dim)",
      "    out_ptrs += stride_out_chunk",
      "    seq_idx = 0",
      "    for c in range(nchunks):",
      "        new_states = tl.load(states_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_new = tl.load(",
      "                seq_idx_ptr",
      "                + (min((c + 1) * chunk_size, seqlen) - 1) * stride_seq_idx_seqlen",
      "            )",
      "            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)",
      "            seq_idx = seq_idx_new",
      "        states = scale * states + new_states",
      "        if c < nchunks - 1:",
      "            tl.store(out_ptrs, states, mask=offs_m < dim)",
      "        else:",
      "            tl.store(final_states_ptrs, states, mask=offs_m < dim)",
      "        states_ptrs += stride_states_chunk",
      "        dA_cs_ptr += stride_dA_cs_chunk",
      "        out_ptrs += stride_out_chunk"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/49.py"
  },
  {
    "name": "_state_passing_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 64}), triton.Config({'BLOCK_SIZE': 128}), triton.Config({'BLOCK_SIZE': 256}), triton.Config({'BLOCK_SIZE': 512}), triton.Config({'BLOCK_SIZE': 1024}), triton.Config({'BLOCK_SIZE': 2048})], key=['dim'])"
    ],
    "args": [
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dfinal_states_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "dstates_ptr",
        "annotation": null
      },
      {
        "name": "ddA_cs_ptr",
        "annotation": null
      },
      {
        "name": "dinitstates_ptr",
        "annotation": null
      },
      {
        "name": "states_converted_ptr",
        "annotation": null
      },
      {
        "name": "dim",
        "annotation": null
      },
      {
        "name": "nchunks",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_dim",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_out_dim",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dfinal_states_batch",
        "annotation": null
      },
      {
        "name": "stride_dfinal_states_head",
        "annotation": null
      },
      {
        "name": "stride_dfinal_states_dim",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dstates_chunk",
        "annotation": null
      },
      {
        "name": "stride_dstates_head",
        "annotation": null
      },
      {
        "name": "stride_dstates_dim",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_ddA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dinitstates_batch",
        "annotation": null
      },
      {
        "name": "stride_dinitstates_head",
        "annotation": null
      },
      {
        "name": "stride_dinitstates_dim",
        "annotation": null
      },
      {
        "name": "CONVERT_STATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DFINAL_STATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DINITSTATES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _state_passing_bwd_kernel(",
      "    dout_ptr,",
      "    out_ptr,",
      "    dA_cs_ptr,",
      "    dfinal_states_ptr,",
      "    seq_idx_ptr,",
      "    dstates_ptr,",
      "    ddA_cs_ptr,",
      "    dinitstates_ptr,",
      "    states_converted_ptr,",
      "    dim,",
      "    nchunks,",
      "    seqlen,",
      "    chunk_size,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_dim,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_out_dim,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dfinal_states_batch,",
      "    stride_dfinal_states_head,",
      "    stride_dfinal_states_dim,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    stride_dstates_batch,",
      "    stride_dstates_chunk,",
      "    stride_dstates_head,",
      "    stride_dstates_dim,",
      "    stride_ddA_cs_batch,",
      "    stride_ddA_cs_chunk,",
      "    stride_ddA_cs_head,",
      "    stride_dinitstates_batch,",
      "    stride_dinitstates_head,",
      "    stride_dinitstates_dim,",
      "    CONVERT_STATES: tl.constexpr,",
      "    HAS_DFINAL_STATES: tl.constexpr,",
      "    HAS_DINITSTATES: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    pid_m = tl.program_id(axis=0)",
      "    dstates_ptr += (",
      "        pid_b * stride_dstates_batch",
      "        + pid_h * stride_dstates_head",
      "        + (nchunks - 1) * stride_dstates_chunk",
      "    )",
      "    dA_cs_ptr += (",
      "        pid_b * stride_dA_cs_batch",
      "        + pid_h * stride_dA_cs_head",
      "        + (nchunks - 1) * stride_dA_cs_chunk",
      "    )",
      "    ddA_cs_ptr += (",
      "        pid_b * stride_ddA_cs_batch",
      "        + pid_h * stride_ddA_cs_head",
      "        + (nchunks - 1) * stride_ddA_cs_chunk",
      "        + pid_m",
      "    )",
      "    out_ptr += (",
      "        pid_b * stride_out_batch",
      "        + pid_h * stride_out_head",
      "        + (nchunks - 1) * stride_out_chunk",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch",
      "        + pid_h * stride_dout_head",
      "        + (nchunks - 1) * stride_dout_chunk",
      "    )",
      "    if CONVERT_STATES:",
      "        states_converted_ptr += (",
      "            pid_b * stride_out_batch",
      "            + pid_h * stride_out_head",
      "            + (nchunks - 1) * stride_out_chunk",
      "        )",
      "    if HAS_DFINAL_STATES:",
      "        dfinal_states_ptr += (",
      "            pid_b * stride_dfinal_states_batch + pid_h * stride_dfinal_states_head",
      "        )",
      "    if HAS_DINITSTATES:",
      "        dinitstates_ptr += (",
      "            pid_b * stride_dinitstates_batch + pid_h * stride_dinitstates_head",
      "        )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += pid_b * stride_seq_idx_batch",
      "",
      "    offs_m = pid_m * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    dstates_ptrs = dstates_ptr + offs_m * stride_dstates_dim",
      "    out_ptrs = out_ptr + offs_m * stride_out_dim",
      "    dout_ptrs = dout_ptr + offs_m * stride_dout_dim",
      "    if CONVERT_STATES:",
      "        states_converted_ptrs = states_converted_ptr + offs_m * stride_out_dim",
      "",
      "    if HAS_DFINAL_STATES:",
      "        dstates = tl.load(",
      "            dfinal_states_ptr + offs_m * stride_dfinal_states_dim,",
      "            mask=offs_m < dim,",
      "            other=0.0,",
      "        ).to(tl.float32)",
      "    else:",
      "        dstates = tl.zeros((BLOCK_SIZE,), dtype=tl.float32)",
      "    tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "    if HAS_SEQ_IDX:",
      "        seq_idx = tl.load(seq_idx_ptr + (seqlen - 1) * stride_seq_idx_seqlen)",
      "    dstates_ptrs -= stride_dstates_chunk",
      "    for c in range(nchunks - 1):",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        if HAS_SEQ_IDX:",
      "            seq_idx_new = tl.load(",
      "                seq_idx_ptr",
      "                + (((nchunks - c - 1) * chunk_size - 1) * stride_seq_idx_seqlen)",
      "            )",
      "            scale = tl.where(seq_idx_new == seq_idx, scale, 0.0)",
      "            seq_idx = seq_idx_new",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        if CONVERT_STATES:",
      "            tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "        ddA = tl.sum(out * dstates) * scale",
      "        tl.store(ddA_cs_ptr, ddA)",
      "        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dstates = scale * dstates + dout",
      "        tl.store(dstates_ptrs, dstates, mask=offs_m < dim)",
      "        dout_ptrs -= stride_dout_chunk",
      "        dstates_ptrs -= stride_dstates_chunk",
      "        dA_cs_ptr -= stride_dA_cs_chunk",
      "        ddA_cs_ptr -= stride_ddA_cs_chunk",
      "        out_ptrs -= stride_out_chunk",
      "        if CONVERT_STATES:",
      "            states_converted_ptrs -= stride_out_chunk",
      "    if CONVERT_STATES:",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        tl.store(states_converted_ptrs, out, mask=offs_m < dim)",
      "    if not HAS_DINITSTATES:",
      "        tl.store(ddA_cs_ptr, 0.0)",
      "    else:",
      "        dA_cs = tl.load(dA_cs_ptr).to(tl.float32)",
      "        scale = tl.exp(dA_cs)",
      "        if HAS_SEQ_IDX:",
      "            scale = tl.where(seq_idx == 0, scale, 0.0)",
      "        out = tl.load(out_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        ddA = tl.sum(out * dstates) * scale",
      "        tl.store(ddA_cs_ptr, ddA)",
      "        dout = tl.load(dout_ptrs, mask=offs_m < dim, other=0.0).to(tl.float32)",
      "        dstates = scale * dstates + dout",
      "        tl.store(",
      "            dinitstates_ptr + offs_m * stride_dinitstates_dim,",
      "            dstates,",
      "            mask=offs_m < dim,",
      "        )"
    ],
    "file": "triton_repos/Hprairie_Bi-Mamba2/src/ssd/uni/49.py"
  },
  {
    "name": "tuned_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 0, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 1, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 2, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 3, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 4, 'pre_load_v': True}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 0, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 1, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 2, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 3, 'pre_load_v': False}, num_stages=1, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'waves_per_eu': 4, 'pre_load_v': False}, num_stages=1, num_warps=4)], key=['seqlen_q', 'seqlen_k', 'STAGE'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "encoded_softmax",
        "annotation": null
      },
      {
        "name": "STAGE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "pre_load_v",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RETURN_ENCODED_SOFTMAX",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    encoded_softmax,",
      "    STAGE: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    pre_load_v: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "):",
      "    bare_attn_fwd(",
      "        Q,",
      "        K,",
      "        V,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        seqlen_q,",
      "        seqlen_k,",
      "        dropout_p,",
      "        philox_seed,",
      "        philox_offset_base,",
      "        encoded_softmax,",
      "        STAGE,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        pre_load_v,",
      "        ENABLE_DROPOUT,",
      "        RETURN_ENCODED_SOFTMAX,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/test/116.py"
  },
  {
    "name": "tuned_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_FWD, key=['max_seqlen_q', 'max_seqlen_k', 'CAUSAL'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "num_head_q",
        "annotation": null
      },
      {
        "name": "num_head_k",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed_ptr",
        "annotation": null
      },
      {
        "name": "philox_offset1",
        "annotation": null
      },
      {
        "name": "philox_offset2",
        "annotation": null
      },
      {
        "name": "philox_seed_output",
        "annotation": null
      },
      {
        "name": "philox_offset_output",
        "annotation": null
      },
      {
        "name": "encoded_softmax",
        "annotation": null
      },
      {
        "name": "CAUSAL_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "Window_left",
        "annotation": null
      },
      {
        "name": "Window_right",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "pre_load_v",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RETURN_ENCODED_SOFTMAX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    num_head_q,",
      "    num_head_k,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed_ptr,",
      "    philox_offset1,",
      "    philox_offset2,",
      "    philox_seed_output,",
      "    philox_offset_output,",
      "    encoded_softmax,",
      "    CAUSAL_TYPE: tl.constexpr,",
      "    Window_left,",
      "    Window_right,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    pre_load_v: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "):",
      "    bare_attn_fwd(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        M,",
      "        Out,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_on,",
      "        num_head_q,",
      "        num_head_k,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed_ptr,",
      "        philox_offset1,",
      "        philox_offset2,",
      "        philox_seed_output,",
      "        philox_offset_output,",
      "        encoded_softmax,",
      "        CAUSAL_TYPE,",
      "        Window_left,",
      "        Window_right,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        pre_load_v,",
      "        ENABLE_DROPOUT,",
      "        RETURN_ENCODED_SOFTMAX,",
      "        PADDED_HEAD,",
      "        BIAS_TYPE=BIAS_TYPE,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/tritonsrc/117.py"
  },
  {
    "name": "tuned_attn_bwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_FUSED, key=['max_seqlen_q', 'max_seqlen_k', 'head_dim'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_ok",
        "annotation": null
      },
      {
        "name": "stride_dkz",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dkk",
        "annotation": null
      },
      {
        "name": "stride_dvz",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvk",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "stride_dqz",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dqk",
        "annotation": null
      },
      {
        "name": "stride_dbz",
        "annotation": null
      },
      {
        "name": "stride_dbh",
        "annotation": null
      },
      {
        "name": "stride_dbm",
        "annotation": null
      },
      {
        "name": "stride_dbn",
        "annotation": null
      },
      {
        "name": "num_head_q",
        "annotation": null
      },
      {
        "name": "num_head_k",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed_ptr",
        "annotation": null
      },
      {
        "name": "philox_offset1",
        "annotation": null
      },
      {
        "name": "philox_offset2",
        "annotation": null
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLK_SLICE_FACTOR",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_attn_bwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    Out,",
      "    DO,",
      "    DK,",
      "    DV,",
      "    DQ,",
      "    DB,",
      "    L,",
      "    D,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_ok,",
      "    stride_dkz,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dkk,",
      "    stride_dvz,",
      "    stride_dvh,",
      "    stride_dvk,",
      "    stride_dvn,",
      "    stride_dqz,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dqk,",
      "    stride_dbz,",
      "    stride_dbh,",
      "    stride_dbm,",
      "    stride_dbn,",
      "    num_head_q,",
      "    num_head_k,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed_ptr,",
      "    philox_offset1,",
      "    philox_offset2,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "    BLOCK_M1: tl.constexpr,",
      "    BLOCK_N1: tl.constexpr,",
      "    BLOCK_M2: tl.constexpr,",
      "    BLOCK_N2: tl.constexpr,",
      "    BLK_SLICE_FACTOR: tl.constexpr,",
      "):",
      "    bare_attn_bwd(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        Out,",
      "        DO,",
      "        DK,",
      "        DV,",
      "        DQ,",
      "        DB,",
      "        L,",
      "        D,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_ok,",
      "        stride_dkz,",
      "        stride_dkh,",
      "        stride_dkn,",
      "        stride_dkk,",
      "        stride_dvz,",
      "        stride_dvh,",
      "        stride_dvk,",
      "        stride_dvn,",
      "        stride_dqz,",
      "        stride_dqh,",
      "        stride_dqm,",
      "        stride_dqk,",
      "        stride_dbz,",
      "        stride_dbh,",
      "        stride_dbm,",
      "        stride_dbn,",
      "        num_head_q,",
      "        num_head_k,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed_ptr,",
      "        philox_offset_base,",
      "        BLOCK_DMODEL,",
      "        CAUSAL,",
      "        ENABLE_DROPOUT,",
      "        PADDED_HEAD,",
      "        BIAS_TYPE,",
      "        BLOCK_M1,",
      "        BLOCK_N1,",
      "        BLOCK_M2,",
      "        BLOCK_N2,",
      "        BLK_SLICE_FACTOR,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/tritonsrc/117.py"
  },
  {
    "name": "sized_tuned_bwd_kernel_dk_dv",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_SIZED, key=['BLOCK_DMODEL', 'max_seqlen_q', 'max_seqlen_k'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_ok",
        "annotation": null
      },
      {
        "name": "stride_dkz",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dkk",
        "annotation": null
      },
      {
        "name": "stride_dvz",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvk",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def sized_tuned_bwd_kernel_dk_dv(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    Out,",
      "    DO,",
      "    DK,",
      "    DV,",
      "    L,",
      "    D,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_ok,",
      "    stride_dkz,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dkk,",
      "    stride_dvz,",
      "    stride_dvh,",
      "    stride_dvk,",
      "    stride_dvn,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "):",
      "    bare_bwd_kernel_dk_dv(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        Out,",
      "        DO,",
      "        DK,",
      "        DV,",
      "        L,",
      "        D,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_ok,",
      "        stride_dkz,",
      "        stride_dkh,",
      "        stride_dkn,",
      "        stride_dkk,",
      "        stride_dvz,",
      "        stride_dvh,",
      "        stride_dvk,",
      "        stride_dvn,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed,",
      "        philox_offset_base,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        CAUSAL,",
      "        ENABLE_DROPOUT,",
      "        PADDED_HEAD=PADDED_HEAD,",
      "        BIAS_TYPE=BIAS_TYPE,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/tritonsrc/131.py"
  },
  {
    "name": "sized_tuned_bwd_kernel_dq",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD_SIZED, key=['BLOCK_DMODEL', 'max_seqlen_q', 'max_seqlen_k'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_ok",
        "annotation": null
      },
      {
        "name": "stride_dqz",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dqk",
        "annotation": null
      },
      {
        "name": "stride_dbz",
        "annotation": null
      },
      {
        "name": "stride_dbh",
        "annotation": null
      },
      {
        "name": "stride_dbm",
        "annotation": null
      },
      {
        "name": "stride_dbn",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def sized_tuned_bwd_kernel_dq(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    Out,",
      "    DO,",
      "    DQ,",
      "    DB,",
      "    L,",
      "    D,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_ok,",
      "    stride_dqz,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dqk,",
      "    stride_dbz,",
      "    stride_dbh,",
      "    stride_dbm,",
      "    stride_dbn,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "):",
      "    bare_bwd_kernel_dq(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        Out,",
      "        DO,",
      "        DQ,",
      "        DB,",
      "        L,",
      "        D,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_ok,",
      "        stride_dqz,",
      "        stride_dqh,",
      "        stride_dqm,",
      "        stride_dqk,",
      "        stride_dbz,",
      "        stride_dbh,",
      "        stride_dbm,",
      "        stride_dbn,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed,",
      "        philox_offset_base,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        CAUSAL,",
      "        ENABLE_DROPOUT,",
      "        PADDED_HEAD=PADDED_HEAD,",
      "        BIAS_TYPE=BIAS_TYPE,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/tritonsrc/131.py"
  },
  {
    "name": "tuned_bwd_kernel_dk_dv",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD, key=['BLOCK_DMODEL', 'max_seqlen_q', 'max_seqlen_k'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_ok",
        "annotation": null
      },
      {
        "name": "stride_dkz",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dkk",
        "annotation": null
      },
      {
        "name": "stride_dvz",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvk",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_bwd_kernel_dk_dv(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    Out,",
      "    DO,",
      "    DK,",
      "    DV,",
      "    L,",
      "    D,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_ok,",
      "    stride_dkz,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dkk,",
      "    stride_dvz,",
      "    stride_dvh,",
      "    stride_dvk,",
      "    stride_dvn,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "):",
      "    bare_bwd_kernel_dk_dv(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        Out,",
      "        DO,",
      "        DK,",
      "        DV,",
      "        L,",
      "        D,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_ok,",
      "        stride_dkz,",
      "        stride_dkh,",
      "        stride_dkn,",
      "        stride_dkk,",
      "        stride_dvz,",
      "        stride_dvh,",
      "        stride_dvk,",
      "        stride_dvn,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed,",
      "        philox_offset_base,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        CAUSAL,",
      "        ENABLE_DROPOUT,",
      "        PADDED_HEAD=PADDED_HEAD,",
      "        BIAS_TYPE=BIAS_TYPE,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/tritonsrc/132.py"
  },
  {
    "name": "tuned_bwd_kernel_dq",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=TRITON_CONFIG_LIST_BWD, key=['BLOCK_DMODEL', 'max_seqlen_q', 'max_seqlen_k'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_ok",
        "annotation": null
      },
      {
        "name": "stride_dqz",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dqk",
        "annotation": null
      },
      {
        "name": "stride_dbz",
        "annotation": null
      },
      {
        "name": "stride_dbh",
        "annotation": null
      },
      {
        "name": "stride_dbm",
        "annotation": null
      },
      {
        "name": "stride_dbn",
        "annotation": null
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "num_seqlens",
        "annotation": null
      },
      {
        "name": "max_seqlen_q",
        "annotation": null
      },
      {
        "name": "max_seqlen_k",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEAD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tuned_bwd_kernel_dq(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    sm_scale,",
      "    Out,",
      "    DO,",
      "    DQ,",
      "    DB,",
      "    L,",
      "    D,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_bz,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_ok,",
      "    stride_dqz,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dqk,",
      "    stride_dbz,",
      "    stride_dbh,",
      "    stride_dbm,",
      "    stride_dbn,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    num_seqlens,",
      "    max_seqlen_q,",
      "    max_seqlen_k,",
      "    head_dim,",
      "    dropout_p,",
      "    philox_seed,",
      "    philox_offset_base,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    PADDED_HEAD: tl.constexpr,",
      "    BIAS_TYPE: tl.constexpr,",
      "):",
      "    bare_bwd_kernel_dq(",
      "        Q,",
      "        K,",
      "        V,",
      "        B,",
      "        sm_scale,",
      "        Out,",
      "        DO,",
      "        DQ,",
      "        DB,",
      "        L,",
      "        D,",
      "        stride_qz,",
      "        stride_qh,",
      "        stride_qm,",
      "        stride_qk,",
      "        stride_kz,",
      "        stride_kh,",
      "        stride_kn,",
      "        stride_kk,",
      "        stride_vz,",
      "        stride_vh,",
      "        stride_vk,",
      "        stride_vn,",
      "        stride_bz,",
      "        stride_bh,",
      "        stride_bm,",
      "        stride_bn,",
      "        stride_oz,",
      "        stride_oh,",
      "        stride_om,",
      "        stride_ok,",
      "        stride_dqz,",
      "        stride_dqh,",
      "        stride_dqm,",
      "        stride_dqk,",
      "        stride_dbz,",
      "        stride_dbh,",
      "        stride_dbm,",
      "        stride_dbn,",
      "        cu_seqlens_q,",
      "        cu_seqlens_k,",
      "        num_seqlens,",
      "        max_seqlen_q,",
      "        max_seqlen_k,",
      "        head_dim,",
      "        dropout_p,",
      "        philox_seed,",
      "        philox_offset_base,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        CAUSAL,",
      "        ENABLE_DROPOUT,",
      "        PADDED_HEAD=PADDED_HEAD,",
      "        BIAS_TYPE=BIAS_TYPE,",
      "    )"
    ],
    "file": "triton_repos/ROCm_aotriton/tritonsrc/132.py"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args['BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args['BLOCK_N'] == 0})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "H_KV",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "ROUND_CTX",
        "annotation": null
      },
      {
        "name": "NKV_CTX",
        "annotation": null
      },
      {
        "name": "sliding_window_offset",
        "annotation": null
      },
      {
        "name": "sliding_window_size",
        "annotation": null
      },
      {
        "name": "IS_EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "END",
        "annotation": "tl.constexpr"
      },
      {
        "name": "INIT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "COMPLEMENT_SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    L,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    H_KV,",
      "    N_CTX,",
      "    ROUND_CTX,",
      "    NKV_CTX,",
      "    sliding_window_offset,",
      "    sliding_window_size,",
      "    IS_EVEN_M: tl.constexpr,",
      "    IS_EVEN_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    END: tl.constexpr,",
      "    INIT: tl.constexpr,",
      "    SLIDING_WINDOW: tl.constexpr,",
      "    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,",
      "):",
      "",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    off_hkv = off_h // (H // H_KV)",
      "    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh",
      "    v_offset = off_z.to(tl.int64) * stride_vz + off_hkv.to(tl.int64) * stride_vh",
      "    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + q_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + v_offset,",
      "        shape=(NKV_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + k_offset,",
      "        shape=(BLOCK_DMODEL, NKV_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + o_offset,",
      "        shape=(ROUND_CTX, BLOCK_DMODEL),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    m_ptrs = M + off_hz * ROUND_CTX + offs_m",
      "    l_ptrs = L + off_hz * ROUND_CTX + offs_m",
      "    if INIT:",
      "        m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "        l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0",
      "        acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "    else:",
      "",
      "        m_i = tl.load(m_ptrs).to(tl.float32)",
      "        l_i = tl.load(l_ptrs).to(tl.float32)",
      "        acc = tl.load(O_block_ptr).to(tl.float32)",
      "",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.4426950408889634",
      "",
      "    if IS_EVEN_M:",
      "        q = tl.load(Q_block_ptr)",
      "    else:",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    acc, l_i, m_i = _attn_fwd_inner(",
      "        acc,",
      "        l_i,",
      "        m_i,",
      "        q,",
      "        K_block_ptr,",
      "        V_block_ptr,",
      "        start_m,",
      "        qk_scale,",
      "        NKV_CTX,",
      "        sliding_window_offset,",
      "        sliding_window_size,",
      "        BLOCK_M,",
      "        BLOCK_DMODEL,",
      "        BLOCK_N,",
      "        SLIDING_WINDOW,",
      "        IS_EVEN_M,",
      "        IS_EVEN_N,",
      "        COMPLEMENT_SLIDING_WINDOW,",
      "    )",
      "",
      "    if END:",
      "        m_i += tl.math.log2(l_i)",
      "        acc = acc / l_i[:, None]",
      "    else:",
      "        tl.store(l_ptrs, l_i)",
      "",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/480.py"
  },
  {
    "name": "_score_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'IS_EVEN_M': lambda args: args['N_CTX'] % args['BLOCK_M'] == 0, 'IS_EVEN_N': lambda args: args['NKV_CTX'] % args['BLOCK_N'] == 0})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "H_KV",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "ROUND_CTX",
        "annotation": null
      },
      {
        "name": "NKV_CTX",
        "annotation": null
      },
      {
        "name": "sliding_window_offset",
        "annotation": null
      },
      {
        "name": "sliding_window_size",
        "annotation": null
      },
      {
        "name": "SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "COMPLEMENT_SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _score_kernel(",
      "    Q,",
      "    K,",
      "    M,",
      "    sm_scale,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    H_KV,",
      "    N_CTX,",
      "    ROUND_CTX,",
      "    NKV_CTX,",
      "    sliding_window_offset,",
      "    sliding_window_size,",
      "    SLIDING_WINDOW: tl.constexpr,",
      "    COMPLEMENT_SLIDING_WINDOW: tl.constexpr,",
      "    IS_EVEN_M: tl.constexpr,",
      "    IS_EVEN_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_n = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    off_hkv = off_h // (H // H_KV)",
      "    q_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "    k_offset = off_z.to(tl.int64) * stride_kz + off_hkv.to(tl.int64) * stride_kh",
      "    m_ptrs = M + off_hz * ROUND_CTX + tl.arange(0, BLOCK_M)",
      "    o = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + q_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + k_offset,",
      "        shape=(BLOCK_DMODEL, NKV_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, start_n * BLOCK_N),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "",
      "    if IS_EVEN_N:",
      "        k = tl.load(K_block_ptr)",
      "    else:",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "    lo = 0",
      "    hi = ROUND_CTX",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.4426950408889634",
      "",
      "    for start_m in range(lo, hi, BLOCK_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_M)",
      "        if IS_EVEN_M:",
      "            q = tl.load(Q_block_ptr)",
      "        else:",
      "            q = tl.load(Q_block_ptr, boundary_check=(0, 1), padding_option=\"zero\")",
      "",
      "        m = tl.load(m_ptrs)",
      "",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k)",
      "        qk = qk * qk_scale",
      "",
      "        if SLIDING_WINDOW:",
      "            dist = (",
      "                tl.arange(0, BLOCK_M)[:, None]",
      "                - tl.arange(0, BLOCK_N)[None, :]",
      "                + start_m",
      "                - start_n * BLOCK_N",
      "                + sliding_window_offset",
      "            )",
      "",
      "            if COMPLEMENT_SLIDING_WINDOW:",
      "                mask = dist >= sliding_window_size",
      "            else:",
      "                mask = (dist >= 0) & (dist < sliding_window_size)",
      "",
      "        qk = qk - m[:, None]",
      "        p = tl.math.exp2(qk)",
      "",
      "        if SLIDING_WINDOW:",
      "            p = tl.where(mask, p, 0)",
      "",
      "        if not IS_EVEN_N:",
      "            p = tl.where(((tl.arange(0, BLOCK_M) + start_m) < N_CTX)[:, None], p, 0)",
      "",
      "        o += tl.sum(p, axis=0)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, offsets=(BLOCK_M, 0))",
      "        m_ptrs = m_ptrs + BLOCK_M",
      "",
      "    o_offset = off_z.to(tl.int64) * stride_oz + off_h.to(tl.int64) * stride_oh",
      "    o_range = tl.arange(0, BLOCK_N) + start_n * BLOCK_N",
      "    o_ptrs = Out + o_offset + o_range",
      "    tl.store(o_ptrs, o.to(Out.type.element_ty), mask=o_range < NKV_CTX)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/484.py"
  },
  {
    "name": "_bmm_chunk_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_CS': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "dout_ptr",
        "annotation": null
      },
      {
        "name": "db_ptr",
        "annotation": null
      },
      {
        "name": "res_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_dout_batch",
        "annotation": null
      },
      {
        "name": "stride_dout_chunk",
        "annotation": null
      },
      {
        "name": "stride_dout_head",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_m",
        "annotation": null
      },
      {
        "name": "stride_dout_csize_n",
        "annotation": null
      },
      {
        "name": "stride_db_batch",
        "annotation": null
      },
      {
        "name": "stride_db_seqlen",
        "annotation": null
      },
      {
        "name": "stride_db_head",
        "annotation": null
      },
      {
        "name": "stride_db_k",
        "annotation": null
      },
      {
        "name": "stride_res_batch",
        "annotation": null
      },
      {
        "name": "stride_res_seqlen",
        "annotation": null
      },
      {
        "name": "stride_res_head",
        "annotation": null
      },
      {
        "name": "stride_res_k",
        "annotation": null
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_bwd_kernel(",
      "    a_ptr,",
      "    dout_ptr,",
      "    db_ptr,",
      "    res_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_dout_batch,",
      "    stride_dout_chunk,",
      "    stride_dout_head,",
      "    stride_dout_csize_m,",
      "    stride_dout_csize_n,",
      "    stride_db_batch,",
      "    stride_db_seqlen,",
      "    stride_db_head,",
      "    stride_db_k,",
      "    stride_res_batch,",
      "    stride_res_seqlen,",
      "    stride_res_head,",
      "    stride_res_k,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_CS: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(K, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    dout_ptr += (",
      "        pid_b * stride_dout_batch + pid_c * stride_dout_chunk + pid_h * stride_dout_head",
      "    )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_cs = tl.arange(0, BLOCK_SIZE_CS)",
      "    dout_ptrs = dout_ptr + (",
      "        offs_m[:, None] * stride_dout_csize_n + offs_cs[None, :] * stride_dout_csize_m",
      "    )",
      "    a_ptrs = a_ptr + (offs_cs[:, None] * stride_a_seqlen + offs_n[None, :] * stride_ak)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for cs in range(0, tl.cdiv(chunk_size_limit, BLOCK_SIZE_CS)):",
      "        dout = tl.load(",
      "            dout_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size)",
      "            & (offs_cs[None, :] < chunk_size_limit - cs * BLOCK_SIZE_CS),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_cs[:, None] < chunk_size_limit - cs * BLOCK_SIZE_CS)",
      "            & (offs_n[None, :] < K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(dout, a)",
      "        dout_ptrs += BLOCK_SIZE_CS * stride_dout_csize_m",
      "        a_ptrs += BLOCK_SIZE_CS * stride_a_seqlen",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_RESIDUAL:",
      "        res_ptr += (",
      "            pid_b * stride_res_batch",
      "            + pid_c * chunk_size * stride_res_seqlen",
      "            + pid_h * stride_res_head",
      "        )",
      "        res_ptrs = res_ptr + (",
      "            offs_m[:, None] * stride_res_seqlen + offs_n[None, :] * stride_res_k",
      "        )",
      "        res = tl.load(",
      "            res_ptrs, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "        ).to(tl.float32)",
      "        acc += res",
      "    db = acc.to(db_ptr.dtype.element_ty)",
      "",
      "    db_ptr += (",
      "        pid_b * stride_db_batch",
      "        + pid_c * chunk_size * stride_db_seqlen",
      "        + pid_h * stride_db_head",
      "    )",
      "    db_ptrs = db_ptr + (",
      "        offs_m[:, None] * stride_db_seqlen + offs_n[None, :] * stride_db_k",
      "    )",
      "    tl.store(",
      "        db_ptrs, db, mask=(offs_m[:, None] < chunk_size_limit) & (offs_n[None, :] < K)",
      "    )"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/491.py"
  },
  {
    "name": "_bmm_chunk_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=2)], key=['chunk_size', 'K', 'IS_CAUSAL'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "seq_idx_ptr",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "ngroups",
        "annotation": null
      },
      {
        "name": "stride_a_batch",
        "annotation": null
      },
      {
        "name": "stride_a_seqlen",
        "annotation": null
      },
      {
        "name": "stride_a_head",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_b_batch",
        "annotation": null
      },
      {
        "name": "stride_b_seqlen",
        "annotation": null
      },
      {
        "name": "stride_b_head",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_out_batch",
        "annotation": null
      },
      {
        "name": "stride_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_out_head",
        "annotation": null
      },
      {
        "name": "stride_outm",
        "annotation": null
      },
      {
        "name": "stride_outn",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_batch",
        "annotation": null
      },
      {
        "name": "stride_seq_idx_seqlen",
        "annotation": null
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "dot_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SEQ_IDX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bmm_chunk_fwd_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    out_ptr,",
      "    seq_idx_ptr,",
      "    seqlen,",
      "    chunk_size,",
      "    K,",
      "    ngroups,",
      "    stride_a_batch,",
      "    stride_a_seqlen,",
      "    stride_a_head,",
      "    stride_ak,",
      "    stride_b_batch,",
      "    stride_b_seqlen,",
      "    stride_b_head,",
      "    stride_bk,",
      "    stride_out_batch,",
      "    stride_out_chunk,",
      "    stride_out_head,",
      "    stride_outm,",
      "    stride_outn,",
      "    stride_seq_idx_batch,",
      "    stride_seq_idx_seqlen,",
      "    IS_CAUSAL: tl.constexpr,",
      "    dot_dtype: tl.constexpr,",
      "    HAS_SEQ_IDX: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_b = tl.program_id(axis=1)",
      "    pid_ch = tl.program_id(axis=2)",
      "    pid_c = pid_ch // ngroups",
      "    pid_h = pid_ch - pid_c * ngroups",
      "    num_pid_n = tl.cdiv(chunk_size, BLOCK_SIZE_N)",
      "    pid_m = tl.program_id(axis=0) // num_pid_n",
      "    pid_n = tl.program_id(axis=0) % num_pid_n",
      "    if IS_CAUSAL:",
      "        if pid_n * BLOCK_SIZE_N >= (pid_m + 1) * BLOCK_SIZE_M:",
      "            return",
      "    a_ptr += (",
      "        pid_b * stride_a_batch",
      "        + pid_c * chunk_size * stride_a_seqlen",
      "        + pid_h * stride_a_head",
      "    )",
      "    b_ptr += (",
      "        pid_b * stride_b_batch",
      "        + pid_c * chunk_size * stride_b_seqlen",
      "        + pid_h * stride_b_head",
      "    )",
      "    if HAS_SEQ_IDX:",
      "        seq_idx_ptr += (",
      "            pid_b * stride_seq_idx_batch + pid_c * chunk_size * stride_seq_idx_seqlen",
      "        )",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_m[:, None] * stride_a_seqlen + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_n[None, :] * stride_b_seqlen)",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(",
      "            a_ptrs,",
      "            mask=(offs_m[:, None] < chunk_size_limit)",
      "            & (offs_k[None, :] < K - k * BLOCK_SIZE_K),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        b = tl.load(",
      "            b_ptrs,",
      "            mask=(offs_k[:, None] < K - k * BLOCK_SIZE_K)",
      "            & (offs_n[None, :] < chunk_size_limit),",
      "            other=0.0,",
      "        ).to(dot_dtype)",
      "        acc += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    if HAS_SEQ_IDX:",
      "        chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "        seq_idx_m = tl.load(",
      "            seq_idx_ptr + offs_m * stride_seq_idx_seqlen,",
      "            mask=offs_m < chunk_size_limit,",
      "            other=-1,",
      "        )",
      "        seq_idx_n = tl.load(",
      "            seq_idx_ptr + offs_n * stride_seq_idx_seqlen,",
      "            mask=offs_n < chunk_size_limit,",
      "            other=-2,",
      "        )",
      "        acc = tl.where(seq_idx_m[:, None] == seq_idx_n[None, :], acc, 0.0)",
      "    out = acc.to(out_ptr.dtype.element_ty)",
      "",
      "    out_ptr += (",
      "        pid_b * stride_out_batch + pid_c * stride_out_chunk + pid_h * stride_out_head",
      "    )",
      "    out_ptrs = out_ptr + (stride_outm * offs_m[:, None] + offs_n[None, :] * stride_outn)",
      "    tl.store(",
      "        out_ptrs,",
      "        out,",
      "        mask=(offs_m[:, None] < chunk_size) & (offs_n[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/492.py"
  },
  {
    "name": "bmm_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'TILE_M': 32, 'TILE_N': 32, 'TILE_K': 32, 'GROUP_M': 1}, num_warps=4, num_stages=2), triton.Config({'TILE_M': 64, 'TILE_N': 32, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=2), triton.Config({'TILE_M': 64, 'TILE_N': 64, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=2), triton.Config({'TILE_M': 128, 'TILE_N': 32, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=2), triton.Config({'TILE_M': 128, 'TILE_N': 64, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=2), triton.Config({'TILE_M': 128, 'TILE_N': 128, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=2), triton.Config({'TILE_M': 32, 'TILE_N': 32, 'TILE_K': 32, 'GROUP_M': 1}, num_warps=4, num_stages=3), triton.Config({'TILE_M': 64, 'TILE_N': 32, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=3), triton.Config({'TILE_M': 64, 'TILE_N': 64, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=3), triton.Config({'TILE_M': 128, 'TILE_N': 32, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=3), triton.Config({'TILE_M': 128, 'TILE_N': 64, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=3), triton.Config({'TILE_M': 128, 'TILE_N': 128, 'TILE_K': 32, 'GROUP_M': 2}, num_warps=4, num_stages=3)], key=['M', 'N', 'K'])",
      "@triton.heuristics({'DIVISIBLE_M': heur_divisible_m, 'DIVISIBLE_N': heur_divisible_n, 'DIVISIBLE_K': heur_divisible_k})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "TILE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DIVISIBLE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DIVISIBLE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DIVISIBLE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def bmm_kernel(",
      "    A,",
      "    B,",
      "    O,",
      "    M,",
      "    N,",
      "    K,",
      "    TILE_M: tl.constexpr,",
      "    TILE_N: tl.constexpr,",
      "    TILE_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    DIVISIBLE_M: tl.constexpr,",
      "    DIVISIBLE_N: tl.constexpr,",
      "    DIVISIBLE_K: tl.constexpr,",
      "):",
      "",
      "    pid_b = tl.program_id(2)",
      "    A += pid_b * M * K",
      "    B += pid_b * K * N",
      "    O += pid_b * M * N",
      "",
      "    pidx = tl.program_id(0)",
      "    pidy = tl.program_id(1)",
      "",
      "    if GROUP_M == 1:",
      "        pid_m, pid_n = pidx, pidy",
      "    else:",
      "",
      "        gridx = tl.num_programs(0)",
      "        gridy = tl.num_programs(1)",
      "        pid = pidx + pidy * gridx",
      "",
      "        num_CTA_per_group = gridy * GROUP_M",
      "",
      "        group_id = pid // num_CTA_per_group",
      "        inner_group_id = pid % num_CTA_per_group",
      "        if (group_id * GROUP_M + GROUP_M) > gridx:",
      "            GROUP_SIZE = gridx % GROUP_M",
      "        else:",
      "            GROUP_SIZE = GROUP_M",
      "        pid_m = group_id * GROUP_M + inner_group_id % GROUP_SIZE",
      "        pid_n = inner_group_id // GROUP_SIZE",
      "",
      "    offs_m = pid_m * TILE_M + tl.arange(0, TILE_M)",
      "    offs_n = pid_n * TILE_N + tl.arange(0, TILE_N)",
      "    offs_k = tl.arange(0, TILE_K)",
      "",
      "    if not DIVISIBLE_M:",
      "        mask_m = offs_m < M",
      "    if not DIVISIBLE_N:",
      "        mask_n = offs_n < N",
      "",
      "    a_ptrs = A + offs_m[:, None] * K + offs_k[None, :]",
      "    b_ptrs = B + offs_k[:, None] * N + offs_n[None, :]",
      "    o_ptrs = O + offs_m[:, None] * N + offs_n[None, :]",
      "",
      "    num_iters = tl.cdiv(K, TILE_K)",
      "    o = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)",
      "    for _ in range(num_iters):",
      "        if DIVISIBLE_K:",
      "            if DIVISIBLE_M:",
      "                mask_a = None",
      "            else:",
      "                mask_a = mask_m[:, None]",
      "            if DIVISIBLE_N:",
      "                mask_b = None",
      "            else:",
      "                mask_b = mask_n[None, :]",
      "        else:",
      "            mask_k = offs_k < K",
      "            if DIVISIBLE_M:",
      "                mask_a = mask_k[None, :]",
      "            else:",
      "                mask_a = mask_m[:, None] & mask_k[None, :]",
      "            if DIVISIBLE_N:",
      "                mask_b = mask_k[:, None]",
      "            else:",
      "                mask_b = mask_k[:, None] & mask_n[None, :]",
      "",
      "        a = tl.load(a_ptrs, mask_a)",
      "        b = tl.load(b_ptrs, mask_b)",
      "",
      "        offs_k += TILE_K",
      "        a_ptrs += TILE_K",
      "        b_ptrs += TILE_K * N",
      "",
      "        o += tl.dot(a, b, allow_tf32=False)",
      "",
      "    if DIVISIBLE_M and DIVISIBLE_N:",
      "        mask_c = None",
      "    elif DIVISIBLE_M and not DIVISIBLE_N:",
      "        mask_c = mask_n[None, :]",
      "    elif not DIVISIBLE_M and DIVISIBLE_N:",
      "        mask_c = mask_m[:, None]",
      "    else:",
      "        mask_c = mask_m[:, None] & mask_n[None, :]",
      "    tl.store(o_ptrs, o, mask_c)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/493.py"
  },
  {
    "name": "chunk_simple_gla_bwd_kernel_dqkg",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "s_k_h",
        "annotation": null
      },
      {
        "name": "s_k_t",
        "annotation": null
      },
      {
        "name": "s_v_h",
        "annotation": null
      },
      {
        "name": "s_v_t",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_simple_gla_bwd_kernel_dqkg(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dg,",
      "    s_k_h,",
      "    s_k_t,",
      "    s_v_h,",
      "    s_v_t,",
      "    s_h_h,",
      "    s_h_t,",
      "    scale,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    n_bh = tl.num_programs(2)",
      "    o_i = tl.arange(0, BT)",
      "",
      "    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    b_g = tl.load(p_g, boundary_check=(0,))",
      "    last_idx = min(i_t * BT + BT, T) - 1",
      "    b_g_last = tl.load(g + i_bh * T + last_idx)",
      "",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    b_dg_last = tl.zeros(",
      "        [",
      "            1,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "    b_dg = tl.zeros(",
      "        [",
      "            BT,",
      "        ],",
      "        dtype=tl.float32,",
      "    )",
      "",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h,",
      "            (V, NT * K),",
      "            (1, s_h_t),",
      "            (i_v * BV, i_t * K + i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * s_v_h,",
      "            (T, V),",
      "            (s_v_t, 1),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * s_h_h,",
      "            (V, NT * K),",
      "            (1, s_h_t),",
      "            (i_v * BV, i_t * K + i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "",
      "        b_dg_last += tl.sum(b_h * b_dh)",
      "        b_ds += tl.dot(b_do, tl.trans(b_v))",
      "        b_dq += tl.dot(b_do, b_h.to(b_do.dtype))",
      "        b_dk += tl.dot(b_v, b_dh.to(b_v.dtype))",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_dg_last *= tl.exp(b_g_last)",
      "    b_dq = b_dq * tl.exp(b_g)[:, None] * scale",
      "    b_dk = b_dk * tl.exp(-b_g + b_g_last)[:, None]",
      "    b_dg_last += tl.sum(b_dk * b_k)",
      "    b_ds = tl.where(",
      "        o_i[:, None] >= o_i[None, :],",
      "        b_ds * scale * tl.exp(b_g[:, None] - b_g[None, :]),",
      "        0,",
      "    )",
      "    b_ds = b_ds.to(b_k.dtype)",
      "",
      "    b_dq += tl.dot(b_ds, b_k)",
      "    b_dk += tl.dot(tl.trans(b_ds), b_q)",
      "    b_dg += tl.sum(b_q * b_dq - b_k * b_dk, axis=1)",
      "",
      "    b_dg = tl.where(o_i < min(BT, T - i_t * BT) - 1, b_dg, b_dg + b_dg_last)",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "    )",
      "    p_dg = tl.make_block_ptr(",
      "        dg + (i_k * n_bh + i_bh) * T, (T,), (1,), (i_t * BT,), (BT,), (0,)",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/495.py"
  },
  {
    "name": "chunk_global_cumsum_scalar_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64}, num_warps=4)], key=[])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    T: tl.constexpr,",
      "    BT: tl.constexpr,",
      "):",
      "    i_bh = tl.program_id(0)",
      "    b_z = tl.zeros([], dtype=tl.float32)",
      "    for i_t in range(tl.cdiv(T, BT)):",
      "        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)",
      "        b_o = tl.cumsum(b_s, axis=0) + b_z[None]",
      "        b_zz = tl.sum(b_s, axis=0)",
      "        b_z += b_zz",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/496.py"
  },
  {
    "name": "chunk_global_cumsum_vector_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8), triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton.Config({'BT': 64}, num_warps=8)], key=['S'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "s_s_h",
        "annotation": null
      },
      {
        "name": "s_s_t",
        "annotation": null
      },
      {
        "name": "s_s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_cumsum_vector_kernel(",
      "    s,",
      "    z,",
      "    s_s_h,",
      "    s_s_t,",
      "    s_s_d,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "):",
      "    i_s, i_bh = tl.program_id(0), tl.program_id(1)",
      "    o_i = tl.arange(0, BT)",
      "    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0)",
      "    b_z = tl.zeros([BS], dtype=tl.float32)",
      "    for i_t in range(tl.cdiv(T, BT)):",
      "        p_s = tl.make_block_ptr(",
      "            s + i_bh * s_s_h,",
      "            (T, S),",
      "            (s_s_t, s_s_d),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * s_s_h,",
      "            (T, S),",
      "            (s_s_t, s_s_d),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)",
      "        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))",
      "        if i_t >= 0:",
      "            b_z += tl.sum(b_s, 0)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/497.py"
  },
  {
    "name": "chunk_delta_rule_fwd_kernel_h",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "d",
        "annotation": null
      },
      {
        "name": "v_new",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "initial_state",
        "annotation": null
      },
      {
        "name": "final_state",
        "annotation": null
      },
      {
        "name": "s_qk_h",
        "annotation": null
      },
      {
        "name": "s_qk_t",
        "annotation": null
      },
      {
        "name": "s_qk_d",
        "annotation": null
      },
      {
        "name": "s_vo_h",
        "annotation": null
      },
      {
        "name": "s_vo_t",
        "annotation": null
      },
      {
        "name": "s_vo_d",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_delta_rule_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    d,",
      "    v_new,",
      "    h,",
      "    initial_state,",
      "    final_state,",
      "    s_qk_h,",
      "    s_qk_t,",
      "    s_qk_d,",
      "    s_vo_h,",
      "    s_vo_t,",
      "    s_vo_d,",
      "    s_h_h,",
      "    s_h_t,",
      "    H: tl.constexpr,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            initial_state + i_bh * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "",
      "    for i_t in range(NT):",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h + i_t * K * V,",
      "            (K, V),",
      "            (s_h_t, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_h_cumsum = tl.zeros([BK, BV], dtype=tl.float32)",
      "        for i_c in range(tl.cdiv(BT, BC)):",
      "            p_k = tl.make_block_ptr(",
      "                k + i_bh * s_qk_h,",
      "                (K, T),",
      "                (s_qk_d, s_qk_t),",
      "                (i_k * BK, i_t * BT + i_c * BC),",
      "                (BK, BC),",
      "                (0, 1),",
      "            )",
      "            p_d = tl.make_block_ptr(",
      "                d + i_bh * s_qk_h,",
      "                (T, K),",
      "                (s_qk_t, s_qk_d),",
      "                (i_t * BT + i_c * BC, i_k * BK),",
      "                (BC, BK),",
      "                (1, 0),",
      "            )",
      "            p_v = tl.make_block_ptr(",
      "                v + i_bh * s_vo_h,",
      "                (T, V),",
      "                (s_vo_t, s_vo_d),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            p_v_new = tl.make_block_ptr(",
      "                v_new + i_bh * s_vo_h,",
      "                (T, V),",
      "                (s_vo_t, s_vo_d),",
      "                (i_t * BT + i_c * BC, i_v * BV),",
      "                (BC, BV),",
      "                (1, 0),",
      "            )",
      "            b_k = tl.load(p_k, boundary_check=(0, 1))",
      "            b_d = tl.load(p_d, boundary_check=(0, 1))",
      "            b_v = tl.load(p_v, boundary_check=(0, 1))",
      "            b_v -= tl.dot(b_d, b_h.to(b_k.dtype), allow_tf32=False)",
      "            tl.store(p_v_new, b_v.to(p_v_new.dtype.element_ty), boundary_check=(0, 1))",
      "            b_h_cumsum += tl.dot(b_k, b_v.to(b_k.dtype), allow_tf32=False)",
      "        b_h += b_h_cumsum",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            final_state + i_bh * K * V,",
      "            (K, V),",
      "            (V, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/498.py"
  },
  {
    "name": "chunk_gated_abc_fwd_kernel_cum",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BS': 16}, num_warps=2), triton.Config({'BS': 16}, num_warps=4), triton.Config({'BS': 16}, num_warps=8), triton.Config({'BS': 32}, num_warps=2), triton.Config({'BS': 32}, num_warps=4), triton.Config({'BS': 32}, num_warps=8), triton.Config({'BS': 64}, num_warps=2), triton.Config({'BS': 64}, num_warps=4), triton.Config({'BS': 64}, num_warps=8)], key=['S'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "s_s_h",
        "annotation": null
      },
      {
        "name": "s_s_t",
        "annotation": null
      },
      {
        "name": "s_s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gated_abc_fwd_kernel_cum(",
      "    s,",
      "    o,",
      "    s_s_h,",
      "    s_s_t,",
      "    s_s_d,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "):",
      "    i_s, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    o_i = tl.arange(0, BT)",
      "    m_s = tl.where(o_i[:, None] >= o_i[None, :], 1.0, 0.0).to(tl.float32)",
      "",
      "    p_s = tl.make_block_ptr(",
      "        s + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * s_s_h, (T, S), (s_s_t, s_s_d), (i_t * BT, i_s * BS), (BT, BS), (1, 0)",
      "    )",
      "",
      "    b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "    b_o = tl.dot(m_s, b_s, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/500.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_inter",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC', 'BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "s_k_h",
        "annotation": null
      },
      {
        "name": "s_k_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_inter(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    s_k_h,",
      "    s_k_t,",
      "    scale,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_i, i_j = i_c // NC, i_c % NC",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "    if i_i <= i_j:",
      "        return",
      "",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        o_k = i_k * BK + tl.arange(0, BK)",
      "        m_k = o_k < K",
      "",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * s_k_h,",
      "            (T, K),",
      "            (s_k_t, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + i_bh * s_k_h,",
      "            (T, K),",
      "            (s_k_t, 1),",
      "            (i_t * BT + i_i * BC, i_k * BK),",
      "            (BC, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * s_k_h,",
      "            (K, T),",
      "            (1, s_k_t),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gk = tl.make_block_ptr(",
      "            g + i_bh * s_k_h,",
      "            (K, T),",
      "            (1, s_k_t),",
      "            (i_k * BK, i_t * BT + i_j * BC),",
      "            (BK, BC),",
      "            (0, 1),",
      "        )",
      "        p_gn = tl.max_contiguous(",
      "            tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_i * BC) * K + o_k, BK), BK",
      "        )",
      "        b_gn = tl.load(p_gn, mask=m_k, other=0)",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_qg = b_q * tl.exp(b_g - b_gn[None, :]) * scale",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_gk = tl.load(p_gk, boundary_check=(0, 1))",
      "        b_kg = b_k * tl.exp(b_gn[:, None] - b_gk)",
      "        b_A += tl.dot(b_qg, b_kg)",
      "",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT,",
      "        (T, BT),",
      "        (BT, 1),",
      "        (i_t * BT + i_i * BC, i_j * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A, b_A.to(A.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/501.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BK', 'BT'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "s_k_h",
        "annotation": null
      },
      {
        "name": "s_k_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    s_k_h,",
      "    s_k_t,",
      "    scale,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "):",
      "    i_t, i_i, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_j = i_i",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = tl.arange(0, BK)",
      "    o_A = i_bh * T * BT + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BT + i_j * BC",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0)",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT + i_i * BC, 0), (BC, BK), (1, 0)",
      "    )",
      "",
      "    p_k = tl.max_contiguous(",
      "        tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK",
      "    )",
      "    p_gk = tl.max_contiguous(",
      "        tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A = tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i >= j, b_A * scale, 0.0)",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_k += K",
      "        p_gk += K"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/501.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra_split",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC', 'BK'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "s_k_h",
        "annotation": null
      },
      {
        "name": "s_k_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NC",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra_split(",
      "    q,",
      "    k,",
      "    g,",
      "    A,",
      "    s_k_h,",
      "    s_k_t,",
      "    scale,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BC: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    NC: tl.constexpr,",
      "):",
      "    i_k, i_tc, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_t, i_i = i_tc // NC, i_tc % NC",
      "    i_j = i_i",
      "    n_bh = tl.num_programs(2)",
      "    if i_t * BT + i_i * BC >= T:",
      "        return",
      "",
      "    o_i = tl.arange(0, BC)",
      "    o_k = i_k * BK + tl.arange(0, BK)",
      "    o_A = (i_bh + i_k * n_bh) * T * BC + (i_t * BT + i_i * BC + tl.arange(0, BC)) * BC",
      "    m_k = o_k < K",
      "    m_A = (i_t * BT + i_i * BC + tl.arange(0, BC)) < T",
      "",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * s_k_h,",
      "        (T, K),",
      "        (s_k_t, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_g = tl.make_block_ptr(",
      "        g + i_bh * s_k_h,",
      "        (T, K),",
      "        (s_k_t, 1),",
      "        (i_t * BT + i_i * BC, i_k * BK),",
      "        (BC, BK),",
      "        (1, 0),",
      "    )",
      "    p_k = tl.max_contiguous(",
      "        tl.multiple_of(k + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK",
      "    )",
      "    p_gk = tl.max_contiguous(",
      "        tl.multiple_of(g + i_bh * s_k_h + (i_t * BT + i_j * BC) * K + o_k, BK), BK",
      "    )",
      "",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_g = tl.load(p_g, boundary_check=(0, 1))",
      "    for j in range(0, min(BC, T - i_t * BT - i_i * BC)):",
      "        b_A = tl.zeros([BC], dtype=tl.float32)",
      "        b_k = tl.load(p_k, mask=m_k, other=0).to(tl.float32)",
      "        b_gk = tl.load(p_gk, mask=m_k, other=0).to(tl.float32)",
      "        b_A += tl.sum(b_q * b_k[None, :] * tl.exp(b_g - b_gk[None, :]), 1)",
      "        b_A = tl.where(o_i >= j, b_A * scale, 0.0)",
      "        tl.store(A + o_A + j, b_A, mask=m_A)",
      "        p_k += K",
      "        p_gk += K"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/501.py"
  },
  {
    "name": "chunk_gla_fwd_A_kernel_intra_sub_intra_merge",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BC'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "A2",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_A_kernel_intra_sub_intra_merge(",
      "    A, A2, T: tl.constexpr, BT: tl.constexpr, BC: tl.constexpr, NK: tl.constexpr",
      "):",
      "    i_t, i_c, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    if i_t * BT + i_c * BC >= T:",
      "        return",
      "    n_bh = tl.num_programs(2)",
      "    b_A = tl.zeros([BC, BC], dtype=tl.float32)",
      "    for i_k in range(0, NK):",
      "        p_A = tl.make_block_ptr(",
      "            A + (i_bh + i_k * n_bh) * T * BC,",
      "            (T, BC),",
      "            (BC, 1),",
      "            (i_t * BT + i_c * BC, 0),",
      "            (BC, BC),",
      "            (1, 0),",
      "        )",
      "        b_A += tl.load(p_A, boundary_check=(0, 1))",
      "    p_A2 = tl.make_block_ptr(",
      "        A2 + i_bh * T * BT,",
      "        (T, BT),",
      "        (BT, 1),",
      "        (i_t * BT + i_c * BC, i_c * BC),",
      "        (BC, BC),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_A2, b_A.to(A2.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/501.py"
  },
  {
    "name": "chunk_gla_fwd_kernel_o",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['BK', 'BV', 'BT'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "s_k_h",
        "annotation": null
      },
      {
        "name": "s_k_t",
        "annotation": null
      },
      {
        "name": "s_v_h",
        "annotation": null
      },
      {
        "name": "s_v_t",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_gla_fwd_kernel_o(",
      "    q,",
      "    v,",
      "    g,",
      "    h,",
      "    o,",
      "    A,",
      "    s_k_h,",
      "    s_k_t,",
      "    s_v_h,",
      "    s_v_t,",
      "    s_h_h,",
      "    s_h_t,",
      "    scale,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    m_s = tl.arange(0, BT)[:, None] >= tl.arange(0, BT)[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_g = tl.make_block_ptr(",
      "            g + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h + i_t * K * V,",
      "            (K, V),",
      "            (s_h_t, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_g = tl.load(p_g, boundary_check=(0, 1))",
      "        b_qg = (b_q * tl.exp(b_g)).to(b_q.dtype)",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        if i_k >= 0:",
      "            b_o += tl.dot(b_qg, b_h.to(b_qg.dtype))",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    p_A = tl.make_block_ptr(",
      "        A + i_bh * T * BT, (T, BT), (BT, 1), (i_t * BT, 0), (BT, BT), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_A = tl.load(p_A, boundary_check=(0, 1))",
      "    b_A = tl.where(m_s, b_A, 0.0).to(b_v.dtype)",
      "    b_o += tl.dot(b_A, b_v, allow_tf32=False)",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/501.py"
  },
  {
    "name": "chunk_simple_gla_fwd_kernel_o",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "s_k_h",
        "annotation": null
      },
      {
        "name": "s_k_t",
        "annotation": null
      },
      {
        "name": "s_v_h",
        "annotation": null
      },
      {
        "name": "s_v_t",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_simple_gla_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    g,",
      "    o,",
      "    s_k_h,",
      "    s_k_t,",
      "    s_v_h,",
      "    s_v_t,",
      "    s_h_h,",
      "    s_h_t,",
      "    scale,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    o_i = tl.arange(0, BT)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_s = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * s_k_h, (T, K), (s_k_t, 1), (i_t * BT, i_k * BK), (BT, BK), (1, 0)",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * s_k_h, (K, T), (1, s_k_t), (i_k * BK, i_t * BT), (BK, BT), (0, 1)",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h + i_t * K * V,",
      "            (K, V),",
      "            (s_h_t, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_o += tl.dot(b_q, b_h, allow_tf32=False)",
      "        b_s += tl.dot(b_q, b_k, allow_tf32=False)",
      "",
      "    p_g = tl.make_block_ptr(g + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "    b_g = tl.load(p_g, boundary_check=(0,))",
      "    b_o = b_o * tl.exp(b_g)[:, None]",
      "    b_s = b_s * tl.exp(b_g[:, None] - b_g[None, :])",
      "    b_s = tl.where(m_s, b_s, 0)",
      "",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * s_v_h, (T, V), (s_v_t, 1), (i_t * BT, i_v * BV), (BT, BV), (1, 0)",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/502.py"
  },
  {
    "name": "chunk_retention_fwd_kernel_h",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "s_qk_h",
        "annotation": null
      },
      {
        "name": "s_qk_t",
        "annotation": null
      },
      {
        "name": "s_qk_d",
        "annotation": null
      },
      {
        "name": "s_vo_h",
        "annotation": null
      },
      {
        "name": "s_vo_t",
        "annotation": null
      },
      {
        "name": "s_vo_d",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_retention_fwd_kernel_h(",
      "    k,",
      "    v,",
      "    h,",
      "    h0,",
      "    ht,",
      "    s_qk_h,",
      "    s_qk_t,",
      "    s_qk_d,",
      "    s_vo_h,",
      "    s_vo_t,",
      "    s_vo_d,",
      "    s_h_h,",
      "    s_h_t,",
      "    H: tl.constexpr,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "",
      "    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "    o_i = tl.arange(0, BT)",
      "    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((BT - o_i - 1) * b_b)",
      "    b_h = tl.zeros([BK, BV], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = tl.make_block_ptr(",
      "            h0 + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        b_h = tl.load(p_h0, boundary_check=(0, 1)).to(tl.float32)",
      "    for i_t in range(NT):",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * s_qk_h,",
      "            (K, T),",
      "            (s_qk_d, s_qk_t),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * s_vo_h,",
      "            (T, V),",
      "            (s_vo_t, s_vo_d),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h + i_t * K * V,",
      "            (K, V),",
      "            (s_h_t, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_h, b_h.to(p_h.dtype.element_ty), boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        if i_t == NT - 1 and (T % BT) != 0:",
      "            d_b = tl.math.exp2((T % BT) * b_b)",
      "            d_i = tl.math.exp2(((T % BT) - o_i - 1) * b_b)",
      "        b_h = d_b * b_h + tl.dot(",
      "            b_k, (b_v * d_i[:, None]).to(b_k.dtype), allow_tf32=False",
      "        )",
      "    if STORE_FINAL_STATE:",
      "        p_ht = tl.make_block_ptr(",
      "            ht + i_bh * K * V, (K, V), (V, 1), (i_k * BK, i_v * BV), (BK, BV), (1, 0)",
      "        )",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/505.py"
  },
  {
    "name": "chunk_retention_fwd_kernel_o",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "s_qk_h",
        "annotation": null
      },
      {
        "name": "s_qk_t",
        "annotation": null
      },
      {
        "name": "s_qk_d",
        "annotation": null
      },
      {
        "name": "s_vo_h",
        "annotation": null
      },
      {
        "name": "s_vo_t",
        "annotation": null
      },
      {
        "name": "s_vo_d",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_retention_fwd_kernel_o(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    o,",
      "    s_qk_h,",
      "    s_qk_t,",
      "    s_qk_d,",
      "    s_vo_h,",
      "    s_vo_t,",
      "    s_vo_d,",
      "    s_h_h,",
      "    s_h_t,",
      "    scale,",
      "    H: tl.constexpr,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    i_v, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "    o_i = tl.arange(0, BT)",
      "    d_i = tl.math.exp2((o_i + 1) * b_b)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0)",
      "    b_o = tl.zeros([BT, BV], dtype=tl.float32)",
      "    b_s = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_k in range(tl.cdiv(K, BK)):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * s_qk_h,",
      "            (T, K),",
      "            (s_qk_t, s_qk_d),",
      "            (i_t * BT, i_k * BK),",
      "            (BT, BK),",
      "            (1, 0),",
      "        )",
      "        p_k = tl.make_block_ptr(",
      "            k + i_bh * s_qk_h,",
      "            (K, T),",
      "            (s_qk_d, s_qk_t),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h + i_t * K * V,",
      "            (K, V),",
      "            (s_h_t, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_k = tl.load(p_k, boundary_check=(0, 1))",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_o += tl.dot((b_q * d_i[:, None]).to(b_q.dtype), b_h, allow_tf32=False)",
      "        b_s += tl.dot(b_q, b_k, allow_tf32=False)",
      "    b_s *= d_s",
      "    p_v = tl.make_block_ptr(",
      "        v + i_bh * s_vo_h,",
      "        (T, V),",
      "        (s_vo_t, s_vo_d),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    b_v = tl.load(p_v, boundary_check=(0, 1))",
      "    b_o = (b_o + tl.dot(b_s.to(b_v.dtype), b_v, allow_tf32=False)) * scale",
      "    p_o = tl.make_block_ptr(",
      "        o + i_bh * s_vo_h,",
      "        (T, V),",
      "        (s_vo_t, s_vo_d),",
      "        (i_t * BT, i_v * BV),",
      "        (BT, BV),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/505.py"
  },
  {
    "name": "chunk_retention_bwd_kernel_dh",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "s_qk_h",
        "annotation": null
      },
      {
        "name": "s_qk_t",
        "annotation": null
      },
      {
        "name": "s_qk_d",
        "annotation": null
      },
      {
        "name": "s_vo_h",
        "annotation": null
      },
      {
        "name": "s_vo_t",
        "annotation": null
      },
      {
        "name": "s_vo_d",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_retention_bwd_kernel_dh(",
      "    q,",
      "    do,",
      "    dh,",
      "    s_qk_h,",
      "    s_qk_t,",
      "    s_qk_d,",
      "    s_vo_h,",
      "    s_vo_t,",
      "    s_vo_d,",
      "    s_h_h,",
      "    s_h_t,",
      "    scale,",
      "    H: tl.constexpr,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "",
      "    i_k, i_v, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "    o_i = tl.arange(0, BT)",
      "    d_b, d_i = tl.math.exp2(BT * b_b), tl.math.exp2((o_i + 1) * b_b)",
      "    b_dh = tl.zeros([BK, BV], dtype=tl.float32)",
      "    for i_t in range(NT - 1, -1, -1):",
      "        p_q = tl.make_block_ptr(",
      "            q + i_bh * s_qk_h,",
      "            (K, T),",
      "            (s_qk_d, s_qk_t),",
      "            (i_k * BK, i_t * BT),",
      "            (BK, BT),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * s_vo_h,",
      "            (T, V),",
      "            (s_vo_t, s_vo_d),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * s_h_h + i_t * K * V,",
      "            (K, V),",
      "            (s_h_t, 1),",
      "            (i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        tl.store(p_dh, b_dh.to(p_dh.dtype.element_ty), boundary_check=(0, 1))",
      "        b_q = tl.load(p_q, boundary_check=(0, 1))",
      "        b_q = (b_q * scale).to(b_q.dtype)",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_dh = d_b * b_dh + tl.dot(",
      "            b_q, (b_do * d_i[:, None]).to(b_q.dtype), allow_tf32=False",
      "        )"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/505.py"
  },
  {
    "name": "chunk_retention_bwd_kernel_dqkv",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4)], key=['BT', 'BK', 'BV'])"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "h",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "dh",
        "annotation": null
      },
      {
        "name": "dq",
        "annotation": null
      },
      {
        "name": "dk",
        "annotation": null
      },
      {
        "name": "dv",
        "annotation": null
      },
      {
        "name": "s_qk_h",
        "annotation": null
      },
      {
        "name": "s_qk_t",
        "annotation": null
      },
      {
        "name": "s_qk_d",
        "annotation": null
      },
      {
        "name": "s_vo_h",
        "annotation": null
      },
      {
        "name": "s_vo_t",
        "annotation": null
      },
      {
        "name": "s_vo_d",
        "annotation": null
      },
      {
        "name": "s_h_h",
        "annotation": null
      },
      {
        "name": "s_h_t",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_retention_bwd_kernel_dqkv(",
      "    q,",
      "    k,",
      "    v,",
      "    h,",
      "    do,",
      "    dh,",
      "    dq,",
      "    dk,",
      "    dv,",
      "    s_qk_h,",
      "    s_qk_t,",
      "    s_qk_d,",
      "    s_vo_h,",
      "    s_vo_t,",
      "    s_vo_d,",
      "    s_h_h,",
      "    s_h_t,",
      "    scale,",
      "    H: tl.constexpr,",
      "    T: tl.constexpr,",
      "    K: tl.constexpr,",
      "    V: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "    NT: tl.constexpr,",
      "):",
      "",
      "    i_k, i_t, i_bh = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "    i_h = i_bh % H",
      "    n_bh = tl.num_programs(2)",
      "    b_b = tl.math.log2(1 - tl.math.exp2(-5 - i_h * 1.0))",
      "    o_i = tl.arange(0, BT)",
      "    d_q, d_k = tl.math.exp2((o_i + 1) * b_b), tl.math.exp2((BT - o_i - 1) * b_b)",
      "    d_q = (d_q * scale).to(d_q.dtype)",
      "    m_s = o_i[:, None] >= o_i[None, :]",
      "    d_s = tl.where(m_s, tl.math.exp2((o_i[:, None] - o_i[None, :]) * b_b), 0) * scale",
      "    p_q = tl.make_block_ptr(",
      "        q + i_bh * s_qk_h,",
      "        (K, T),",
      "        (s_qk_d, s_qk_t),",
      "        (i_k * BK, i_t * BT),",
      "        (BK, BT),",
      "        (0, 1),",
      "    )",
      "    p_k = tl.make_block_ptr(",
      "        k + i_bh * s_qk_h,",
      "        (T, K),",
      "        (s_qk_t, s_qk_d),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    b_q = tl.load(p_q, boundary_check=(0, 1))",
      "    b_k = tl.load(p_k, boundary_check=(0, 1))",
      "    b_s = tl.dot(b_k, b_q, allow_tf32=False) * tl.trans(d_s)",
      "    b_dq = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_dk = tl.zeros([BT, BK], dtype=tl.float32)",
      "    b_ds = tl.zeros([BT, BT], dtype=tl.float32)",
      "    for i_v in range(tl.cdiv(V, BV)):",
      "        p_v = tl.make_block_ptr(",
      "            v + i_bh * s_vo_h,",
      "            (T, V),",
      "            (s_vo_t, s_vo_d),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_h = tl.make_block_ptr(",
      "            h + i_bh * s_h_h,",
      "            (V, NT * K),",
      "            (1, s_h_t),",
      "            (i_v * BV, i_t * K + i_k * BK),",
      "            (BV, BK),",
      "            (0, 1),",
      "        )",
      "        p_do = tl.make_block_ptr(",
      "            do + i_bh * s_vo_h,",
      "            (T, V),",
      "            (s_vo_t, s_vo_d),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        p_dh = tl.make_block_ptr(",
      "            dh + i_bh * s_h_h,",
      "            (NT * K, V),",
      "            (s_h_t, 1),",
      "            (i_t * K + i_k * BK, i_v * BV),",
      "            (BK, BV),",
      "            (1, 0),",
      "        )",
      "        p_dv = tl.make_block_ptr(",
      "            dv + (i_k * n_bh + i_bh) * s_vo_h,",
      "            (T, V),",
      "            (s_vo_t, s_vo_d),",
      "            (i_t * BT, i_v * BV),",
      "            (BT, BV),",
      "            (1, 0),",
      "        )",
      "        b_v = tl.load(p_v, boundary_check=(0, 1))",
      "        b_do = tl.load(p_do, boundary_check=(0, 1))",
      "        b_h = tl.load(p_h, boundary_check=(0, 1))",
      "        b_dh = tl.load(p_dh, boundary_check=(0, 1))",
      "        b_ds += tl.dot(b_do, tl.trans(b_v), allow_tf32=False)",
      "        b_dq += tl.dot(b_do, b_h, allow_tf32=False)",
      "        b_dk += tl.dot(b_v, tl.trans(b_dh), allow_tf32=False)",
      "        b_dv = tl.dot(b_k, b_dh, allow_tf32=False) * d_k[:, None] + tl.dot(",
      "            b_s.to(b_q.dtype), b_do, allow_tf32=False",
      "        )",
      "        tl.store(p_dv, b_dv.to(p_dv.dtype.element_ty), boundary_check=(0, 1))",
      "    b_ds = (b_ds * d_s).to(b_q.dtype)",
      "    b_dq = b_dq * d_q[:, None] + tl.dot(b_ds, b_k, allow_tf32=False)",
      "    b_dk = b_dk * d_k[:, None] + tl.trans(tl.dot(b_q, b_ds, allow_tf32=False))",
      "    p_dq = tl.make_block_ptr(",
      "        dq + i_bh * s_qk_h,",
      "        (T, K),",
      "        (s_qk_t, s_qk_d),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    p_dk = tl.make_block_ptr(",
      "        dk + i_bh * s_qk_h,",
      "        (T, K),",
      "        (s_qk_t, s_qk_d),",
      "        (i_t * BT, i_k * BK),",
      "        (BT, BK),",
      "        (1, 0),",
      "    )",
      "    tl.store(p_dq, b_dq.to(p_dq.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(p_dk, b_dk.to(p_dk.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/505.py"
  },
  {
    "name": "_chunk_cumsum_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_H': 1}), triton.Config({'BLOCK_SIZE_H': 2}), triton.Config({'BLOCK_SIZE_H': 4}), triton.Config({'BLOCK_SIZE_H': 8}), triton.Config({'BLOCK_SIZE_H': 16}), triton.Config({'BLOCK_SIZE_H': 32}), triton.Config({'BLOCK_SIZE_H': 64})], key=['chunk_size', 'nheads'])"
    ],
    "args": [
      {
        "name": "dt_ptr",
        "annotation": null
      },
      {
        "name": "A_ptr",
        "annotation": null
      },
      {
        "name": "dt_bias_ptr",
        "annotation": null
      },
      {
        "name": "dt_out_ptr",
        "annotation": null
      },
      {
        "name": "dA_cumsum_ptr",
        "annotation": null
      },
      {
        "name": "batch",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "chunk_size",
        "annotation": null
      },
      {
        "name": "dt_min",
        "annotation": null
      },
      {
        "name": "dt_max",
        "annotation": null
      },
      {
        "name": "stride_dt_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_seqlen",
        "annotation": null
      },
      {
        "name": "stride_dt_head",
        "annotation": null
      },
      {
        "name": "stride_A_head",
        "annotation": null
      },
      {
        "name": "stride_dt_bias_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_batch",
        "annotation": null
      },
      {
        "name": "stride_dt_out_chunk",
        "annotation": null
      },
      {
        "name": "stride_dt_out_head",
        "annotation": null
      },
      {
        "name": "stride_dt_out_csize",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_batch",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_chunk",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_head",
        "annotation": null
      },
      {
        "name": "stride_dA_cs_csize",
        "annotation": null
      },
      {
        "name": "DT_SOFTPLUS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DT_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_H",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_CHUNK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunk_cumsum_fwd_kernel(",
      "    dt_ptr,",
      "    A_ptr,",
      "    dt_bias_ptr,",
      "    dt_out_ptr,",
      "    dA_cumsum_ptr,",
      "    batch,",
      "    seqlen,",
      "    nheads,",
      "    chunk_size,",
      "    dt_min,",
      "    dt_max,",
      "    stride_dt_batch,",
      "    stride_dt_seqlen,",
      "    stride_dt_head,",
      "    stride_A_head,",
      "    stride_dt_bias_head,",
      "    stride_dt_out_batch,",
      "    stride_dt_out_chunk,",
      "    stride_dt_out_head,",
      "    stride_dt_out_csize,",
      "    stride_dA_cs_batch,",
      "    stride_dA_cs_chunk,",
      "    stride_dA_cs_head,",
      "    stride_dA_cs_csize,",
      "    DT_SOFTPLUS: tl.constexpr,",
      "    HAS_DT_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_H: tl.constexpr,",
      "    BLOCK_SIZE_CHUNK: tl.constexpr,",
      "):",
      "",
      "    pid_b = tl.program_id(axis=0)",
      "    pid_c = tl.program_id(axis=1)",
      "    pid_h = tl.program_id(axis=2)",
      "    dt_ptr += pid_b * stride_dt_batch + pid_c * chunk_size * stride_dt_seqlen",
      "    dt_out_ptr += pid_b * stride_dt_out_batch + pid_c * stride_dt_out_chunk",
      "    dA_cumsum_ptr += pid_b * stride_dA_cs_batch + pid_c * stride_dA_cs_chunk",
      "",
      "    offs_h = pid_h * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)",
      "    offs_c = tl.arange(0, BLOCK_SIZE_CHUNK)",
      "    dt_ptrs = dt_ptr + (",
      "        offs_h[:, None] * stride_dt_head + offs_c[None, :] * stride_dt_seqlen",
      "    )",
      "    A_ptrs = A_ptr + offs_h * stride_A_head",
      "    dt_out_ptrs = dt_out_ptr + (",
      "        offs_h[:, None] * stride_dt_out_head + offs_c[None, :] * stride_dt_out_csize",
      "    )",
      "    dA_cs_ptrs = dA_cumsum_ptr + (",
      "        offs_h[:, None] * stride_dA_cs_head + offs_c[None, :] * stride_dA_cs_csize",
      "    )",
      "    chunk_size_limit = min(chunk_size, seqlen - pid_c * chunk_size)",
      "",
      "    dt = tl.load(",
      "        dt_ptrs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit),",
      "        other=0.0,",
      "    ).to(tl.float32)",
      "    if HAS_DT_BIAS:",
      "        dt_bias = tl.load(",
      "            dt_bias_ptr + offs_h * stride_dt_bias_head, mask=offs_h < nheads, other=0.0",
      "        ).to(tl.float32)",
      "        dt += dt_bias[:, None]",
      "    if DT_SOFTPLUS:",
      "        dt = tl.where(dt <= 20.0, tl.log(1 + tl.exp(dt)), dt)",
      "",
      "    dt = tl.minimum(tl.maximum(dt, dt_min), dt_max)",
      "    dt = tl.where(",
      "        (offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size_limit), dt, 0.0",
      "    )",
      "    tl.store(",
      "        dt_out_ptrs,",
      "        dt,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )",
      "    A = tl.load(A_ptrs, mask=offs_h < nheads, other=0.0).to(tl.float32)",
      "    dA = dt * A[:, None]",
      "    dA_cs = tl.cumsum(dA, axis=1)",
      "    tl.store(",
      "        dA_cs_ptrs,",
      "        dA_cs,",
      "        mask=(offs_h[:, None] < nheads) & (offs_c[None, :] < chunk_size),",
      "    )"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/506.py"
  },
  {
    "name": "cross_entropy_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "smoothing",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignored_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "n_rows",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_fwd_kernel(",
      "    loss_ptr,",
      "    lse_ptr,",
      "    logits_ptr,",
      "    labels_ptr,",
      "    smoothing,",
      "    lse_square_scale,",
      "    ignored_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    n_rows,",
      "    logits_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "    SPLIT: tl.constexpr,",
      "):",
      "",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    logits = tl.load(",
      "        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "    ).to(tl.float32)",
      "    max_logits = tl.max(logits, 0)",
      "    if HAS_SMOOTHING:",
      "        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)",
      "    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits",
      "    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)",
      "    if label_idx == ignored_index:",
      "        loss = 0.0",
      "    else:",
      "        label_idx -= class_start_idx",
      "        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(",
      "            n_cols, (col_block_idx + 1) * BLOCK_SIZE",
      "        ):",
      "            logits_label = tl.load(logits_ptr + label_idx)",
      "            if HAS_SMOOTHING:",
      "                loss = (",
      "                    (lse if not SPLIT else 0.0)",
      "                    - smoothing * sum_logits / total_classes",
      "                    - (1 - smoothing) * logits_label",
      "                )",
      "            else:",
      "                loss = (lse if not SPLIT else 0.0) - logits_label",
      "        else:",
      "            if HAS_SMOOTHING:",
      "                loss = smoothing * (",
      "                    (lse if not SPLIT else 0.0) - sum_logits / total_classes",
      "                )",
      "            else:",
      "                loss = 0.0",
      "        if not SPLIT:",
      "            loss += lse_square_scale * lse * lse",
      "    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/513.py"
  },
  {
    "name": "cross_entropy_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "dlogits_ptr",
        "annotation": null
      },
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "smoothing",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignored_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "dlogits_row_stride",
        "annotation": null
      },
      {
        "name": "dloss_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_bwd_kernel(",
      "    dlogits_ptr,",
      "    dloss_ptr,",
      "    logits_ptr,",
      "    lse_ptr,",
      "    labels_ptr,",
      "    smoothing,",
      "    lse_square_scale,",
      "    ignored_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    logits_row_stride,",
      "    dlogits_row_stride,",
      "    dloss_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "):",
      "",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    if label_idx != ignored_index:",
      "        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)",
      "    else:",
      "        dloss = 0.0",
      "    logits = tl.load(",
      "        logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "    ).to(tl.float32)",
      "    lse = tl.load(lse_ptr + row_idx)",
      "    probs = tl.exp(logits - lse)",
      "    probs += 2.0 * lse_square_scale * lse * probs",
      "    label_idx -= class_start_idx",
      "    if HAS_SMOOTHING:",
      "        smooth_negative = smoothing / total_classes",
      "        probs = (",
      "            tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs)",
      "            - smooth_negative",
      "        )",
      "    else:",
      "        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)",
      "    tl.store(dlogits_ptr + col_offsets, dloss * probs, mask=col_offsets < n_cols)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/513.py"
  },
  {
    "name": "cross_entropy_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "z_loss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "smoothing",
        "annotation": null
      },
      {
        "name": "logit_scale",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignored_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "n_rows",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_fwd_kernel(",
      "    loss_ptr,",
      "    lse_ptr,",
      "    z_loss_ptr,",
      "    logits_ptr,",
      "    labels_ptr,",
      "    smoothing,",
      "    logit_scale,",
      "    lse_square_scale,",
      "    ignored_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    n_rows,",
      "    logits_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "    SPLIT: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    logits = (",
      "        tl.load(",
      "            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "        ).to(tl.float32)",
      "        * logit_scale",
      "    )",
      "    max_logits = tl.max(logits, 0)",
      "    if HAS_SMOOTHING:",
      "        sum_logits = tl.sum(tl.where(col_offsets < n_cols, logits, 0.0), 0)",
      "    lse = tl.log(tl.sum(tl.exp(logits - max_logits), 0)) + max_logits",
      "    tl.store(lse_ptr + col_block_idx * n_rows + row_idx, lse)",
      "    if label_idx == ignored_index:",
      "        loss = 0.0",
      "        z_loss = 0.0",
      "    else:",
      "        label_idx -= class_start_idx",
      "        if label_idx >= col_block_idx * BLOCK_SIZE and label_idx < min(",
      "            n_cols, (col_block_idx + 1) * BLOCK_SIZE",
      "        ):",
      "            logits_label = tl.load(logits_ptr + label_idx) * logit_scale",
      "            if HAS_SMOOTHING:",
      "                loss = (",
      "                    (lse if not SPLIT else 0.0)",
      "                    - smoothing * sum_logits / total_classes",
      "                    - (1 - smoothing) * logits_label",
      "                )",
      "            else:",
      "                loss = (lse if not SPLIT else 0.0) - logits_label",
      "        else:",
      "            if HAS_SMOOTHING:",
      "                loss = smoothing * (",
      "                    (lse if not SPLIT else 0.0) - sum_logits / total_classes",
      "                )",
      "            else:",
      "                loss = 0.0",
      "        if not SPLIT:",
      "            z_loss = lse_square_scale * lse * lse",
      "            loss += z_loss",
      "        else:",
      "            z_loss = 0.0",
      "    tl.store(loss_ptr + col_block_idx * n_rows + row_idx, loss)",
      "    if not SPLIT:",
      "        tl.store(z_loss_ptr + col_block_idx * n_rows + row_idx, z_loss)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/515.py"
  },
  {
    "name": "cross_entropy_bwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'HAS_SMOOTHING': lambda args: args['smoothing'] > 0.0})"
    ],
    "args": [
      {
        "name": "dlogits_ptr",
        "annotation": null
      },
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "lse_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "smoothing",
        "annotation": null
      },
      {
        "name": "logit_scale",
        "annotation": null
      },
      {
        "name": "lse_square_scale",
        "annotation": null
      },
      {
        "name": "ignored_index",
        "annotation": null
      },
      {
        "name": "total_classes",
        "annotation": null
      },
      {
        "name": "class_start_idx",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "dlogits_row_stride",
        "annotation": null
      },
      {
        "name": "dloss_row_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SMOOTHING",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def cross_entropy_bwd_kernel(",
      "    dlogits_ptr,",
      "    dloss_ptr,",
      "    logits_ptr,",
      "    lse_ptr,",
      "    labels_ptr,",
      "    smoothing,",
      "    logit_scale,",
      "    lse_square_scale,",
      "    ignored_index,",
      "    total_classes,",
      "    class_start_idx,",
      "    n_cols,",
      "    logits_row_stride,",
      "    dlogits_row_stride,",
      "    dloss_row_stride,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HAS_SMOOTHING: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    col_block_idx = tl.program_id(1)",
      "    logits_ptr = logits_ptr + row_idx * logits_row_stride.to(tl.int64)",
      "    dlogits_ptr = dlogits_ptr + row_idx * dlogits_row_stride.to(tl.int64)",
      "    col_offsets = col_block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    label_idx = tl.load(labels_ptr + row_idx)",
      "    if label_idx != ignored_index:",
      "        dloss = tl.load(dloss_ptr + row_idx * dloss_row_stride)",
      "    else:",
      "        dloss = 0.0",
      "    logits = (",
      "        tl.load(",
      "            logits_ptr + col_offsets, mask=col_offsets < n_cols, other=-float(\"inf\")",
      "        ).to(tl.float32)",
      "        * logit_scale",
      "    )",
      "    lse = tl.load(lse_ptr + row_idx)",
      "    probs = tl.exp(logits - lse)",
      "    probs += 2.0 * lse_square_scale * lse * probs",
      "    label_idx -= class_start_idx",
      "    if HAS_SMOOTHING:",
      "        smooth_positive = 1.0 - smoothing",
      "        smooth_negative = smoothing / total_classes",
      "        probs = (",
      "            tl.where(col_offsets == label_idx, probs - (1 - smoothing), probs)",
      "            - smooth_negative",
      "        )",
      "    else:",
      "        probs = tl.where(col_offsets == label_idx, probs - 1.0, probs)",
      "    tl.store(",
      "        dlogits_ptr + col_offsets,",
      "        (dloss * logit_scale) * probs,",
      "        mask=col_offsets < n_cols,",
      "    )"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/515.py"
  },
  {
    "name": "dequantize_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 128}, num_stages=3, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 256}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])"
    ],
    "args": [
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "b_scale_ptr",
        "annotation": null
      },
      {
        "name": "fpb_ptr",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_fpbk",
        "annotation": null
      },
      {
        "name": "stride_fpbn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dequantize_kernel(",
      "    b_ptr,",
      "    b_scale_ptr,",
      "    fpb_ptr,",
      "    K,",
      "    N,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_fpbk,",
      "    stride_fpbn,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    k_block_idx = tl.program_id(axis=0)",
      "    n_block_idx = tl.program_id(axis=1)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    offs_n = tl.arange(0, BLOCK_SIZE_N)",
      "    b_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_bk + (",
      "        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]",
      "    ) * stride_bn",
      "    fpb_offs = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None]) * stride_fpbk + (",
      "        n_block_idx * BLOCK_SIZE_N + offs_n[None, :]",
      "    ) * stride_fpbn",
      "    bs_offs = n_block_idx * BLOCK_SIZE_N + offs_n[None, :]",
      "    n_mask = n_block_idx * BLOCK_SIZE_N + offs_n[None, :] < N",
      "    mask = (k_block_idx * BLOCK_SIZE_K + offs_k[:, None] < K) & n_mask",
      "    int_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)",
      "    scale_b = tl.load(b_scale_ptr + bs_offs, mask=n_mask, other=0.0)",
      "    tl.store(fpb_ptr + fpb_offs, int_b * scale_b, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/517.py"
  },
  {
    "name": "_cross_entropy_forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'DO_SOFTCAPPING': lambda args: args['DO_SOFTCAPPING'], 'DO_LOGIT_SCALING': lambda args: args['DO_LOGIT_SCALING']})"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "logsumexp_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "VOCAB_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DO_SOFTCAPPING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SOFTCAP",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DO_LOGIT_SCALING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOGIT_SCALE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _cross_entropy_forward(",
      "    logits_ptr,",
      "    logits_row_stride,",
      "    loss_ptr,",
      "    logsumexp_ptr,",
      "    labels_ptr,",
      "    VOCAB_SIZE: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    DO_SOFTCAPPING: tl.constexpr,",
      "    SOFTCAP: tl.constexpr,",
      "    DO_LOGIT_SCALING: tl.constexpr,",
      "    LOGIT_SCALE: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    logits_ptr += row_idx * logits_row_stride.to(tl.int64)",
      "    loss_ptr += row_idx",
      "    logsumexp_ptr += row_idx",
      "    labels_ptr += row_idx",
      "",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    mask = col_offsets < VOCAB_SIZE",
      "",
      "    label_idx = tl.load(labels_ptr).to(tl.int32)",
      "    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))",
      "",
      "    if DO_LOGIT_SCALING:",
      "        logits = LOGIT_SCALE * logits",
      "    if DO_SOFTCAPPING:",
      "        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)",
      "",
      "    logits = logits.to(tl.float32)",
      "    c = tl.max(logits, 0)",
      "    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))",
      "",
      "    if label_idx != -100:",
      "        x = tl.load(logits_ptr + label_idx)",
      "        if DO_LOGIT_SCALING:",
      "            x = LOGIT_SCALE * x",
      "        if DO_SOFTCAPPING:",
      "            x = SOFTCAP * triton_tanh(x / SOFTCAP)",
      "        loss = logsumexp - x.to(tl.float32)",
      "    else:",
      "        loss = 0.0",
      "    tl.store(logsumexp_ptr, logsumexp)",
      "    tl.store(loss_ptr, loss)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/526.py"
  },
  {
    "name": "_chunked_cross_entropy_forward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'DO_SOFTCAPPING': lambda args: args['DO_SOFTCAPPING'], 'DO_LOGIT_SCALING': lambda args: args['DO_LOGIT_SCALING']})"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "loss_ptr",
        "annotation": null
      },
      {
        "name": "logsumexp_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "VOCAB_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "N_CHUNKS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DO_SOFTCAPPING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SOFTCAP",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DO_LOGIT_SCALING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOGIT_SCALE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _chunked_cross_entropy_forward(",
      "    logits_ptr,",
      "    logits_row_stride,",
      "    loss_ptr,",
      "    logsumexp_ptr,",
      "    labels_ptr,",
      "    VOCAB_SIZE: tl.constexpr,",
      "    N_CHUNKS: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    DO_SOFTCAPPING: tl.constexpr,",
      "    SOFTCAP: tl.constexpr,",
      "    DO_LOGIT_SCALING: tl.constexpr,",
      "    LOGIT_SCALE: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    chunk_idx = tl.program_id(1)",
      "    logits_ptr += row_idx * logits_row_stride.to(tl.int64)",
      "    loss_ptr += row_idx",
      "    logsumexp_ptr += row_idx * N_CHUNKS + chunk_idx",
      "    labels_ptr += row_idx",
      "",
      "    col_offsets = chunk_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = col_offsets < VOCAB_SIZE",
      "",
      "    label_idx = tl.load(labels_ptr).to(tl.int32)",
      "    logits = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))",
      "",
      "    if DO_LOGIT_SCALING:",
      "        logits = LOGIT_SCALE * logits",
      "    if DO_SOFTCAPPING:",
      "        logits = SOFTCAP * triton_tanh(logits / SOFTCAP)",
      "",
      "    logits = logits.to(tl.float32)",
      "    c = tl.max(logits, 0)",
      "    logsumexp = c + tl.log(tl.sum(tl.exp(logits - c), 0))",
      "",
      "    if chunk_idx == 0:",
      "        if label_idx != -100:",
      "            x = tl.load(logits_ptr + label_idx).to(tl.float32)",
      "            if DO_LOGIT_SCALING:",
      "                x = LOGIT_SCALE * x",
      "            if DO_SOFTCAPPING:",
      "                x = SOFTCAP * triton_tanh(x / SOFTCAP)",
      "            loss = -1.0 * x.to(tl.float32)",
      "        else:",
      "            loss = 0.0",
      "        tl.store(loss_ptr, loss)",
      "        tl.store(logsumexp_ptr, logsumexp)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/526.py"
  },
  {
    "name": "_cross_entropy_backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'DO_SOFTCAPPING': lambda args: args['DO_SOFTCAPPING'], 'DO_LOGIT_SCALING': lambda args: args['DO_LOGIT_SCALING']})"
    ],
    "args": [
      {
        "name": "logits_ptr",
        "annotation": null
      },
      {
        "name": "logits_row_stride",
        "annotation": null
      },
      {
        "name": "dloss_ptr",
        "annotation": null
      },
      {
        "name": "dloss_row_stride",
        "annotation": null
      },
      {
        "name": "logsumexp_ptr",
        "annotation": null
      },
      {
        "name": "labels_ptr",
        "annotation": null
      },
      {
        "name": "VOCAB_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DO_SOFTCAPPING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SOFTCAP",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DO_LOGIT_SCALING",
        "annotation": "tl.constexpr"
      },
      {
        "name": "LOGIT_SCALE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _cross_entropy_backward(",
      "    logits_ptr,",
      "    logits_row_stride,",
      "    dloss_ptr,",
      "    dloss_row_stride,",
      "    logsumexp_ptr,",
      "    labels_ptr,",
      "    VOCAB_SIZE: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    DO_SOFTCAPPING: tl.constexpr,",
      "    SOFTCAP: tl.constexpr,",
      "    DO_LOGIT_SCALING: tl.constexpr,",
      "    LOGIT_SCALE: tl.constexpr,",
      "):",
      "    row_idx = tl.program_id(0)",
      "    block_idx = tl.program_id(1)",
      "",
      "    logits_ptr += row_idx * logits_row_stride.to(tl.int64)",
      "    dloss_ptr += row_idx * dloss_row_stride",
      "    col_offsets = block_idx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = col_offsets < VOCAB_SIZE",
      "    label_idx = tl.load(labels_ptr + row_idx).to(tl.int32)",
      "",
      "    if label_idx != -100:",
      "        dloss = tl.load(dloss_ptr)",
      "    else:",
      "        dloss = 0.0",
      "",
      "    x = tl.load(logits_ptr + col_offsets, mask=mask, other=-float(\"inf\"))",
      "",
      "    if DO_LOGIT_SCALING:",
      "        x = x * LOGIT_SCALE",
      "",
      "    if DO_SOFTCAPPING:",
      "        partial = triton_tanh(x / SOFTCAP)",
      "        x = SOFTCAP * partial",
      "",
      "    logsumexp = tl.load(logsumexp_ptr + row_idx)",
      "    y = tl.exp(x.to(tl.float32) - logsumexp)",
      "    y = tl.where(",
      "        col_offsets == label_idx,",
      "        y - 1.0,",
      "        y,",
      "    )",
      "",
      "    if DO_LOGIT_SCALING:",
      "        y = y * LOGIT_SCALE",
      "",
      "    if DO_SOFTCAPPING:",
      "        y = y * (1.0 - partial * partial)",
      "",
      "    tl.store(logits_ptr + col_offsets, dloss * y, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/526.py"
  },
  {
    "name": "_rms_layernorm_backward",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'GEMMA': lambda args: args['GEMMA']})"
    ],
    "args": [
      {
        "name": "dY",
        "annotation": null
      },
      {
        "name": "dY_row_stride",
        "annotation": null
      },
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "X_row_stride",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "W_row_stride",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "r_row_stride",
        "annotation": null
      },
      {
        "name": "dW",
        "annotation": null
      },
      {
        "name": "dW_row_stride",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "GEMMA",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_layernorm_backward(",
      "    dY,",
      "    dY_row_stride,",
      "    X,",
      "    X_row_stride,",
      "    W,",
      "    W_row_stride,",
      "    r,",
      "    r_row_stride,",
      "    dW,",
      "    dW_row_stride,",
      "    n_cols,",
      "    eps,",
      "    GEMMA: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    row_idx = tl.program_id(0)",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    mask = col_offsets < n_cols",
      "",
      "    dY += row_idx * dY_row_stride",
      "    X += row_idx * X_row_stride",
      "    r += row_idx * r_row_stride",
      "",
      "    dY_row = tl.load(dY + col_offsets, mask=mask, other=0).to(tl.float32)",
      "    X_row = tl.load(X + col_offsets, mask=mask, other=0).to(tl.float32)",
      "    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)",
      "",
      "    inv_var = tl.load(r).to(tl.float32)",
      "    normed = X_row * inv_var",
      "",
      "    if GEMMA:",
      "        dY_W = dY_row * (W_row + 1.0)",
      "    else:",
      "        dY_W = dY_row * W_row",
      "",
      "    rowsum_dY_normed = tl.sum(dY_W * normed, axis=0)",
      "    output = inv_var / n_cols * (n_cols * dY_W - normed * rowsum_dY_normed)",
      "    tl.store(dY + col_offsets, output, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/528.py"
  },
  {
    "name": "triton_f4_to_scaled_bf16_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_IN': 128}), triton.Config({'BLOCK_SIZE_IN': 256}), triton.Config({'BLOCK_SIZE_IN': 512}), triton.Config({'BLOCK_SIZE_IN': 1024}), triton.Config({'BLOCK_SIZE_IN': 2048})], key=['n_elements_in'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "s_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "n_elements_in",
        "annotation": null
      },
      {
        "name": "mx_block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "sign_mask_f4",
        "annotation": "tl.constexpr"
      },
      {
        "name": "mantissa_mask_f4",
        "annotation": "tl.constexpr"
      },
      {
        "name": "mbits_f4_e2m1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ebits_f4_e2m1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "f4_e2m1_exp_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "mbits_f32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ebits_f32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "f32_exp_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "zero_bits_f32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "zero_point_five_bits_f32",
        "annotation": "tl.constexpr"
      },
      {
        "name": "e8m0_exponent_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "e8m0_exponent_nan_val",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_IN",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_f4_to_scaled_bf16_kernel(",
      "    x_ptr,",
      "    s_ptr,",
      "    output_ptr,",
      "    n_elements_in,",
      "    mx_block_size: tl.constexpr,",
      "    sign_mask_f4: tl.constexpr,",
      "    mantissa_mask_f4: tl.constexpr,",
      "    mbits_f4_e2m1: tl.constexpr,",
      "    ebits_f4_e2m1: tl.constexpr,",
      "    f4_e2m1_exp_bias: tl.constexpr,",
      "    mbits_f32: tl.constexpr,",
      "    ebits_f32: tl.constexpr,",
      "    f32_exp_bias: tl.constexpr,",
      "    zero_bits_f32: tl.constexpr,",
      "    zero_point_five_bits_f32: tl.constexpr,",
      "    e8m0_exponent_bias: tl.constexpr,",
      "    e8m0_exponent_nan_val: tl.constexpr,",
      "    BLOCK_SIZE_IN: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    n_elements_out = n_elements_in * 2",
      "    n_elements_s = n_elements_out // 32",
      "",
      "    BLOCK_SIZE_S: tl.constexpr = BLOCK_SIZE_IN // 16",
      "    BLOCK_SIZE_OUT: tl.constexpr = BLOCK_SIZE_IN * 2",
      "",
      "    block_start_in = pid * BLOCK_SIZE_IN",
      "    offsets_in = block_start_in + tl.arange(0, BLOCK_SIZE_IN)",
      "    mask_in = offsets_in < n_elements_in",
      "",
      "    x_packed = tl.load(x_ptr + offsets_in, mask=mask_in)",
      "    output = _fp4_packed_to_bf16(",
      "        x_packed,",
      "        sign_mask_f4,",
      "        mantissa_mask_f4,",
      "        mbits_f4_e2m1,",
      "        ebits_f4_e2m1,",
      "        f4_e2m1_exp_bias,",
      "        mbits_f32,",
      "        ebits_f32,",
      "        f32_exp_bias,",
      "        zero_bits_f32,",
      "        zero_point_five_bits_f32,",
      "    )",
      "",
      "    block_start_s = pid * BLOCK_SIZE_S",
      "    offsets_s = block_start_s + tl.arange(0, BLOCK_SIZE_S)",
      "    mask_s = offsets_s < n_elements_s",
      "    s = tl.load(s_ptr + offsets_s, mask=mask_s)",
      "",
      "    s_offset = s.to(tl.int16) - e8m0_exponent_bias",
      "    s_fp = tl.extra.cuda.libdevice.pow(2.0, s_offset).to(tl.bfloat16)",
      "    s_fp = tl.where(s != e8m0_exponent_nan_val, s_fp, float(\"nan\"))",
      "",
      "    output = tl.reshape(output, (BLOCK_SIZE_OUT // mx_block_size, mx_block_size))",
      "    s_fp = tl.reshape(s_fp, (BLOCK_SIZE_S // 1, 1))",
      "    output = output * s_fp",
      "    output = tl.reshape(output, (BLOCK_SIZE_OUT,))",
      "",
      "    block_start_out = pid * BLOCK_SIZE_OUT",
      "    offsets_out = block_start_out + tl.arange(0, BLOCK_SIZE_OUT)",
      "    mask_out = offsets_out < n_elements_out",
      "",
      "    tl.store(output_ptr + offsets_out, output, mask=mask_out)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/535.py"
  },
  {
    "name": "triton_red_fused_native_layer_norm_0",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_out_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "out_ptr1",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_native_layer_norm_0(",
      "    in_out_ptr0,",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr0,",
      "    out_ptr1,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :]",
      "    x0 = xindex",
      "    tmp3_mean = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    tmp3_m2 = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    tmp3_weight = tl.zeros([XBLOCK, RBLOCK], tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp0 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp1 = tmp0.to(tl.float32)",
      "        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])",
      "        tmp3_mean_next, tmp3_m2_next, tmp3_weight_next = triton_helpers.welford_reduce(",
      "            tmp2, tmp3_mean, tmp3_m2, tmp3_weight, roffset == 0",
      "        )",
      "        tmp3_mean = tl.where(rmask, tmp3_mean_next, tmp3_mean)",
      "        tmp3_m2 = tl.where(rmask, tmp3_m2_next, tmp3_m2)",
      "        tmp3_weight = tl.where(rmask, tmp3_weight_next, tmp3_weight)",
      "    tmp3_tmp, tmp4_tmp, tmp5_tmp = triton_helpers.welford(",
      "        tmp3_mean, tmp3_m2, tmp3_weight, 1",
      "    )",
      "    tmp3 = tmp3_tmp[:, None]",
      "    tmp4 = tmp4_tmp[:, None]",
      "    tmp5 = tmp5_tmp[:, None]",
      "    tl.store(out_ptr0 + (x0), tmp3, None)",
      "    tmp6 = rnumel",
      "    tmp7 = tmp4 / tmp6",
      "    tmp8 = 1e-05",
      "    tmp9 = tmp7 + tmp8",
      "    tmp10 = libdevice.rsqrt(tmp9)",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr0 + (x0), tmp10, None)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp11 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"",
      "        ).to(tl.float32)",
      "        tmp15 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp18 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp12 = tmp11.to(tl.float32)",
      "        tmp13 = tmp12 - tmp3",
      "        tmp14 = tmp13 * tmp10",
      "        tmp16 = tmp15.to(tl.float32)",
      "        tmp17 = tmp14 * tmp16",
      "        tmp19 = tmp18.to(tl.float32)",
      "        tmp20 = tmp17 + tmp19",
      "        tmp21 = tmp20.to(tl.float32)",
      "        tl.store(out_ptr1 + (r1 + (rnumel * x0)), tmp21, rmask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/537.py"
  },
  {
    "name": "fused_recurrent_hgrn_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4), triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64}, num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton.Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps=2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128}, num_warps=8)], key=['D'])"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "ht",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_FINAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_hgrn_fwd_kernel(",
      "    x,",
      "    g,",
      "    o,",
      "    h0,",
      "    ht,",
      "    T: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "    STORE_FINAL_STATE: tl.constexpr,",
      "):",
      "    i_d, i_bh = tl.program_id(0), tl.program_id(1)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_x = x + i_bh * T * D + o_d",
      "    p_g = g + i_bh * T * D + o_d",
      "    p_o = o + i_bh * T * D + o_d",
      "",
      "    b_h = tl.zeros([BD], dtype=tl.float32)",
      "    if USE_INITIAL_STATE:",
      "        p_h0 = h0 + i_bh * D + o_d",
      "        b_h += tl.load(p_h0, mask=mask, other=0).to(tl.float32)",
      "    for _ in range(0, T):",
      "        b_x = tl.load(p_x, mask=mask, other=0).to(tl.float32)",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_h = b_g * b_h + b_x",
      "        tl.store(p_o, b_h.to(p_o.dtype.element_ty), mask=mask)",
      "",
      "        p_x += D",
      "        p_g += D",
      "        p_o += D",
      "",
      "    if STORE_FINAL_STATE:",
      "        p_ht = ht + i_bh * D + o_d",
      "        tl.store(p_ht, b_h.to(p_ht.dtype.element_ty), mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/539.py"
  },
  {
    "name": "fused_recurrent_hgrn_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BD': 32}, num_warps=1), triton.Config({'BD': 32}, num_warps=2), triton.Config({'BD': 32}, num_warps=4), triton.Config({'BD': 32}, num_warps=8), triton.Config({'BD': 64}, num_warps=1), triton.Config({'BD': 64}, num_warps=2), triton.Config({'BD': 64}, num_warps=4), triton.Config({'BD': 64}, num_warps=8), triton.Config({'BD': 128}, num_warps=1), triton.Config({'BD': 128}, num_warps=2), triton.Config({'BD': 128}, num_warps=4), triton.Config({'BD': 128}, num_warps=8)], key=['D'])"
    ],
    "args": [
      {
        "name": "g",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "dx",
        "annotation": null
      },
      {
        "name": "dg",
        "annotation": null
      },
      {
        "name": "do",
        "annotation": null
      },
      {
        "name": "h0",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_INITIAL_STATE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_recurrent_hgrn_bwd_kernel(",
      "    g,",
      "    o,",
      "    dx,",
      "    dg,",
      "    do,",
      "    h0,",
      "    T: tl.constexpr,",
      "    D: tl.constexpr,",
      "    BD: tl.constexpr,",
      "    USE_INITIAL_STATE: tl.constexpr,",
      "):",
      "    i_d, i_bh = tl.program_id(0), tl.program_id(1)",
      "    o_d = i_d * BD + tl.arange(0, BD)",
      "    mask = o_d < D",
      "",
      "    p_g = g + (i_bh * T + T - 1) * D + o_d",
      "    p_o = o + (i_bh * T + T - 2) * D + o_d",
      "    p_dx = dx + (i_bh * T + T - 1) * D + o_d",
      "    p_dg = dg + (i_bh * T + T - 1) * D + o_d",
      "    p_do = do + (i_bh * T + T - 1) * D + o_d",
      "",
      "    b_dh = tl.zeros([BD], dtype=tl.float32)",
      "    for i in range(T - 1, -1, -1):",
      "        b_g = tl.load(p_g, mask=mask, other=0).to(tl.float32)",
      "        b_do = tl.load(p_do, mask=mask, other=0).to(tl.float32)",
      "        if i > 0:",
      "            b_o = tl.load(p_o, mask=mask, other=0).to(tl.float32)",
      "        elif USE_INITIAL_STATE:",
      "            b_o = tl.load(h0 + i_bh * D + o_d, mask=mask, other=0).to(tl.float32)",
      "        else:",
      "            b_o = tl.zeros([BD], dtype=tl.float32)",
      "",
      "        b_dh = b_dh + b_do",
      "        b_dx = b_dh",
      "        b_dg = b_dh * b_o",
      "        b_dh = b_dh * b_g",
      "        tl.store(p_dx, b_dx.to(p_dx.dtype.element_ty), mask=mask)",
      "        tl.store(p_dg, b_dg.to(p_dg.dtype.element_ty), mask=mask)",
      "",
      "        p_g -= D",
      "        p_o -= D",
      "        p_dx -= D",
      "        p_dg -= D",
      "        p_do -= D"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/539.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 16, 'BLOCK_SIZE_K': 512, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "bs_ptr",
        "annotation": null
      },
      {
        "name": "bzp_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "stride_bsk",
        "annotation": null
      },
      {
        "name": "stride_bsn",
        "annotation": null
      },
      {
        "name": "stride_bzpk",
        "annotation": null
      },
      {
        "name": "stride_bzpn",
        "annotation": null
      },
      {
        "name": "group_size",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    bs_ptr,",
      "    bzp_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_bsk,",
      "    stride_bsn,",
      "    stride_bzpk,",
      "    stride_bzpn,",
      "    group_size,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    pid_sp_k = tl.program_id(axis=1)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak",
      "",
      "    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):",
      "",
      "        bs_ptrs = (",
      "            bs_ptr",
      "            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)",
      "            * stride_bsk",
      "            + offs_bn[None, :] * stride_bsn",
      "        )",
      "",
      "        bzp_ptrs = (",
      "            bzp_ptr",
      "            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)",
      "            * stride_bzpk",
      "            + (offs_bn[None, :] // 8) * stride_bzpn",
      "        )",
      "        b_shift_bits = (offs_k[:, None] % 8) * 4",
      "        bzp_shift_bits = (offs_bn[None, :] % 8) * 4",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "        bs = tl.load(bs_ptrs)",
      "        bzp = tl.load(bzp_ptrs)",
      "",
      "        int_b = (b >> b_shift_bits) & 0xF",
      "        int_bzp = (bzp >> bzp_shift_bits) & 0xF",
      "        b = ((int_b - int_bzp) * bs).to(a.dtype)",
      "        accumulator += tl.dot(a, b.to(a.dtype))",
      "",
      "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk // 8",
      "",
      "    c = accumulator.to(c_ptr.dtype.element_ty)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    if SPLIT_K == 1:",
      "        tl.store(c_ptrs, c, mask=c_mask)",
      "    else:",
      "        tl.atomic_add(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/546.py"
  },
  {
    "name": "_int8_matmul_rowwise_dequantize",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2), *get_configs_io_bound()], key=['M', 'N', 'K'], prune_configs_by={'early_config_prune': early_config_prune, 'perf_model': estimate_matmul_time, 'top_k': 10})",
      "@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "state_x_ptr",
        "annotation": null
      },
      {
        "name": "state_w_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "divfactor",
        "annotation": null
      },
      {
        "name": "has_bias",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACC_TYPE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _int8_matmul_rowwise_dequantize(",
      "    A,",
      "    B,",
      "    C,",
      "    bias,",
      "    state_x_ptr,",
      "    state_w_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    divfactor,",
      "    has_bias: tl.constexpr,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    ACC_TYPE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    pid_z = tl.program_id(1)",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "    width = GROUP_M * grid_n",
      "    group_id = pid // width",
      "    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (pid % group_size)",
      "    pid_n = (pid % width) // (group_size)",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)",
      "    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)",
      "    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)",
      "    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)",
      "    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    w_factor = tl.load(state_w_ptr + rbn)[None, :]",
      "    x_factor = tl.load(state_x_ptr + ram)[:, None]",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):",
      "        if EVEN_K:",
      "            a = tl.load(A)",
      "            b = tl.load(B)",
      "        else:",
      "            k_remaining = K - k * (BLOCK_K * SPLIT_K)",
      "            a = tl.load(A, mask=rk[None, :] < k_remaining, other=0.0)",
      "            b = tl.load(B, mask=rk[:, None] < k_remaining, other=0.0)",
      "        acc += tl.dot(a, b)",
      "        A += BLOCK_K * SPLIT_K * stride_ak",
      "        B += BLOCK_K * SPLIT_K * stride_bk",
      "    acc = w_factor * (x_factor * (acc * divfactor))",
      "    acc = acc.to(C.dtype.element_ty)",
      "    if has_bias:",
      "        bias = tl.load(bias + rn).to(C.dtype.element_ty)",
      "        acc = acc + bias[None, :]",
      "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "    mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "    if SPLIT_K == 1:",
      "        tl.store(C, acc, mask=mask)",
      "    else:",
      "        tl.atomic_add(C, acc, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/547.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K: tl.constexpr,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    tl.static_assert(",
      "        K % (4 * BLOCK_SIZE_K) == 0,",
      "        \"K / 4 must be divisible by BLOCK_SIZE_K => K divisible by 4*BLOCK_SIZE_K\",",
      "    )",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)",
      "    for i in range(4):",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "        for j in range(0, tl.cdiv(K // 4, BLOCK_SIZE_K)):",
      "            k = i * tl.cdiv(K // 4, BLOCK_SIZE_K) + j",
      "            a = tl.load(",
      "                a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0",
      "            ).to(tl.int8)",
      "            b_uint8 = tl.load(b_ptrs, mask=offs_k[:, None] < K, other=0)",
      "            mask = 3 << (2 * i)",
      "            b = ((b_uint8 & mask) >> (2 * i)).to(tl.int8)",
      "            tensor_full = tl.full((1,), 1, dtype=tl.int8)",
      "            accumulator += tl.dot(a, (b - tensor_full), out_dtype=tl.int32)",
      "            a_ptrs += BLOCK_SIZE_K * stride_ak",
      "            b_ptrs += BLOCK_SIZE_K * stride_bk",
      "    c = accumulator",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/548.py"
  },
  {
    "name": "quantize_int8_perrow_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_stages=2, num_warps=8), triton.Config({}, num_stages=2, num_warps=4), triton.Config({}, num_stages=2, num_warps=2), triton.Config({}, num_stages=2, num_warps=1)], key=['K'])"
    ],
    "args": [
      {
        "name": "fpa_ptr",
        "annotation": null
      },
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "as_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_fpam",
        "annotation": null
      },
      {
        "name": "stride_fpak",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_asm",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def quantize_int8_perrow_kernel(",
      "    fpa_ptr,",
      "    a_ptr,",
      "    as_ptr,",
      "    M,",
      "    K,",
      "    stride_fpam,",
      "    stride_fpak,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_asm,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(axis=0)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "",
      "    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak",
      "    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak",
      "    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        a_max = tl.maximum(a_max, tl.max(tl.abs(fpa), axis=1))",
      "        fpa_ptrs += BLOCK_SIZE_K * stride_fpak",
      "    a_scale = a_max / 127.0",
      "    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        inta = (fpa / a_scale[:, None]).to(tl.int8)",
      "        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)",
      "        fpa_ptrs += BLOCK_SIZE_K * stride_fpak",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)",
      "    tl.store(as_ptr + as_offs, a_scale)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/549.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8), triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "as_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "bs_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_asm",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_bsn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    as_ptr,",
      "    b_ptr,",
      "    bs_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_asm,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_bsn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    pid_sp_k = tl.program_id(axis=1)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "    as_ptrs = as_ptr + offs_am * stride_asm",
      "    bs_ptrs = bs_ptr + offs_bn * stride_bsn",
      "    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)",
      "    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.int32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):",
      "",
      "        a = tl.load(",
      "            a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0",
      "        )",
      "        b = tl.load(",
      "            b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0",
      "        )",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk",
      "",
      "    c = (accumulator.to(tl.float32) * a_scale[:, None] * b_scale[None, :]).to(",
      "        tl.float16",
      "    )",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    if SPLIT_K == 1:",
      "        tl.store(c_ptrs, c, mask=c_mask)",
      "    else:",
      "        tl.atomic_add(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/549.py"
  },
  {
    "name": "_softmax",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['K'])",
      "@triton.heuristics({'DEPTH': lambda nargs: get_depth(nargs['K'])})",
      "@triton.heuristics({'IS_FP16': lambda nargs: nargs['Y'].dtype == torch.float16})"
    ],
    "args": [
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "stride_ym",
        "annotation": null
      },
      {
        "name": "stride_yn",
        "annotation": null
      },
      {
        "name": "stride_xm",
        "annotation": null
      },
      {
        "name": "stride_xn",
        "annotation": null
      },
      {
        "name": "stride_m",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "LOG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "MASK_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DEPTH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_FP16",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _softmax(",
      "    Y,",
      "    X,",
      "    M,",
      "    stride_ym,",
      "    stride_yn,",
      "    stride_xm,",
      "    stride_xn,",
      "    stride_m,",
      "    K,",
      "    LOG: tl.constexpr,",
      "    MASK_TYPE: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    DEPTH: tl.constexpr,",
      "    IS_FP16: tl.constexpr,",
      "):",
      "",
      "    m = tl.program_id(0)",
      "    n = tl.program_id(1)",
      "    k = tl.arange(0, DEPTH)",
      "    x_ptrs = X + m * stride_xm + n * stride_xn + k",
      "    io_mask = k < K",
      "    if CAUSAL:",
      "        io_mask = io_mask & (k <= n)",
      "    x = tl.load(x_ptrs, mask=io_mask, other=float(\"-inf\"))",
      "    if CAUSAL:",
      "        off = float(\"-inf\")",
      "        off = off.to(x.dtype)",
      "        x = tl.where(k > n, off, x)",
      "    if MASK_TYPE is not None:",
      "        if MASK_TYPE == \"qk\":",
      "            mask_ptrs = M + n * stride_m + k",
      "        elif MASK_TYPE == \"bk\":",
      "            mask_ptrs = M + m * stride_m + k",
      "        add_mask = tl.load(mask_ptrs, io_mask, other=float(\"-inf\"))",
      "        x += add_mask",
      "    z = x - tl.max(x, axis=0)",
      "    if IS_FP16:",
      "        z = z.to(tl.float32)",
      "    num = tl.exp(z)",
      "    denom = tl.sum(num, axis=0)",
      "    if LOG:",
      "        y = z - tl.log(denom)",
      "    else:",
      "        y = num / denom",
      "    y_ptrs = Y + m * stride_ym + n * stride_yn + k",
      "    tl.store(y_ptrs, y, mask=k < K)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/557.py"
  },
  {
    "name": "_softmax_backward",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['K'])",
      "@triton.heuristics({'DEPTH': lambda nargs: get_depth(nargs['K'])})",
      "@triton.heuristics({'IS_FP16': lambda nargs: nargs['GradIn'].dtype == torch.float16})"
    ],
    "args": [
      {
        "name": "GradIn",
        "annotation": null
      },
      {
        "name": "GradOut",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_gm",
        "annotation": null
      },
      {
        "name": "stride_gn",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "LOG",
        "annotation": "tl.constexpr"
      },
      {
        "name": "CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DEPTH",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_FP16",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _softmax_backward(",
      "    GradIn,",
      "    GradOut,",
      "    Out,",
      "    stride_bm,",
      "    stride_bn,",
      "    stride_gm,",
      "    stride_gn,",
      "    stride_om,",
      "    stride_on,",
      "    K,",
      "    LOG: tl.constexpr,",
      "    CAUSAL: tl.constexpr,",
      "    DEPTH: tl.constexpr,",
      "    IS_FP16: tl.constexpr,",
      "):",
      "",
      "    m = tl.program_id(0)",
      "    n = tl.program_id(1)",
      "    k = tl.arange(0, DEPTH)",
      "    grad_out_ptrs = GradOut + m * stride_gm + n * stride_gn + k",
      "    out_ptrs = Out + m * stride_om + n * stride_on + k",
      "    io_mask = k < K",
      "    if CAUSAL:",
      "        io_mask = io_mask & (k <= n)",
      "    g = tl.load(grad_out_ptrs, mask=io_mask, other=float(0))",
      "    o = tl.load(out_ptrs, mask=io_mask, other=float(0))",
      "    if CAUSAL:",
      "        zero = float(0)",
      "        zero = zero.to(g.dtype)",
      "        g = tl.where(k > n, zero, g)",
      "        o = tl.where(k > n, zero, o)",
      "    if LOG:",
      "        s = tl.sum(g, 0)",
      "        if IS_FP16:",
      "            o = o.to(tl.float32)",
      "        grad_in = g - tl.exp(o) * s",
      "    else:",
      "        s = tl.sum(g * o, 0)",
      "        grad_in = o * (g - s)",
      "    grad_in_ptrs = GradIn + m * stride_bm + n * stride_bn + k",
      "    tl.store(grad_in_ptrs, grad_in, mask=k < K)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/557.py"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])",
      "@triton.heuristics({'HAS_X1': lambda args: args['X1'] is not None})",
      "@triton.heuristics({'HAS_W1': lambda args: args['W1'] is not None})",
      "@triton.heuristics({'HAS_B1': lambda args: args['B1'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "X1",
        "annotation": null
      },
      {
        "name": "W1",
        "annotation": null
      },
      {
        "name": "B1",
        "annotation": null
      },
      {
        "name": "Y1",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "ROWSCALE",
        "annotation": null
      },
      {
        "name": "SEEDS",
        "annotation": null
      },
      {
        "name": "DROPOUT_MASK",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "stride_x1_row",
        "annotation": null
      },
      {
        "name": "stride_y1_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DROPOUT_MASK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_ROWSCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_X1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_W1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_B1",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    X1,",
      "    W1,",
      "    B1,",
      "    Y1,",
      "    RESIDUAL_OUT,",
      "    ROWSCALE,",
      "    SEEDS,",
      "    DROPOUT_MASK,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    stride_x1_row,",
      "    stride_y1_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    dropout_p,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    HAS_DROPOUT: tl.constexpr,",
      "    STORE_DROPOUT_MASK: tl.constexpr,",
      "    HAS_ROWSCALE: tl.constexpr,",
      "    HAS_X1: tl.constexpr,",
      "    HAS_W1: tl.constexpr,",
      "    HAS_B1: tl.constexpr,",
      "):",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "    if HAS_X1:",
      "        X1 += row * stride_x1_row",
      "    if HAS_W1:",
      "        Y1 += row * stride_y1_row",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_ROWSCALE:",
      "        rowscale = tl.load(ROWSCALE + row).to(tl.float32)",
      "        x *= rowscale",
      "    if HAS_DROPOUT:",
      "        keep_mask = (",
      "            tl.rand(tl.load(SEEDS + row).to(tl.uint32), cols, n_rounds=7) > dropout_p",
      "        )",
      "        x = tl.where(keep_mask, x / (1.0 - dropout_p), 0.0)",
      "        if STORE_DROPOUT_MASK:",
      "            tl.store(DROPOUT_MASK + row * N + cols, keep_mask, mask=cols < N)",
      "    if HAS_X1:",
      "        x1 = tl.load(X1 + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        if HAS_ROWSCALE:",
      "            rowscale = tl.load(ROWSCALE + M + row).to(tl.float32)",
      "            x1 *= rowscale",
      "        if HAS_DROPOUT:",
      "            keep_mask = (",
      "                tl.rand(tl.load(SEEDS + M + row).to(tl.uint32), cols, n_rounds=7)",
      "                > dropout_p",
      "            )",
      "            x1 = tl.where(keep_mask, x1 / (1.0 - dropout_p), 0.0)",
      "            if STORE_DROPOUT_MASK:",
      "                tl.store(DROPOUT_MASK + (M + row) * N + cols, keep_mask, mask=cols < N)",
      "        x += x1",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    tl.store(Y + cols, y, mask=mask)",
      "    if HAS_W1:",
      "        w1 = tl.load(W1 + cols, mask=mask).to(tl.float32)",
      "        if HAS_B1:",
      "            b1 = tl.load(B1 + cols, mask=mask).to(tl.float32)",
      "        y1 = x_hat * w1 + b1 if HAS_B1 else x_hat * w1",
      "        tl.store(Y1 + cols, y1, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/563.py"
  },
  {
    "name": "_layer_norm_fwd_1pass_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_RESIDUAL', 'STORE_RESIDUAL_OUT', 'IS_RMS_NORM', 'HAS_BIAS'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "RESIDUAL",
        "annotation": null
      },
      {
        "name": "RESIDUAL_OUT",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_res_row",
        "annotation": null
      },
      {
        "name": "stride_res_out_row",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_RESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_RESIDUAL_OUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_fwd_1pass_kernel(",
      "    X,",
      "    Y,",
      "    W,",
      "    B,",
      "    RESIDUAL,",
      "    RESIDUAL_OUT,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_res_row,",
      "    stride_res_out_row,",
      "    N,",
      "    eps,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_RESIDUAL: tl.constexpr,",
      "    STORE_RESIDUAL_OUT: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "):",
      "    row = tl.program_id(0)",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    if HAS_RESIDUAL:",
      "        RESIDUAL += row * stride_res_row",
      "    if STORE_RESIDUAL_OUT:",
      "        RESIDUAL_OUT += row * stride_res_out_row",
      "    cols = tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "    if HAS_RESIDUAL:",
      "        residual = tl.load(RESIDUAL + cols, mask=cols < N, other=0.0).to(tl.float32)",
      "        x += residual",
      "    if STORE_RESIDUAL_OUT:",
      "        tl.store(RESIDUAL_OUT + cols, x, mask=cols < N)",
      "    if not IS_RMS_NORM:",
      "        mean = tl.sum(x, axis=0) / N",
      "        tl.store(Mean + row, mean)",
      "        xbar = tl.where(cols < N, x - mean, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    else:",
      "        xbar = tl.where(cols < N, x, 0.0)",
      "        var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "    tl.store(Rstd + row, rstd)",
      "    mask = cols < N",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask).to(tl.float32)",
      "    x_hat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "    y = x_hat * w + b if HAS_BIAS else x_hat * w",
      "    tl.store(Y + cols, y, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/565.py"
  },
  {
    "name": "_layer_norm_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N', 'HAS_DRESIDUAL', 'STORE_DRESIDUAL', 'IS_RMS_NORM', 'HAS_BIAS'])",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['Y'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "DB",
        "annotation": null
      },
      {
        "name": "DRESIDUAL",
        "annotation": null
      },
      {
        "name": "DRESIDUAL_IN",
        "annotation": null
      },
      {
        "name": "Mean",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dres_row",
        "annotation": null
      },
      {
        "name": "stride_dres_in_row",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "IS_RMS_NORM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "STORE_DRESIDUAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _layer_norm_bwd_kernel(",
      "    X,",
      "    W,",
      "    B,",
      "    Y,",
      "    DY,",
      "    DX,",
      "    DW,",
      "    DB,",
      "    DRESIDUAL,",
      "    DRESIDUAL_IN,",
      "    Mean,",
      "    Rstd,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dy_row,",
      "    stride_dx_row,",
      "    stride_dres_row,",
      "    stride_dres_in_row,",
      "    M,",
      "    N,",
      "    eps,",
      "    rows_per_program,",
      "    IS_RMS_NORM: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    HAS_DRESIDUAL: tl.constexpr,",
      "    STORE_DRESIDUAL: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, BLOCK_N)",
      "    mask = cols < N",
      "    X += row_start * stride_x_row",
      "    if HAS_DRESIDUAL:",
      "        DRESIDUAL += row_start * stride_dres_row",
      "    if STORE_DRESIDUAL:",
      "        DRESIDUAL_IN += row_start * stride_dres_in_row",
      "    DY += row_start * stride_dy_row",
      "    DX += row_start * stride_dx_row",
      "    if RECOMPUTE_OUTPUT:",
      "        Y += row_start * stride_y_row",
      "    w = tl.load(W + cols, mask=mask).to(tl.float32)",
      "    if RECOMPUTE_OUTPUT and HAS_BIAS:",
      "        b = tl.load(B + cols, mask=mask, other=0.0).to(tl.float32)",
      "    dw = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    if HAS_BIAS:",
      "        db = tl.zeros((BLOCK_N,), dtype=tl.float32)",
      "    row_end = min((row_block_id + 1) * rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "        x = tl.load(X + cols, mask=mask, other=0).to(tl.float32)",
      "        dy = tl.load(DY + cols, mask=mask, other=0).to(tl.float32)",
      "        if not IS_RMS_NORM:",
      "            mean = tl.load(Mean + row) if Mean is not None else 0.0",
      "        rstd = tl.load(Rstd + row)",
      "        xhat = (x - mean) * rstd if not IS_RMS_NORM else x * rstd",
      "        xhat = tl.where(mask, xhat, 0.0)",
      "        if RECOMPUTE_OUTPUT:",
      "            y = xhat * w + b if HAS_BIAS else xhat * w",
      "            tl.store(Y + cols, y, mask=mask)",
      "        wdy = w * dy",
      "        dw += dy * xhat",
      "        if HAS_BIAS:",
      "            db += dy",
      "        if not IS_RMS_NORM:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            c2 = tl.sum(wdy, axis=0) / N",
      "            dx = (wdy - (xhat * c1 + c2)) * rstd",
      "        else:",
      "            c1 = tl.sum(xhat * wdy, axis=0) / N",
      "            dx = (wdy - xhat * c1) * rstd",
      "        if HAS_DRESIDUAL:",
      "            dres = tl.load(DRESIDUAL + cols, mask=mask, other=0).to(tl.float32)",
      "            dx += dres",
      "        if STORE_DRESIDUAL:",
      "            tl.store(DRESIDUAL_IN + cols, dx, mask=mask)",
      "        tl.store(DX + cols, dx, mask=mask)",
      "        X += stride_x_row",
      "        if HAS_DRESIDUAL:",
      "            DRESIDUAL += stride_dres_row",
      "        if STORE_DRESIDUAL:",
      "            DRESIDUAL_IN += stride_dres_in_row",
      "        if RECOMPUTE_OUTPUT:",
      "            Y += stride_y_row",
      "        DY += stride_dy_row",
      "        DX += stride_dx_row",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)",
      "    if HAS_BIAS:",
      "        tl.store(DB + row_block_id * N + cols, db, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/565.py"
  },
  {
    "name": "triton_red_fused_native_layer_norm_no_welford",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'XBLOCK': 1, 'RBLOCK': 1024}, num_stages=1, num_warps=8), triton.Config({'XBLOCK': 1, 'RBLOCK': 2048}, num_stages=1, num_warps=8)], key=['xnumel', 'rnumel'])"
    ],
    "args": [
      {
        "name": "in_out_ptr0",
        "annotation": null
      },
      {
        "name": "in_out_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr0",
        "annotation": null
      },
      {
        "name": "in_ptr1",
        "annotation": null
      },
      {
        "name": "in_ptr2",
        "annotation": null
      },
      {
        "name": "out_ptr0",
        "annotation": null
      },
      {
        "name": "xnumel",
        "annotation": null
      },
      {
        "name": "rnumel",
        "annotation": null
      },
      {
        "name": "XBLOCK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RBLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def triton_red_fused_native_layer_norm_no_welford(",
      "    in_out_ptr0,",
      "    in_out_ptr1,",
      "    in_ptr0,",
      "    in_ptr1,",
      "    in_ptr2,",
      "    out_ptr0,",
      "    xnumel,",
      "    rnumel,",
      "    XBLOCK: tl.constexpr,",
      "    RBLOCK: tl.constexpr,",
      "):",
      "    xoffset = tl.program_id(0) * XBLOCK",
      "    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]",
      "    xmask = xindex < xnumel",
      "    rbase = tl.arange(0, RBLOCK)[None, :]",
      "    x0 = xindex",
      "    _tmp3 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp0 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp1 = tmp0.to(tl.float32)",
      "        tmp2 = tl.broadcast_to(tmp1, [XBLOCK, RBLOCK])",
      "        tmp4 = _tmp3 + tmp2",
      "        _tmp3 = tmp4",
      "    tmp3 = tl.sum(_tmp3, 1)[:, None]",
      "    tmp5 = rnumel",
      "    tmp6 = tmp3 / tmp5",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr0 + (x0), tmp6, None)",
      "    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp7 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_last\"",
      "        ).to(tl.float32)",
      "        tmp8 = tmp7.to(tl.float32)",
      "        tmp9 = tmp8 - tmp6",
      "        tmp10 = tmp9 * tmp9",
      "        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])",
      "        tmp13 = _tmp12 + tmp11",
      "        _tmp12 = tmp13",
      "    tmp12 = tl.sum(_tmp12, 1)[:, None]",
      "    tmp14 = rnumel",
      "    tmp15 = tmp12 / tmp14",
      "    tmp16 = 1e-05",
      "    tmp17 = tmp15 + tmp16",
      "    tmp18 = libdevice.rsqrt(tmp17)",
      "    tl.debug_barrier()",
      "    tl.store(in_out_ptr1 + (x0), tmp18, None)",
      "    for roffset in range(0, rnumel, RBLOCK):",
      "        rindex = roffset + rbase",
      "        rmask = rindex < rnumel",
      "        r1 = rindex",
      "        tmp19 = tl.load(",
      "            in_ptr0 + (r1 + (rnumel * x0)), rmask, eviction_policy=\"evict_first\"",
      "        ).to(tl.float32)",
      "        tmp23 = tl.load(in_ptr1 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp26 = tl.load(in_ptr2 + (r1), rmask, eviction_policy=\"evict_last\").to(",
      "            tl.float32",
      "        )",
      "        tmp20 = tmp19.to(tl.float32)",
      "        tmp21 = tmp20 - tmp6",
      "        tmp22 = tmp21 * tmp18",
      "        tmp24 = tmp23.to(tl.float32)",
      "        tmp25 = tmp22 * tmp24",
      "        tmp27 = tmp26.to(tl.float32)",
      "        tmp28 = tmp25 + tmp27",
      "        tmp29 = tmp28.to(tl.float32)",
      "        tl.store(out_ptr0 + (r1 + (rnumel * x0)), tmp29, rmask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/567.py"
  },
  {
    "name": "log_softmax_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 1}), triton.Config({'BLOCK_M': 2}), triton.Config({'BLOCK_M': 4}), triton.Config({'BLOCK_M': 8})], key=['M', 'N'])",
      "@triton.heuristics({'BLOCK_N': heur_block_n, 'num_warps': heur_num_warps})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def log_softmax_kernel(",
      "    output_ptr,",
      "    input_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offset = tl.arange(0, BLOCK_N)",
      "    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "    mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "    input_ptrs = input_ptr + offset",
      "    inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(tl.float32)",
      "    row_minus_max = inp - tl.max(inp, axis=1)[:, None]",
      "    numerator = tl.exp(row_minus_max)",
      "    denominator = tl.sum(numerator, axis=1)[:, None]",
      "    softmax_output = tl.log(numerator / denominator)",
      "    output_ptrs = output_ptr + offset",
      "    tl.store(output_ptrs, softmax_output, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/571.py"
  },
  {
    "name": "log_softmax_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 1}), triton.Config({'BLOCK_M': 2}), triton.Config({'BLOCK_M': 4}), triton.Config({'BLOCK_M': 8})], key=['M', 'N'])",
      "@triton.heuristics({'BLOCK_N': heur_block_n, 'num_warps': heur_num_warps})"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_grad_ptr",
        "annotation": null
      },
      {
        "name": "in_grad_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def log_softmax_backward_kernel(",
      "    out_ptr,",
      "    out_grad_ptr,",
      "    in_grad_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offset = tl.arange(0, BLOCK_N)",
      "",
      "    offsets = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "    mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "    out_ptrs = out_ptr + offsets",
      "    out = tl.load(out_ptrs, mask=mask).to(tl.float32)",
      "    out_grad_ptrs = out_grad_ptr + offsets",
      "    out_grad = tl.load(out_grad_ptrs, mask=mask).to(tl.float32)",
      "",
      "    scale = tl.sum(out_grad, 1)",
      "    in_grad = out_grad - tl.exp(out.to(tl.float32)) * scale[:, None]",
      "",
      "    in_grad_ptrs = in_grad_ptr + offsets",
      "    tl.store(in_grad_ptrs, in_grad, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/571.py"
  },
  {
    "name": "logsumexp_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['D'])",
      "@triton.heuristics({'HAS_SCALE': lambda args: args['scale'] is not None})"
    ],
    "args": [
      {
        "name": "x",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_SCALE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def logsumexp_fwd_kernel(",
      "    x, z, scale, D: tl.constexpr, B: tl.constexpr, HAS_SCALE: tl.constexpr",
      "):",
      "    i_n, i_d = tl.program_id(0).to(tl.int64), tl.program_id(1).to(tl.int64)",
      "    o_d = i_d * B + tl.arange(0, B)",
      "    m_d = o_d < D",
      "",
      "    b_x = tl.load(x + i_n * D + o_d, mask=m_d, other=-float(\"inf\"))",
      "    if HAS_SCALE:",
      "        b_x = b_x * scale",
      "    b_m = tl.max(b_x, 0)",
      "    b_z = tl.log(tl.sum(tl.exp(b_x - b_m), 0)) + b_m",
      "    tl.store(z + i_n * tl.cdiv(D, B) + i_d, b_z)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/572.py"
  },
  {
    "name": "masked_select_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=cfggen(), key=['n_elements'])"
    ],
    "args": [
      {
        "name": "inp_ptr",
        "annotation": null
      },
      {
        "name": "select_mask_ptr",
        "annotation": null
      },
      {
        "name": "prefix_sum_ptr",
        "annotation": null
      },
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "n_elements",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def masked_select_kernel(",
      "    inp_ptr,",
      "    select_mask_ptr,",
      "    prefix_sum_ptr,",
      "    out_ptr,",
      "    n_elements,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "    mask = offsets < n_elements",
      "",
      "    inp = tl.load(inp_ptr + offsets, mask=mask, other=0.0)",
      "    select_mask = tl.load(select_mask_ptr + offsets, mask=mask, other=0.0).to(tl.int1)",
      "    out_offset = tl.load(prefix_sum_ptr + offsets, mask=mask, other=0.0) - 1",
      "",
      "    tl.store(out_ptr + out_offset, inp, mask=(select_mask and mask))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/575.py"
  },
  {
    "name": "matmul4_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K', 'NO_GROUPS'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "scales_ptr",
        "annotation": null
      },
      {
        "name": "zeros_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "stride_scales_g",
        "annotation": null
      },
      {
        "name": "stride_scales_n",
        "annotation": null
      },
      {
        "name": "stride_zeros_g",
        "annotation": null
      },
      {
        "name": "stride_zeros_n",
        "annotation": null
      },
      {
        "name": "groupsize",
        "annotation": null
      },
      {
        "name": "NO_GROUPS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul4_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    scales_ptr,",
      "    zeros_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_scales_g,",
      "    stride_scales_n,",
      "    stride_zeros_g,",
      "    stride_zeros_n,",
      "    groupsize,",
      "    NO_GROUPS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    bits = 4",
      "    infearure_per_bits = 8",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    a_mask = offs_am[:, None] < M",
      "",
      "    b_ptrs = b_ptr + (",
      "        (offs_k[:, None] // infearure_per_bits) * stride_bk",
      "        + offs_bn[None, :] * stride_bn",
      "    )",
      "    scales_ptrs = scales_ptr + offs_bn * stride_scales_n",
      "",
      "    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)",
      "",
      "    shifter = (offs_k % infearure_per_bits) * bits",
      "    zeros_shifter = (offs_bn % infearure_per_bits) * bits",
      "",
      "    if NO_GROUPS:",
      "",
      "        scales = tl.load(scales_ptrs)",
      "        zeros = tl.load(zeros_ptrs)",
      "",
      "        zeros = (zeros >> zeros_shifter) & 0xF",
      "",
      "        zeros = zeros * scales",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, num_pid_k):",
      "        a = tl.load(a_ptrs, mask=a_mask, other=0.0)",
      "        b = tl.load(b_ptrs)",
      "        if not NO_GROUPS:",
      "            g_id = k // (groupsize // BLOCK_SIZE_K)",
      "            ptr = scales_ptrs + g_id * stride_scales_g",
      "            scales = tl.load(ptr)",
      "            ptr = zeros_ptrs + g_id * stride_zeros_g",
      "            zeros = tl.load(ptr)",
      "",
      "            zeros = (zeros >> zeros_shifter) & 0xF",
      "            zeros = (zeros) * scales",
      "",
      "        b = (b >> shifter[:, None]) & 0xF",
      "        b = b * scales[None, :] - zeros[None, :]",
      "",
      "        accumulator += tl.dot(a, b.to(a.dtype))",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk",
      "    c = accumulator.to(c_ptr.dtype.element_ty)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/576.py"
  },
  {
    "name": "dequantize_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5, num_warps=2)], key=['K', 'N'])"
    ],
    "args": [
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "b_scale_ptr",
        "annotation": null
      },
      {
        "name": "b_zp_ptr",
        "annotation": null
      },
      {
        "name": "fpb_ptr",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "group_size",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_bsk",
        "annotation": null
      },
      {
        "name": "stride_bsn",
        "annotation": null
      },
      {
        "name": "stride_bzpk",
        "annotation": null
      },
      {
        "name": "stride_bzpn",
        "annotation": null
      },
      {
        "name": "stride_fpbk",
        "annotation": null
      },
      {
        "name": "stride_fpbn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dequantize_kernel(",
      "    b_ptr,",
      "    b_scale_ptr,",
      "    b_zp_ptr,",
      "    fpb_ptr,",
      "    K,",
      "    N,",
      "    group_size,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_bsk,",
      "    stride_bsn,",
      "    stride_bzpk,",
      "    stride_bzpn,",
      "    stride_fpbk,",
      "    stride_fpbn,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "",
      "    k_block_idx = tl.program_id(axis=0)",
      "    n_block_idx = tl.program_id(axis=1)",
      "    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn",
      "    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn",
      "    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (",
      "        offs_n[None, :] // 8",
      "    ) * stride_bzpn",
      "    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[",
      "        None, :",
      "    ] * stride_bsn",
      "    n_mask = offs_n[None, :] < N",
      "    k_mask = offs_k[:, None] < K",
      "    mask = n_mask & k_mask",
      "    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)",
      "    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)",
      "    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)",
      "    b_shift = (offs_k[:, None] % 8) * 4",
      "    bzp_shift = (offs_n[None, :] % 8) * 4",
      "    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b",
      "    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/576.py"
  },
  {
    "name": "matmul4_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N', 'K', 'NO_GROUPS'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "scales_ptr",
        "annotation": null
      },
      {
        "name": "zeros_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "stride_scales_g",
        "annotation": null
      },
      {
        "name": "stride_scales_n",
        "annotation": null
      },
      {
        "name": "stride_zeros_g",
        "annotation": null
      },
      {
        "name": "stride_zeros_n",
        "annotation": null
      },
      {
        "name": "groupsize",
        "annotation": null
      },
      {
        "name": "NO_GROUPS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul4_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    scales_ptr,",
      "    zeros_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_scales_g,",
      "    stride_scales_n,",
      "    stride_zeros_g,",
      "    stride_zeros_n,",
      "    groupsize,",
      "    NO_GROUPS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    bits = 4",
      "    infearure_per_bits = 8",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    a_mask = offs_am[:, None] < M",
      "    b_ptrs = b_ptr + (",
      "        (offs_k[:, None] // infearure_per_bits) * stride_bk",
      "        + offs_bn[None, :] * stride_bn",
      "    )",
      "    scales_ptrs = scales_ptr + offs_bn * stride_scales_n",
      "    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)",
      "    shifter = (offs_k % infearure_per_bits) * bits",
      "    zeros_shifter = (offs_bn % infearure_per_bits) * bits",
      "    if NO_GROUPS:",
      "        scales = tl.load(scales_ptrs)",
      "        zeros = tl.load(zeros_ptrs)",
      "        zeros = (zeros >> zeros_shifter) & 0xF",
      "        zeros = zeros * scales",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, num_pid_k):",
      "        a = tl.load(a_ptrs, mask=a_mask, other=0.0)",
      "        b = tl.load(b_ptrs)",
      "        if not NO_GROUPS:",
      "            g_id = k // (groupsize // BLOCK_SIZE_K)",
      "            ptr = scales_ptrs + g_id * stride_scales_g",
      "            scales = tl.load(ptr)",
      "            ptr = zeros_ptrs + g_id * stride_zeros_g",
      "            zeros = tl.load(ptr)",
      "            zeros = (zeros >> zeros_shifter) & 0xF",
      "            zeros = (zeros) * scales",
      "        b = (b >> shifter[:, None]) & 0xF",
      "        b = b * scales[None, :] - zeros[None, :]",
      "        accumulator += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk",
      "    c = accumulator.to(tl.float16)",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/577.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4)], key=['M', 'N', 'K'], reset_to_zero=['c_ptr'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "bs_ptr",
        "annotation": null
      },
      {
        "name": "bzp_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "stride_bsk",
        "annotation": null
      },
      {
        "name": "stride_bsn",
        "annotation": null
      },
      {
        "name": "stride_bzpk",
        "annotation": null
      },
      {
        "name": "stride_bzpn",
        "annotation": null
      },
      {
        "name": "group_size",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    bs_ptr,",
      "    bzp_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_bsk,",
      "    stride_bsn,",
      "    stride_bzpk,",
      "    stride_bzpn,",
      "    group_size,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    pid_sp_k = tl.program_id(axis=1)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak",
      "    b_ptrs = b_ptr + (offs_k[:, None] // 8) * stride_bk + offs_bn[None, :] * stride_bn",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):",
      "        bs_ptrs = (",
      "            bs_ptr",
      "            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)",
      "            * stride_bsk",
      "            + offs_bn[None, :] * stride_bsn",
      "        )",
      "        bzp_ptrs = (",
      "            bzp_ptr",
      "            + ((offs_k[:, None] + k * BLOCK_SIZE_K * SPLIT_K) // group_size)",
      "            * stride_bzpk",
      "            + (offs_bn[None, :] // 8) * stride_bzpn",
      "        )",
      "        b_shift_bits = (offs_k[:, None] % 8) * 4",
      "        bzp_shift_bits = (offs_bn[None, :] % 8) * 4",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "        bs = tl.load(bs_ptrs)",
      "        bzp = tl.load(bzp_ptrs)",
      "",
      "        int_b = (b >> b_shift_bits) & 0xF",
      "        int_bzp = (bzp >> bzp_shift_bits) & 0xF",
      "        b = ((int_b - int_bzp) * bs).to(tl.float16)",
      "        accumulator += tl.dot(a.to(tl.float16), b.to(tl.float16))",
      "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk // 8",
      "",
      "    c = accumulator.to(tl.float16)",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    if SPLIT_K == 1:",
      "        tl.store(c_ptrs, c, mask=c_mask)",
      "    else:",
      "        tl.atomic_add(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/577.py"
  },
  {
    "name": "dequantize_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64}, num_stages=4, num_warps=4)], key=['K', 'N'])"
    ],
    "args": [
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "b_scale_ptr",
        "annotation": null
      },
      {
        "name": "b_zp_ptr",
        "annotation": null
      },
      {
        "name": "fpb_ptr",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "group_size",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_bsk",
        "annotation": null
      },
      {
        "name": "stride_bsn",
        "annotation": null
      },
      {
        "name": "stride_bzpk",
        "annotation": null
      },
      {
        "name": "stride_bzpn",
        "annotation": null
      },
      {
        "name": "stride_fpbk",
        "annotation": null
      },
      {
        "name": "stride_fpbn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def dequantize_kernel(",
      "    b_ptr,",
      "    b_scale_ptr,",
      "    b_zp_ptr,",
      "    fpb_ptr,",
      "    K,",
      "    N,",
      "    group_size,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_bsk,",
      "    stride_bsn,",
      "    stride_bzpk,",
      "    stride_bzpn,",
      "    stride_fpbk,",
      "    stride_fpbn,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "):",
      "",
      "    k_block_idx = tl.program_id(axis=0)",
      "    n_block_idx = tl.program_id(axis=1)",
      "    offs_k = k_block_idx * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "    offs_n = n_block_idx * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    fpb_offs = offs_k[:, None] * stride_fpbk + offs_n[None, :] * stride_fpbn",
      "    b_offs = (offs_k[:, None] // 8) * stride_bk + offs_n[None, :] * stride_bn",
      "    bzp_offs = (offs_k[:, None] // group_size) * stride_bzpk + (",
      "        offs_n[None, :] // 8",
      "    ) * stride_bzpn",
      "    bs_offs = (offs_k[:, None] // group_size) * stride_bsk + offs_n[",
      "        None, :",
      "    ] * stride_bsn",
      "    n_mask = offs_n[None, :] < N",
      "    k_mask = offs_k[:, None] < K",
      "    mask = n_mask & k_mask",
      "    int32_b = tl.load(b_ptr + b_offs, mask=mask, other=0.0)",
      "    zp_b = tl.load(b_zp_ptr + bzp_offs, mask=mask, other=0.0)",
      "    scale_b = tl.load(b_scale_ptr + bs_offs, mask=mask, other=0.0)",
      "    b_shift = (offs_k[:, None] % 8) * 4",
      "    bzp_shift = (offs_n[None, :] % 8) * 4",
      "    fp_weight = (((int32_b >> b_shift) & 0xF) - ((zp_b >> bzp_shift) & 0xF)) * scale_b",
      "    tl.store(fpb_ptr + fpb_offs, fp_weight, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/577.py"
  },
  {
    "name": "matmul4_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=8), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K', 'NO_GROUPS'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "scales_ptr",
        "annotation": null
      },
      {
        "name": "zeros_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "stride_scales_g",
        "annotation": null
      },
      {
        "name": "stride_scales_n",
        "annotation": null
      },
      {
        "name": "stride_zeros_g",
        "annotation": null
      },
      {
        "name": "stride_zeros_n",
        "annotation": null
      },
      {
        "name": "groupsize",
        "annotation": null
      },
      {
        "name": "NO_GROUPS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul4_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    scales_ptr,",
      "    zeros_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    stride_scales_g,",
      "    stride_scales_n,",
      "    stride_zeros_g,",
      "    stride_zeros_n,",
      "    groupsize,",
      "    NO_GROUPS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    bits = 4",
      "    infearure_per_bits = 8",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    a_mask = offs_am[:, None] < M",
      "",
      "    b_ptrs = b_ptr + (",
      "        (offs_k[:, None] // infearure_per_bits) * stride_bk",
      "        + offs_bn[None, :] * stride_bn",
      "    )",
      "    scales_ptrs = scales_ptr + offs_bn * stride_scales_n",
      "",
      "    zeros_ptrs = zeros_ptr + ((offs_bn // infearure_per_bits) * stride_zeros_n)",
      "",
      "    shifter = (offs_k % infearure_per_bits) * bits",
      "    zeros_shifter = (offs_bn % infearure_per_bits) * bits",
      "",
      "    if NO_GROUPS:",
      "",
      "        scales = tl.load(scales_ptrs)",
      "        zeros = tl.load(zeros_ptrs)",
      "",
      "        zeros = (zeros >> zeros_shifter) & 0xF",
      "",
      "        zeros = zeros * scales",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, num_pid_k):",
      "        a = tl.load(a_ptrs, mask=a_mask, other=0.0)",
      "        b = tl.load(b_ptrs)",
      "        if not NO_GROUPS:",
      "            g_id = k // (groupsize // BLOCK_SIZE_K)",
      "            ptr = scales_ptrs + g_id * stride_scales_g",
      "            scales = tl.load(ptr)",
      "            ptr = zeros_ptrs + g_id * stride_zeros_g",
      "            zeros = tl.load(ptr)",
      "",
      "            zeros = (zeros >> zeros_shifter) & 0xF",
      "            zeros = (zeros) * scales",
      "",
      "        b = (b >> shifter[:, None]) & 0xF",
      "        b = b * scales[None, :] - zeros[None, :]",
      "",
      "        accumulator += tl.dot(a, b.to(a.dtype))",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += (BLOCK_SIZE_K // infearure_per_bits) * stride_bk",
      "    c = accumulator.to(c_ptr.dtype.element_ty)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/578.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        accumulator = leaky_relu(accumulator)",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/581.py"
  },
  {
    "name": "matmul_kernel_persistent",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_SMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel_persistent(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    NUM_SMS: tl.constexpr,",
      "):",
      "    start_pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)",
      "    num_tiles = num_pid_m * num_pid_n",
      "",
      "    tiles_per_SM = num_tiles // NUM_SMS",
      "    if start_pid < num_tiles % NUM_SMS:",
      "        tiles_per_SM += 1",
      "",
      "    tile_id = start_pid - NUM_SMS",
      "    ki = -1",
      "",
      "    offs_k_for_mask = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "",
      "    pid_m = 0",
      "    pid_n = 0",
      "    offs_am = tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = tl.arange(0, BLOCK_SIZE_N)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for _ in range(0, k_tiles * tiles_per_SM):",
      "        ki = tl.where(ki == k_tiles - 1, 0, ki + 1)",
      "        if ki == 0:",
      "            tile_id += NUM_SMS",
      "            group_id = tile_id // num_pid_in_group",
      "            first_pid_m = group_id * GROUP_SIZE_M",
      "            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "            pid_m = first_pid_m + (tile_id % group_size_m)",
      "            pid_n = (tile_id % num_pid_in_group) // group_size_m",
      "",
      "            start_m = pid_m * BLOCK_SIZE_M",
      "            start_n = pid_n * BLOCK_SIZE_N",
      "            offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)",
      "            offs_am = tl.where(offs_am < M, offs_am, 0)",
      "            offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "            offs_am = tl.max_contiguous(",
      "                tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M",
      "            )",
      "            offs_bn = tl.max_contiguous(",
      "                tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N",
      "            )",
      "        offs_k = ki * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)",
      "        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "        a = tl.load(",
      "            a_ptrs, mask=offs_k_for_mask[None, :] < K - ki * BLOCK_SIZE_K, other=0.0",
      "        )",
      "        b = tl.load(",
      "            b_ptrs, mask=offs_k_for_mask[:, None] < K - ki * BLOCK_SIZE_K, other=0.0",
      "        )",
      "        accumulator = tl.dot(a, b, accumulator)",
      "",
      "        if ki == k_tiles - 1:",
      "            offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "            offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "            c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "            c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "            if c_ptr.dtype.element_ty == tl.float8e4nv:",
      "                c = accumulator.to(tl.float8e4nv)",
      "            else:",
      "                c = accumulator.to(tl.float16)",
      "            tl.store(c_ptrs, c, mask=c_mask)",
      "            accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/582.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4), triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2), triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator += tl.dot(a, b)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c_ptrs = c_ptr + (offs_am[:, None] * stride_cm + offs_bn[None, :] * stride_cn)",
      "    c_mask = (offs_am[:, None] < M) & (offs_bn[None, :] < N)",
      "    tl.store(c_ptrs, accumulator, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/585.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if ACTIVATION == \"leaky_relu\":",
      "        accumulator = leaky_relu(accumulator)",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/586.py"
  },
  {
    "name": "mv_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': m, 'BLOCK_N': n}, num_stages=s, num_warps=w) for m in [32, 64, 128] for n in [1, 2, 4, 8] for s in [3, 4] for w in [4, 8]], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "stride_an",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def mv_kernel(",
      "    A,",
      "    B,",
      "    C,",
      "    N,",
      "    M,",
      "    stride_an,",
      "    stride_am,",
      "    stride_bm,",
      "    stride_cn,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    offset_n = pid * BLOCK_N + tl.arange(0, BLOCK_N)[:, None]",
      "    offset_m = tl.arange(0, BLOCK_M)[None, :]",
      "    n_mask = offset_n < N",
      "    A_ptrs = A + offset_n * stride_an + offset_m * stride_am",
      "    B_ptrs = B + offset_m * stride_bm",
      "    acc = tl.zeros((BLOCK_N, BLOCK_M), dtype=tl.float32)",
      "    for m in range(0, M, BLOCK_M):",
      "        m_mask = m + offset_m < M",
      "        a = tl.load(A_ptrs, mask=n_mask & m_mask, other=0.0).to(tl.float32)",
      "        b = tl.load(B_ptrs, mask=m_mask, other=0.0).to(tl.float32)",
      "        acc += a * b",
      "        A_ptrs += BLOCK_M * stride_am",
      "        B_ptrs += BLOCK_M * stride_bm",
      "",
      "    acc = tl.sum(acc, axis=1)",
      "    C_ptrs = C + offset_n * stride_cn",
      "    tl.store(C_ptrs, acc[:, None], mask=n_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/589.py"
  },
  {
    "name": "max_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 8}, num_warps=8), triton.Config({'BLOCK_M': 16}, num_warps=8), triton.Config({'BLOCK_M': 32}, num_warps=8)], key=['M', 'N'])",
      "@triton.heuristics({'BLOCK_N': heur_block_n})"
    ],
    "args": [
      {
        "name": "inp",
        "annotation": null
      },
      {
        "name": "out_value",
        "annotation": null
      },
      {
        "name": "out_index",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def max_kernel(",
      "    inp,",
      "    out_value,",
      "    out_index,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offset = tl.arange(0, BLOCK_N)",
      "    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "    offset_index = m_offset * K + pid_k",
      "",
      "    mask1 = m_offset < M",
      "    mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "    inp_ptrs = inp + offset",
      "    inp_vals = tl.load(inp_ptrs, mask=mask, other=-float(\"inf\"))",
      "    result_value, result_index = tl.max(inp_vals, axis=1, return_indices=True)",
      "",
      "    out_value_ptrs = out_value + offset_index",
      "    out_index_ptrs = out_index + offset_index",
      "",
      "    tl.store(out_value_ptrs, result_value, mask=mask1)",
      "    tl.store(out_index_ptrs, result_index, mask=mask1)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/590.py"
  },
  {
    "name": "_quantize_global_transpose",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'GROUP_M': 8}, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'GROUP_M': 8}, num_warps=4)], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "absmax_inv_ptr",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_an",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _quantize_global_transpose(",
      "    A,",
      "    absmax_inv_ptr,",
      "    B,",
      "    stride_am,",
      "    stride_an,",
      "    stride_bn,",
      "    stride_bm,",
      "    M,",
      "    N,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    grid_m = (M + BLOCK_M - 1) // BLOCK_M",
      "    grid_n = (N + BLOCK_N - 1) // BLOCK_N",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = pid // width",
      "    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (pid % group_size)",
      "    pid_n = (pid % width) // group_size",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    A = A + (rm[:, None] * stride_am + rn[None, :] * stride_an)",
      "    mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "    a = tl.load(A, mask=mask)",
      "    absmax_inv = tl.load(absmax_inv_ptr)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    B = B + (rm[:, None] * stride_bm + rn[None, :] * stride_bn)",
      "    mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "",
      "    output = tl.extra.cuda.libdevice.llrint(127.0 * (a * absmax_inv))",
      "",
      "    tl.store(B, output, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/598.py"
  },
  {
    "name": "_quantize_global",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 1024}, num_warps=4), triton.Config({'BLOCK_SIZE': 2048}, num_stages=1)], key=['n_elements'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "absmax_inv_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "n_elements",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _quantize_global(",
      "    x_ptr,",
      "    absmax_inv_ptr,",
      "    output_ptr,",
      "    n_elements,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    block_start = pid * BLOCK_SIZE",
      "    offsets = block_start + tl.arange(0, BLOCK_SIZE)",
      "    mask = offsets < n_elements",
      "    x = tl.load(x_ptr + offsets, mask=mask)",
      "    absmax_inv = tl.load(absmax_inv_ptr)",
      "    output = tl.extra.cuda.libdevice.llrint(127.0 * (x * absmax_inv))",
      "    tl.store(output_ptr + offsets, output, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/600.py"
  },
  {
    "name": "chunk_global_reversed_cumsum_vector_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.Config({'BT': 16}, num_warps=4), triton.Config({'BT': 16}, num_warps=8), triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=8), triton.Config({'BT': 64}, num_warps=2), triton.Config({'BT': 64}, num_warps=4), triton.Config({'BT': 64}, num_warps=8)], key=['S'])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "z",
        "annotation": null
      },
      {
        "name": "s_s_h",
        "annotation": null
      },
      {
        "name": "s_s_t",
        "annotation": null
      },
      {
        "name": "s_s_d",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "S",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_reversed_cumsum_vector_kernel(",
      "    s,",
      "    z,",
      "    s_s_h,",
      "    s_s_t,",
      "    s_s_d,",
      "    T: tl.constexpr,",
      "    S: tl.constexpr,",
      "    BT: tl.constexpr,",
      "    BS: tl.constexpr,",
      "):",
      "    i_s, i_bh = tl.program_id(0), tl.program_id(1)",
      "    o_i = tl.arange(0, BT)",
      "    m_s = tl.where(o_i[:, None] <= o_i[None, :], 1.0, 0.0)",
      "",
      "    b_z = tl.zeros([BS], dtype=tl.float32)",
      "    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):",
      "        p_s = tl.make_block_ptr(",
      "            s + i_bh * s_s_h,",
      "            (T, S),",
      "            (s_s_t, s_s_d),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "        p_z = tl.make_block_ptr(",
      "            z + i_bh * s_s_h,",
      "            (T, S),",
      "            (s_s_t, s_s_d),",
      "            (i_t * BT, i_s * BS),",
      "            (BT, BS),",
      "            (1, 0),",
      "        )",
      "",
      "        b_s = tl.load(p_s, boundary_check=(0, 1)).to(tl.float32)",
      "        b_c = b_z[None, :] + tl.dot(m_s, b_s, allow_tf32=False)",
      "        tl.store(p_z, b_c.to(p_z.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "        if i_t >= 0:",
      "            b_z += tl.sum(b_s, 0)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/606.py"
  },
  {
    "name": "chunk_global_reversed_cumsum_scalar_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BT': 16}, num_warps=2), triton.Config({'BT': 32}, num_warps=4), triton.Config({'BT': 32}, num_warps=2), triton.Config({'BT': 64}, num_warps=8), triton.Config({'BT': 64}, num_warps=4)], key=[])"
    ],
    "args": [
      {
        "name": "s",
        "annotation": null
      },
      {
        "name": "o",
        "annotation": null
      },
      {
        "name": "T",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def chunk_global_reversed_cumsum_scalar_kernel(",
      "    s,",
      "    o,",
      "    T: tl.constexpr,",
      "    BT: tl.constexpr,",
      "):",
      "    i_bh = tl.program_id(0)",
      "    b_z = tl.zeros([], dtype=tl.float32)",
      "    for i_t in range(tl.cdiv(T, BT) - 1, -1, -1):",
      "        p_s = tl.make_block_ptr(s + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        p_o = tl.make_block_ptr(o + i_bh * T, (T,), (1,), (i_t * BT,), (BT,), (0,))",
      "        b_s = tl.load(p_s, boundary_check=(0,)).to(tl.float32)",
      "        b_zz = tl.sum(b_s, axis=0)",
      "        b_z += b_zz",
      "        b_o = b_s - tl.cumsum(b_s, axis=0) + b_z[None]",
      "        tl.store(p_o, b_o.to(p_o.dtype.element_ty), boundary_check=(0,))"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/607.py"
  },
  {
    "name": "rms_norm_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(do_not_specialize=['eps'])"
    ],
    "args": [
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "y_stride_r",
        "annotation": null
      },
      {
        "name": "y_stride_c",
        "annotation": null
      },
      {
        "name": "x_stride_r",
        "annotation": null
      },
      {
        "name": "x_stride_c",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def rms_norm_kernel(",
      "    Y,",
      "    X,",
      "    W,",
      "    y_stride_r,",
      "    y_stride_c,",
      "    x_stride_r,",
      "    x_stride_c,",
      "    N,",
      "    eps,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    Y += pid * y_stride_r",
      "    X += pid * x_stride_r",
      "",
      "    mask = tl.arange(0, BLOCK_SIZE) < N",
      "    cols = tl.arange(0, BLOCK_SIZE)",
      "    x = tl.load(X + cols * x_stride_c, mask, other=0.0).to(tl.float32)",
      "",
      "    var = tl.sum(x * x, axis=0) / N",
      "    rrms = 1 / tl.sqrt(var + eps)",
      "",
      "    w = tl.load(W + tl.arange(0, BLOCK_SIZE), mask=mask, other=0.0)",
      "    y = (x * rrms).to(Y.dtype.element_ty) * w",
      "    tl.store(Y + cols * y_stride_c, y, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/609.py"
  },
  {
    "name": "_rope_embedding",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'BACKWARD_PASS': lambda args: args['BACKWARD_PASS']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "Q_row_stride",
        "annotation": null
      },
      {
        "name": "cos",
        "annotation": null
      },
      {
        "name": "cos_row_stride",
        "annotation": null
      },
      {
        "name": "sin",
        "annotation": null
      },
      {
        "name": "sin_row_stride",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_heads",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BACKWARD_PASS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ROPE_GROUP_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rope_embedding(",
      "    Q,",
      "    Q_row_stride,",
      "    cos,",
      "    cos_row_stride,",
      "    sin,",
      "    sin_row_stride,",
      "    seqlen,",
      "    head_dim: tl.constexpr,",
      "    n_heads: tl.constexpr,",
      "    BACKWARD_PASS: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    ROPE_GROUP_SIZE: tl.constexpr = 4,",
      "):",
      "",
      "    row_position = tl.program_id(0)",
      "    group_head_position = tl.program_id(1)",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    half_head_dim = head_dim // 2",
      "    mask = col_offsets < half_head_dim",
      "",
      "    sin1 = tl.load(",
      "        sin",
      "        + (row_position % seqlen) * sin_row_stride",
      "        + half_head_dim * 0",
      "        + col_offsets,",
      "        mask=mask,",
      "        other=0,",
      "    )",
      "    cos1 = tl.load(",
      "        cos",
      "        + (row_position % seqlen) * cos_row_stride",
      "        + half_head_dim * 0",
      "        + col_offsets,",
      "        mask=mask,",
      "        other=0,",
      "    )",
      "",
      "    if BACKWARD_PASS:",
      "",
      "        sin1 = -sin1",
      "",
      "    head_start = group_head_position * ROPE_GROUP_SIZE",
      "    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)",
      "",
      "    for k in range(head_start, head_end):",
      "        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets",
      "        offs_q2 = (",
      "            row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim",
      "        )",
      "",
      "        Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)",
      "        Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)",
      "",
      "        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)",
      "        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/616.py"
  },
  {
    "name": "_quantize_rowwise",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_stages=1, num_warps=8), triton.Config({}, num_stages=2, num_warps=8), triton.Config({}, num_stages=4, num_warps=8), triton.Config({}, num_stages=8, num_warps=8), triton.Config({}, num_stages=1), triton.Config({}, num_stages=2), triton.Config({}, num_stages=4), triton.Config({}, num_stages=8), triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8)], key=['n_elements'])"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "output_maxs",
        "annotation": null
      },
      {
        "name": "n_elements",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "P2",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _quantize_rowwise(",
      "    x_ptr,",
      "    output_ptr,",
      "    output_maxs,",
      "    n_elements,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    P2: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    block_start = pid * BLOCK_SIZE",
      "    arange = tl.arange(0, P2)",
      "    offsets = block_start + arange",
      "    row_mask = arange < BLOCK_SIZE",
      "    x = tl.load(x_ptr + offsets, mask=row_mask)",
      "",
      "    abs_x = tl.abs(x)",
      "    max_val = tl.max(tl.where(row_mask, abs_x, 0), axis=0)",
      "    output = tl.extra.cuda.libdevice.llrint(127.0 * (x / max_val))",
      "    tl.store(output_ptr + offsets, output, mask=row_mask)",
      "    tl.store(output_maxs + pid, max_val)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/622.py"
  },
  {
    "name": "softmax_kernel_non_inner",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'TILE_K': heur_tile_k, 'TILE_N': heur_tile_n_non_inner, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta, 'num_warps': heur_num_warps_non_inner})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_kernel_non_inner(",
      "    output_ptr,",
      "    input_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    TILE_N: tl.constexpr,",
      "    TILE_K: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_k = tl.program_id(1)",
      "    pid_m = tl.program_id(0)",
      "",
      "    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)",
      "",
      "    if ONE_TILE_PER_CTA:",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offset = pid_m * N * K + n_offsets[:, None] * K + k_offsets",
      "        mask = (n_offsets[:, None] < N) & (k_offsets < K)",
      "        input_ptrs = input_ptr + offset",
      "        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))",
      "        m = tl.max(inp, 0)",
      "        e = tl.exp(inp - m[None, :])",
      "        z = tl.sum(e, 0)",
      "        out = e / z",
      "        output_ptrs = output_ptr + offset",
      "        tl.store(output_ptrs, out, mask=mask)",
      "    else:",
      "        m = tl.full([TILE_N, TILE_K], value=float(\"-inf\"), dtype=tl.float32)",
      "        z = tl.full([TILE_N, TILE_K], value=0.0, dtype=tl.float32)",
      "",
      "        for start_n in range(0, N, TILE_N):",
      "            n_offsets = start_n + tl.arange(0, TILE_N)",
      "            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets",
      "            mask = (n_offsets[:, None] < N) & (k_offsets < K)",
      "            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))",
      "            m_new = tl.maximum(m, inp)",
      "            alpha = tl.exp(m - m_new)",
      "            z = z * alpha + tl.exp(inp - m_new)",
      "            m = m_new",
      "",
      "        m_reduced = tl.max(m, 0)",
      "        z = tl.sum(z * tl.exp(m - m_reduced[None, :]), 0)",
      "        m = m_reduced",
      "",
      "        previous_multiple = prev_multiple_of(N, TILE_N)",
      "        for start_n in range(0, N, TILE_N):",
      "            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)",
      "            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets",
      "            mask = (n_offsets[:, None] < N) & (k_offsets[None, :] < K)",
      "            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))",
      "            o = tl.exp(inp - m[None, :]) / z[None, :]",
      "            tl.store(output_ptr + offsets, o, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/627.py"
  },
  {
    "name": "softmax_kernel_inner",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'TILE_N': heur_tile_n_inner, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta, 'num_warps': heur_num_warps_inner})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_kernel_inner(",
      "    output_ptr,",
      "    input_ptr,",
      "    M,",
      "    N,",
      "    TILE_N: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    if ONE_TILE_PER_CTA:",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offset = pid_m * N + n_offsets",
      "        input_ptrs = input_ptr + offset",
      "        mask = n_offsets < N",
      "        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(",
      "            output_ptr.dtype.element_ty",
      "        )",
      "        m = tl.max(inp, 0)",
      "        e = tl.exp(inp - m)",
      "        z = tl.sum(e, 0)",
      "        out = e / z",
      "        output_ptrs = output_ptr + offset",
      "        tl.store(output_ptrs, out, mask=mask)",
      "    else:",
      "        m = tl.full([TILE_N], value=float(\"-inf\"), dtype=tl.float32)",
      "        z = tl.full([TILE_N], value=0.0, dtype=tl.float32)",
      "        input_ptr += pid_m * N",
      "        output_ptr += pid_m * N",
      "",
      "        previous_multiple = prev_multiple_of(N, TILE_N)",
      "        for start_n in range(0, previous_multiple, TILE_N):",
      "            n_offsets = start_n + tl.arange(0, TILE_N)",
      "            inp = tl.load(input_ptr + n_offsets)",
      "            m_new = tl.maximum(m, inp)",
      "            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)",
      "            m = m_new",
      "",
      "        for start_n in range(previous_multiple, N, TILE_N):",
      "            n_offsets = start_n + tl.arange(0, TILE_N)",
      "            mask = n_offsets < N",
      "            inp = tl.load(input_ptr + n_offsets, mask=mask, other=-float(\"inf\"))",
      "            m_new = tl.maximum(m, inp)",
      "            z = z * tl.exp(m - m_new) + tl.exp(inp - m_new)",
      "            m = m_new",
      "",
      "        m_reduced = tl.max(m, 0)",
      "        z = tl.sum(z * tl.exp(m - m_reduced), 0)",
      "        m = m_reduced",
      "",
      "        previous_multiple = prev_multiple_of(N, TILE_N)",
      "",
      "        for start_n in range(0, TILE_N, TILE_N):",
      "            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)",
      "            mask = n_offsets < N",
      "            inp = tl.load(",
      "                input_ptr + n_offsets,",
      "                mask=mask,",
      "                other=-float(\"inf\"),",
      "                eviction_policy=\"evict_first\",",
      "            )",
      "            o = tl.exp(inp - m) / z",
      "            tl.store(output_ptr + n_offsets, o, mask=mask)",
      "        for start_n in range(TILE_N, N, TILE_N):",
      "            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)",
      "            inp = tl.load(input_ptr + n_offsets, eviction_policy=\"evict_first\")",
      "            o = tl.exp(inp - m) / z",
      "            tl.store(output_ptr + n_offsets, o)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/627.py"
  },
  {
    "name": "softmax_backward_kernel_non_inner",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'TILE_K': 32}), triton.Config({'TILE_K': 64}), triton.Config({'TILE_K': 128}), triton.Config({'TILE_K': 256}), triton.Config({'TILE_K': 512}), triton.Config({'TILE_K': 1024})], key=['M', 'N', 'K'])",
      "@triton.heuristics({'TILE_N': heur_tile_n_bwd_non_inner, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta})"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_grad_ptr",
        "annotation": null
      },
      {
        "name": "in_grad_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_backward_kernel_non_inner(",
      "    out_ptr,",
      "    out_grad_ptr,",
      "    in_grad_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    TILE_N: tl.constexpr,",
      "    TILE_K: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    offsets_k = pid_k * TILE_K + tl.arange(0, TILE_K)",
      "",
      "    if ONE_TILE_PER_CTA:",
      "        offsets_n = tl.arange(0, TILE_N)",
      "        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k",
      "        mask = (offsets_n < N)[:, None] & (offsets_k < K)",
      "        out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "        scale = tl.sum(out_tile * out_grad_tile, axis=0)",
      "        in_grad_tile = out_tile * (out_grad_tile - scale[None, :])",
      "        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "    else:",
      "        offsets_n = tl.arange(0, TILE_N)",
      "        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k",
      "        scale = tl.zeros([TILE_N, TILE_K], dtype=tl.float32)",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (offsets_n < N)[:, None] & (offsets_k < K)",
      "            out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            scale += out_tile * out_grad_tile",
      "            offsets_n += TILE_N",
      "            offsets += TILE_N * K",
      "        scale = tl.sum(scale, axis=0)",
      "",
      "        offsets_n = tl.arange(0, TILE_N)",
      "        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (offsets_n < N)[:, None] & (offsets_k < K)",
      "            out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            in_grad_tile = out_tile * (out_grad_tile - scale[None, :])",
      "            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "            offsets_n += TILE_N",
      "            offsets += TILE_N * K"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/627.py"
  },
  {
    "name": "softmax_backward_kernel_inner",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'TILE_N': 32}), triton.Config({'TILE_N': 64}), triton.Config({'TILE_N': 128}), triton.Config({'TILE_N': 256}), triton.Config({'TILE_N': 512}), triton.Config({'TILE_N': 1024})], key=['M', 'N'])",
      "@triton.heuristics(values={'TILE_M': heru_tile_m, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta})"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_grad_ptr",
        "annotation": null
      },
      {
        "name": "in_grad_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "TILE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def softmax_backward_kernel_inner(",
      "    out_ptr,",
      "    out_grad_ptr,",
      "    in_grad_ptr,",
      "    M,",
      "    N,",
      "    TILE_M: tl.constexpr,",
      "    TILE_N: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    m_offsets = pid_m * TILE_M + tl.arange(0, TILE_M)",
      "    if ONE_TILE_PER_CTA:",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offsets = m_offsets[:, None] * N + n_offsets",
      "        mask = (m_offsets[:, None] < M) & (n_offsets < N)",
      "        out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "        scale = tl.sum(out_tile * out_grad_tile, 1)",
      "        in_grad_tile = out_tile * (out_grad_tile - scale[:, None])",
      "        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "    else:",
      "        scale = tl.zeros([TILE_M, TILE_N], dtype=tl.float32)",
      "",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offsets = m_offsets[:, None] * N + n_offsets",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (m_offsets[:, None] < M) & (n_offsets < N)",
      "            out_tile = tl.load(",
      "                out_ptr + offsets, mask=mask, eviction_policy=\"evict_last\"",
      "            )",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            scale += out_tile * out_grad_tile",
      "            n_offsets += TILE_N",
      "            offsets += TILE_N",
      "        scale = tl.sum(scale, 1)",
      "",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offsets = m_offsets[:, None] * N + n_offsets",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (m_offsets[:, None] < M) & (n_offsets < N)",
      "            out_tile = tl.load(",
      "                out_ptr + offsets, mask=mask, eviction_policy=\"evict_first\"",
      "            )",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            in_grad_tile = out_tile * (out_grad_tile - scale[:, None])",
      "            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "            n_offsets += TILE_N",
      "            offsets += TILE_N"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/627.py"
  },
  {
    "name": "swizzle_tile",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def swizzle_tile(",
      "    tile_id,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = tile_id // width",
      "    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (tile_id % group_size)",
      "    pid_n = (tile_id % width) // group_size",
      "    return pid_m, pid_n"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/635.py"
  },
  {
    "name": "linear_tile",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_tile(",
      "    tile_id,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    pid_m = tile_id // tl.cdiv(N, BLOCK_N)",
      "    pid_n = tile_id % tl.cdiv(N, BLOCK_N)",
      "    return pid_m, pid_n"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/635.py"
  },
  {
    "name": "_swiglu_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])",
      "@triton.heuristics({'RECOMPUTE_OUTPUT': lambda args: args['OUT'] is not None})"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "DOUT",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_dout_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "stride_dx_row",
        "annotation": null
      },
      {
        "name": "stride_dy_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RECOMPUTE_OUTPUT",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_bwd_kernel(",
      "    X,",
      "    Y,",
      "    DOUT,",
      "    OUT,",
      "    DX,",
      "    DY,",
      "    stride_x_row,",
      "    stride_y_row,",
      "    stride_dout_row,",
      "    stride_out_row,",
      "    stride_dx_row,",
      "    stride_dy_row,",
      "    ncols,",
      "    BLOCK_N: tl.constexpr,",
      "    RECOMPUTE_OUTPUT: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    DOUT += row * stride_dout_row",
      "    if RECOMPUTE_OUTPUT:",
      "        OUT += row * stride_out_row",
      "    DX += row * stride_dx_row",
      "    DY += row * stride_dy_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    dout = tl.load(DOUT + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    x_sigmoid = tl.sigmoid(x)",
      "    dx = x_sigmoid * (1 + x * (1 - x_sigmoid)) * y * dout",
      "    dy = x * x_sigmoid * dout",
      "    tl.store(DX + cols, dx, mask=cols < ncols)",
      "    tl.store(DY + cols, dy, mask=cols < ncols)",
      "    if RECOMPUTE_OUTPUT:",
      "        out = x * x_sigmoid * y",
      "        tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/636.py"
  },
  {
    "name": "_swiglu_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_N': 32}), triton.Config({'BLOCK_N': 64}), triton.Config({'BLOCK_N': 128}), triton.Config({'BLOCK_N': 256}), triton.Config({'BLOCK_N': 512}), triton.Config({'BLOCK_N': 1024})], key=['ncols'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "OUT",
        "annotation": null
      },
      {
        "name": "stride_x_row",
        "annotation": null
      },
      {
        "name": "stride_y_row",
        "annotation": null
      },
      {
        "name": "stride_out_row",
        "annotation": null
      },
      {
        "name": "ncols",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _swiglu_fwd_kernel(",
      "    X, Y, OUT, stride_x_row, stride_y_row, stride_out_row, ncols, BLOCK_N: tl.constexpr",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    start_col = tl.program_id(1) * BLOCK_N",
      "    X += row * stride_x_row",
      "    Y += row * stride_y_row",
      "    OUT += row * stride_out_row",
      "    cols = start_col + tl.arange(0, BLOCK_N)",
      "    x = tl.load(X + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    y = tl.load(Y + cols, mask=cols < ncols, other=0.0).to(tl.float32)",
      "    out = x * tl.sigmoid(x) * y",
      "    tl.store(OUT + cols, out, mask=cols < ncols)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/637.py"
  },
  {
    "name": "update_fn_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128}, num_warps=4), triton.Config({'BLOCK_SIZE': 1024}, num_warps=8)], key=['n_elements'], restore_value=['p_ptr', 'exp_avg_ptr'])"
    ],
    "args": [
      {
        "name": "p_ptr",
        "annotation": null
      },
      {
        "name": "grad_ptr",
        "annotation": null
      },
      {
        "name": "exp_avg_ptr",
        "annotation": null
      },
      {
        "name": "lr",
        "annotation": null
      },
      {
        "name": "wd",
        "annotation": null
      },
      {
        "name": "beta1",
        "annotation": null
      },
      {
        "name": "beta2",
        "annotation": null
      },
      {
        "name": "n_elements",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def update_fn_kernel(",
      "    p_ptr,",
      "    grad_ptr,",
      "    exp_avg_ptr,",
      "    lr,",
      "    wd,",
      "    beta1,",
      "    beta2,",
      "    n_elements,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "",
      "    block_start = pid * BLOCK_SIZE",
      "    offsets = block_start + tl.arange(0, BLOCK_SIZE)",
      "",
      "    mask = offsets < n_elements",
      "",
      "    offset_p_ptr = p_ptr + offsets",
      "    offset_grad_ptr = grad_ptr + offsets",
      "    offset_exp_avg_ptr = exp_avg_ptr + offsets",
      "",
      "    p = tl.load(offset_p_ptr, mask=mask)",
      "    grad = tl.load(offset_grad_ptr, mask=mask)",
      "    exp_avg = tl.load(offset_exp_avg_ptr, mask=mask)",
      "",
      "    p = p * (1 - lr * wd)",
      "",
      "    diff = exp_avg - grad",
      "",
      "    update = diff * beta1 + grad",
      "",
      "    can_update = update != 0",
      "    update_sign = tl.where(update > 0, -lr, lr)",
      "",
      "    p = p + update_sign * can_update",
      "",
      "    exp_avg = diff * beta2 + grad",
      "",
      "    tl.store(offset_p_ptr, p, mask=mask)",
      "    tl.store(offset_exp_avg_ptr, exp_avg, mask=mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/473.py"
  },
  {
    "name": "kernel_fma",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2)] + get_configs_io_bound(), key=['CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={'early_config_prune': early_config_prune, 'perf_model': estimate_matmul_time, 'top_k': 10})",
      "@triton.heuristics({'K_LOAD_MASK_NEEDED': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0})"
    ],
    "args": [
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "ACT_INPUTS",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_M",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_N",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_K",
        "annotation": null
      },
      {
        "name": "output_m_stride",
        "annotation": null
      },
      {
        "name": "output_n_stride",
        "annotation": null
      },
      {
        "name": "act_inputs_m_stride",
        "annotation": null
      },
      {
        "name": "act_inputs_n_stride",
        "annotation": null
      },
      {
        "name": "a_m_stride",
        "annotation": null
      },
      {
        "name": "a_k_stride",
        "annotation": null
      },
      {
        "name": "b_n_stride",
        "annotation": null
      },
      {
        "name": "b_k_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K_LOAD_MASK_NEEDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SHOULD_SAVE_ACT_INPUTS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_fma(",
      "    C,",
      "    ACT_INPUTS,",
      "    A,",
      "    B,",
      "    bias,",
      "    M,",
      "    N,",
      "    K,",
      "    CACHE_KEY_M,",
      "    CACHE_KEY_N,",
      "    CACHE_KEY_K,",
      "    output_m_stride,",
      "    output_n_stride,",
      "    act_inputs_m_stride,",
      "    act_inputs_n_stride,",
      "    a_m_stride,",
      "    a_k_stride,",
      "    b_n_stride,",
      "    b_k_stride,",
      "    BLOCK_M: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "    K_LOAD_MASK_NEEDED: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    SHOULD_SAVE_ACT_INPUTS: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "",
      "    program_idx = tl.program_id(axis=0)",
      "",
      "    grid_m = (M + BLOCK_M - 1) // BLOCK_M",
      "    grid_n = (N + BLOCK_N - 1) // BLOCK_N",
      "",
      "    width = GROUP_M * grid_n",
      "    group_idx = program_idx // width",
      "    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)",
      "    block_m_idx = group_idx * GROUP_M + (program_idx % group_size)",
      "    block_n_idx = (program_idx % width) // group_size",
      "",
      "    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)",
      "",
      "    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M), BLOCK_M)",
      "    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N), BLOCK_N)",
      "",
      "    k_range_offs = tl.arange(0, BLOCK_K)",
      "",
      "    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)",
      "    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)",
      "",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "",
      "    if HAS_BIAS:",
      "        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0).to(tl.float32)",
      "        acc += bias[None, :]",
      "",
      "    for k in range(K, 0, -BLOCK_K):",
      "        if K_LOAD_MASK_NEEDED:",
      "            a = tl.load(A)",
      "            b = tl.load(B)",
      "        else:",
      "            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)",
      "            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)",
      "        acc += tl.dot(a, b)",
      "",
      "        A += BLOCK_K * a_k_stride",
      "        B += BLOCK_K * b_k_stride",
      "",
      "    if SHOULD_SAVE_ACT_INPUTS:",
      "        act_in_ptrs = (",
      "            ACT_INPUTS",
      "            + m_offs[:, None] * act_inputs_m_stride",
      "            + n_offs[None, :] * act_inputs_n_stride",
      "        )",
      "        tl.store(act_in_ptrs, acc)",
      "",
      "    if ACTIVATION == \"tanh\":",
      "        acc = tanh(acc)",
      "    if ACTIVATION == \"gelu\":",
      "        acc = gelu(acc)",
      "    if ACTIVATION == \"fast_gelu\":",
      "        acc = fast_gelu(acc)",
      "    if ACTIVATION == \"relu\":",
      "        acc = relu(acc)",
      "",
      "    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :] * output_n_stride",
      "    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]",
      "    tl.store(C, acc, mask=c_ptr_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/647.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_matmul_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    start_m = pid_m * BLOCK_SIZE_M",
      "    start_n = pid_n * BLOCK_SIZE_N",
      "",
      "    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_am = tl.where(offs_am < M, offs_am, 0)",
      "    offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "",
      "    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if c_ptr.dtype.element_ty == tl.float8e4nv:",
      "        c = accumulator.to(tl.float8e4nv)",
      "    else:",
      "        c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/648.py"
  },
  {
    "name": "uniform_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      },
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'BLOCK': heur_block, 'num_warps': heur_num_warps})",
      "@triton.jit(do_not_specialize=['philox_seed', 'philox_offset'])"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "philox_offset",
        "annotation": null
      },
      {
        "name": "from_",
        "annotation": null
      },
      {
        "name": "to",
        "annotation": null
      },
      {
        "name": "BLOCK",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def uniform_kernel(",
      "    out_ptr,",
      "    N,",
      "    philox_seed,",
      "    philox_offset,",
      "    from_,",
      "    to,",
      "    BLOCK: tl.constexpr,",
      "):",
      "    philox_seed = philox_seed.to(tl.int64)",
      "    philox_offset = philox_offset.to(tl.int64)",
      "    c0 = (philox_offset & 0xFFFFFFFF).to(tl.uint32)",
      "    c1 = ((philox_offset >> 32) & 0xFFFFFFFF).to(tl.uint32)",
      "    i4 = tl.program_id(0) * BLOCK + tl.arange(0, BLOCK)",
      "    c0 += i4",
      "    _O = c0 * 0",
      "    r0, r1, r2, r3 = tl.philox(philox_seed, c0, c1, _O, _O)",
      "    r0 = uint_to_uniform_float(r0) * (to - from_) + from_",
      "    r1 = uint_to_uniform_float(r1) * (to - from_) + from_",
      "    r2 = uint_to_uniform_float(r2) * (to - from_) + from_",
      "    r3 = uint_to_uniform_float(r3) * (to - from_) + from_",
      "    off_0 = tl.program_id(0) * BLOCK * 4 + tl.arange(0, BLOCK)",
      "    off_1 = off_0 + BLOCK",
      "    off_2 = off_1 + BLOCK",
      "    off_3 = off_2 + BLOCK",
      "    tl.store(out_ptr + off_0, r0, mask=off_0 < N, eviction_policy=\"evict_first\")",
      "    tl.store(out_ptr + off_1, r1, mask=off_1 < N, eviction_policy=\"evict_first\")",
      "    tl.store(out_ptr + off_2, r2, mask=off_2 < N, eviction_policy=\"evict_first\")",
      "    tl.store(out_ptr + off_3, r3, mask=off_3 < N, eviction_policy=\"evict_first\")"
    ],
    "file": "triton_repos/thunlp_TritonBench/data/TritonBench_G_v1/651.py"
  },
  {
    "name": "fused_fwd_kernel_s_km",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64, 128] for BK in [64] for BV in [64] for num_warps in [2, 4, 8] for num_stages in [2]], key=['L'])"
    ],
    "args": [
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "KM",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "stride_km_bh",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_fwd_kernel_s_km(",
      "    K,",
      "    V,",
      "    S,",
      "    KM,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    stride_km_bh,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + off_bs_head * stride_qk_bh,",
      "        shape=(DK, L),",
      "        strides=(stride_qk_d, stride_qk_l),",
      "        offsets=(start_k * BK, 0),",
      "        block_shape=(BK, BL),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    s = tl.zeros([BK, BV], dtype=tl.float32)",
      "    km = tl.zeros([BK], dtype=tl.float32)",
      "",
      "    for _ in range(0, L, BL):",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1))",
      "        v = tl.load(V_block_ptr, boundary_check=(0, 1))",
      "",
      "        v = (v * scale).to(v.dtype)",
      "        s += tl.dot(k, v, allow_tf32=False)",
      "        km += tl.sum(k, axis=1) / L",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BL))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))",
      "",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(start_k * BK, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))",
      "",
      "    KM_block_ptr = KM + off_bs_head * stride_km_bh + start_k * BK + tl.arange(0, BK)",
      "    tl.store(",
      "        KM_block_ptr,",
      "        km.to(KM.dtype.element_ty),",
      "        mask=((start_k * BK + tl.arange(0, BK)) < DK),",
      "    )"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/linear_attn/208.py"
  },
  {
    "name": "fused_fwd_kernel_o",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64, 128] for BK in [64] for BV in [64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "KM",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "stride_km_bh",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fused_fwd_kernel_o(",
      "    Q,",
      "    S,",
      "    O,",
      "    KM,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    stride_km_bh,",
      "    eps,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "",
      "    start_v, start_l, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_l * BL, 0),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    KM_block_ptr = KM + off_bs_head * stride_km_bh + tl.arange(0, BK)",
      "",
      "    o = tl.zeros([BL, BV], dtype=tl.float32)",
      "    z = tl.zeros([BL], dtype=tl.float32)",
      "",
      "    for offset_k in range(0, DK, BK):",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1))",
      "        s = tl.load(S_block_ptr, boundary_check=(0, 1))",
      "        km = tl.load(KM_block_ptr, mask=((offset_k + tl.arange(0, BK)) < DK))",
      "",
      "        z += tl.sum(q * km[None, :], axis=1, keep_dims=False)",
      "        o += tl.dot(q, s, allow_tf32=False)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))",
      "        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))",
      "        KM_block_ptr = KM_block_ptr + tl.arange(0, BK)",
      "",
      "    o = o / (z[:, None] + eps)",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=O + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_l * BL, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/linear_attn/208.py"
  },
  {
    "name": "_fwd_kv_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kv_kernel(",
      "    K,",
      "    V,",
      "    S,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + off_bs_head * stride_qk_bh,",
      "        shape=(DK, L),",
      "        strides=(stride_qk_d, stride_qk_l),",
      "        offsets=(start_k * BK, 0),",
      "        block_shape=(BK, BL),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    s = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for _ in range(0, L, BL):",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1))",
      "        v = tl.load(V_block_ptr, boundary_check=(0, 1))",
      "",
      "        v = (v * scale).to(v.dtype)",
      "        s += tl.dot(k, v, allow_tf32=False)",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BL))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BL, 0))",
      "",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(start_k * BK, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(S_block_ptr, s.to(S.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/simple_la/209.py"
  },
  {
    "name": "_fwd_qs_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_qs_kernel(",
      "    Q,",
      "    S,",
      "    O,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    o = tl.zeros([BL, BV], dtype=tl.float32)",
      "",
      "    for _ in range(0, DK, BK):",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1))",
      "        s = tl.load(S_block_ptr, boundary_check=(0, 1))",
      "",
      "        o += tl.dot(q, s, allow_tf32=False)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, (0, BK))",
      "        S_block_ptr = tl.advance(S_block_ptr, (BK, 0))",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=O + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(O_block_ptr, o.to(O.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/simple_la/209.py"
  },
  {
    "name": "_bwd_ds_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BK': BK, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BK in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DS",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_ds_kernel(",
      "    Q,",
      "    DO,",
      "    DS,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_k, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + off_bs_head * stride_qk_bh,",
      "        shape=(DK, L),",
      "        strides=(stride_qk_d, stride_qk_l),",
      "        offsets=(start_k * BK, 0),",
      "        block_shape=(BK, BL),",
      "        order=(0, 1),",
      "    )",
      "    DO_block_ptr = tl.make_block_ptr(",
      "        base=DO + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    ds = tl.zeros([BK, BV], dtype=tl.float32)",
      "",
      "    for i in range(0, L, BL):",
      "        q = tl.load(Q_block_ptr, boundary_check=(0, 1))",
      "        do = tl.load(DO_block_ptr, boundary_check=(0, 1))",
      "",
      "        ds += tl.dot(q, do, allow_tf32=False)",
      "",
      "        Q_block_ptr = tl.advance(Q_block_ptr, (0, BL))",
      "        DO_block_ptr = tl.advance(DO_block_ptr, (BL, 0))",
      "",
      "    DS_block_ptr = tl.make_block_ptr(",
      "        base=DS + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(start_k * BK, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(DS_block_ptr, ds.to(DS.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/simple_la/209.py"
  },
  {
    "name": "_bwd_dqk_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BK': BK}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64] for BK in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "S",
        "annotation": null
      },
      {
        "name": "dQ",
        "annotation": null
      },
      {
        "name": "dK",
        "annotation": null
      },
      {
        "name": "dS",
        "annotation": null
      },
      {
        "name": "dO",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_dqk_kernel(",
      "    V,",
      "    S,",
      "    dQ,",
      "    dK,",
      "    dS,",
      "    dO,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_k, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    dO_block_ptr = tl.make_block_ptr(",
      "        base=dO + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    S_block_ptr = tl.make_block_ptr(",
      "        base=S + off_bs_head * stride_s_bh,",
      "        shape=(DV, DK),",
      "        strides=(stride_s_dv, stride_s_dk),",
      "        offsets=(0, start_k * BK),",
      "        block_shape=(BV, BK),",
      "        order=(0, 1),",
      "    )",
      "    dS_block_ptr = tl.make_block_ptr(",
      "        base=dS + off_bs_head * stride_s_bh,",
      "        shape=(DV, DK),",
      "        strides=(stride_s_dv, stride_s_dk),",
      "        offsets=(0, start_k * BK),",
      "        block_shape=(BV, BK),",
      "        order=(0, 1),",
      "    )",
      "",
      "    dq = tl.zeros([BL, BK], dtype=tl.float32)",
      "    dk = tl.zeros([BL, BK], dtype=tl.float32)",
      "",
      "    for _ in range(0, DV, BV):",
      "        v = tl.load(V_block_ptr, boundary_check=(0, 1))",
      "        do = tl.load(dO_block_ptr, boundary_check=(0, 1))",
      "",
      "        s = tl.load(S_block_ptr, boundary_check=(0, 1))",
      "        ds = tl.load(dS_block_ptr, boundary_check=(0, 1))",
      "",
      "        v = (v * scale).to(v.dtype)",
      "        dq += tl.dot(do, s.to(do.dtype), allow_tf32=False)",
      "        dk += tl.dot(v, ds.to(v.dtype), allow_tf32=False)",
      "",
      "        V_block_ptr = tl.advance(V_block_ptr, (0, BV))",
      "        dS_block_ptr = tl.advance(dS_block_ptr, (BV, 0))",
      "",
      "        dO_block_ptr = tl.advance(dO_block_ptr, (0, BV))",
      "        S_block_ptr = tl.advance(S_block_ptr, (BV, 0))",
      "",
      "    dQ_block_ptr = tl.make_block_ptr(",
      "        base=dQ + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, start_k * BK),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    dK_block_ptr = tl.make_block_ptr(",
      "        base=dK + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, start_k * BK),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(dQ_block_ptr, dq.to(dQ.dtype.element_ty), boundary_check=(0, 1))",
      "    tl.store(dK_block_ptr, dk.to(dK.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/simple_la/209.py"
  },
  {
    "name": "_bwd_dv_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BL': BL, 'BV': BV}, num_warps=num_warps, num_stages=num_stages) for BL in [32, 64] for BV in [32, 64] for num_warps in [2, 4, 8] for num_stages in [2, 3, 4]], key=['L'])"
    ],
    "args": [
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "dV",
        "annotation": null
      },
      {
        "name": "dS",
        "annotation": null
      },
      {
        "name": "stride_qk_bh",
        "annotation": null
      },
      {
        "name": "stride_qk_l",
        "annotation": null
      },
      {
        "name": "stride_qk_d",
        "annotation": null
      },
      {
        "name": "stride_vo_bh",
        "annotation": null
      },
      {
        "name": "stride_vo_l",
        "annotation": null
      },
      {
        "name": "stride_vo_d",
        "annotation": null
      },
      {
        "name": "stride_s_bh",
        "annotation": null
      },
      {
        "name": "stride_s_dk",
        "annotation": null
      },
      {
        "name": "stride_s_dv",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "L",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "DV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BV",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_dv_kernel(",
      "    K,",
      "    dV,",
      "    dS,",
      "    stride_qk_bh,",
      "    stride_qk_l,",
      "    stride_qk_d,",
      "    stride_vo_bh,",
      "    stride_vo_l,",
      "    stride_vo_d,",
      "    stride_s_bh,",
      "    stride_s_dk,",
      "    stride_s_dv,",
      "    scale,",
      "    L: tl.constexpr,",
      "    DK: tl.constexpr,",
      "    DV: tl.constexpr,",
      "    BL: tl.constexpr,",
      "    BK: tl.constexpr,",
      "    BV: tl.constexpr,",
      "):",
      "    start_v, start_m, off_bs_head = tl.program_id(0), tl.program_id(1), tl.program_id(2)",
      "",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + off_bs_head * stride_qk_bh,",
      "        shape=(L, DK),",
      "        strides=(stride_qk_l, stride_qk_d),",
      "        offsets=(start_m * BL, 0),",
      "        block_shape=(BL, BK),",
      "        order=(1, 0),",
      "    )",
      "    dS_block_ptr = tl.make_block_ptr(",
      "        base=dS + off_bs_head * stride_s_bh,",
      "        shape=(DK, DV),",
      "        strides=(stride_s_dk, stride_s_dv),",
      "        offsets=(0, start_v * BV),",
      "        block_shape=(BK, BV),",
      "        order=(1, 0),",
      "    )",
      "",
      "    dv = tl.zeros([BL, BV], dtype=tl.float32)",
      "",
      "    for _ in range(0, DK, BK):",
      "        k = tl.load(K_block_ptr, boundary_check=(0, 1))",
      "        ds = tl.load(dS_block_ptr, boundary_check=(0, 1))",
      "",
      "        dv += tl.dot(k, ds.to(k.dtype), allow_tf32=False) * scale",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BK))",
      "        dS_block_ptr = tl.advance(dS_block_ptr, (BK, 0))",
      "",
      "    dV_block_ptr = tl.make_block_ptr(",
      "        base=dV + off_bs_head * stride_vo_bh,",
      "        shape=(L, DV),",
      "        strides=(stride_vo_l, stride_vo_d),",
      "        offsets=(start_m * BL, start_v * BV),",
      "        block_shape=(BL, BV),",
      "        order=(1, 0),",
      "    )",
      "    tl.store(dV_block_ptr, dv.to(dV.dtype.element_ty), boundary_check=(0, 1))"
    ],
    "file": "triton_repos/fla-org_flash-bidirectional-linear-attention/fbi_la/ops/simple_la/209.py"
  },
  {
    "name": "_angular_lsh_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen'] % args['BLOCK_M'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "in_mat",
        "annotation": null
      },
      {
        "name": "proj_dir",
        "annotation": null
      },
      {
        "name": "perm",
        "annotation": null
      },
      {
        "name": "enc_vec",
        "annotation": null
      },
      {
        "name": "buckets",
        "annotation": null
      },
      {
        "name": "stride_in_matb",
        "annotation": null
      },
      {
        "name": "stride_in_math",
        "annotation": null
      },
      {
        "name": "stride_in_matm",
        "annotation": null
      },
      {
        "name": "stride_proj_dirb",
        "annotation": null
      },
      {
        "name": "stride_proj_dirh",
        "annotation": null
      },
      {
        "name": "stride_proj_dird",
        "annotation": null
      },
      {
        "name": "stride_bucketsb",
        "annotation": null
      },
      {
        "name": "stride_bucketsh",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "seqlen_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "NUM_PROJ_ROUNDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_projs",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _angular_lsh_kernel(",
      "    in_mat,",
      "    proj_dir,",
      "    perm,",
      "    enc_vec,",
      "    buckets,",
      "    stride_in_matb,",
      "    stride_in_math,",
      "    stride_in_matm,",
      "    stride_proj_dirb,",
      "    stride_proj_dirh,",
      "    stride_proj_dird,",
      "    stride_bucketsb,",
      "    stride_bucketsh,",
      "    nheads,",
      "    seqlen,",
      "    seqlen_rounded,",
      "    headdim,",
      "    NUM_PROJ_ROUNDED: tl.constexpr,",
      "    num_projs: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, NUM_PROJ_ROUNDED)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    in_mat_ptrs = (",
      "        in_mat",
      "        + off_b * stride_in_matb",
      "        + off_h * stride_in_math",
      "        + (offs_m[:, None] * stride_in_matm + offs_d[None, :])",
      "    )",
      "    proj_dir_ptrs = (",
      "        proj_dir",
      "        + off_b * stride_proj_dirb",
      "        + off_h * stride_proj_dirh",
      "        + (offs_d[:, None] * stride_proj_dird + offs_n[None, :])",
      "    )",
      "",
      "    if EVEN_M:",
      "        if EVEN_HEADDIM:",
      "            mat = tl.load(in_mat_ptrs)",
      "        else:",
      "            mat = tl.load(in_mat_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            mat = tl.load(in_mat_ptrs, mask=offs_m[:, None] < seqlen, other=0.0)",
      "        else:",
      "            mat = tl.load(",
      "                in_mat_ptrs,",
      "                mask=(offs_m[:, None] < seqlen) & (offs_d[None, :] < headdim),",
      "                other=0.0,",
      "            )",
      "",
      "    if EVEN_HEADDIM:",
      "        proj_dir_block = tl.load(",
      "            proj_dir_ptrs, mask=offs_n[None, :] < num_projs, other=0.0",
      "        )",
      "    else:",
      "        proj_dir_block = tl.load(",
      "            proj_dir_ptrs,",
      "            mask=(offs_n[None, :] < num_projs)",
      "            & (offs_d[:, None] * stride_proj_dird < headdim),",
      "            other=0.0,",
      "        )",
      "",
      "    mask = tl.dot(mat, proj_dir_block)",
      "    mask = tl.where(mask > 0.0, 1.0, 0.0)",
      "",
      "    encoding_vectors = tl.load(enc_vec + offs_n, mask=offs_n < num_projs, other=0.0)",
      "",
      "    bin_ids = tl.sum(mask * encoding_vectors[None, :], 1).to(tl.int32)",
      "",
      "    hash_buckets = tl.load(perm + bin_ids)",
      "",
      "    buckets_ptrs = buckets + off_b * stride_bucketsb + off_h * stride_bucketsh + offs_m",
      "    if EVEN_M:",
      "        tl.store(buckets_ptrs, hash_buckets)",
      "    else:",
      "        tl.store(buckets_ptrs, hash_buckets, mask=offs_m < seqlen)"
    ],
    "file": "triton_repos/amirzandieh_HyperAttention/src/159.py"
  },
  {
    "name": "_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "Lse",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_ob",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    Out,",
      "    Lse,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_ob,",
      "    stride_oh,",
      "    stride_om,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    q_ptrs = (",
      "        Q",
      "        + off_b * stride_qb",
      "        + off_h * stride_qh",
      "        + (offs_m[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "    k_ptrs = (",
      "        K",
      "        + off_b * stride_kb",
      "        + off_h * stride_kh",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "    v_ptrs = (",
      "        V",
      "        + off_b * stride_vb",
      "        + off_h * stride_vh",
      "        + (offs_n[:, None] * stride_vn + offs_d[None, :])",
      "    )",
      "    if BIAS_TYPE == \"vector\":",
      "        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n",
      "    elif BIAS_TYPE == \"matrix\":",
      "        b_ptrs = (",
      "            Bias",
      "            + off_b * stride_bb",
      "            + off_h * stride_bh",
      "            + (offs_m[:, None] * stride_bm + offs_n[None, :])",
      "        )",
      "",
      "    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    if EVEN_M & EVEN_N:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs)",
      "        else:",
      "            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)",
      "        else:",
      "            q = tl.load(",
      "                q_ptrs,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "                other=0.0,",
      "            )",
      "",
      "    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)",
      "    for start_n in range(0, end_n, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(k_ptrs + start_n * stride_kn)",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, tl.trans(k))",
      "",
      "        if not EVEN_N:",
      "            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float(\"-inf\"))",
      "        if IS_CAUSAL:",
      "            qk += tl.where(",
      "                offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float(\"-inf\")",
      "            )",
      "        if BIAS_TYPE != \"none\":",
      "            if BIAS_TYPE == \"vector\":",
      "                if EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n, mask=(start_n + offs_n) < seqlen_k, other=0.0",
      "                    ).to(tl.float32)",
      "                bias = bias[None, :]",
      "            elif BIAS_TYPE == \"matrix\":",
      "                if EVEN_M & EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n,",
      "                        mask=(offs_m[:, None] < seqlen_q)",
      "                        & ((start_n + offs_n)[None, :] < seqlen_k),",
      "                        other=0.0,",
      "                    ).to(tl.float32)",
      "",
      "            qk = qk * softmax_scale + bias",
      "            m_ij = tl.maximum(tl.max(qk, 1), lse_i)",
      "            p = tl.exp(qk - m_ij[:, None])",
      "        else:",
      "            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "            p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(v_ptrs + start_n * stride_vn)",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    o_scale = tl.exp(m_i - lse_i)",
      "    acc_o = acc_o * o_scale[:, None]",
      "",
      "    start_m = tl.program_id(0)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m",
      "    tl.store(lse_ptrs, lse_i)",
      "",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    out_ptrs = (",
      "        Out",
      "        + off_b * stride_ob",
      "        + off_h * stride_oh",
      "        + (offs_m[:, None] * stride_om + offs_d[None, :])",
      "    )",
      "    if EVEN_M:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o)",
      "        else:",
      "            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)",
      "        else:",
      "            tl.store(",
      "                out_ptrs,",
      "                acc_o,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "            )"
    ],
    "file": "triton_repos/amirzandieh_HyperAttention/src/160.py"
  },
  {
    "name": "_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K', 'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])",
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEQUENCE_PARALLEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    SEQUENCE_PARALLEL: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    K += off_b * stride_kb + off_h * stride_kh",
      "    V += off_b * stride_vb + off_h * stride_vh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "    DK += off_b * stride_dkb + off_h * stride_dkh",
      "    DV += off_b * stride_dvb + off_h * stride_dvh",
      "    if BIAS_TYPE != \"none\":",
      "        Bias += off_b * stride_bb + off_h * stride_bh",
      "",
      "    D += off_hb * seqlen_q_rounded",
      "    LSE += off_hb * seqlen_q_rounded",
      "    if not SEQUENCE_PARALLEL:",
      "        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)",
      "        for start_n in range(0, num_block_n):",
      "            _bwd_kernel_one_col_block(",
      "                start_n,",
      "                Q,",
      "                K,",
      "                V,",
      "                Bias,",
      "                DO,",
      "                DQ,",
      "                DK,",
      "                DV,",
      "                LSE,",
      "                D,",
      "                softmax_scale,",
      "                stride_qm,",
      "                stride_kn,",
      "                stride_vn,",
      "                stride_bm,",
      "                stride_dom,",
      "                stride_dqm,",
      "                stride_dkn,",
      "                stride_dvn,",
      "                seqlen_q,",
      "                seqlen_k,",
      "                headdim,",
      "                ATOMIC_ADD=False,",
      "                BIAS_TYPE=BIAS_TYPE,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "                EVEN_M=EVEN_M,",
      "                EVEN_N=EVEN_N,",
      "                EVEN_HEADDIM=EVEN_HEADDIM,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "    else:",
      "        start_n = tl.program_id(0)",
      "        _bwd_kernel_one_col_block(",
      "            start_n,",
      "            Q,",
      "            K,",
      "            V,",
      "            Bias,",
      "            DO,",
      "            DQ,",
      "            DK,",
      "            DV,",
      "            LSE,",
      "            D,",
      "            softmax_scale,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dqm,",
      "            stride_dkn,",
      "            stride_dvn,",
      "            seqlen_q,",
      "            seqlen_k,",
      "            headdim,",
      "            ATOMIC_ADD=True,",
      "            BIAS_TYPE=BIAS_TYPE,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "            EVEN_M=EVEN_M,",
      "            EVEN_N=EVEN_N,",
      "            EVEN_HEADDIM=EVEN_HEADDIM,",
      "            BLOCK_M=BLOCK_M,",
      "            BLOCK_N=BLOCK_N,",
      "        )"
    ],
    "file": "triton_repos/amirzandieh_HyperAttention/src/160.py"
  },
  {
    "name": "_fwd_hyper_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] == args['V_BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "q_sort_idx",
        "annotation": null
      },
      {
        "name": "k_sort_idx",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "Lse",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxm",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxn",
        "annotation": null
      },
      {
        "name": "stride_ob",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "block_size",
        "annotation": null
      },
      {
        "name": "sample_size",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "v_headdim",
        "annotation": null
      },
      {
        "name": "smooth_block",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_V_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_hyper_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    q_sort_idx,",
      "    k_sort_idx,",
      "    Out,",
      "    Lse,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_q_sort_idxb,",
      "    stride_q_sort_idxh,",
      "    stride_q_sort_idxm,",
      "    stride_k_sort_idxb,",
      "    stride_k_sort_idxh,",
      "    stride_k_sort_idxn,",
      "    stride_ob,",
      "    stride_oh,",
      "    stride_om,",
      "    nheads,",
      "    block_size,",
      "    sample_size,",
      "    seqlen_k,",
      "    seqlen_q,",
      "    headdim,",
      "    v_headdim,",
      "    smooth_block,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    V_BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    EVEN_V_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)",
      "",
      "    q_idx_ptrs = (",
      "        q_sort_idx",
      "        + off_b * stride_q_sort_idxb",
      "        + off_h * stride_q_sort_idxh",
      "        + offs_m * stride_q_sort_idxm",
      "    )",
      "    q_idx = tl.load(q_idx_ptrs).to(tl.int32)",
      "",
      "    k_sort_idx += off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh",
      "",
      "    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, V_BLOCK_HEADDIM], dtype=tl.float32)",
      "    q_ptrs = (",
      "        Q",
      "        + off_b * stride_qb",
      "        + off_h * stride_qh",
      "        + (q_idx[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "    if EVEN_HEADDIM:",
      "        q = tl.load(q_ptrs)",
      "    else:",
      "        q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "",
      "    block_id = start_m // block_size",
      "    block_offs = (",
      "        seqlen_k + (start_m % block_size) * BLOCK_N - (block_size - 1) * BLOCK_N // 2",
      "    )",
      "    end_n = tl.minimum((block_id + 1) * BLOCK_N * block_size, seqlen_k)",
      "    for start_n in range(block_id * BLOCK_N * block_size, end_n, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "        if smooth_block:",
      "            k_idx_ptrs = (",
      "                (start_n + block_offs + offs_n) * stride_k_sort_idxn",
      "            ) % seqlen_k",
      "        else:",
      "            k_idx_ptrs = (start_n + offs_n) * stride_k_sort_idxn",
      "",
      "        k_idx = tl.load(k_sort_idx + k_idx_ptrs).to(tl.int32)",
      "        k_ptrs = (",
      "            K",
      "            + off_b * stride_kb",
      "            + off_h * stride_kh",
      "            + (k_idx[:, None] * stride_kn + offs_d[None, :])",
      "        )",
      "",
      "        if EVEN_HEADDIM:",
      "            k = tl.load(k_ptrs)",
      "        else:",
      "            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, tl.trans(k))",
      "        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "        p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        v_ptrs = (",
      "            V",
      "            + off_b * stride_vb",
      "            + off_h * stride_vh",
      "            + (k_idx[:, None] * stride_vn + offs_vd[None, :])",
      "        )",
      "        if EVEN_V_HEADDIM:",
      "            v = tl.load(v_ptrs)",
      "        else:",
      "            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    for col_block in range(0, sample_size):",
      "        curr_offs_n = col_block * BLOCK_N * stride_kn + offs_n",
      "        k_ptrs = (",
      "            K",
      "            + off_b * stride_kb",
      "            + off_h * stride_kh",
      "            + (curr_offs_n[:, None] * stride_kn + offs_d[None, :])",
      "        )",
      "",
      "        if EVEN_HEADDIM:",
      "            k = tl.load(k_ptrs)",
      "        else:",
      "            k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, tl.trans(k))",
      "        m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "        p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        v_ptrs = (",
      "            V",
      "            + off_b * stride_vb",
      "            + off_h * stride_vh",
      "            + (curr_offs_n[:, None] * stride_vn + offs_vd[None, :])",
      "        )",
      "        if EVEN_V_HEADDIM:",
      "            v = tl.load(v_ptrs)",
      "        else:",
      "            v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    o_scale = tl.exp(m_i - lse_i)",
      "    acc_o = acc_o * o_scale[:, None]",
      "",
      "    lse_ptrs = Lse + off_hb * seqlen_q + q_idx",
      "    out_ptrs = (",
      "        Out",
      "        + off_b * stride_ob",
      "        + off_h * stride_oh",
      "        + (q_idx[:, None] * stride_om + offs_vd[None, :])",
      "    )",
      "",
      "    tl.store(lse_ptrs, lse_i)",
      "    if EVEN_V_HEADDIM:",
      "        tl.store(out_ptrs, acc_o)",
      "    else:",
      "        tl.store(out_ptrs, acc_o, mask=offs_vd[None, :] < v_headdim)"
    ],
    "file": "triton_repos/amirzandieh_HyperAttention/src/161.py"
  },
  {
    "name": "_bwd_permuted_block_diagonal_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] == args['V_BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "q_sort_idx",
        "annotation": null
      },
      {
        "name": "k_sort_idx",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_q_sort_idxm",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxb",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxh",
        "annotation": null
      },
      {
        "name": "stride_k_sort_idxn",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "block_size",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "v_headdim",
        "annotation": null
      },
      {
        "name": "smooth_block",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_V_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_permuted_block_diagonal_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    q_sort_idx,",
      "    k_sort_idx,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_q_sort_idxb,",
      "    stride_q_sort_idxh,",
      "    stride_q_sort_idxm,",
      "    stride_k_sort_idxb,",
      "    stride_k_sort_idxh,",
      "    stride_k_sort_idxn,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    block_size,",
      "    headdim,",
      "    v_headdim,",
      "    smooth_block,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    V_BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    EVEN_V_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    K += off_b * stride_kb + off_h * stride_kh",
      "    V += off_b * stride_vb + off_h * stride_vh",
      "    Q_idx = q_sort_idx + off_b * stride_q_sort_idxb + off_h * stride_q_sort_idxh",
      "    K_idx = k_sort_idx + off_b * stride_k_sort_idxb + off_h * stride_k_sort_idxh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "    DK += off_b * stride_dkb + off_h * stride_dkh",
      "    DV += off_b * stride_dvb + off_h * stride_dvh",
      "",
      "    D += off_hb * seqlen_q",
      "    LSE += off_hb * seqlen_q",
      "",
      "    start_n = tl.program_id(0)",
      "    _bwd_blocked_kernel_one_col(",
      "        start_n=start_n,",
      "        Q=Q,",
      "        K=K,",
      "        V=V,",
      "        Q_idx=Q_idx,",
      "        K_idx=K_idx,",
      "        DO=DO,",
      "        DQ=DQ,",
      "        DK=DK,",
      "        DV=DV,",
      "        LSE=LSE,",
      "        D=D,",
      "        softmax_scale=softmax_scale,",
      "        stride_qm=stride_qm,",
      "        stride_kn=stride_kn,",
      "        stride_vn=stride_vn,",
      "        stride_dom=stride_dom,",
      "        stride_dqm=stride_dqm,",
      "        stride_dkn=stride_dkn,",
      "        stride_dvn=stride_dvn,",
      "        stride_q_idxm=stride_q_sort_idxm,",
      "        stride_k_idxn=stride_k_sort_idxn,",
      "        seqlen_q=seqlen_q,",
      "        block_size=block_size // BLOCK_N,",
      "        headdim=headdim,",
      "        v_headdim=v_headdim,",
      "        smooth_block=smooth_block,",
      "        BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "        V_BLOCK_HEADDIM=V_BLOCK_HEADDIM,",
      "        EVEN_HEADDIM=EVEN_HEADDIM,",
      "        EVEN_V_HEADDIM=EVEN_V_HEADDIM,",
      "        BLOCK_M=BLOCK_M,",
      "        BLOCK_N=BLOCK_N,",
      "    )"
    ],
    "file": "triton_repos/amirzandieh_HyperAttention/src/161.py"
  },
  {
    "name": "_bwd_sampled_col_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM'], 'EVEN_V_HEADDIM': lambda args: args['v_headdim'] == args['V_BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "v_headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "V_BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_V_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_sampled_col_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    headdim,",
      "    v_headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    V_BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    EVEN_V_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "",
      "    D += off_hb * seqlen_q",
      "    LSE += off_hb * seqlen_q",
      "",
      "    start_n = tl.program_id(0)",
      "",
      "    offs_n = start_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    offs_m = tl.arange(0, BLOCK_M)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    offs_vd = tl.arange(0, V_BLOCK_HEADDIM)",
      "",
      "    k_ptrs = (",
      "        K",
      "        + off_b * stride_kb",
      "        + off_h * stride_kh",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "    v_ptrs = (",
      "        V",
      "        + off_b * stride_vb",
      "        + off_h * stride_vh",
      "        + (offs_n[:, None] * stride_vn + offs_vd[None, :])",
      "    )",
      "",
      "    dv = tl.zeros([BLOCK_N, V_BLOCK_HEADDIM], dtype=tl.float32)",
      "    dk = tl.zeros([BLOCK_N, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    if EVEN_HEADDIM:",
      "        k = tl.load(k_ptrs)",
      "    else:",
      "        k = tl.load(k_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    if EVEN_V_HEADDIM:",
      "        v = tl.load(v_ptrs)",
      "    else:",
      "        v = tl.load(v_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "",
      "    for start_m in range(0, seqlen_q, BLOCK_M):",
      "        start_m = tl.multiple_of(start_m, BLOCK_M)",
      "        offs_m_curr = start_m + offs_m",
      "        q_ptrs = Q + (offs_m_curr[:, None] * stride_qm + offs_d[None, :])",
      "",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs)",
      "        else:",
      "            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "",
      "        qk = tl.dot(q, tl.trans(k))",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "        lse_i = tl.load(LSE + offs_m_curr)",
      "        p = tl.exp(qk * softmax_scale - lse_i[:, None])",
      "",
      "        do_ptrs = DO + (offs_m_curr[:, None] * stride_dom + offs_vd[None, :])",
      "        if EVEN_V_HEADDIM:",
      "            do = tl.load(do_ptrs)",
      "        else:",
      "            do = tl.load(do_ptrs, mask=offs_vd[None, :] < v_headdim, other=0.0)",
      "        dv += tl.dot(tl.trans(p.to(do.dtype)), do)",
      "",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "        dp = tl.dot(do, tl.trans(v))",
      "",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "",
      "        Di = tl.load(D + offs_m_curr)",
      "",
      "        ds = (p * (dp - Di[:, None]) * softmax_scale).to(q.dtype)",
      "",
      "        dk += tl.dot(tl.trans(ds), q)",
      "",
      "        if not EVEN_HEADDIM:",
      "            tl.debug_barrier()",
      "",
      "        dq_ptrs = DQ + (offs_m_curr[:, None] * stride_dqm + offs_d[None, :])",
      "        dq = tl.dot(ds, k)",
      "        if EVEN_HEADDIM:",
      "            tl.atomic_add(dq_ptrs, dq)",
      "        else:",
      "            tl.atomic_add(dq_ptrs, dq, mask=offs_d[None, :] < headdim)",
      "",
      "    dv_ptrs = (",
      "        DV",
      "        + off_b * stride_dvb",
      "        + off_h * stride_dvh",
      "        + (offs_n[:, None] * stride_dvn + offs_vd[None, :])",
      "    )",
      "    dk_ptrs = (",
      "        DK",
      "        + off_b * stride_dkb",
      "        + off_h * stride_dkh",
      "        + (offs_n[:, None] * stride_dkn + offs_d[None, :])",
      "    )",
      "    dk += tl.load(dk_ptrs)",
      "    dv += tl.load(dv_ptrs)",
      "    _bwd_store_dx(",
      "        dk_ptrs,",
      "        dk,",
      "        offs_d,",
      "        headdim,",
      "        even_headdim=EVEN_HEADDIM,",
      "    )",
      "    _bwd_store_dx(",
      "        dv_ptrs,",
      "        dv,",
      "        offs_vd,",
      "        v_headdim,",
      "        even_headdim=EVEN_V_HEADDIM,",
      "    )",
      "",
      "    return"
    ],
    "file": "triton_repos/amirzandieh_HyperAttention/src/161.py"
  },
  {
    "name": "_rms_layernorm_backward",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': 128, 'NUM_WARPS': 4}), triton.Config({'BLOCK_SIZE': 256, 'NUM_WARPS': 8}), triton.Config({'BLOCK_SIZE': 512, 'NUM_WARPS': 16}), triton.Config({'BLOCK_SIZE': 1024, 'NUM_WARPS': 16}), triton.Config({'BLOCK_SIZE': 2048, 'NUM_WARPS': 32}), triton.Config({'BLOCK_SIZE': 4096, 'NUM_WARPS': 32}), triton.Config({'BLOCK_SIZE': 8192, 'NUM_WARPS': 48})], key=['n_cols'])"
    ],
    "args": [
      {
        "name": "dY",
        "annotation": null
      },
      {
        "name": "dY_row_stride",
        "annotation": null
      },
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "X_row_stride",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "W_row_stride",
        "annotation": null
      },
      {
        "name": "r",
        "annotation": null
      },
      {
        "name": "r_row_stride",
        "annotation": null
      },
      {
        "name": "dX",
        "annotation": null
      },
      {
        "name": "dX_row_stride",
        "annotation": null
      },
      {
        "name": "dW",
        "annotation": null
      },
      {
        "name": "n_cols",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_WARPS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_layernorm_backward(",
      "    dY,",
      "    dY_row_stride,",
      "    X,",
      "    X_row_stride,",
      "    W,",
      "    W_row_stride,",
      "    r,",
      "    r_row_stride,",
      "    dX,",
      "    dX_row_stride,",
      "    dW,",
      "    n_cols,",
      "    eps,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    NUM_WARPS: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    num_pids = tl.num_programs(0)",
      "",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    mask = col_offsets < n_cols",
      "",
      "    dY_ptr = dY + pid * dY_row_stride + col_offsets",
      "    X_ptr = X + pid * X_row_stride + col_offsets",
      "    dX_ptr = dX + pid * dX_row_stride + col_offsets",
      "",
      "    dY_row = tl.load(dY_ptr, mask=mask, other=0).to(tl.float32)",
      "    X_row = tl.load(X_ptr, mask=mask, other=0).to(tl.float32)",
      "    W_row = tl.load(W + col_offsets, mask=mask, other=0).to(tl.float32)",
      "    rms = tl.load(r + pid).to(tl.float32)",
      "",
      "    X_norm = X_row * rms",
      "    dY_W = dY_row * W_row",
      "    sum_dY_X = tl.sum(dY_W * X_norm, axis=0)",
      "    dX = rms * (dY_W - X_norm * (sum_dY_X / n_cols))",
      "    dW_row = dY_row * X_norm",
      "    tl.atomic_add(dW + col_offsets, dW_row, mask=mask)",
      "    tl.store(dX_ptr, dX, mask=mask)"
    ],
    "file": "triton_repos/dame-cell_Triformer/triformer/192.py"
  },
  {
    "name": "_rope_embedding",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'BACKWARD_PASS': lambda args: bool(args['BACKWARD_PASS'])})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "Q_row_stride",
        "annotation": null
      },
      {
        "name": "cos",
        "annotation": null
      },
      {
        "name": "cos_row_stride",
        "annotation": null
      },
      {
        "name": "sin",
        "annotation": null
      },
      {
        "name": "sin_row_stride",
        "annotation": null
      },
      {
        "name": "seqlen",
        "annotation": null
      },
      {
        "name": "head_dim",
        "annotation": "tl.constexpr"
      },
      {
        "name": "n_heads",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BACKWARD_PASS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rope_embedding(",
      "    Q,",
      "    Q_row_stride,",
      "    cos,",
      "    cos_row_stride,",
      "    sin,",
      "    sin_row_stride,",
      "    seqlen,",
      "    head_dim: tl.constexpr,",
      "    n_heads: tl.constexpr,",
      "    BACKWARD_PASS: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    ROPE_GROUP_SIZE = 4",
      "    row_position = tl.program_id(0)",
      "    group_head_position = tl.program_id(1)",
      "    col_offsets = tl.arange(0, BLOCK_SIZE)",
      "    half_head_dim = head_dim // 2",
      "    mask = col_offsets < half_head_dim",
      "",
      "    sin1 = tl.load(",
      "        sin",
      "        + (row_position % seqlen) * sin_row_stride",
      "        + half_head_dim * 0",
      "        + col_offsets,",
      "        mask=mask,",
      "        other=0,",
      "    )",
      "    cos1 = tl.load(",
      "        cos",
      "        + (row_position % seqlen) * cos_row_stride",
      "        + half_head_dim * 0",
      "        + col_offsets,",
      "        mask=mask,",
      "        other=0,",
      "    )",
      "",
      "    if BACKWARD_PASS:",
      "",
      "        sin1 = -sin1",
      "    pass",
      "",
      "    head_start = group_head_position * ROPE_GROUP_SIZE",
      "    head_end = min((head_start + ROPE_GROUP_SIZE), n_heads)",
      "",
      "    for k in range(head_start, head_end):",
      "        offs_q1 = row_position * Q_row_stride + k * head_dim + col_offsets",
      "        offs_q2 = (",
      "            row_position * Q_row_stride + k * head_dim + col_offsets + half_head_dim",
      "        )",
      "",
      "        Q1 = tl.load(Q + offs_q1, mask=mask, other=0).to(sin1.dtype)",
      "        Q2 = tl.load(Q + offs_q2, mask=mask, other=0).to(sin1.dtype)",
      "",
      "        tl.store(Q + offs_q1, Q1 * cos1 - Q2 * sin1, mask=mask)",
      "        tl.store(Q + offs_q2, Q2 * cos1 + Q1 * sin1, mask=mask)",
      "    pass"
    ],
    "file": "triton_repos/dame-cell_Triformer/triformer/193.py"
  },
  {
    "name": "apply_clip_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size, num_warps in itertools.product([32, 64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=['n_audios', 'audio_len'])"
    ],
    "args": [
      {
        "name": "samples_ptr",
        "annotation": null
      },
      {
        "name": "min",
        "annotation": null
      },
      {
        "name": "max",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "n_audios",
        "annotation": null
      },
      {
        "name": "audio_len",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def apply_clip_kernel(",
      "    samples_ptr, min, max, output_ptr, n_audios, audio_len, BLOCK_SIZE: tl.constexpr",
      "):",
      "    audio_idx = tl.program_id(0)",
      "",
      "    if audio_idx >= n_audios:",
      "        return",
      "",
      "    for i in range(0, audio_len, BLOCK_SIZE):",
      "        sample_idx = i + tl.arange(0, BLOCK_SIZE)",
      "        mask = sample_idx < audio_len",
      "",
      "        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)",
      "        result = tl.where(samples > max, max, samples)",
      "        result = tl.where(result < min, min, result)",
      "        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)"
    ],
    "file": "triton_repos/Lallapallooza_fast-audiomentations/fast_audiomentations/transforms/_impl/101.py"
  },
  {
    "name": "unfold_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size, num_warps in itertools.product([32, 64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=['length', 'kernel_size', 'stride', 'n_frames'])"
    ],
    "args": [
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "length",
        "annotation": null
      },
      {
        "name": "kernel_size",
        "annotation": null
      },
      {
        "name": "stride",
        "annotation": null
      },
      {
        "name": "n_frames",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def unfold_kernel(",
      "    input_ptr,",
      "    output_ptr,",
      "    length,",
      "    kernel_size,",
      "    stride,",
      "    n_frames,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    batch_idx = tl.program_id(0)",
      "",
      "    frame_idx = tl.program_id(1) * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "",
      "    mask = frame_idx < n_frames",
      "",
      "    input_pos = frame_idx * stride",
      "",
      "    for i in range(kernel_size):",
      "        in_bounds = mask & ((input_pos + i) < length)",
      "",
      "        val = tl.where(",
      "            in_bounds,",
      "            tl.load(input_ptr + batch_idx * length + input_pos + i, mask=in_bounds),",
      "            0,",
      "        )",
      "",
      "        out_idx = batch_idx * n_frames * kernel_size + frame_idx * kernel_size + i",
      "        tl.store(output_ptr + out_idx, val, mask=in_bounds)"
    ],
    "file": "triton_repos/Lallapallooza_fast-audiomentations/fast_audiomentations/transforms/_impl/102.py"
  },
  {
    "name": "complex_mul_conjugate_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=num_warps) for num_warps in [1, 2, 4, 8, 16, 32]], key=['num_batches', 'num_frames', 'fft_size'])"
    ],
    "args": [
      {
        "name": "a_real_ptr",
        "annotation": null
      },
      {
        "name": "b_real_ptr",
        "annotation": null
      },
      {
        "name": "a_imag_ptr",
        "annotation": null
      },
      {
        "name": "b_imag_ptr",
        "annotation": null
      },
      {
        "name": "output1_ptr",
        "annotation": null
      },
      {
        "name": "output2_ptr",
        "annotation": null
      },
      {
        "name": "num_batches",
        "annotation": null
      },
      {
        "name": "num_frames",
        "annotation": null
      },
      {
        "name": "fft_size",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def complex_mul_conjugate_kernel(",
      "    a_real_ptr,",
      "    b_real_ptr,",
      "    a_imag_ptr,",
      "    b_imag_ptr,",
      "    output1_ptr,",
      "    output2_ptr,",
      "    num_batches,",
      "    num_frames,",
      "    fft_size,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "",
      "    batch_idx = tl.program_id(0)",
      "",
      "    if batch_idx >= num_batches:",
      "        return",
      "",
      "    fft_idx = tl.arange(0, BLOCK_SIZE)",
      "    fft_mask = fft_idx < fft_size",
      "",
      "    batch_by_fft = batch_idx * fft_size",
      "",
      "    b_real_val = tl.load(b_real_ptr + batch_by_fft + fft_idx, mask=fft_mask)",
      "    b_imag_val = tl.load(b_imag_ptr + batch_by_fft + fft_idx, mask=fft_mask)",
      "",
      "    for frame_idx in range(num_frames):",
      "        global_idx = num_frames * batch_by_fft + frame_idx * fft_size + fft_idx",
      "",
      "        a_real_val = tl.load(a_real_ptr + global_idx, mask=fft_mask)",
      "        a_imag_val = tl.load(a_imag_ptr + global_idx, mask=fft_mask)",
      "",
      "        result1 = a_real_val * b_real_val + a_imag_val * b_imag_val",
      "        result2 = a_imag_val * b_real_val - a_real_val * b_imag_val",
      "",
      "        tl.store(output1_ptr + global_idx, result1, mask=fft_mask)",
      "        tl.store(output2_ptr + global_idx, result2, mask=fft_mask)"
    ],
    "file": "triton_repos/Lallapallooza_fast-audiomentations/fast_audiomentations/transforms/_impl/102.py"
  },
  {
    "name": "apply_gain_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE': block_size}, num_warps=num_warps) for block_size, num_warps in itertools.product([32, 64, 128, 256, 512, 1024, 2048, 4096], [1, 2, 4, 8, 16, 32])], key=['n_audios', 'audio_len'])"
    ],
    "args": [
      {
        "name": "samples_ptr",
        "annotation": null
      },
      {
        "name": "amplitude_ratios_ptr",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "n_audios",
        "annotation": null
      },
      {
        "name": "audio_len",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def apply_gain_kernel(",
      "    samples_ptr,",
      "    amplitude_ratios_ptr,",
      "    output_ptr,",
      "    n_audios,",
      "    audio_len,",
      "    BLOCK_SIZE: tl.constexpr,",
      "):",
      "    audio_idx = tl.program_id(0)",
      "",
      "    if audio_idx >= n_audios:",
      "        return",
      "",
      "    gain = tl.load(amplitude_ratios_ptr + audio_idx)",
      "",
      "    for i in range(0, audio_len, BLOCK_SIZE):",
      "        sample_idx = i + tl.arange(0, BLOCK_SIZE)",
      "        mask = sample_idx < audio_len",
      "        samples = tl.load(samples_ptr + audio_idx * audio_len + sample_idx, mask=mask)",
      "        result = samples * gain",
      "        tl.store(output_ptr + audio_idx * audio_len + sample_idx, result, mask=mask)"
    ],
    "file": "triton_repos/Lallapallooza_fast-audiomentations/fast_audiomentations/transforms/_impl/103.py"
  },
  {
    "name": "sum_with_snr_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_SUM': block_size_sum}, num_warps=num_warps) for block_size_sum, num_warps in itertools.product([512, 1024], [2, 4, 8, 16])], key=['clean_audio_max_len', 'noisy_audio_max_len'])"
    ],
    "args": [
      {
        "name": "clean_audio",
        "annotation": null
      },
      {
        "name": "clean_audio_real_lens",
        "annotation": null
      },
      {
        "name": "clean_audio_max_len",
        "annotation": null
      },
      {
        "name": "desired_rms",
        "annotation": null
      },
      {
        "name": "noisy_audio_ptr",
        "annotation": null
      },
      {
        "name": "noisy_audio_real_lens",
        "annotation": null
      },
      {
        "name": "noisy_audio_max_len",
        "annotation": null
      },
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_SUM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_RMS",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def sum_with_snr_kernel(",
      "    clean_audio,",
      "    clean_audio_real_lens,",
      "    clean_audio_max_len,",
      "    desired_rms,",
      "    noisy_audio_ptr,",
      "    noisy_audio_real_lens,",
      "    noisy_audio_max_len,",
      "    output_ptr,",
      "    BLOCK_SIZE_SUM: tl.constexpr,",
      "    BLOCK_SIZE_RMS: tl.constexpr,",
      "):",
      "    batch_idx = tl.program_id(0)",
      "",
      "    clean_audio_real_lens_val = tl.load(clean_audio_real_lens + batch_idx)",
      "    clean_audio_rms = rms_kernel(",
      "        clean_audio,",
      "        clean_audio_real_lens,",
      "        clean_audio_max_len,",
      "        batch_idx,",
      "        BLOCK_SIZE_RMS,",
      "    )",
      "",
      "    noisy_audio_real_lens_val = tl.load(noisy_audio_real_lens + batch_idx)",
      "",
      "    noisy_audio_rms = rms_kernel(",
      "        noisy_audio_ptr,",
      "        noisy_audio_real_lens,",
      "        noisy_audio_max_len,",
      "        batch_idx,",
      "        BLOCK_SIZE_RMS,",
      "    )",
      "",
      "    desired_rms_val = tl.load(desired_rms + batch_idx)",
      "    relative_rms = clean_audio_rms / tl.math.pow(10.0, desired_rms_val / 20.0)",
      "",
      "    for offset in range(0, clean_audio_max_len, BLOCK_SIZE_SUM):",
      "        clean_audio_block_ptr = offset + tl.arange(0, BLOCK_SIZE_SUM)",
      "        clean_audio_mask = clean_audio_block_ptr < clean_audio_real_lens_val",
      "        clean_audio_vals = tl.load(",
      "            clean_audio + batch_idx * clean_audio_max_len + clean_audio_block_ptr,",
      "            mask=clean_audio_mask,",
      "        )",
      "",
      "        offset_over_max = offset % noisy_audio_real_lens_val",
      "",
      "        offset_adjusted = offset_over_max - tl.math.min(",
      "            offset_over_max,",
      "            tl.math.max(",
      "                0, (offset_over_max + BLOCK_SIZE_SUM) - noisy_audio_real_lens_val",
      "            ),",
      "        )",
      "",
      "        noisy_audio_block_ptr = offset_adjusted + tl.arange(0, BLOCK_SIZE_SUM)",
      "",
      "        noisy_audio_val = tl.load(",
      "            noisy_audio_ptr + batch_idx * noisy_audio_max_len + noisy_audio_block_ptr,",
      "            mask=noisy_audio_block_ptr < noisy_audio_real_lens_val,",
      "        )",
      "",
      "        tl.store(",
      "            output_ptr + batch_idx * clean_audio_max_len + clean_audio_block_ptr,",
      "            clean_audio_vals + noisy_audio_val * (relative_rms / noisy_audio_rms),",
      "            mask=clean_audio_mask,",
      "        )"
    ],
    "file": "triton_repos/Lallapallooza_fast-audiomentations/fast_audiomentations/transforms/_impl/104.py"
  },
  {
    "name": "min_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 512, 'BLOCK_N': 512}), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 512}), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32}), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64}), triton.Config({'BLOCK_M': 32, 'BLOCK_N': 128}), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 16}), triton.Config({'BLOCK_M': 16, 'BLOCK_N': 256})], key=['M', 'N'])"
    ],
    "args": [
      {
        "name": "inp",
        "annotation": null
      },
      {
        "name": "out_value",
        "annotation": null
      },
      {
        "name": "out_index",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def min_kernel(",
      "    inp,",
      "    out_value,",
      "    out_index,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    min_values = tl.full([BLOCK_M], dtype=tl.float32, value=float(\"inf\"))",
      "    argmin_values = tl.full([BLOCK_M], dtype=tl.int64, value=0)",
      "    for start_n in range(0, N, BLOCK_N):",
      "",
      "        n_offset = start_n + tl.arange(0, BLOCK_N)",
      "",
      "        offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "",
      "        mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "        inp_ptrs = inp + offset",
      "",
      "        inp_vals = tl.load(inp_ptrs, mask=mask, other=float(\"inf\"))",
      "        local_min, local_argmin = tl.min(inp_vals, 1, return_indices=True)",
      "",
      "        update = local_min < min_values",
      "        min_values = tl.where(update, local_min, min_values)",
      "        argmin_values = tl.where(update, start_n + local_argmin, argmin_values)",
      "",
      "    offset_index = m_offset * K + pid_k",
      "    out_value_ptrs = out_value + offset_index",
      "    out_index_ptrs = out_index + offset_index",
      "    mask1 = m_offset < M",
      "",
      "    tl.store(out_value_ptrs, min_values, mask=mask1)",
      "    tl.store(out_index_ptrs, argmin_values, mask=mask1)"
    ],
    "file": "triton_repos/zjhellofss_triton_course/course16/660.py"
  },
  {
    "name": "fp8_gemm_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=fp8_gemm_configs, key=['N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "a_s_ptr",
        "annotation": null
      },
      {
        "name": "b_s_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def fp8_gemm_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    a_s_ptr,",
      "    b_s_ptr,",
      "    M,",
      "    N: tl.constexpr,",
      "    K: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "):",
      "",
      "    pid_m = tl.program_id(axis=0)",
      "    pid_n = tl.program_id(axis=1)",
      "    k = tl.cdiv(K, BLOCK_SIZE_K)",
      "    offs_m = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_n = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + offs_m[:, None] * K + offs_k[None, :]",
      "    b_ptrs = b_ptr + offs_n[None, :] * K + offs_k[:, None]",
      "    a_s_ptrs = a_s_ptr + offs_m * k",
      "    b_s_ptrs = b_s_ptr + (offs_n // BLOCK_SIZE_K) * k",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for i in range(k):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - i * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - i * BLOCK_SIZE_K, other=0.0)",
      "        a_s = tl.load(a_s_ptrs)",
      "        b_s = tl.load(b_s_ptrs)",
      "        accumulator += tl.dot(a, b) * a_s[:, None] * b_s[None, :]",
      "        a_ptrs += BLOCK_SIZE_K",
      "        b_ptrs += BLOCK_SIZE_K",
      "        a_s_ptrs += 1",
      "        b_s_ptrs += 1",
      "    c = accumulator.to(c_ptr.dtype.element_ty)",
      "    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + offs_m[:, None] * N + offs_n[None, :]",
      "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=mask)"
    ],
    "file": "triton_repos/zjhellofss_triton_course/course5/664.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=get_autotune_config(), key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "need_silu",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    need_silu,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M",
      "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if need_silu:",
      "        sigmoid_x = 1.0 / (1.0 + tl.exp(-accumulator))",
      "        c = accumulator.to(tl.float16) * sigmoid_x.to(tl.float16)",
      "    else:",
      "        c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/zjhellofss_triton_course/course6/666.py"
  },
  {
    "name": "flash_attention2_nopad_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=attention_configs, key=['BLOCK_DHEAD_SIZE', 'heads', 'num_kv_groups'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "O",
        "annotation": null
      },
      {
        "name": "B_Start_Loc",
        "annotation": null
      },
      {
        "name": "B_Seqlen",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "heads",
        "annotation": null
      },
      {
        "name": "num_kv_groups",
        "annotation": null
      },
      {
        "name": "stride_q_bs",
        "annotation": null
      },
      {
        "name": "stride_q_heads",
        "annotation": null
      },
      {
        "name": "stride_q_dim",
        "annotation": null
      },
      {
        "name": "stride_k_bs",
        "annotation": null
      },
      {
        "name": "stride_k_heads",
        "annotation": null
      },
      {
        "name": "stride_k_dim",
        "annotation": null
      },
      {
        "name": "stride_v_bs",
        "annotation": null
      },
      {
        "name": "stride_v_heads",
        "annotation": null
      },
      {
        "name": "stride_v_dim",
        "annotation": null
      },
      {
        "name": "stride_o_bs",
        "annotation": null
      },
      {
        "name": "stride_o_heads",
        "annotation": null
      },
      {
        "name": "stride_o_dim",
        "annotation": null
      },
      {
        "name": "BLOCK_DHEAD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N_SIZE",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def flash_attention2_nopad_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    O,",
      "    B_Start_Loc,",
      "    B_Seqlen,",
      "    sm_scale,",
      "    heads,",
      "    num_kv_groups,",
      "    stride_q_bs,",
      "    stride_q_heads,",
      "    stride_q_dim,",
      "    stride_k_bs,",
      "    stride_k_heads,",
      "    stride_k_dim,",
      "    stride_v_bs,",
      "    stride_v_heads,",
      "    stride_v_dim,",
      "    stride_o_bs,",
      "    stride_o_heads,",
      "    stride_o_dim,",
      "    BLOCK_DHEAD_SIZE: tl.constexpr,",
      "    BLOCK_M_SIZE: tl.constexpr,",
      "    BLOCK_N_SIZE: tl.constexpr,",
      "):",
      "",
      "    block_m_idx = tl.program_id(0)",
      "    cur_bh = tl.program_id(1)",
      "    cur_batch_idx = cur_bh // heads",
      "    cur_head_idx = cur_bh % heads",
      "    cur_kv_head_idx = cur_head_idx // num_kv_groups",
      "",
      "    cur_seq_len = tl.load(B_Seqlen + cur_batch_idx)",
      "",
      "    cur_seq_start_loc = tl.load(B_Start_Loc + cur_batch_idx)",
      "",
      "    block_start_loc = block_m_idx * BLOCK_M_SIZE",
      "",
      "    offs_n = tl.arange(0, BLOCK_N_SIZE)",
      "    offs_d = tl.arange(0, BLOCK_DHEAD_SIZE)",
      "    offs_m = block_start_loc + tl.arange(0, BLOCK_M_SIZE)",
      "",
      "    q_offs = (",
      "        (cur_seq_start_loc + offs_m[:, None]) * stride_q_bs",
      "        + cur_head_idx * stride_q_heads",
      "        + offs_d[None, :] * stride_q_dim",
      "    )",
      "    q = tl.load(Q + q_offs, mask=offs_m[:, None] < cur_seq_len, other=0.0)",
      "",
      "    k_offs = (",
      "        offs_n[None, :] * stride_k_bs",
      "        + cur_kv_head_idx * stride_k_heads",
      "        + offs_d[:, None] * stride_k_dim",
      "    )",
      "    v_offs = (",
      "        offs_n[:, None] * stride_v_bs",
      "        + cur_kv_head_idx * stride_v_heads",
      "        + offs_d[None, :] * stride_v_dim",
      "    )",
      "",
      "    k_ptrs = K + k_offs",
      "    v_ptrs = V + v_offs",
      "",
      "    m_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32) - float(\"inf\")",
      "    d_i = tl.zeros((BLOCK_M_SIZE,), dtype=tl.float32)",
      "    acc = tl.zeros((BLOCK_M_SIZE, BLOCK_DHEAD_SIZE), dtype=tl.float32)",
      "",
      "    block_mask = tl.where(block_start_loc < cur_seq_len, 1, 0)",
      "    block_end_loc = tl.minimum(block_start_loc + BLOCK_M_SIZE, cur_seq_len)",
      "",
      "    for start_n in range(0, block_mask * block_end_loc, BLOCK_N_SIZE):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N_SIZE)",
      "",
      "        k = tl.load(",
      "            k_ptrs + (cur_seq_start_loc + start_n) * stride_k_bs,",
      "            mask=(start_n + offs_n[None, :]) < block_end_loc,",
      "            other=0.0,",
      "        )",
      "",
      "        qk = tl.dot(q, k)",
      "",
      "        casual_mask = offs_m[:, None] >= (start_n + offs_n[None, :])",
      "        qk = tl.where(casual_mask, qk * sm_scale, -1.0e8)",
      "",
      "        m_ij = tl.maximum(m_i, tl.max(qk, 1))",
      "        qk -= m_ij[:, None]",
      "        p = tl.math.exp2(qk)",
      "        d_ij = tl.sum(p, 1)",
      "",
      "        alpha = tl.math.exp2(m_i - m_ij)",
      "        d_i = d_i * alpha + d_ij",
      "",
      "        acc = acc * alpha[:, None]",
      "",
      "        v = tl.load(",
      "            v_ptrs + (cur_seq_start_loc + start_n) * stride_v_bs,",
      "            mask=(start_n + offs_n[:, None]) < block_end_loc,",
      "            other=0.0,",
      "        )",
      "        p = p.to(v.dtype)",
      "        acc = tl.dot(p, v, acc)",
      "",
      "        m_i = m_ij",
      "",
      "    acc = acc / d_i[:, None]",
      "    off_o = (",
      "        (cur_seq_start_loc + offs_m[:, None]) * stride_o_bs",
      "        + cur_head_idx * stride_o_heads",
      "        + offs_d[None, :] * stride_o_dim",
      "    )",
      "    out_ptrs = O + off_o",
      "    tl.store(out_ptrs, acc, mask=offs_m[:, None] < cur_seq_len)"
    ],
    "file": "triton_repos/zjhellofss_triton_course/course8/669.py"
  },
  {
    "name": "_fwd_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "Lse",
        "annotation": null
      },
      {
        "name": "TMP",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_ob",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    Out,",
      "    Lse,",
      "    TMP,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_ob,",
      "    stride_oh,",
      "    stride_om,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    q_ptrs = (",
      "        Q",
      "        + off_b * stride_qb",
      "        + off_h * stride_qh",
      "        + (offs_m[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "    k_ptrs = (",
      "        K",
      "        + off_b * stride_kb",
      "        + off_h * stride_kh",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "    v_ptrs = (",
      "        V",
      "        + off_b * stride_vb",
      "        + off_h * stride_vh",
      "        + (offs_n[:, None] * stride_vn + offs_d[None, :])",
      "    )",
      "    if BIAS_TYPE == \"vector\":",
      "        b_ptrs = Bias + off_b * stride_bb + off_h * stride_bh + offs_n",
      "    elif BIAS_TYPE == \"matrix\":",
      "        b_ptrs = (",
      "            Bias",
      "            + off_b * stride_bb",
      "            + off_h * stride_bh",
      "            + (offs_m[:, None] * stride_bm + offs_n[None, :])",
      "        )",
      "",
      "    t_ptrs = TMP + off_hb * seqlen_q_rounded + offs_m",
      "    lse_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    if EVEN_M & EVEN_N:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs)",
      "        else:",
      "            q = tl.load(q_ptrs, mask=offs_d[None, :] < headdim, other=0.0)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            q = tl.load(q_ptrs, mask=offs_m[:, None] < seqlen_q, other=0.0)",
      "        else:",
      "            q = tl.load(",
      "                q_ptrs,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "                other=0.0,",
      "            )",
      "",
      "    end_n = seqlen_k if not IS_CAUSAL else tl.minimum((start_m + 1) * BLOCK_M, seqlen_k)",
      "    for start_n in range(0, end_n, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(k_ptrs + start_n * stride_kn)",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                k = tl.load(",
      "                    k_ptrs + start_n * stride_kn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)",
      "        qk += tl.dot(q, k, trans_b=True)",
      "",
      "        if not EVEN_N:",
      "            qk += tl.where((start_n + offs_n)[None, :] < seqlen_k, 0, float(\"-inf\"))",
      "        if IS_CAUSAL:",
      "            qk += tl.where(",
      "                offs_m[:, None] >= (start_n + offs_n)[None, :], 0, float(\"-inf\")",
      "            )",
      "        if BIAS_TYPE != \"none\":",
      "            if BIAS_TYPE == \"vector\":",
      "                if EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n, mask=(start_n + offs_n) < seqlen_k, other=0.0",
      "                    ).to(tl.float32)",
      "                bias = bias[None, :]",
      "            elif BIAS_TYPE == \"matrix\":",
      "                if EVEN_M & EVEN_N:",
      "                    bias = tl.load(b_ptrs + start_n).to(tl.float32)",
      "                else:",
      "                    bias = tl.load(",
      "                        b_ptrs + start_n,",
      "                        mask=(offs_m[:, None] < seqlen_q)",
      "                        & ((start_n + offs_n)[None, :] < seqlen_k),",
      "                        other=0.0,",
      "                    ).to(tl.float32)",
      "",
      "            qk = qk * softmax_scale + bias",
      "            m_ij = tl.maximum(tl.max(qk, 1), lse_i)",
      "            p = tl.exp(qk - m_ij[:, None])",
      "        else:",
      "            m_ij = tl.maximum(tl.max(qk, 1) * softmax_scale, lse_i)",
      "            p = tl.exp(qk * softmax_scale - m_ij[:, None])",
      "        l_ij = tl.sum(p, 1)",
      "",
      "        acc_o_scale = tl.exp(m_i - m_ij)",
      "",
      "        tl.store(t_ptrs, acc_o_scale)",
      "        acc_o_scale = tl.load(t_ptrs)",
      "        acc_o = acc_o * acc_o_scale[:, None]",
      "",
      "        if EVEN_N & EVEN_M:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(v_ptrs + start_n * stride_vn)",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=offs_d[None, :] < headdim,",
      "                    other=0.0,",
      "                )",
      "        else:",
      "            if EVEN_HEADDIM:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=(start_n + offs_n)[:, None] < seqlen_k,",
      "                    other=0.0,",
      "                )",
      "            else:",
      "                v = tl.load(",
      "                    v_ptrs + start_n * stride_vn,",
      "                    mask=((start_n + offs_n)[:, None] < seqlen_k)",
      "                    & (offs_d[None, :] < headdim),",
      "                    other=0.0,",
      "                )",
      "        p = p.to(v.dtype)",
      "        acc_o += tl.dot(p, v)",
      "",
      "        m_i = m_ij",
      "        l_i_new = tl.exp(lse_i - m_ij) + l_ij",
      "        lse_i = m_ij + tl.log(l_i_new)",
      "",
      "    o_scale = tl.exp(m_i - lse_i)",
      "",
      "    tl.store(t_ptrs, o_scale)",
      "    o_scale = tl.load(t_ptrs)",
      "    acc_o = acc_o * o_scale[:, None]",
      "",
      "    start_m = tl.program_id(0)",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    lse_ptrs = Lse + off_hb * seqlen_q_rounded + offs_m",
      "    tl.store(lse_ptrs, lse_i)",
      "",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    out_ptrs = (",
      "        Out",
      "        + off_b * stride_ob",
      "        + off_h * stride_oh",
      "        + (offs_m[:, None] * stride_om + offs_d[None, :])",
      "    )",
      "    if EVEN_M:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o)",
      "        else:",
      "            tl.store(out_ptrs, acc_o, mask=offs_d[None, :] < headdim)",
      "    else:",
      "        if EVEN_HEADDIM:",
      "            tl.store(out_ptrs, acc_o, mask=offs_m[:, None] < seqlen_q)",
      "        else:",
      "            tl.store(",
      "                out_ptrs,",
      "                acc_o,",
      "                mask=(offs_m[:, None] < seqlen_q) & (offs_d[None, :] < headdim),",
      "            )"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/dev/395.py"
  },
  {
    "name": "_bwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'SEQUENCE_PARALLEL': False}, num_warps=8, num_stages=1, pre_hook=init_to_zero('DQ')), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'SEQUENCE_PARALLEL': True}, num_warps=8, num_stages=1, pre_hook=init_to_zero('DQ'))], key=['CACHE_KEY_SEQLEN_Q', 'CACHE_KEY_SEQLEN_K', 'BIAS_TYPE', 'IS_CAUSAL', 'BLOCK_HEADDIM'])",
      "@triton.heuristics({'EVEN_M': lambda args: args['seqlen_q'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['seqlen_k'] % args['BLOCK_N'] == 0, 'EVEN_HEADDIM': lambda args: args['headdim'] == args['BLOCK_HEADDIM']})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "Bias",
        "annotation": null
      },
      {
        "name": "DO",
        "annotation": null
      },
      {
        "name": "DQ",
        "annotation": null
      },
      {
        "name": "DK",
        "annotation": null
      },
      {
        "name": "DV",
        "annotation": null
      },
      {
        "name": "LSE",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "stride_qb",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_kb",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_vb",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_bb",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_dob",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_dqb",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dkb",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dvb",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "nheads",
        "annotation": null
      },
      {
        "name": "seqlen_q",
        "annotation": null
      },
      {
        "name": "seqlen_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_Q",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_SEQLEN_K",
        "annotation": null
      },
      {
        "name": "BIAS_TYPE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SEQUENCE_PARALLEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _bwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    Bias,",
      "    DO,",
      "    DQ,",
      "    DK,",
      "    DV,",
      "    LSE,",
      "    D,",
      "    softmax_scale,",
      "    stride_qb,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_kb,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_vb,",
      "    stride_vh,",
      "    stride_vn,",
      "    stride_bb,",
      "    stride_bh,",
      "    stride_bm,",
      "    stride_dob,",
      "    stride_doh,",
      "    stride_dom,",
      "    stride_dqb,",
      "    stride_dqh,",
      "    stride_dqm,",
      "    stride_dkb,",
      "    stride_dkh,",
      "    stride_dkn,",
      "    stride_dvb,",
      "    stride_dvh,",
      "    stride_dvn,",
      "    nheads,",
      "    seqlen_q,",
      "    seqlen_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CACHE_KEY_SEQLEN_Q,",
      "    CACHE_KEY_SEQLEN_K,",
      "    BIAS_TYPE: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    SEQUENCE_PARALLEL: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    EVEN_HEADDIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    off_hb = tl.program_id(1)",
      "    off_b = off_hb // nheads",
      "    off_h = off_hb % nheads",
      "",
      "    Q += off_b * stride_qb + off_h * stride_qh",
      "    K += off_b * stride_kb + off_h * stride_kh",
      "    V += off_b * stride_vb + off_h * stride_vh",
      "    DO += off_b * stride_dob + off_h * stride_doh",
      "    DQ += off_b * stride_dqb + off_h * stride_dqh",
      "    DK += off_b * stride_dkb + off_h * stride_dkh",
      "    DV += off_b * stride_dvb + off_h * stride_dvh",
      "    if BIAS_TYPE != \"none\":",
      "        Bias += off_b * stride_bb + off_h * stride_bh",
      "",
      "    D += off_hb * seqlen_q_rounded",
      "    LSE += off_hb * seqlen_q_rounded",
      "    if not SEQUENCE_PARALLEL:",
      "        num_block_n = tl.cdiv(seqlen_k, BLOCK_N)",
      "        for start_n in range(0, num_block_n):",
      "            _bwd_kernel_one_col_block(",
      "                start_n,",
      "                Q,",
      "                K,",
      "                V,",
      "                Bias,",
      "                DO,",
      "                DQ,",
      "                DK,",
      "                DV,",
      "                LSE,",
      "                D,",
      "                softmax_scale,",
      "                stride_qm,",
      "                stride_kn,",
      "                stride_vn,",
      "                stride_bm,",
      "                stride_dom,",
      "                stride_dqm,",
      "                stride_dkn,",
      "                stride_dvn,",
      "                seqlen_q,",
      "                seqlen_k,",
      "                headdim,",
      "                ATOMIC_ADD=False,",
      "                BIAS_TYPE=BIAS_TYPE,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "                EVEN_M=EVEN_M,",
      "                EVEN_N=EVEN_N,",
      "                EVEN_HEADDIM=EVEN_HEADDIM,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "    else:",
      "        start_n = tl.program_id(0)",
      "        _bwd_kernel_one_col_block(",
      "            start_n,",
      "            Q,",
      "            K,",
      "            V,",
      "            Bias,",
      "            DO,",
      "            DQ,",
      "            DK,",
      "            DV,",
      "            LSE,",
      "            D,",
      "            softmax_scale,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dqm,",
      "            stride_dkn,",
      "            stride_dvn,",
      "            seqlen_q,",
      "            seqlen_k,",
      "            headdim,",
      "            ATOMIC_ADD=True,",
      "            BIAS_TYPE=BIAS_TYPE,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "            EVEN_M=EVEN_M,",
      "            EVEN_N=EVEN_N,",
      "            EVEN_HEADDIM=EVEN_HEADDIM,",
      "            BLOCK_M=BLOCK_M,",
      "            BLOCK_N=BLOCK_N,",
      "        )"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/dev/395.py"
  },
  {
    "name": "swizzle_tile",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def swizzle_tile(",
      "    tile_id,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = tile_id // width",
      "    group_size = tl.minimum(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (tile_id % group_size)",
      "    pid_n = (tile_id % width) // group_size",
      "    return pid_m, pid_n"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/stream-k/403.py"
  },
  {
    "name": "linear_tile",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def linear_tile(",
      "    tile_id,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "):",
      "    pid_m = tile_id // tl.cdiv(N, BLOCK_N)",
      "    pid_n = tile_id % tl.cdiv(N, BLOCK_N)",
      "    return pid_m, pid_n"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/stream-k/403.py"
  },
  {
    "name": "matmul_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8)], key=['M', 'N', 'K'])"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def matmul_kernel(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(axis=0)",
      "",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    offs_am = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, K, BLOCK_SIZE_K):",
      "",
      "        a = tl.load(a_ptrs)",
      "        b = tl.load(b_ptrs)",
      "",
      "        accumulator += tl.dot(a, b)",
      "",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    c = accumulator.to(tl.float16)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/stream-k/404.py"
  },
  {
    "name": "tile_swizzling",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": null
      },
      {
        "name": "BLOCK_K",
        "annotation": null
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tile_swizzling(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = tile_id // width",
      "    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (tile_id % group_size)",
      "    pid_n = (tile_id % width) // group_size",
      "    return pid_m, pid_n"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/stream-k/406.py"
  },
  {
    "name": "tile_classic",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit"
    ],
    "args": [
      {
        "name": "tile_id",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": null
      },
      {
        "name": "BLOCK_N",
        "annotation": null
      },
      {
        "name": "BLOCK_K",
        "annotation": null
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def tile_classic(tile_id, M, N, K, BLOCK_M, BLOCK_N, BLOCK_K, GROUP_M: tl.constexpr):",
      "    pid_m = tile_id // tl.cdiv(N, BLOCK_N)",
      "    pid_n = tile_id % tl.cdiv(N, BLOCK_N)",
      "    return pid_m, pid_n"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/stream-k/406.py"
  },
  {
    "name": "_rms_norm_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "stride_x",
        "annotation": null
      },
      {
        "name": "Y",
        "annotation": null
      },
      {
        "name": "stride_y",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "block_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_norm_fwd_kernel(",
      "    X,",
      "    stride_x,",
      "    Y,",
      "    stride_y,",
      "    W,",
      "    Rstd,",
      "    eps,",
      "    M,",
      "    N,",
      "    block_N: tl.constexpr,",
      "):",
      "",
      "    row = tl.program_id(0)",
      "    cols = tl.arange(0, block_N)",
      "",
      "    mask = cols < N",
      "    x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)",
      "    w = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)",
      "",
      "    xbar = tl.where(cols < N, x, 0.0)",
      "    var = tl.sum(xbar * xbar, axis=0) / N",
      "    rstd = 1 / tl.sqrt(var + eps)",
      "",
      "    tl.store(Rstd + row, rstd)",
      "",
      "    x_hat = x * rstd",
      "    y = x_hat * w",
      "",
      "    tl.store(Y + row * stride_y + cols, y, mask=mask)"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/workspace/409.py"
  },
  {
    "name": "_rms_norm_bwd_kernel_sm",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({}, num_warps=1), triton.Config({}, num_warps=2), triton.Config({}, num_warps=4), triton.Config({}, num_warps=8), triton.Config({}, num_warps=16), triton.Config({}, num_warps=32)], key=['N'])"
    ],
    "args": [
      {
        "name": "X",
        "annotation": null
      },
      {
        "name": "stride_x",
        "annotation": null
      },
      {
        "name": "W",
        "annotation": null
      },
      {
        "name": "DY",
        "annotation": null
      },
      {
        "name": "stride_dy",
        "annotation": null
      },
      {
        "name": "DX",
        "annotation": null
      },
      {
        "name": "stride_dx",
        "annotation": null
      },
      {
        "name": "Rstd",
        "annotation": null
      },
      {
        "name": "DW",
        "annotation": null
      },
      {
        "name": "eps",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "rows_per_program",
        "annotation": null
      },
      {
        "name": "block_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _rms_norm_bwd_kernel_sm(",
      "    X,",
      "    stride_x,",
      "    W,",
      "    DY,",
      "    stride_dy,",
      "    DX,",
      "    stride_dx,",
      "    Rstd,",
      "    DW,",
      "    eps,",
      "    M,",
      "    N,",
      "    rows_per_program,",
      "    block_N: tl.constexpr,",
      "):",
      "    row_block_id = tl.program_id(0)",
      "    row_start = row_block_id * rows_per_program",
      "    cols = tl.arange(0, block_N)",
      "    mask = cols < N",
      "",
      "    w = tl.load(W + cols, mask=mask, other=0.0).to(tl.float32)",
      "",
      "    dw = tl.zeros((block_N,), dtype=tl.float32)",
      "",
      "    row_end = min(row_start + rows_per_program, M)",
      "    for row in range(row_start, row_end):",
      "",
      "        x = tl.load(X + row * stride_x + cols, mask=mask, other=0.0).to(tl.float32)",
      "        dy = tl.load(DY + row * stride_dy + cols, mask=mask, other=0.0).to(tl.float32)",
      "        rstd = tl.load(Rstd + row)",
      "",
      "        x_hat = x * rstd",
      "        wdy = w * dy",
      "        dw += dy * x_hat",
      "        c1 = tl.sum(x_hat * wdy, axis=0) / N",
      "        dx = (wdy - x_hat * c1) * rstd",
      "",
      "        tl.store(DX + row * stride_dx + cols, dx, mask=mask)",
      "",
      "    tl.store(DW + row_block_id * N + cols, dw, mask=mask)"
    ],
    "file": "triton_repos/lessw2020_triton_kernels_for_fun_and_profit/workspace/409.py"
  },
  {
    "name": "_attn_bwd",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M1': lambda args: args['QSeq'] % args['BLOCK_M1'] == 0, 'EVEN_N1': lambda args: args['KSeq'] % args['BLOCK_N1'] == 0, 'EVEN_M2': lambda args: args['QSeq'] % args['BLOCK_M2'] == 0, 'EVEN_N2': lambda args: args['KSeq'] % args['BLOCK_N2'] == 0, 'HEADS_PADDED': lambda args: args['headdim'] != args['BLOCK_HEADDIM'], 'NUM_BLOCKS_KV': lambda args: math.ceil(args['KSeq'] / args['BLOCK_N1'])})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "Do",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "D",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "dropout_prob",
        "annotation": null
      },
      {
        "name": "dropout_seed",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "stride_doz",
        "annotation": null
      },
      {
        "name": "stride_dom",
        "annotation": null
      },
      {
        "name": "stride_doh",
        "annotation": null
      },
      {
        "name": "stride_dqz",
        "annotation": null
      },
      {
        "name": "stride_dqm",
        "annotation": null
      },
      {
        "name": "stride_dqh",
        "annotation": null
      },
      {
        "name": "stride_dkz",
        "annotation": null
      },
      {
        "name": "stride_dkn",
        "annotation": null
      },
      {
        "name": "stride_dkh",
        "annotation": null
      },
      {
        "name": "stride_dvz",
        "annotation": null
      },
      {
        "name": "stride_dvn",
        "annotation": null
      },
      {
        "name": "stride_dvh",
        "annotation": null
      },
      {
        "name": "nheads_q",
        "annotation": null
      },
      {
        "name": "num_repeats",
        "annotation": null
      },
      {
        "name": "QSeq",
        "annotation": null
      },
      {
        "name": "cum_seqlens_q",
        "annotation": null
      },
      {
        "name": "KSeq",
        "annotation": null
      },
      {
        "name": "cum_seqlens_k",
        "annotation": null
      },
      {
        "name": "seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CQSeq",
        "annotation": null
      },
      {
        "name": "CKSeq",
        "annotation": null
      },
      {
        "name": "DRuntime",
        "annotation": null
      },
      {
        "name": "Dq",
        "annotation": null
      },
      {
        "name": "Dk",
        "annotation": null
      },
      {
        "name": "Dv",
        "annotation": null
      },
      {
        "name": "VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_ON",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "NUM_BLOCKS_KV",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEADS_PADDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N2",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_bwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    B,",
      "    Do,",
      "    M,",
      "    D,",
      "    softmax_scale,",
      "    dropout_prob,",
      "    dropout_seed,",
      "    stride_qz,",
      "    stride_qm,",
      "    stride_qh,",
      "    stride_kz,",
      "    stride_kn,",
      "    stride_kh,",
      "    stride_vz,",
      "    stride_vn,",
      "    stride_vh,",
      "    stride_bz,",
      "    stride_bm,",
      "    stride_bh,",
      "    stride_doz,",
      "    stride_dom,",
      "    stride_doh,",
      "    stride_dqz,",
      "    stride_dqm,",
      "    stride_dqh,",
      "    stride_dkz,",
      "    stride_dkn,",
      "    stride_dkh,",
      "    stride_dvz,",
      "    stride_dvn,",
      "    stride_dvh,",
      "    nheads_q,",
      "    num_repeats,",
      "    QSeq,",
      "    cum_seqlens_q,",
      "    KSeq,",
      "    cum_seqlens_k,",
      "    seqlen_q_rounded,",
      "    headdim,",
      "    CQSeq,",
      "    CKSeq,",
      "    DRuntime,",
      "    Dq,",
      "    Dk,",
      "    Dv,",
      "    VARLEN: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BIAS_ON: tl.constexpr,",
      "    USE_DROPOUT: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    EVEN_M1: tl.constexpr,",
      "    EVEN_N1: tl.constexpr,",
      "    EVEN_M2: tl.constexpr,",
      "    EVEN_N2: tl.constexpr,",
      "    NUM_BLOCKS_KV: tl.constexpr,",
      "    HEADS_PADDED: tl.constexpr,",
      "    BLOCK_M1: tl.constexpr,",
      "    BLOCK_N1: tl.constexpr,",
      "    BLOCK_M2: tl.constexpr,",
      "    BLOCK_N2: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    off_zh = tl.program_id(1)",
      "    off_z = off_zh // nheads_q",
      "    off_head_q = off_zh % nheads_q",
      "    off_head_kv = off_head_q // num_repeats",
      "",
      "    if VARLEN:",
      "        cu_seq_start_q = tl.load(cum_seqlens_q + off_z)",
      "        cu_seq_start_k = tl.load(cum_seqlens_k + off_z)",
      "        actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q",
      "        actual_seqlen_k = tl.load(cum_seqlens_k + off_z + 1) - cu_seq_start_k",
      "        off_z = 0",
      "    else:",
      "        cu_seq_start_q = 0",
      "        cu_seq_start_k = 0",
      "        actual_seqlen_q = QSeq",
      "        actual_seqlen_k = KSeq",
      "",
      "    Q += off_z * stride_qz + off_head_q * stride_qh + cu_seq_start_q * stride_qm",
      "    K += off_z * stride_kz + off_head_kv * stride_kh + cu_seq_start_k * stride_kn",
      "    V += off_z * stride_vz + off_head_kv * stride_vh + cu_seq_start_k * stride_vn",
      "",
      "    Do += off_z * stride_doz + off_head_q * stride_doh + cu_seq_start_q * stride_dom",
      "    Dq += off_z * stride_dqz + off_head_q * stride_dqh + cu_seq_start_q * stride_dqm",
      "    Dk += off_z * stride_dkz + off_head_q * stride_dkh + cu_seq_start_k * stride_dkn",
      "    Dv += off_z * stride_dvz + off_head_q * stride_dvh + cu_seq_start_k * stride_dvn",
      "",
      "    if BIAS_ON:",
      "        B += off_z * stride_bz + off_head_q * stride_bh + cu_seq_start_q * stride_bm",
      "    if USE_DROPOUT:",
      "        Dropout = actual_seqlen_k * (",
      "            cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z)",
      "        )",
      "    else:",
      "        Dropout = None",
      "",
      "    D += off_zh * seqlen_q_rounded",
      "    M += off_zh * seqlen_q_rounded",
      "",
      "    if pid < NUM_BLOCKS_KV:",
      "        i_start_n = pid",
      "        pad_cols = (not EVEN_N1) or (",
      "            VARLEN and ((i_start_n + 1) * BLOCK_N1 > actual_seqlen_k)",
      "        )",
      "        _attn_bwd_block_dkdv(",
      "            i_start_n * BLOCK_N1,",
      "            Q,",
      "            K,",
      "            V,",
      "            B,",
      "            Dropout,",
      "            Do,",
      "            Dk,",
      "            Dv,",
      "            M,",
      "            D,",
      "            softmax_scale,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dkn,",
      "            stride_dvn,",
      "            actual_seqlen_q,",
      "            actual_seqlen_k,",
      "            headdim,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BIAS_ON=BIAS_ON,",
      "            USE_DROPOUT=USE_DROPOUT,",
      "            PAD_COLS=pad_cols,",
      "            HEADS_PADDED=HEADS_PADDED,",
      "            BLOCK_M=BLOCK_M1,",
      "            BLOCK_N=BLOCK_N1,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "        )",
      "",
      "    else:",
      "        i_start_m = pid - NUM_BLOCKS_KV",
      "        pad_rows = (not EVEN_M2) or (",
      "            VARLEN and ((i_start_m + 1) * BLOCK_M2 > actual_seqlen_q)",
      "        )",
      "        _attn_bwd_block_dq(",
      "            i_start_m * BLOCK_M2,",
      "            Q,",
      "            K,",
      "            V,",
      "            B,",
      "            Dropout,",
      "            Do,",
      "            Dq,",
      "            M,",
      "            D,",
      "            softmax_scale,",
      "            dropout_prob,",
      "            dropout_seed,",
      "            stride_qm,",
      "            stride_kn,",
      "            stride_vn,",
      "            stride_bm,",
      "            stride_dom,",
      "            stride_dqm,",
      "            actual_seqlen_q,",
      "            actual_seqlen_k,",
      "            headdim,",
      "            VARLEN=VARLEN,",
      "            IS_CAUSAL=IS_CAUSAL,",
      "            BIAS_ON=BIAS_ON,",
      "            USE_DROPOUT=USE_DROPOUT,",
      "            PAD_ROWS=pad_rows,",
      "            HEADS_PADDED=HEADS_PADDED,",
      "            BLOCK_M=BLOCK_M2,",
      "            BLOCK_N=BLOCK_N2,",
      "            BLOCK_HEADDIM=BLOCK_HEADDIM,",
      "            EVEN_N=EVEN_N2,",
      "        )"
    ],
    "file": "triton_repos/erfanzar_jax-flash-attn2/jax_flash_attn2/flash_attention_triton/198.py"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'EVEN_M': lambda args: args['QSeq'] % args['BLOCK_M'] == 0, 'EVEN_N': lambda args: args['KSeq'] % args['BLOCK_N'] == 0})"
    ],
    "args": [
      {
        "name": "q",
        "annotation": null
      },
      {
        "name": "k",
        "annotation": null
      },
      {
        "name": "v",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "softmax_scale",
        "annotation": null
      },
      {
        "name": "dropout_prob",
        "annotation": null
      },
      {
        "name": "dropout_seed",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_bz",
        "annotation": null
      },
      {
        "name": "stride_bm",
        "annotation": null
      },
      {
        "name": "stride_bh",
        "annotation": null
      },
      {
        "name": "nheads_q",
        "annotation": null
      },
      {
        "name": "num_repeats",
        "annotation": null
      },
      {
        "name": "QSeq",
        "annotation": null
      },
      {
        "name": "cum_seqlens_q",
        "annotation": null
      },
      {
        "name": "KSeq",
        "annotation": null
      },
      {
        "name": "max_seqlen_q_rounded",
        "annotation": null
      },
      {
        "name": "headdim",
        "annotation": null
      },
      {
        "name": "CQSeq",
        "annotation": null
      },
      {
        "name": "CKSeq",
        "annotation": null
      },
      {
        "name": "DRuntime",
        "annotation": null
      },
      {
        "name": "Po",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_ON",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_HEADDIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PADDED_HEADS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    q,",
      "    k,",
      "    v,",
      "    B,",
      "    softmax_scale,",
      "    dropout_prob,",
      "    dropout_seed,",
      "    stride_qz,",
      "    stride_qm,",
      "    stride_qh,",
      "    stride_kz,",
      "    stride_kn,",
      "    stride_kh,",
      "    stride_vz,",
      "    stride_vn,",
      "    stride_vh,",
      "    stride_oz,",
      "    stride_om,",
      "    stride_oh,",
      "    stride_bz,",
      "    stride_bm,",
      "    stride_bh,",
      "    nheads_q,",
      "    num_repeats,",
      "    QSeq,",
      "    cum_seqlens_q,",
      "    KSeq,",
      "    max_seqlen_q_rounded,",
      "    headdim,",
      "    CQSeq,",
      "    CKSeq,",
      "    DRuntime,",
      "    Po,",
      "    M,",
      "    VARLEN: tl.constexpr,",
      "    USE_DROPOUT: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BIAS_ON: tl.constexpr,",
      "    BLOCK_HEADDIM: tl.constexpr,",
      "    PADDED_HEADS: tl.constexpr,",
      "    EVEN_M: tl.constexpr,",
      "    EVEN_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    LN2: tl.constexpr = 1.44269504089",
      "    i_start_m = tl.program_id(0)",
      "    off_zh = tl.program_id(1)",
      "    off_head_q = off_zh % nheads_q",
      "    off_head_kv = off_head_q // num_repeats",
      "    off_z = off_zh // nheads_q",
      "",
      "    if VARLEN:",
      "        cu_seq_start_q = tl.load(cum_seqlens_q + off_z)",
      "        actual_seqlen_q = tl.load(cum_seqlens_q + off_z + 1) - cu_seq_start_q",
      "        if i_start_m * BLOCK_M >= actual_seqlen_q:",
      "            return",
      "        actual_seqlen_k = actual_seqlen_q",
      "        cu_seq_start_k = cu_seq_start_q",
      "        off_z = 0",
      "    else:",
      "        actual_seqlen_q = QSeq",
      "        actual_seqlen_k = KSeq",
      "        cu_seq_start_q = 0",
      "        cu_seq_start_k = 0",
      "",
      "    softmax_scale = softmax_scale * LN2",
      "",
      "    offs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    offs_n = tl.arange(0, BLOCK_N)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "",
      "    fully_masked_lines = actual_seqlen_q - actual_seqlen_k if IS_CAUSAL else 0",
      "    if fully_masked_lines >= (i_start_m + 1) * BLOCK_M:",
      "        return",
      "",
      "    q_ptrs = (",
      "        q",
      "        + off_z * stride_qz",
      "        + off_head_q * stride_qh",
      "        + cu_seq_start_q * stride_qm",
      "        + (offs_m[:, None] * stride_qm + offs_d[None, :])",
      "    )",
      "",
      "    k_ptrs = (",
      "        k",
      "        + off_z * stride_kz",
      "        + off_head_kv * stride_kh",
      "        + cu_seq_start_k * stride_kn",
      "        + (offs_n[:, None] * stride_kn + offs_d[None, :])",
      "    )",
      "",
      "    v_ptrs = (",
      "        v",
      "        + off_z * stride_vz",
      "        + off_head_kv * stride_vh",
      "        + cu_seq_start_k * stride_vn",
      "        + (offs_n[:, None] * stride_vn + offs_d[None, :])",
      "    )",
      "",
      "    if BIAS_ON:",
      "        bias_ptrs = (",
      "            B",
      "            + off_z * stride_bz",
      "            + off_head_kv * stride_bh",
      "            + cu_seq_start_q * stride_bm",
      "            + (offs_m[:, None] * stride_bm + offs_n[None, :])",
      "        )",
      "    else:",
      "        bias_ptrs = None",
      "    if USE_DROPOUT:",
      "        dropout_off = actual_seqlen_k * (",
      "            cu_seq_start_q + actual_seqlen_q * (off_head_q + nheads_q * off_z)",
      "        )",
      "        dropout_offs = dropout_off + offs_m[:, None] * actual_seqlen_k + offs_n[None, :]",
      "    else:",
      "        dropout_offs = None",
      "",
      "    me_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    acc_o = tl.zeros([BLOCK_M, BLOCK_HEADDIM], dtype=tl.float32)",
      "",
      "    pad_rows = (not EVEN_M) or (VARLEN and (i_start_m * BLOCK_M > actual_seqlen_q))",
      "    q = padded_load(",
      "        q_ptrs,",
      "        offs_m,",
      "        offs_d,",
      "        PA0=pad_rows,",
      "        PA1=PADDED_HEADS,",
      "        LA0=actual_seqlen_q,",
      "        LA1=headdim,",
      "    )",
      "    if IS_CAUSAL:",
      "        end_n = min(",
      "            actual_seqlen_k - actual_seqlen_q + (i_start_m + 1) * BLOCK_M,",
      "            actual_seqlen_k,",
      "        )",
      "        if end_n < 0:",
      "            return",
      "    else:",
      "        end_n = actual_seqlen_k",
      "",
      "    uneven_n = actual_seqlen_k % BLOCK_N != 0",
      "    attention_padding = VARLEN & uneven_n",
      "    if IS_CAUSAL:",
      "        first_masked_col = i_start_m * BLOCK_M + 1 + actual_seqlen_k - actual_seqlen_q",
      "    elif attention_padding:",
      "        first_masked_col = actual_seqlen_k",
      "    else:",
      "        first_masked_col = end_n",
      "    nb_full_blocks = first_masked_col // BLOCK_N",
      "",
      "    next_start_n = 0",
      "    if nb_full_blocks > 0:",
      "        for _ in range(0, nb_full_blocks):",
      "            m_i, me_i, acc_o = _attn_fwd_inner(",
      "                q,",
      "                m_i,",
      "                me_i,",
      "                k_ptrs,",
      "                v_ptrs,",
      "                bias_ptrs,",
      "                acc_o,",
      "                offs_m,",
      "                offs_n,",
      "                offs_d,",
      "                softmax_scale,",
      "                dropout_prob,",
      "                dropout_seed,",
      "                dropout_offs,",
      "                stride_kn,",
      "                stride_vn,",
      "                next_start_n,",
      "                actual_seqlen_q,",
      "                actual_seqlen_k,",
      "                headdim,",
      "                USE_DROPOUT=USE_DROPOUT,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BIAS_ON=BIAS_ON,",
      "                MASKED=False,",
      "                PADDED_COLS=False,",
      "                PADDED_HEADS=PADDED_HEADS,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "            next_start_n += BLOCK_N",
      "    if next_start_n < end_n:",
      "        for index_start_n in range(next_start_n, end_n, BLOCK_N):",
      "            pad_cols = (not EVEN_N) or VARLEN",
      "            m_i, me_i, acc_o = _attn_fwd_inner(",
      "                q,",
      "                m_i,",
      "                me_i,",
      "                k_ptrs,",
      "                v_ptrs,",
      "                bias_ptrs,",
      "                acc_o,",
      "                offs_m,",
      "                offs_n,",
      "                offs_d,",
      "                softmax_scale,",
      "                dropout_prob,",
      "                dropout_seed,",
      "                dropout_offs,",
      "                stride_kn,",
      "                stride_vn,",
      "                index_start_n,",
      "                actual_seqlen_q,",
      "                actual_seqlen_k,",
      "                headdim,",
      "                USE_DROPOUT=USE_DROPOUT,",
      "                IS_CAUSAL=IS_CAUSAL,",
      "                BIAS_ON=BIAS_ON,",
      "                MASKED=True,",
      "                PADDED_COLS=pad_cols,",
      "                PADDED_HEADS=PADDED_HEADS,",
      "                BLOCK_M=BLOCK_M,",
      "                BLOCK_N=BLOCK_N,",
      "            )",
      "",
      "    if USE_DROPOUT:",
      "        o_scale = tl.exp2((m_i - me_i) - tl.log2(1 - dropout_prob))",
      "    else:",
      "        o_scale = tl.exp2(m_i - me_i)",
      "    acc_o = acc_o * o_scale[:, None]",
      "    if fully_masked_lines > i_start_m * BLOCK_M:",
      "        acc_o = tl.where(offs_m[:, None] < fully_masked_lines, 0, acc_o)",
      "    i_start_m = tl.program_id(0)",
      "    offs_m = i_start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    lse_ptrs = M + off_zh * max_seqlen_q_rounded + offs_m",
      "    tl.store(lse_ptrs, me_i)",
      "    offs_d = tl.arange(0, BLOCK_HEADDIM)",
      "    out_ptrs = (",
      "        Po",
      "        + off_z * stride_oz",
      "        + off_head_q * stride_oh",
      "        + cu_seq_start_q * stride_om",
      "        + (offs_m[:, None] * stride_om + offs_d[None, :])",
      "    )",
      "",
      "    tl.store(",
      "        out_ptrs,",
      "        acc_o,",
      "        mask=(offs_m[:, None] < actual_seqlen_q) & (offs_d[None, :] < headdim),",
      "    )"
    ],
    "file": "triton_repos/erfanzar_jax-flash-attn2/jax_flash_attn2/flash_attention_triton/199.py"
  },
  {
    "name": "_attn_fwd",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(list(filter(keep, configs)), key=['N_CTX', 'HEAD_DIM'])"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "HEAD_DIM",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    M,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    HEAD_DIM: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    tl.static_assert(BLOCK_N <= HEAD_DIM)",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "    v_order: tl.constexpr = (1, 0)",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, HEAD_DIM),",
      "        order=v_order,",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(HEAD_DIM, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(HEAD_DIM, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + qvk_offset,",
      "        shape=(N_CTX, HEAD_DIM),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, HEAD_DIM),",
      "        order=(1, 0),",
      "    )",
      "",
      "    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32) + 1.0",
      "    acc = tl.zeros([BLOCK_M, HEAD_DIM], dtype=tl.float32)",
      "",
      "    qk_scale = sm_scale",
      "    qk_scale *= 1.44269504",
      "",
      "    q = tl.load(Q_block_ptr)",
      "    acc, l_i, m_i = _attn_fwd_inner(",
      "        acc,",
      "        l_i,",
      "        m_i,",
      "        q,",
      "        K_block_ptr,",
      "        V_block_ptr,",
      "        qk_scale,",
      "        BLOCK_N,",
      "        N_CTX,",
      "    )",
      "",
      "    m_i += tl.math.log2(l_i)",
      "    acc = acc / l_i[:, None]",
      "    m_ptrs = M + off_hz * N_CTX + offs_m",
      "    tl.store(m_ptrs, m_i)",
      "    tl.store(O_block_ptr, acc.to(Out.type.element_ty))"
    ],
    "file": "triton_repos/ai-compiler-study_triton-kernels/triton_kernels/kernels/148.py"
  },
  {
    "name": "_linear_fwd",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=_linear_launch_metadata)"
    ],
    "args": [
      {
        "name": "a_ptr",
        "annotation": null
      },
      {
        "name": "b_ptr",
        "annotation": null
      },
      {
        "name": "c_ptr",
        "annotation": null
      },
      {
        "name": "bias_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_SIZE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _linear_fwd(",
      "    a_ptr,",
      "    b_ptr,",
      "    c_ptr,",
      "    bias_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    HAS_BIAS: tl.constexpr,",
      "    BLOCK_SIZE_M: tl.constexpr,",
      "    BLOCK_SIZE_N: tl.constexpr,",
      "    BLOCK_SIZE_K: tl.constexpr,",
      "    GROUP_SIZE_M: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "    pid = tl.program_id(axis=0)",
      "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)",
      "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)",
      "    num_pid_in_group = GROUP_SIZE_M * num_pid_n",
      "    group_id = pid // num_pid_in_group",
      "    first_pid_m = group_id * GROUP_SIZE_M",
      "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)",
      "    pid_m = first_pid_m + (pid % group_size_m)",
      "    pid_n = (pid % num_pid_in_group) // group_size_m",
      "",
      "    start_m = pid_m * BLOCK_SIZE_M",
      "    start_n = pid_n * BLOCK_SIZE_N",
      "",
      "    offs_am = start_m + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_bn = start_n + tl.arange(0, BLOCK_SIZE_N)",
      "    offs_am = tl.where(offs_am < M, offs_am, 0)",
      "    offs_bn = tl.where(offs_bn < N, offs_bn, 0)",
      "",
      "    offs_am = tl.max_contiguous(tl.multiple_of(offs_am, BLOCK_SIZE_M), BLOCK_SIZE_M)",
      "    offs_bn = tl.max_contiguous(tl.multiple_of(offs_bn, BLOCK_SIZE_N), BLOCK_SIZE_N)",
      "    offs_k = tl.arange(0, BLOCK_SIZE_K)",
      "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)",
      "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)",
      "",
      "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):",
      "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)",
      "        accumulator = tl.dot(a, b, accumulator)",
      "        a_ptrs += BLOCK_SIZE_K * stride_ak",
      "        b_ptrs += BLOCK_SIZE_K * stride_bk",
      "",
      "    if HAS_BIAS:",
      "        bias_ptrs = bias_ptr + offs_bn[None, :]",
      "        bias = tl.load(bias_ptrs).to(tl.float32)",
      "        bias = tl.broadcast_to(bias, [BLOCK_SIZE_M, BLOCK_SIZE_N])",
      "        accumulator += bias",
      "",
      "    c = accumulator.to(tl.bfloat16)",
      "",
      "    if ACTIVATION == \"GELU\":",
      "        c = gelu(c)",
      "",
      "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)",
      "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)",
      "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]",
      "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)",
      "    tl.store(c_ptrs, c, mask=c_mask)"
    ],
    "file": "triton_repos/ai-compiler-study_triton-kernels/triton_kernels/kernels/149.py"
  },
  {
    "name": "add_kernel",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'boundary_check': lambda args: args['x_size'] % args['block_size']})"
    ],
    "args": [
      {
        "name": "x_ptr",
        "annotation": null
      },
      {
        "name": "y_ptr",
        "annotation": null
      },
      {
        "name": "z_ptr",
        "annotation": null
      },
      {
        "name": "size",
        "annotation": null
      },
      {
        "name": "block_size",
        "annotation": "tl.constexpr"
      },
      {
        "name": "boundary_check",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def add_kernel(",
      "    x_ptr, y_ptr, z_ptr, size, block_size: tl.constexpr, boundary_check: tl.constexpr",
      "):",
      "    offset = tl.program_id(0) * block_size",
      "",
      "    x_block_ptr = tl.make_block_ptr(",
      "        x_ptr,",
      "        shape=(size,),",
      "        strides=(1,),",
      "        offsets=(offset,),",
      "        block_shape=(block_size,),",
      "        order=(0,),",
      "    )",
      "    y_block_ptr = tl.make_block_ptr(",
      "        y_ptr,",
      "        shape=(size,),",
      "        strides=(1,),",
      "        offsets=(offset,),",
      "        block_shape=(block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if boundary_check:",
      "        x = tl.load(x_block_ptr, boundary_check=(0,))",
      "        y = tl.load(y_block_ptr, boundary_check=(0,))",
      "    else:",
      "        x = tl.load(x_block_ptr)",
      "        y = tl.load(y_block_ptr)",
      "",
      "    z = x + y",
      "",
      "    z_block_ptr = tl.make_block_ptr(",
      "        z_ptr,",
      "        shape=(size,),",
      "        strides=(1,),",
      "        offsets=(offset,),",
      "        block_shape=(block_size,),",
      "        order=(0,),",
      "    )",
      "",
      "    if boundary_check:",
      "        tl.store(z_block_ptr, z, boundary_check=(0,))",
      "    else:",
      "        tl.store(z_block_ptr, z)"
    ],
    "file": "triton_repos/daemyung_practice-triton/184.py"
  },
  {
    "name": "_fwd_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=_get_configs(), key=['N_CTX', 'H', 'Z'])",
      "@triton.heuristics({'EVEN_CTX': lambda args: args['N_CTX'] % args['BLOCK_M'] == 0})"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "sm_scale",
        "annotation": null
      },
      {
        "name": "qkv_scale_ptr",
        "annotation": null
      },
      {
        "name": "out_scale_ptr",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": null
      },
      {
        "name": "stride_qh",
        "annotation": null
      },
      {
        "name": "stride_qm",
        "annotation": null
      },
      {
        "name": "stride_qk",
        "annotation": null
      },
      {
        "name": "stride_kz",
        "annotation": null
      },
      {
        "name": "stride_kh",
        "annotation": null
      },
      {
        "name": "stride_kn",
        "annotation": null
      },
      {
        "name": "stride_kk",
        "annotation": null
      },
      {
        "name": "stride_vz",
        "annotation": null
      },
      {
        "name": "stride_vh",
        "annotation": null
      },
      {
        "name": "stride_vk",
        "annotation": null
      },
      {
        "name": "stride_vn",
        "annotation": null
      },
      {
        "name": "stride_oz",
        "annotation": null
      },
      {
        "name": "stride_oh",
        "annotation": null
      },
      {
        "name": "stride_om",
        "annotation": null
      },
      {
        "name": "stride_on",
        "annotation": null
      },
      {
        "name": "Z",
        "annotation": null
      },
      {
        "name": "H",
        "annotation": null
      },
      {
        "name": "N_CTX",
        "annotation": null
      },
      {
        "name": "EVEN_CTX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _fwd_kernel(",
      "    Q,",
      "    K,",
      "    V,",
      "    sm_scale,",
      "    qkv_scale_ptr,",
      "    out_scale_ptr,",
      "    Out,",
      "    stride_qz,",
      "    stride_qh,",
      "    stride_qm,",
      "    stride_qk,",
      "    stride_kz,",
      "    stride_kh,",
      "    stride_kn,",
      "    stride_kk,",
      "    stride_vz,",
      "    stride_vh,",
      "    stride_vk,",
      "    stride_vn,",
      "    stride_oz,",
      "    stride_oh,",
      "    stride_om,",
      "    stride_on,",
      "    Z,",
      "    H,",
      "    N_CTX,",
      "    EVEN_CTX: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    start_m = tl.program_id(0)",
      "    off_hz = tl.program_id(1)",
      "    off_z = off_hz // H",
      "    off_h = off_hz % H",
      "    qvk_offset = off_z.to(tl.int64) * stride_qz + off_h.to(tl.int64) * stride_qh",
      "",
      "    Q_block_ptr = tl.make_block_ptr(",
      "        base=Q + qvk_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_qm, stride_qk),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    K_block_ptr = tl.make_block_ptr(",
      "        base=K + qvk_offset,",
      "        shape=(BLOCK_DMODEL, N_CTX),",
      "        strides=(stride_kk, stride_kn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_DMODEL, BLOCK_N),",
      "        order=(0, 1),",
      "    )",
      "    V_block_ptr = tl.make_block_ptr(",
      "        base=V + qvk_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_vk, stride_vn),",
      "        offsets=(0, 0),",
      "        block_shape=(BLOCK_N, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "",
      "    m_i = tl.zeros([BLOCK_M], dtype=tl.float32) - float(\"inf\")",
      "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)",
      "    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "    qkv_scale = tl.load(qkv_scale_ptr)",
      "    qk_scale = qkv_scale * qkv_scale * sm_scale * 1.44269504",
      "",
      "    if EVEN_CTX:",
      "        q = tl.load(Q_block_ptr)",
      "    else:",
      "        q = tl.load(Q_block_ptr, boundary_check=(0,), padding_option=\"zero\")",
      "    for start_n in range(0, N_CTX, BLOCK_N):",
      "        start_n = tl.multiple_of(start_n, BLOCK_N)",
      "",
      "        if EVEN_CTX:",
      "            k = tl.load(K_block_ptr)",
      "        else:",
      "            k = tl.load(K_block_ptr, boundary_check=(1,), padding_option=\"zero\")",
      "",
      "        qk = tl.dot(q, k, allow_tf32=False, out_dtype=tl.int32)",
      "        qk_fp32 = qk * qk_scale",
      "",
      "        m_ij = tl.maximum(m_i, tl.max(qk_fp32, 1))",
      "        p = tl.math.exp2(qk_fp32 - m_ij[:, None])",
      "",
      "        alpha = tl.math.exp2(m_i - m_ij)",
      "        m_i = m_ij",
      "",
      "        if EVEN_CTX:",
      "            v = tl.load(V_block_ptr)",
      "        else:",
      "            v = tl.load(V_block_ptr, boundary_check=(0,), padding_option=\"zero\")",
      "        v = (v * qkv_scale).to(tl.bfloat16)",
      "        acc *= alpha[:, None]",
      "        acc += tl.dot(",
      "            p.to(tl.bfloat16),",
      "            v,",
      "            allow_tf32=True,",
      "        )",
      "        l_i = l_i * alpha + tl.sum(p, 1)",
      "",
      "        K_block_ptr = tl.advance(K_block_ptr, (0, BLOCK_N))",
      "        V_block_ptr = tl.advance(V_block_ptr, (BLOCK_N, 0))",
      "",
      "    out_scale = tl.load(out_scale_ptr)",
      "    acc = tl.math.llrint(acc / (l_i[:, None] * out_scale)).to(tl.int8)",
      "",
      "    O_block_ptr = tl.make_block_ptr(",
      "        base=Out + qvk_offset,",
      "        shape=(N_CTX, BLOCK_DMODEL),",
      "        strides=(stride_om, stride_on),",
      "        offsets=(start_m * BLOCK_M, 0),",
      "        block_shape=(BLOCK_M, BLOCK_DMODEL),",
      "        order=(1, 0),",
      "    )",
      "    if EVEN_CTX:",
      "        tl.store(O_block_ptr, acc)",
      "    else:",
      "        tl.store(O_block_ptr, acc, boundary_check=(0,))"
    ],
    "file": "triton_repos/IBM_qattn/qattn/nn/functional/50.py"
  },
  {
    "name": "_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=int8_configs(), key=['M', 'N', 'K'], prune_configs_by={'early_config_prune': early_config_prune, 'perf_model': _estimate_matmul_time, 'top_k': 10})",
      "@triton.heuristics({'EVEN_K': lambda args: args['K'] % args['BLOCK_K'] == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "a_scale_ptr",
        "annotation": null
      },
      {
        "name": "b_scale_ptr",
        "annotation": null
      },
      {
        "name": "out_scale_ptr",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_ADD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "A_PER_CHANNEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B_PER_CHANNEL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _kernel(",
      "    A,",
      "    B,",
      "    C,",
      "    bias,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    a_scale_ptr,",
      "    b_scale_ptr,",
      "    out_scale_ptr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    BIAS_ADD: tl.constexpr,",
      "    A_PER_CHANNEL: tl.constexpr,",
      "    B_PER_CHANNEL: tl.constexpr,",
      "):",
      "    pid = tl.program_id(0)",
      "    pid_z = tl.program_id(1)",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = pid // width",
      "    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (pid % group_size)",
      "    pid_n = (pid % width) // (group_size)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)",
      "    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)",
      "    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)",
      "",
      "    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)",
      "    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)",
      "    for k in range(0, tl.cdiv(K, BLOCK_K)):",
      "        if EVEN_K:",
      "            a = tl.load(A)",
      "            b = tl.load(B)",
      "        else:",
      "            k_remaining = K - k * BLOCK_K",
      "            _0 = tl.zeros((1, 1), dtype=tl.int8)",
      "            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)",
      "            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)",
      "        acc += tl.dot(a, b, allow_tf32=True, out_dtype=tl.int32)",
      "        A += BLOCK_K * stride_ak",
      "        B += BLOCK_K * stride_bk",
      "    if A_PER_CHANNEL:",
      "        _0 = tl.zeros((1,), dtype=a_scale_ptr.dtype.element_ty)",
      "        mask = ram < M",
      "        a_scale = tl.load(a_scale_ptr + ram, mask=mask, other=_0)",
      "    else:",
      "        a_scale = tl.load(a_scale_ptr)",
      "    if B_PER_CHANNEL:",
      "        _0 = tl.zeros((1,), dtype=b_scale_ptr.dtype.element_ty)",
      "        mask = rbn < N",
      "        b_scale = tl.load(b_scale_ptr + rbn, mask=mask, other=_0)",
      "    else:",
      "        b_scale = tl.load(b_scale_ptr)",
      "    if BIAS_ADD:",
      "        bias = tl.load(bias + rn)",
      "        if A_PER_CHANNEL and B_PER_CHANNEL:",
      "            bias = tl.math.llrint(bias / (a_scale[:, None] * b_scale[None, :])).to(",
      "                tl.int32",
      "            )",
      "            acc = acc + bias",
      "        else:",
      "            bias = tl.math.llrint(bias / (a_scale * b_scale)).to(tl.int32)",
      "            acc = acc + bias[None, :]",
      "",
      "    if A_PER_CHANNEL and B_PER_CHANNEL:",
      "        mask = ram < M",
      "        _0 = tl.zeros((1,), dtype=out_scale_ptr.dtype.element_ty)",
      "        out_scale = tl.load(out_scale_ptr + ram, mask=mask, other=_0)",
      "        acc = tl.math.llrint(",
      "            (",
      "                acc.to(tl.float32)",
      "                * a_scale[:, None]",
      "                * b_scale[None, :]",
      "                * out_scale[:, None]",
      "            )",
      "        ).to(tl.int8)",
      "    else:",
      "        out_scale = tl.load(out_scale_ptr)",
      "        acc = tl.math.llrint((acc.to(tl.float32) * (a_scale * b_scale * out_scale))).to(",
      "            tl.int8",
      "        )",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "    mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "    tl.store(C, acc, mask=mask)"
    ],
    "file": "triton_repos/IBM_qattn/qattn/nn/functional/51.py"
  },
  {
    "name": "_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=int8_dynamic_configs(), key=['M', 'N', 'K'], prune_configs_by={'early_config_prune': early_config_prune, 'perf_model': estimate_matmul_time, 'top_k': 10})",
      "@triton.heuristics({'EVEN_K': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0})"
    ],
    "args": [
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "stride_am",
        "annotation": null
      },
      {
        "name": "stride_ak",
        "annotation": null
      },
      {
        "name": "stride_bk",
        "annotation": null
      },
      {
        "name": "stride_bn",
        "annotation": null
      },
      {
        "name": "stride_cm",
        "annotation": null
      },
      {
        "name": "stride_cn",
        "annotation": null
      },
      {
        "name": "a_scale_ptr",
        "annotation": null
      },
      {
        "name": "b_scale_ptr",
        "annotation": null
      },
      {
        "name": "out_dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "EVEN_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BIAS_ADD",
        "annotation": "tl.constexpr"
      },
      {
        "name": "A_PER_CHANNEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B_PER_CHANNEL",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _kernel(",
      "    A,",
      "    B,",
      "    C,",
      "    bias,",
      "    M,",
      "    N,",
      "    K,",
      "    stride_am,",
      "    stride_ak,",
      "    stride_bk,",
      "    stride_bn,",
      "    stride_cm,",
      "    stride_cn,",
      "    a_scale_ptr,",
      "    b_scale_ptr,",
      "    out_dtype: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "    EVEN_K: tl.constexpr,",
      "    BIAS_ADD: tl.constexpr,",
      "    A_PER_CHANNEL: tl.constexpr,",
      "    B_PER_CHANNEL: tl.constexpr,",
      "):",
      "",
      "    pid = tl.program_id(0)",
      "    pid_z = tl.program_id(1)",
      "    grid_m = tl.cdiv(M, BLOCK_M)",
      "    grid_n = tl.cdiv(N, BLOCK_N)",
      "",
      "    width = GROUP_M * grid_n",
      "    group_id = pid // width",
      "    group_size = min(grid_m - group_id * GROUP_M, GROUP_M)",
      "    pid_m = group_id * GROUP_M + (pid % group_size)",
      "    pid_n = (pid % width) // (group_size)",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    ram = tl.max_contiguous(tl.multiple_of(rm % M, BLOCK_M), BLOCK_M)",
      "    rbn = tl.max_contiguous(tl.multiple_of(rn % N, BLOCK_N), BLOCK_N)",
      "    rk = pid_z * BLOCK_K + tl.arange(0, BLOCK_K)",
      "",
      "    A = A + (ram[:, None] * stride_am + rk[None, :] * stride_ak)",
      "    B = B + (rk[:, None] * stride_bk + rbn[None, :] * stride_bn)",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.int32)",
      "    if A_PER_CHANNEL:",
      "        a_scale = tl.load(a_scale_ptr + ram)",
      "    else:",
      "        a_scale = tl.load(a_scale_ptr)",
      "",
      "    for k in range(0, tl.cdiv(K, BLOCK_K * SPLIT_K)):",
      "        if EVEN_K:",
      "            a = tl.load(A)",
      "            b = tl.load(B)",
      "        else:",
      "            k_remaining = K - k * BLOCK_K",
      "            _0 = tl.zeros((1, 1), dtype=tl.int8)",
      "            a = tl.load(A, mask=rk[None, :] < k_remaining, other=_0)",
      "            b = tl.load(B, mask=rk[:, None] < k_remaining, other=_0)",
      "        if A_PER_CHANNEL:",
      "            a = tl.math.llrint((a / a_scale[:, None])).to(tl.int8)",
      "        else:",
      "            a = tl.math.llrint((a / a_scale)).to(tl.int8)",
      "        acc += tl.dot(a, b, allow_tf32=True, out_dtype=tl.int32)",
      "        A += BLOCK_K * SPLIT_K * stride_ak",
      "        B += BLOCK_K * SPLIT_K * stride_bk",
      "    if B_PER_CHANNEL:",
      "        b_scale = tl.load(b_scale_ptr + rbn)",
      "    else:",
      "        b_scale = tl.load(b_scale_ptr)",
      "    if A_PER_CHANNEL and B_PER_CHANNEL:",
      "        acc = (acc.to(tl.float32) * (a_scale[:, None] * b_scale[None, :])).to(out_dtype)",
      "    else:",
      "        acc = (acc.to(tl.float32) * (a_scale * b_scale)).to(out_dtype)",
      "    if BIAS_ADD:",
      "        bias = tl.load(bias + rn)",
      "        acc = acc + bias[None, :]",
      "",
      "    rm = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    rn = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)",
      "    C = C + (rm[:, None] * stride_cm + rn[None, :] * stride_cn)",
      "    mask = (rm < M)[:, None] & (rn < N)[None, :]",
      "",
      "    if SPLIT_K == 1:",
      "        tl.store(C, acc, mask=mask)",
      "    else:",
      "        tl.atomic_add(C, acc, mask=mask)"
    ],
    "file": "triton_repos/IBM_qattn/qattn/nn/functional/dynamic/53.py"
  },
  {
    "name": "_safe_softmax_forward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 1}, num_stages=4), triton.Config({'BLOCK_M': 1}, num_stages=5), triton.Config({'BLOCK_M': 2}, num_stages=4), triton.Config({'BLOCK_M': 2}, num_stages=5), triton.Config({'BLOCK_M': 4}, num_stages=4), triton.Config({'BLOCK_M': 4}, num_stages=5), triton.Config({'BLOCK_M': 8}, num_stages=4), triton.Config({'BLOCK_M': 8}, num_stages=5)], key=['M', 'N'])",
      "@triton.heuristics(values={'BLOCK_N': lambda args: triton.next_power_of_2(args['N']), 'num_warps': lambda args: 4 if args['N'] <= 1024 else 8 if args['N'] <= 2048 else 16})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _safe_softmax_forward_kernel(",
      "    output_ptr, input_ptr, M, N, K, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offset = tl.arange(0, BLOCK_N)",
      "    offset = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "    mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "    input_ptrs = input_ptr + offset",
      "    inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))",
      "    row_minus_max = inp - tl.max(inp, axis=1)[:, None]",
      "    numerator = tl.exp(row_minus_max)",
      "    denominator = tl.sum(numerator, axis=1)[:, None]",
      "    softmax_output = numerator / denominator",
      "    output_ptrs = output_ptr + offset",
      "    tl.store(output_ptrs, softmax_output, mask=mask)"
    ],
    "file": "triton_repos/phonism_genesis/genesis/nn/triton_ops/431.py"
  },
  {
    "name": "_safe_softmax_backward_kernel",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 1}, num_stages=4), triton.Config({'BLOCK_M': 1}, num_stages=5), triton.Config({'BLOCK_M': 2}, num_stages=4), triton.Config({'BLOCK_M': 2}, num_stages=5), triton.Config({'BLOCK_M': 4}, num_stages=4), triton.Config({'BLOCK_M': 4}, num_stages=5), triton.Config({'BLOCK_M': 8}, num_stages=4), triton.Config({'BLOCK_M': 8}, num_stages=5)], key=['M', 'N'])",
      "@triton.heuristics(values={'BLOCK_N': lambda args: triton.next_power_of_2(args['N']), 'num_warps': lambda args: 4 if args['N'] <= 1024 else 8 if args['N'] <= 2048 else 16})"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_grad_ptr",
        "annotation": null
      },
      {
        "name": "in_grad_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _safe_softmax_backward_kernel(",
      "    out_ptr,",
      "    out_grad_ptr,",
      "    in_grad_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    BLOCK_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    m_offset = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offset = tl.arange(0, BLOCK_N)",
      "    offsets = m_offset[:, None] * N * K + n_offset[None, :] * K + pid_k",
      "    mask = m_offset[:, None] < M and n_offset[None, :] < N",
      "    out_ptrs = out_ptr + offsets",
      "    out = tl.load(out_ptrs, mask=mask)",
      "    out_grad_ptrs = out_grad_ptr + offsets",
      "    out_grad = tl.load(out_grad_ptrs, mask=mask)",
      "",
      "    scale = tl.sum(out * out_grad, 1)",
      "    in_grad = out * (out_grad - scale[:, None])",
      "",
      "    in_grad_ptrs = in_grad_ptr + offsets",
      "    tl.store(in_grad_ptrs, in_grad, mask=mask)"
    ],
    "file": "triton_repos/phonism_genesis/genesis/nn/triton_ops/431.py"
  },
  {
    "name": "_online_softmax_kernel_non_inner",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'TILE_K': heur_tile_k, 'TILE_N': heur_tile_n_non_inner, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta, 'num_warps': heur_num_warps_non_inner})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _online_softmax_kernel_non_inner(",
      "    output_ptr,",
      "    input_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    TILE_N: tl.constexpr,",
      "    TILE_K: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_k = tl.program_id(1)",
      "    pid_m = tl.program_id(0)",
      "",
      "    k_offsets = pid_k * TILE_K + tl.arange(0, TILE_K)",
      "",
      "    if ONE_TILE_PER_CTA:",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offset = pid_m * N * K + n_offsets[:, None] * K + k_offsets",
      "        mask = (n_offsets[:, None] < N) & (k_offsets < K)",
      "        input_ptrs = input_ptr + offset",
      "        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\"))",
      "        m = tl.max(inp, 0)",
      "        e = tl.exp(inp - m[None, :])",
      "        z = tl.sum(e, 0)",
      "        out = e / z",
      "        output_ptrs = output_ptr + offset",
      "        tl.store(output_ptrs, out, mask=mask)",
      "    else:",
      "        m = tl.full([TILE_N, TILE_K], value=float(\"-inf\"), dtype=tl.float32)",
      "        z = tl.full([TILE_N, TILE_K], value=0.0, dtype=tl.float32)",
      "",
      "        for start_n in range(0, N, TILE_N):",
      "            n_offsets = start_n + tl.arange(0, TILE_N)",
      "            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets",
      "            mask = (n_offsets[:, None] < N) & (k_offsets < K)",
      "            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))",
      "            m_new = tl.maximum(m, inp)",
      "            all_neg_inf = m_new == float(\"-inf\")",
      "            z = tl.where(all_neg_inf, z, z * tl.exp(m - m_new) + tl.exp(inp - m_new))",
      "            m = m_new",
      "",
      "        m_reduced = tl.max(m, 0)",
      "        z = tl.sum(z * tl.exp(m - m_reduced[None, :]), 0)",
      "        m = m_reduced",
      "",
      "        previous_multiple = prev_multiple_of(N, TILE_N)",
      "        for start_n in range(0, N, TILE_N):",
      "            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)",
      "            offsets = pid_m * N * K + n_offsets[:, None] * K + k_offsets",
      "            mask = (n_offsets[:, None] < N) & (k_offsets[None, :] < K)",
      "            inp = tl.load(input_ptr + offsets, mask=mask, other=-float(\"inf\"))",
      "            o = tl.exp(inp - m[None, :]) / z[None, :]",
      "            tl.store(output_ptr + offsets, o, mask=mask)"
    ],
    "file": "triton_repos/phonism_genesis/genesis/nn/triton_ops/431.py"
  },
  {
    "name": "_online_softmax_kernel_inner",
    "decorators": [
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.heuristics({'TILE_N': heur_tile_n_inner, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta, 'num_warps': heur_num_warps_inner})"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "input_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _online_softmax_kernel_inner(",
      "    output_ptr,",
      "    input_ptr,",
      "    M,",
      "    N,",
      "    TILE_N: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    if ONE_TILE_PER_CTA:",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offset = pid_m * N + n_offsets",
      "        input_ptrs = input_ptr + offset",
      "        mask = n_offsets < N",
      "        inp = tl.load(input_ptrs, mask=mask, other=-float(\"inf\")).to(",
      "            output_ptr.dtype.element_ty",
      "        )",
      "        m = tl.max(inp, 0)",
      "        e = tl.exp(inp - m)",
      "        z = tl.sum(e, 0)",
      "        out = e / z",
      "        output_ptrs = output_ptr + offset",
      "        tl.store(output_ptrs, out, mask=mask)",
      "    else:",
      "        m = tl.full([TILE_N], value=float(\"-inf\"), dtype=tl.float32)",
      "        z = tl.full([TILE_N], value=0.0, dtype=tl.float32)",
      "        input_ptr += pid_m * N",
      "        output_ptr += pid_m * N",
      "",
      "        previous_multiple = prev_multiple_of(N, TILE_N)",
      "        for start_n in range(0, previous_multiple, TILE_N):",
      "            n_offsets = start_n + tl.arange(0, TILE_N)",
      "            inp = tl.load(input_ptr + n_offsets)",
      "            m_new = tl.maximum(m, inp)",
      "",
      "            all_neg_inf = m_new == float(\"-inf\")",
      "            z = tl.where(all_neg_inf, z, z * tl.exp(m - m_new) + tl.exp(inp - m_new))",
      "            m = m_new",
      "",
      "        for start_n in range(previous_multiple, N, TILE_N):",
      "            n_offsets = start_n + tl.arange(0, TILE_N)",
      "            mask = n_offsets < N",
      "            inp = tl.load(input_ptr + n_offsets, mask=mask, other=-float(\"inf\"))",
      "            m_new = tl.maximum(m, inp)",
      "            all_neg_inf = m_new == float(\"-inf\")",
      "            z = tl.where(all_neg_inf, z, z * tl.exp(m - m_new) + tl.exp(inp - m_new))",
      "            m = m_new",
      "",
      "        m_reduced = tl.max(m, 0)",
      "        z = tl.sum(z * tl.exp(m - m_reduced), 0)",
      "        m = m_reduced",
      "",
      "        previous_multiple = prev_multiple_of(N, TILE_N)",
      "",
      "        for start_n in range(0, TILE_N, TILE_N):",
      "            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)",
      "            mask = n_offsets < N",
      "            inp = tl.load(",
      "                input_ptr + n_offsets,",
      "                mask=mask,",
      "                other=-float(\"inf\"),",
      "                eviction_policy=\"evict_first\",",
      "            )",
      "            o = tl.exp(inp - m) / z",
      "            tl.store(output_ptr + n_offsets, o, mask=mask)",
      "        for start_n in range(TILE_N, N, TILE_N):",
      "            n_offsets = (previous_multiple - start_n) + tl.arange(0, TILE_N)",
      "            inp = tl.load(input_ptr + n_offsets, eviction_policy=\"evict_first\")",
      "            o = tl.exp(inp - m) / z",
      "            tl.store(output_ptr + n_offsets, o)"
    ],
    "file": "triton_repos/phonism_genesis/genesis/nn/triton_ops/431.py"
  },
  {
    "name": "_online_softmax_backward_kernel_non_inner",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'TILE_K': 32}), triton.Config({'TILE_K': 64}), triton.Config({'TILE_K': 128}), triton.Config({'TILE_K': 256}), triton.Config({'TILE_K': 1024})], key=['M', 'N', 'K'])",
      "@triton.heuristics({'TILE_N': heur_tile_n_bwd_non_inner, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta})"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_grad_ptr",
        "annotation": null
      },
      {
        "name": "in_grad_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _online_softmax_backward_kernel_non_inner(",
      "    out_ptr,",
      "    out_grad_ptr,",
      "    in_grad_ptr,",
      "    M,",
      "    N,",
      "    K,",
      "    TILE_N: tl.constexpr,",
      "    TILE_K: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    pid_k = tl.program_id(1)",
      "    offsets_k = pid_k * TILE_K + tl.arange(0, TILE_K)",
      "",
      "    if ONE_TILE_PER_CTA:",
      "        offsets_n = tl.arange(0, TILE_N)",
      "        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k",
      "        mask = (offsets_n < N)[:, None] & (offsets_k < K)",
      "        out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "        scale = tl.sum(out_tile * out_grad_tile, axis=0)",
      "        in_grad_tile = out_tile * (out_grad_tile - scale[None, :])",
      "        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "    else:",
      "        offsets_n = tl.arange(0, TILE_N)",
      "        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k",
      "        scale = tl.zeros([TILE_N, TILE_K], dtype=tl.float32)",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (offsets_n < N)[:, None] & (offsets_k < K)",
      "            out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            scale += out_tile * out_grad_tile",
      "            offsets_n += TILE_N",
      "            offsets += TILE_N * K",
      "        scale = tl.sum(scale, axis=0)",
      "",
      "        offsets_n = tl.arange(0, TILE_N)",
      "        offsets = pid_m * N * K + offsets_n[:, None] * K + offsets_k",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (offsets_n < N)[:, None] & (offsets_k < K)",
      "            out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            in_grad_tile = out_tile * (out_grad_tile - scale[None, :])",
      "            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "            offsets_n += TILE_N",
      "            offsets += TILE_N * K"
    ],
    "file": "triton_repos/phonism_genesis/genesis/nn/triton_ops/431.py"
  },
  {
    "name": "_online_softmax_backward_kernel_inner",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'TILE_N': 32}), triton.Config({'TILE_N': 64}), triton.Config({'TILE_N': 128}), triton.Config({'TILE_N': 256}), triton.Config({'TILE_N': 1024})], key=['M', 'N'])",
      "@triton.heuristics(values={'TILE_M': heru_tile_m, 'ONE_TILE_PER_CTA': heur_one_tile_per_cta})"
    ],
    "args": [
      {
        "name": "out_ptr",
        "annotation": null
      },
      {
        "name": "out_grad_ptr",
        "annotation": null
      },
      {
        "name": "in_grad_ptr",
        "annotation": null
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "TILE_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "TILE_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ONE_TILE_PER_CTA",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def _online_softmax_backward_kernel_inner(",
      "    out_ptr,",
      "    out_grad_ptr,",
      "    in_grad_ptr,",
      "    M,",
      "    N,",
      "    TILE_M: tl.constexpr,",
      "    TILE_N: tl.constexpr,",
      "    ONE_TILE_PER_CTA: tl.constexpr,",
      "):",
      "    pid_m = tl.program_id(0)",
      "    m_offsets = pid_m * TILE_M + tl.arange(0, TILE_M)",
      "    if ONE_TILE_PER_CTA:",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offsets = m_offsets[:, None] * N + n_offsets",
      "        mask = (m_offsets[:, None] < M) & (n_offsets < N)",
      "        out_tile = tl.load(out_ptr + offsets, mask=mask)",
      "        out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "        scale = tl.sum(out_tile * out_grad_tile, 1)",
      "        in_grad_tile = out_tile * (out_grad_tile - scale[:, None])",
      "        tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "    else:",
      "        scale = tl.zeros([TILE_M, TILE_N], dtype=tl.float32)",
      "",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offsets = m_offsets[:, None] * N + n_offsets",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (m_offsets[:, None] < M) & (n_offsets < N)",
      "            out_tile = tl.load(",
      "                out_ptr + offsets, mask=mask, eviction_policy=\"evict_last\"",
      "            )",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            scale += out_tile * out_grad_tile",
      "            n_offsets += TILE_N",
      "            offsets += TILE_N",
      "        scale = tl.sum(scale, 1)",
      "",
      "        n_offsets = tl.arange(0, TILE_N)",
      "        offsets = m_offsets[:, None] * N + n_offsets",
      "        for _ in range(0, N, TILE_N):",
      "            mask = (m_offsets[:, None] < M) & (n_offsets < N)",
      "            out_tile = tl.load(",
      "                out_ptr + offsets, mask=mask, eviction_policy=\"evict_first\"",
      "            )",
      "            out_grad_tile = tl.load(out_grad_ptr + offsets, mask=mask)",
      "            in_grad_tile = out_tile * (out_grad_tile - scale[:, None])",
      "            tl.store(in_grad_ptr + offsets, in_grad_tile, mask=mask)",
      "            n_offsets += TILE_N",
      "            offsets += TILE_N"
    ],
    "file": "triton_repos/phonism_genesis/genesis/nn/triton_ops/431.py"
  },
  {
    "name": "kernel_fma",
    "decorators": [
      {
        "name": "autotune",
        "args": [],
        "keywords": {}
      },
      {
        "name": "heuristics",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.autotune(configs=[triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 32, 'SPLIT_K': 1}, num_stages=5, num_warps=2), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=3, num_warps=8), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 128, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=4, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': 64, 'SPLIT_K': 1}, num_stages=5, num_warps=2)] + get_configs_io_bound(), key=['CACHE_KEY_M', 'CACHE_KEY_N', 'CACHE_KEY_K'], prune_configs_by={'early_config_prune': early_config_prune, 'perf_model': estimate_matmul_time, 'top_k': 10})",
      "@triton.heuristics({'K_LOAD_MASK_NEEDED': lambda args: args['K'] % (args['BLOCK_K'] * args['SPLIT_K']) == 0})"
    ],
    "args": [
      {
        "name": "C",
        "annotation": null
      },
      {
        "name": "A",
        "annotation": null
      },
      {
        "name": "B",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "dtype",
        "annotation": "tl.constexpr"
      },
      {
        "name": "M",
        "annotation": null
      },
      {
        "name": "N",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_M",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_N",
        "annotation": null
      },
      {
        "name": "CACHE_KEY_K",
        "annotation": null
      },
      {
        "name": "output_m_stride",
        "annotation": null
      },
      {
        "name": "output_n_stride",
        "annotation": null
      },
      {
        "name": "a_m_stride",
        "annotation": null
      },
      {
        "name": "a_k_stride",
        "annotation": null
      },
      {
        "name": "b_n_stride",
        "annotation": null
      },
      {
        "name": "b_k_stride",
        "annotation": null
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GROUP_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SPLIT_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "K_LOAD_MASK_NEEDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HAS_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTIVATION",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_fma(",
      "    C,",
      "    A,",
      "    B,",
      "    bias,",
      "    dtype: tl.constexpr,",
      "    M,",
      "    N,",
      "    K,",
      "    CACHE_KEY_M,",
      "    CACHE_KEY_N,",
      "    CACHE_KEY_K,",
      "    output_m_stride,",
      "    output_n_stride,",
      "    a_m_stride,",
      "    a_k_stride,",
      "    b_n_stride,",
      "    b_k_stride,",
      "    BLOCK_M: tl.constexpr,",
      "    GROUP_M: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_K: tl.constexpr,",
      "    SPLIT_K: tl.constexpr,",
      "    K_LOAD_MASK_NEEDED: tl.constexpr,",
      "    HAS_BIAS: tl.constexpr,",
      "    ACTIVATION: tl.constexpr,",
      "):",
      "",
      "    program_idx = tl.program_id(axis=0)",
      "",
      "    grid_m = (M + BLOCK_M - 1) // BLOCK_M",
      "    grid_n = (N + BLOCK_N - 1) // BLOCK_N",
      "",
      "    width = GROUP_M * grid_n",
      "    group_idx = program_idx // width",
      "    group_size = min(grid_m - group_idx * GROUP_M, GROUP_M)",
      "    block_m_idx = group_idx * GROUP_M + (program_idx % group_size)",
      "    block_n_idx = (program_idx % width) // group_size",
      "",
      "    m_offs_untagged = block_m_idx * BLOCK_M + tl.arange(0, BLOCK_M)",
      "    n_offs_untagged = block_n_idx * BLOCK_N + tl.arange(0, BLOCK_N)",
      "",
      "    m_offs = tl.max_contiguous(tl.multiple_of(m_offs_untagged % M, BLOCK_M), BLOCK_M)",
      "    n_offs = tl.max_contiguous(tl.multiple_of(n_offs_untagged % N, BLOCK_N), BLOCK_N)",
      "",
      "    k_range_offs = tl.arange(0, BLOCK_K)",
      "",
      "    A = A + (m_offs[:, None] * a_m_stride + k_range_offs[None, :] * a_k_stride)",
      "    B = B + (k_range_offs[:, None] * b_k_stride + n_offs[None, :] * b_n_stride)",
      "",
      "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)",
      "",
      "    if HAS_BIAS:",
      "        bias = tl.load(bias + n_offs, mask=n_offs < N, other=0.0).to(tl.float32)",
      "        acc += bias[None, :]",
      "",
      "    for k in range(K, 0, -BLOCK_K):",
      "        if K_LOAD_MASK_NEEDED:",
      "            a = tl.load(A)",
      "            b = tl.load(B)",
      "        else:",
      "            a = tl.load(A, mask=k_range_offs[None, :] < k, other=0.0)",
      "            b = tl.load(B, mask=k_range_offs[:, None] < k, other=0.0)",
      "        acc += tl.dot(a, b)",
      "",
      "        A += BLOCK_K * a_k_stride",
      "        B += BLOCK_K * b_k_stride",
      "",
      "    if ACTIVATION:",
      "        acc = silu(acc)",
      "    acc = acc.to(dtype)",
      "",
      "    C = C + m_offs[:, None] * output_m_stride + n_offs[None, :] * output_n_stride",
      "    c_ptr_mask = (m_offs < M)[:, None] & (n_offs < N)[None, :]",
      "    tl.store(C, acc, mask=c_ptr_mask)"
    ],
    "file": "triton_repos/arnavdantuluri_StableTriton/src/stabletriton/kernels/170.py"
  },
  {
    "name": "attn_fwd",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=metadata_fn)"
    ],
    "args": [
      {
        "name": "Q",
        "annotation": null
      },
      {
        "name": "K",
        "annotation": null
      },
      {
        "name": "V",
        "annotation": null
      },
      {
        "name": "bias",
        "annotation": null
      },
      {
        "name": "SM_SCALE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "L",
        "annotation": null
      },
      {
        "name": "Out",
        "annotation": null
      },
      {
        "name": "stride_qz",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_qh",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_qm",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_qk",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_kz",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_kh",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_kn",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_kk",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_vz",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_vh",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_vk",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_vn",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_oz",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_oh",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_om",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_on",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_bz",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_bh",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_bm",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_bn",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_az",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_ah",
        "annotation": "tl.constexpr"
      },
      {
        "name": "cu_seqlens_q",
        "annotation": null
      },
      {
        "name": "cu_seqlens_k",
        "annotation": null
      },
      {
        "name": "dropout_p",
        "annotation": null
      },
      {
        "name": "philox_seed",
        "annotation": null
      },
      {
        "name": "PERSISTENT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PERSISTENT_DYNAMIC",
        "annotation": "tl.constexpr"
      },
      {
        "name": "atomic_counter",
        "annotation": null
      },
      {
        "name": "NUM_CU",
        "annotation": "tl.constexpr"
      },
      {
        "name": "B",
        "annotation": "tl.constexpr"
      },
      {
        "name": "philox_offset_base",
        "annotation": null
      },
      {
        "name": "encoded_softmax",
        "annotation": null
      },
      {
        "name": "alibi_slopes",
        "annotation": null
      },
      {
        "name": "HQ",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HK",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ACTUAL_BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "MAX_SEQLENS_Q",
        "annotation": "tl.constexpr"
      },
      {
        "name": "MAX_SEQLENS_K",
        "annotation": "tl.constexpr"
      },
      {
        "name": "VARLEN",
        "annotation": "tl.constexpr"
      },
      {
        "name": "IS_CAUSAL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_DMODEL",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_BIAS",
        "annotation": "tl.constexpr"
      },
      {
        "name": "ENABLE_DROPOUT",
        "annotation": "tl.constexpr"
      },
      {
        "name": "RETURN_ENCODED_SOFTMAX",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_ALIBI",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_N",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_M",
        "annotation": "tl.constexpr"
      },
      {
        "name": "PRE_LOAD_V",
        "annotation": "tl.constexpr"
      },
      {
        "name": "GRID_CU_MULTIP",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def attn_fwd(",
      "    Q,",
      "    K,",
      "    V,",
      "    bias,",
      "    SM_SCALE: tl.constexpr,",
      "    L,",
      "    Out,",
      "    stride_qz: tl.constexpr,",
      "    stride_qh: tl.constexpr,",
      "    stride_qm: tl.constexpr,",
      "    stride_qk: tl.constexpr,",
      "    stride_kz: tl.constexpr,",
      "    stride_kh: tl.constexpr,",
      "    stride_kn: tl.constexpr,",
      "    stride_kk: tl.constexpr,",
      "    stride_vz: tl.constexpr,",
      "    stride_vh: tl.constexpr,",
      "    stride_vk: tl.constexpr,",
      "    stride_vn: tl.constexpr,",
      "    stride_oz: tl.constexpr,",
      "    stride_oh: tl.constexpr,",
      "    stride_om: tl.constexpr,",
      "    stride_on: tl.constexpr,",
      "    stride_bz: tl.constexpr,",
      "    stride_bh: tl.constexpr,",
      "    stride_bm: tl.constexpr,",
      "    stride_bn: tl.constexpr,",
      "    stride_az: tl.constexpr,",
      "    stride_ah: tl.constexpr,",
      "    cu_seqlens_q,",
      "    cu_seqlens_k,",
      "    dropout_p,",
      "    philox_seed,",
      "    PERSISTENT: tl.constexpr,",
      "    PERSISTENT_DYNAMIC: tl.constexpr,",
      "    atomic_counter,",
      "    NUM_CU: tl.constexpr,",
      "    B: tl.constexpr,",
      "    philox_offset_base,",
      "    encoded_softmax,",
      "    alibi_slopes,",
      "    HQ: tl.constexpr,",
      "    HK: tl.constexpr,",
      "    ACTUAL_BLOCK_DMODEL: tl.constexpr,",
      "    MAX_SEQLENS_Q: tl.constexpr,",
      "    MAX_SEQLENS_K: tl.constexpr,",
      "    VARLEN: tl.constexpr,",
      "    IS_CAUSAL: tl.constexpr,",
      "    BLOCK_DMODEL: tl.constexpr,",
      "    USE_BIAS: tl.constexpr,",
      "    ENABLE_DROPOUT: tl.constexpr,",
      "    RETURN_ENCODED_SOFTMAX: tl.constexpr,",
      "    USE_ALIBI: tl.constexpr,",
      "    BLOCK_N: tl.constexpr,",
      "    BLOCK_M: tl.constexpr,",
      "    PRE_LOAD_V: tl.constexpr,",
      "    GRID_CU_MULTIP: tl.constexpr,",
      "):",
      "",
      "    if PERSISTENT:",
      "        NUM_WG = NUM_CU * GRID_CU_MULTIP",
      "        num_tiles_per_head = tl.cdiv(MAX_SEQLENS_Q, BLOCK_M)",
      "        num_tiles_per_sample = num_tiles_per_head * HQ",
      "        num_tiles_total = num_tiles_per_sample * B",
      "        if PERSISTENT_DYNAMIC:",
      "            tile_id = atomic_counter.atomic_add(1)",
      "        else:",
      "            tile_id = tl.program_id(0)",
      "    else:",
      "        tile_id = 0",
      "        num_tiles_total = 1",
      "",
      "    while tile_id < num_tiles_total:",
      "        if PERSISTENT:",
      "",
      "            off_z = tile_id // num_tiles_per_sample",
      "",
      "            off_h_q = tile_id % num_tiles_per_sample // num_tiles_per_head",
      "",
      "            start_m = tile_id % num_tiles_per_sample % num_tiles_per_head",
      "        else:",
      "            start_m = tl.program_id(0)",
      "            off_h_q = tl.program_id(1)",
      "            off_z = tl.program_id(2)",
      "",
      "        offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)",
      "        offs_n = tl.arange(0, BLOCK_N)",
      "        offs_d = tl.arange(0, BLOCK_DMODEL)",
      "",
      "        continue_condition = True",
      "",
      "        if VARLEN:",
      "            cu_seqlens_q_start = tl.load(cu_seqlens_q + off_z)",
      "            cu_seqlens_q_end = tl.load(cu_seqlens_q + off_z + 1)",
      "            seqlen_q = cu_seqlens_q_end - cu_seqlens_q_start",
      "",
      "            if start_m * BLOCK_M > seqlen_q:",
      "                continue_condition = False",
      "",
      "            cu_seqlens_k_start = tl.load(cu_seqlens_k + off_z)",
      "            cu_seqlens_k_end = tl.load(cu_seqlens_k + off_z + 1)",
      "            seqlen_k = cu_seqlens_k_end - cu_seqlens_k_start",
      "        else:",
      "            cu_seqlens_q_start = 0",
      "            cu_seqlens_k_start = 0",
      "            seqlen_q = MAX_SEQLENS_Q",
      "            seqlen_k = MAX_SEQLENS_K",
      "",
      "        if continue_condition:",
      "",
      "            n_blocks = cdiv_fn(seqlen_k, BLOCK_N)",
      "            if IS_CAUSAL:",
      "",
      "                n_blocks_seqlen = cdiv_fn(",
      "                    (start_m + 1) * BLOCK_M + seqlen_k - seqlen_q, BLOCK_N",
      "                )",
      "",
      "                n_blocks = min(n_blocks, n_blocks_seqlen)",
      "",
      "                if n_blocks <= 0:",
      "                    o_offset = (",
      "                        Out",
      "                        + off_z * stride_oz",
      "                        + off_h_q * stride_oh",
      "                        + cu_seqlens_q_start * stride_om",
      "                    )",
      "                    o_ptrs = (",
      "                        o_offset",
      "                        + offs_m[:, None] * stride_om",
      "                        + offs_d[None, :] * stride_on",
      "                    )",
      "                    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=Out.type.element_ty)",
      "                    o_ptrs_mask = (offs_m[:, None] < seqlen_q).broadcast_to(",
      "                        [BLOCK_M, BLOCK_DMODEL]",
      "                    )",
      "",
      "                    tl.store(o_ptrs, acc, mask=o_ptrs_mask)",
      "",
      "                    l_ptrs = (",
      "                        L",
      "                        + off_z * HQ * MAX_SEQLENS_Q",
      "                        + off_h_q * MAX_SEQLENS_Q",
      "                        + offs_m",
      "                    )",
      "",
      "                    l_value = tl.full([BLOCK_M], value=float(\"inf\"), dtype=tl.float32)",
      "                    l_ptrs_mask = offs_m < MAX_SEQLENS_Q",
      "                    tl.store(l_ptrs, l_value, mask=l_ptrs_mask)",
      "",
      "                    continue_condition = False",
      "",
      "            if continue_condition:",
      "",
      "                GROUP_SIZE: tl.constexpr = HQ // HK",
      "                off_h_k = off_h_q // GROUP_SIZE if GROUP_SIZE != 1 else off_h_q",
      "",
      "                n_extra_tokens = 0",
      "                if seqlen_k < BLOCK_N:",
      "                    n_extra_tokens = BLOCK_N - seqlen_k",
      "                elif seqlen_k % BLOCK_N:",
      "                    n_extra_tokens = seqlen_k % BLOCK_N",
      "                PADDED_HEAD: tl.constexpr = ACTUAL_BLOCK_DMODEL != BLOCK_DMODEL",
      "",
      "                q_offset = (",
      "                    Q",
      "                    + off_z * stride_qz",
      "                    + off_h_q * stride_qh",
      "                    + cu_seqlens_q_start * stride_qm",
      "                )",
      "                q_ptrs = (",
      "                    q_offset + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk",
      "                )",
      "                k_offset = (",
      "                    K",
      "                    + off_z * stride_kz",
      "                    + off_h_k * stride_kh",
      "                    + cu_seqlens_k_start * stride_kn",
      "                )",
      "                k_ptrs = (",
      "                    k_offset + offs_d[:, None] * stride_kk + offs_n[None, :] * stride_kn",
      "                )",
      "                v_offset = (",
      "                    V",
      "                    + off_z * stride_vz",
      "                    + off_h_k * stride_vh",
      "                    + cu_seqlens_k_start * stride_vk",
      "                )",
      "                v_ptrs = (",
      "                    v_offset + offs_n[:, None] * stride_vk + offs_d[None, :] * stride_vn",
      "                )",
      "",
      "                if USE_BIAS:",
      "",
      "                    bias_offset = off_h_q * stride_bh",
      "                    bias_ptrs = (",
      "                        bias",
      "                        + bias_offset",
      "                        + offs_m[:, None] * stride_bm",
      "                        + offs_n[None, :] * stride_bn",
      "                    )",
      "                else:",
      "                    bias_ptrs = None",
      "",
      "                if USE_ALIBI:",
      "                    a_offset = off_z * stride_az + off_h_q * stride_ah",
      "                    alibi_slope = tl.load(alibi_slopes + a_offset)",
      "                else:",
      "                    alibi_slope = None",
      "",
      "                if ENABLE_DROPOUT:",
      "                    off_hz = off_z * HQ + off_h_q",
      "                    batch_philox_offset = (",
      "                        philox_offset_base + off_hz * seqlen_q * seqlen_k",
      "                    )",
      "                else:",
      "                    batch_philox_offset = 0",
      "",
      "                if RETURN_ENCODED_SOFTMAX:",
      "                    encoded_sm_base = encoded_softmax + off_h_q * seqlen_q * seqlen_k",
      "                    encoded_sm_ptrs = (",
      "                        encoded_sm_base + offs_m[:, None] * seqlen_k + offs_n[None, :]",
      "                    )",
      "                else:",
      "                    encoded_sm_ptrs = None",
      "",
      "                m_i = tl.full([BLOCK_M], float(\"-inf\"), dtype=tl.float32)",
      "                l_i = tl.full([BLOCK_M], 1.0, dtype=tl.float32)",
      "                acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)",
      "",
      "                QK_SCALE: tl.constexpr = SM_SCALE * 1.44269504089",
      "",
      "                q_ptrs_mask = offs_m[:, None] < seqlen_q",
      "                if PADDED_HEAD:",
      "                    q_ptrs_mask = q_ptrs_mask & (offs_d[None, :] < ACTUAL_BLOCK_DMODEL)",
      "                q = tl.load(q_ptrs, mask=q_ptrs_mask, other=0.0)",
      "",
      "                padded_block_k = n_extra_tokens != 0",
      "                is_modulo_mn = not padded_block_k and (seqlen_q % BLOCK_M == 0)",
      "                if IS_CAUSAL:",
      "",
      "                    masked_blocks = BLOCK_M // BLOCK_N + (not is_modulo_mn)",
      "                else:",
      "",
      "                    masked_blocks = padded_block_k",
      "",
      "                masked_blocks = min(masked_blocks, n_blocks)",
      "                n_full_blocks = n_blocks - masked_blocks",
      "                block_min = 0",
      "                block_max = n_blocks * BLOCK_N",
      "",
      "                if n_full_blocks > 0:",
      "                    block_max = (n_blocks - masked_blocks) * BLOCK_N",
      "                    acc, l_i, m_i = _attn_fwd_inner(",
      "                        acc,",
      "                        l_i,",
      "                        m_i,",
      "                        q,",
      "                        k_ptrs,",
      "                        v_ptrs,",
      "                        bias_ptrs,",
      "                        stride_kn,",
      "                        stride_vk,",
      "                        stride_bn,",
      "                        start_m,",
      "                        seqlen_k,",
      "                        seqlen_q,",
      "                        dropout_p,",
      "                        philox_seed,",
      "                        batch_philox_offset,",
      "                        encoded_sm_ptrs,",
      "                        block_min,",
      "                        block_max,",
      "                        0,",
      "                        0,",
      "                        0,",
      "                        alibi_slope,",
      "                        False,",
      "                        BLOCK_M,",
      "                        BLOCK_DMODEL,",
      "                        BLOCK_N,",
      "                        offs_m,",
      "                        offs_n,",
      "                        PRE_LOAD_V,",
      "                        False,",
      "                        ENABLE_DROPOUT,",
      "                        RETURN_ENCODED_SOFTMAX,",
      "                        PADDED_HEAD,",
      "                        ACTUAL_BLOCK_DMODEL,",
      "                        QK_SCALE,",
      "                    )",
      "",
      "                    block_min = block_max",
      "                    block_max = n_blocks * BLOCK_N",
      "",
      "                tl.debug_barrier()",
      "",
      "                if masked_blocks > 0:",
      "                    if IS_CAUSAL:",
      "                        offs_n_causal = offs_n + (seqlen_q - seqlen_k)",
      "                    else:",
      "                        offs_n_causal = 0",
      "                    k_ptrs += n_full_blocks * BLOCK_N * stride_kn",
      "                    v_ptrs += n_full_blocks * BLOCK_N * stride_vk",
      "                    if USE_BIAS:",
      "                        bias_ptrs += n_full_blocks * BLOCK_N * stride_bn",
      "                    if RETURN_ENCODED_SOFTMAX:",
      "                        encoded_sm_ptrs += n_full_blocks * BLOCK_N",
      "                    acc, l_i, m_i = _attn_fwd_inner(",
      "                        acc,",
      "                        l_i,",
      "                        m_i,",
      "                        q,",
      "                        k_ptrs,",
      "                        v_ptrs,",
      "                        bias_ptrs,",
      "                        stride_kn,",
      "                        stride_vk,",
      "                        stride_bn,",
      "                        start_m,",
      "                        seqlen_k,",
      "                        seqlen_q,",
      "                        dropout_p,",
      "                        philox_seed,",
      "                        batch_philox_offset,",
      "                        encoded_sm_ptrs,",
      "                        block_min,",
      "                        block_max,",
      "                        offs_n_causal,",
      "                        masked_blocks,",
      "                        n_extra_tokens,",
      "                        alibi_slope,",
      "                        IS_CAUSAL,",
      "                        BLOCK_M,",
      "                        BLOCK_DMODEL,",
      "                        BLOCK_N,",
      "                        offs_m,",
      "                        offs_n,",
      "                        PRE_LOAD_V,",
      "                        True,",
      "                        ENABLE_DROPOUT,",
      "                        RETURN_ENCODED_SOFTMAX,",
      "                        PADDED_HEAD,",
      "                        ACTUAL_BLOCK_DMODEL,",
      "                        QK_SCALE,",
      "                    )",
      "",
      "                l_recip = 1 / l_i[:, None]",
      "                acc = acc * l_recip",
      "",
      "                if ENABLE_DROPOUT:",
      "                    acc = acc / (1 - dropout_p)",
      "",
      "                end_m_idx = (start_m + 1) * BLOCK_M",
      "                start_m_idx = start_m * BLOCK_M",
      "                causal_start_idx = seqlen_q - seqlen_k",
      "                acc = acc.to(Out.type.element_ty)",
      "                if IS_CAUSAL:",
      "                    if causal_start_idx > start_m_idx and causal_start_idx < end_m_idx:",
      "                        out_mask_boundary = tl.full(",
      "                            (BLOCK_DMODEL,), causal_start_idx, dtype=tl.int32",
      "                        )",
      "                        mask_m_offsets = start_m_idx + tl.arange(0, BLOCK_M)",
      "                        out_ptrs_mask = (",
      "                            mask_m_offsets[:, None] >= out_mask_boundary[None, :]",
      "                        )",
      "                        z = 0.0",
      "                        acc = tl.where(out_ptrs_mask, acc, z.to(acc.type.element_ty))",
      "",
      "                l_ptrs = (",
      "                    L + off_z * HQ * MAX_SEQLENS_Q + off_h_q * MAX_SEQLENS_Q + offs_m",
      "                )",
      "",
      "                overflow_size = end_m_idx - seqlen_q",
      "                if overflow_size > 0:",
      "                    boundary = tl.full(",
      "                        (BLOCK_M,), BLOCK_M - overflow_size, dtype=tl.int32",
      "                    )",
      "                    l_ptrs_mask = tl.arange(0, BLOCK_M) < boundary",
      "                    tl.store(l_ptrs, m_i + tl.math.log2(l_i), mask=l_ptrs_mask)",
      "                else:",
      "                    tl.store(l_ptrs, m_i + tl.math.log2(l_i))",
      "",
      "                o_offset = (",
      "                    Out",
      "                    + off_z * stride_oz",
      "                    + off_h_q * stride_oh",
      "                    + cu_seqlens_q_start * stride_om",
      "                )",
      "                o_ptrs = (",
      "                    o_offset + offs_m[:, None] * stride_om + offs_d[None, :] * stride_on",
      "                )",
      "                o_ptrs_mask = tl.full([BLOCK_M, BLOCK_DMODEL], 1, dtype=tl.int1)",
      "                if overflow_size > 0:",
      "                    o_ptrs_mask = o_ptrs_mask & (offs_m[:, None] < seqlen_q)",
      "                if PADDED_HEAD:",
      "                    o_ptrs_mask = o_ptrs_mask & (offs_d[None, :] < ACTUAL_BLOCK_DMODEL)",
      "                tl.store(o_ptrs, acc.to(Out.dtype.element_ty), mask=o_ptrs_mask)",
      "",
      "        if PERSISTENT:",
      "            if PERSISTENT_DYNAMIC:",
      "                tile_id = atomic_counter.atomic_add(1)",
      "            else:",
      "                tile_id += NUM_WG",
      "        else:",
      "            tile_id = num_tiles_total"
    ],
    "file": "triton_repos/foundation-model-stack_vllm-triton-backend/ibm-triton-lib/ibm_triton_lib/kernels/306.py"
  },
  {
    "name": "kernel_paged_attention_2d",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=metadata_fn)"
    ],
    "args": [
      {
        "name": "output_ptr",
        "annotation": null
      },
      {
        "name": "query_ptr",
        "annotation": null
      },
      {
        "name": "key_cache_ptr",
        "annotation": null
      },
      {
        "name": "value_cache_ptr",
        "annotation": null
      },
      {
        "name": "block_tables_ptr",
        "annotation": null
      },
      {
        "name": "seq_lens_ptr",
        "annotation": null
      },
      {
        "name": "alibi_slopes_ptr",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "k_scale",
        "annotation": null
      },
      {
        "name": "v_scale",
        "annotation": null
      },
      {
        "name": "num_query_heads",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_queries_per_kv",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_queries_per_kv_padded",
        "annotation": "tl.constexpr"
      },
      {
        "name": "block_table_stride",
        "annotation": "tl.constexpr"
      },
      {
        "name": "query_stride_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "query_stride_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "output_stride_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "output_stride_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_SIZE_PADDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_ALIBI_SLOPES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_3",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_4",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_3",
        "annotation": "tl.constexpr"
      },
      {
        "name": "filter_by_query_len",
        "annotation": "tl.constexpr"
      },
      {
        "name": "query_start_len_ptr",
        "annotation": null
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_paged_attention_2d(",
      "    output_ptr,",
      "    query_ptr,",
      "    key_cache_ptr,",
      "    value_cache_ptr,",
      "    block_tables_ptr,",
      "    seq_lens_ptr,",
      "    alibi_slopes_ptr,",
      "    scale,",
      "    k_scale,",
      "    v_scale,",
      "    num_query_heads: tl.constexpr,",
      "    num_queries_per_kv: tl.constexpr,",
      "    num_queries_per_kv_padded: tl.constexpr,",
      "    block_table_stride: tl.constexpr,",
      "    query_stride_0: tl.constexpr,",
      "    query_stride_1: tl.constexpr,",
      "    output_stride_0: tl.constexpr,",
      "    output_stride_1: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HEAD_SIZE: tl.constexpr,",
      "    HEAD_SIZE_PADDED: tl.constexpr,",
      "    USE_ALIBI_SLOPES: tl.constexpr,",
      "    SLIDING_WINDOW: tl.constexpr,",
      "    x: tl.constexpr,",
      "    stride_k_cache_0: tl.constexpr,",
      "    stride_k_cache_1: tl.constexpr,",
      "    stride_k_cache_2: tl.constexpr,",
      "    stride_k_cache_3: tl.constexpr,",
      "    stride_k_cache_4: tl.constexpr,",
      "    stride_v_cache_0: tl.constexpr,",
      "    stride_v_cache_1: tl.constexpr,",
      "    stride_v_cache_2: tl.constexpr,",
      "    stride_v_cache_3: tl.constexpr,",
      "    filter_by_query_len: tl.constexpr,",
      "    query_start_len_ptr,",
      "):",
      "    seq_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "",
      "    if filter_by_query_len:",
      "        cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)",
      "        cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)",
      "        cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index",
      "        if cur_batch_query_len > 1:",
      "            return",
      "    else:",
      "        cur_batch_in_all_start_index = seq_idx",
      "",
      "    query_head_idx = kv_head_idx * num_queries_per_kv + tl.arange(",
      "        0, num_queries_per_kv_padded",
      "    )",
      "",
      "    query_offset = (",
      "        cur_batch_in_all_start_index * query_stride_0",
      "        + query_head_idx[:, None] * query_stride_1",
      "    )",
      "",
      "    head_mask = query_head_idx < (kv_head_idx + 1) * num_queries_per_kv",
      "    head_mask = head_mask & (query_head_idx < num_query_heads)",
      "",
      "    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1, 0).to(tl.int1)",
      "",
      "    Q = tl.load(",
      "        query_ptr + query_offset + tl.arange(0, HEAD_SIZE_PADDED)[None, :],",
      "        mask=dim_mask[None, :] & head_mask[:, None],",
      "        other=0.0,",
      "    )",
      "",
      "    block_table_offset = seq_idx * block_table_stride",
      "",
      "    M = tl.full([num_queries_per_kv_padded], float(\"-inf\"), dtype=tl.float32)",
      "    L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED], dtype=tl.float32)",
      "",
      "    seq_len = tl.load(seq_lens_ptr + seq_idx)",
      "",
      "    if USE_ALIBI_SLOPES:",
      "        alibi_slope = tl.load(",
      "            alibi_slopes_ptr + query_head_idx, mask=head_mask, other=0.0",
      "        )",
      "",
      "    num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)",
      "",
      "    for j in range(0, num_blocks):",
      "",
      "        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)",
      "",
      "        offs_n = tl.arange(0, BLOCK_SIZE)",
      "        offs_d = tl.arange(0, HEAD_SIZE_PADDED)",
      "",
      "        v_offset = (",
      "            physical_block_idx * stride_v_cache_0",
      "            + kv_head_idx * stride_v_cache_1",
      "            + offs_d[None, :] * stride_v_cache_2",
      "            + offs_n[:, None] * stride_v_cache_3",
      "        )",
      "",
      "        k_offset = (",
      "            physical_block_idx * stride_k_cache_0",
      "            + kv_head_idx * stride_k_cache_1",
      "            + (offs_d[:, None] // x) * stride_k_cache_2",
      "            + offs_n[None, :] * stride_k_cache_3",
      "            + (offs_d[:, None] % x) * stride_k_cache_4",
      "        )",
      "",
      "        K_load = tl.load(key_cache_ptr + k_offset, mask=dim_mask[:, None], other=0.0)",
      "",
      "        if K_load.dtype.is_fp8():",
      "            K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)",
      "        else:",
      "            K = K_load",
      "",
      "        V_load = tl.load(value_cache_ptr + v_offset, mask=dim_mask[None, :], other=0.0)",
      "",
      "        if V_load.dtype.is_fp8():",
      "            V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)",
      "        else:",
      "            V = V_load",
      "",
      "        seq_offset = j * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "        boundary = tl.full([BLOCK_SIZE], seq_len, dtype=tl.int32)",
      "        seq_mask = seq_offset[None, :] < boundary",
      "",
      "        S = tl.where(head_mask[:, None] & seq_mask, 0.0, float(\"-inf\")).to(tl.float32)",
      "        S += scale * tl.dot(Q, K)",
      "",
      "        context_len = seq_len - 1",
      "",
      "        if SLIDING_WINDOW > 0:",
      "            S = tl.where((context_len - seq_offset) < SLIDING_WINDOW, S, -10000)",
      "",
      "        if USE_ALIBI_SLOPES:",
      "            S += alibi_slope[:, None] * (seq_offset - context_len)",
      "",
      "        m_j = tl.maximum(M, tl.max(S, axis=1))",
      "",
      "        P = tl.exp(S - m_j[:, None])",
      "",
      "        l_j = tl.sum(P, axis=1)",
      "",
      "        alpha = tl.exp(M - m_j)",
      "",
      "        acc = acc * alpha[:, None]",
      "",
      "        L = L * alpha + l_j",
      "        M = m_j",
      "",
      "        acc += tl.dot(P.to(V.dtype), V)",
      "",
      "    acc = acc / L[:, None]",
      "",
      "    output_offset = (",
      "        cur_batch_in_all_start_index * output_stride_0",
      "        + query_head_idx * output_stride_1",
      "    )",
      "",
      "    tl.store(",
      "        output_ptr + output_offset[:, None] + tl.arange(0, HEAD_SIZE_PADDED)[None, :],",
      "        acc,",
      "        mask=dim_mask[None, :] & head_mask[:, None],",
      "    )"
    ],
    "file": "triton_repos/foundation-model-stack_vllm-triton-backend/ibm-triton-lib/ibm_triton_lib/kernels/307.py"
  },
  {
    "name": "kernel_paged_attention_3d",
    "decorators": [
      {
        "name": "jit",
        "args": [],
        "keywords": {}
      }
    ],
    "decorator_source": [
      "@triton.jit(launch_metadata=metadata_fn)"
    ],
    "args": [
      {
        "name": "segm_output_ptr",
        "annotation": null
      },
      {
        "name": "segm_max_ptr",
        "annotation": null
      },
      {
        "name": "segm_expsum_ptr",
        "annotation": null
      },
      {
        "name": "query_ptr",
        "annotation": null
      },
      {
        "name": "key_cache_ptr",
        "annotation": null
      },
      {
        "name": "value_cache_ptr",
        "annotation": null
      },
      {
        "name": "block_tables_ptr",
        "annotation": null
      },
      {
        "name": "seq_lens_ptr",
        "annotation": null
      },
      {
        "name": "alibi_slopes_ptr",
        "annotation": null
      },
      {
        "name": "scale",
        "annotation": null
      },
      {
        "name": "k_scale",
        "annotation": null
      },
      {
        "name": "v_scale",
        "annotation": null
      },
      {
        "name": "num_query_heads",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_queries_per_kv",
        "annotation": "tl.constexpr"
      },
      {
        "name": "num_queries_per_kv_padded",
        "annotation": "tl.constexpr"
      },
      {
        "name": "block_table_stride",
        "annotation": "tl.constexpr"
      },
      {
        "name": "query_stride_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "query_stride_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "BLOCK_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_SIZE",
        "annotation": "tl.constexpr"
      },
      {
        "name": "HEAD_SIZE_PADDED",
        "annotation": "tl.constexpr"
      },
      {
        "name": "USE_ALIBI_SLOPES",
        "annotation": "tl.constexpr"
      },
      {
        "name": "SLIDING_WINDOW",
        "annotation": "tl.constexpr"
      },
      {
        "name": "x",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_3",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_k_cache_4",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_0",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_1",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_2",
        "annotation": "tl.constexpr"
      },
      {
        "name": "stride_v_cache_3",
        "annotation": "tl.constexpr"
      },
      {
        "name": "filter_by_query_len",
        "annotation": "tl.constexpr"
      },
      {
        "name": "query_start_len_ptr",
        "annotation": null
      },
      {
        "name": "MAX_NUM_SEGMENTS_PER_SEQ",
        "annotation": "tl.constexpr"
      }
    ],
    "docstring": null,
    "source": [
      "def kernel_paged_attention_3d(",
      "    segm_output_ptr,",
      "    segm_max_ptr,",
      "    segm_expsum_ptr,",
      "    query_ptr,",
      "    key_cache_ptr,",
      "    value_cache_ptr,",
      "    block_tables_ptr,",
      "    seq_lens_ptr,",
      "    alibi_slopes_ptr,",
      "    scale,",
      "    k_scale,",
      "    v_scale,",
      "    num_query_heads: tl.constexpr,",
      "    num_queries_per_kv: tl.constexpr,",
      "    num_queries_per_kv_padded: tl.constexpr,",
      "    block_table_stride: tl.constexpr,",
      "    query_stride_0: tl.constexpr,",
      "    query_stride_1: tl.constexpr,",
      "    BLOCK_SIZE: tl.constexpr,",
      "    HEAD_SIZE: tl.constexpr,",
      "    HEAD_SIZE_PADDED: tl.constexpr,",
      "    USE_ALIBI_SLOPES: tl.constexpr,",
      "    SLIDING_WINDOW: tl.constexpr,",
      "    x: tl.constexpr,",
      "    stride_k_cache_0: tl.constexpr,",
      "    stride_k_cache_1: tl.constexpr,",
      "    stride_k_cache_2: tl.constexpr,",
      "    stride_k_cache_3: tl.constexpr,",
      "    stride_k_cache_4: tl.constexpr,",
      "    stride_v_cache_0: tl.constexpr,",
      "    stride_v_cache_1: tl.constexpr,",
      "    stride_v_cache_2: tl.constexpr,",
      "    stride_v_cache_3: tl.constexpr,",
      "    filter_by_query_len: tl.constexpr,",
      "    query_start_len_ptr,",
      "    MAX_NUM_SEGMENTS_PER_SEQ: tl.constexpr,",
      "):",
      "    seq_idx = tl.program_id(0)",
      "    kv_head_idx = tl.program_id(1)",
      "    segm_idx = tl.program_id(2)",
      "",
      "    if filter_by_query_len:",
      "        cur_batch_in_all_start_index = tl.load(query_start_len_ptr + seq_idx)",
      "        cur_batch_in_all_stop_index = tl.load(query_start_len_ptr + seq_idx + 1)",
      "        cur_batch_query_len = cur_batch_in_all_stop_index - cur_batch_in_all_start_index",
      "        if cur_batch_query_len > 1:",
      "            return",
      "    else:",
      "        cur_batch_in_all_start_index = seq_idx",
      "",
      "    seq_len = tl.load(seq_lens_ptr + seq_idx)",
      "",
      "    blocks_per_segment = cdiv_fn(seq_len, MAX_NUM_SEGMENTS_PER_SEQ * BLOCK_SIZE)",
      "    if segm_idx * blocks_per_segment * BLOCK_SIZE >= seq_len:",
      "        return",
      "",
      "    query_head_idx = kv_head_idx * num_queries_per_kv + tl.arange(",
      "        0, num_queries_per_kv_padded",
      "    )",
      "",
      "    query_offset = (",
      "        cur_batch_in_all_start_index * query_stride_0",
      "        + query_head_idx[:, None] * query_stride_1",
      "    )",
      "",
      "    head_mask = query_head_idx < (kv_head_idx + 1) * num_queries_per_kv",
      "    head_mask = head_mask & (query_head_idx < num_query_heads)",
      "",
      "    dim_mask = tl.where(tl.arange(0, HEAD_SIZE_PADDED) < HEAD_SIZE, 1, 0).to(tl.int1)",
      "",
      "    Q = tl.load(",
      "        query_ptr + query_offset + tl.arange(0, HEAD_SIZE_PADDED)[None, :],",
      "        mask=dim_mask[None, :] & head_mask[:, None],",
      "        other=0.0,",
      "    )",
      "",
      "    block_table_offset = seq_idx * block_table_stride",
      "",
      "    M = tl.full([num_queries_per_kv_padded], float(\"-inf\"), dtype=tl.float32)",
      "    L = tl.full([num_queries_per_kv_padded], 1.0, dtype=tl.float32)",
      "    acc = tl.zeros([num_queries_per_kv_padded, HEAD_SIZE_PADDED], dtype=tl.float32)",
      "",
      "    if USE_ALIBI_SLOPES:",
      "        alibi_slope = tl.load(",
      "            alibi_slopes_ptr + query_head_idx, mask=head_mask, other=0.0",
      "        )",
      "",
      "    num_blocks = cdiv_fn(seq_len, BLOCK_SIZE)",
      "",
      "    for j in range(",
      "        segm_idx * blocks_per_segment,",
      "        min((segm_idx + 1) * blocks_per_segment, num_blocks),",
      "    ):",
      "        physical_block_idx = tl.load(block_tables_ptr + block_table_offset + j)",
      "",
      "        offs_n = tl.arange(0, BLOCK_SIZE)",
      "        offs_d = tl.arange(0, HEAD_SIZE_PADDED)",
      "",
      "        v_offset = (",
      "            physical_block_idx * stride_v_cache_0",
      "            + kv_head_idx * stride_v_cache_1",
      "            + offs_d[None, :] * stride_v_cache_2",
      "            + offs_n[:, None] * stride_v_cache_3",
      "        )",
      "",
      "        k_offset = (",
      "            physical_block_idx * stride_k_cache_0",
      "            + kv_head_idx * stride_k_cache_1",
      "            + (offs_d[:, None] // x) * stride_k_cache_2",
      "            + offs_n[None, :] * stride_k_cache_3",
      "            + (offs_d[:, None] % x) * stride_k_cache_4",
      "        )",
      "",
      "        K_load = tl.load(key_cache_ptr + k_offset, mask=dim_mask[:, None], other=0.0)",
      "",
      "        if K_load.dtype.is_fp8():",
      "            K = (K_load.to(tl.float32) * tl.load(k_scale)).to(Q.dtype)",
      "        else:",
      "            K = K_load",
      "",
      "        V_load = tl.load(value_cache_ptr + v_offset, mask=dim_mask[None, :], other=0.0)",
      "",
      "        if V_load.dtype.is_fp8():",
      "            V = (V_load.to(tl.float32) * tl.load(v_scale)).to(Q.dtype)",
      "        else:",
      "            V = V_load",
      "",
      "        seq_offset = j * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)",
      "        boundary = tl.full([BLOCK_SIZE], seq_len, dtype=tl.int32)",
      "        seq_mask = seq_offset[None, :] < boundary",
      "",
      "        S = tl.where(head_mask[:, None] & seq_mask, 0.0, float(\"-inf\")).to(tl.float32)",
      "        S += scale * tl.dot(Q, K)",
      "",
      "        context_len = seq_len - 1",
      "",
      "        if SLIDING_WINDOW > 0:",
      "            S = tl.where((context_len - seq_offset) < SLIDING_WINDOW, S, -10000)",
      "",
      "        if USE_ALIBI_SLOPES:",
      "            S += alibi_slope[:, None] * (seq_offset - context_len)",
      "",
      "        m_j = tl.maximum(M, tl.max(S, axis=1))",
      "",
      "        P = tl.exp(S - m_j[:, None])",
      "",
      "        l_j = tl.sum(P, axis=1)",
      "",
      "        alpha = tl.exp(M - m_j)",
      "",
      "        acc = acc * alpha[:, None]",
      "",
      "        L = L * alpha + l_j",
      "        M = m_j",
      "",
      "        acc += tl.dot(P.to(V.dtype), V)",
      "",
      "    segm_output_offset = (",
      "        seq_idx * (num_query_heads * MAX_NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)",
      "        + query_head_idx[:, None] * (MAX_NUM_SEGMENTS_PER_SEQ * HEAD_SIZE_PADDED)",
      "        + segm_idx * HEAD_SIZE_PADDED",
      "        + tl.arange(0, HEAD_SIZE_PADDED)[None, :]",
      "    )",
      "    tl.store(",
      "        segm_output_ptr + segm_output_offset,",
      "        acc,",
      "        mask=dim_mask[None, :] & head_mask[:, None],",
      "    )",
      "",
      "    segm_offset = (",
      "        seq_idx * (num_query_heads * MAX_NUM_SEGMENTS_PER_SEQ)",
      "        + query_head_idx * MAX_NUM_SEGMENTS_PER_SEQ",
      "        + segm_idx",
      "    )",
      "    tl.store(segm_max_ptr + segm_offset, M, mask=head_mask)",
      "    tl.store(segm_expsum_ptr + segm_offset, L, mask=head_mask)"
    ],
    "file": "triton_repos/foundation-model-stack_vllm-triton-backend/ibm-triton-lib/ibm_triton_lib/kernels/308.py"
  }
]